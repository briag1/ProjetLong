{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZV8IJFGT9T",
        "outputId": "98359cb1-736e-4d18-fd5b-38e44547766f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# @title device\n",
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "  return device\n",
        "device=get_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_LSRYqGMO2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "qh5nnjRIFe0h"
      },
      "outputs": [],
      "source": [
        "# @title code\n",
        "from os import makedirs\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_x(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[0])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def get_y(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[1])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def read_dataframe(name):\n",
        "  if not os.path.exists(name+\".pkl\"):\n",
        "    print(\"reading dataframe: \"+name+\".xlsx\")\n",
        "    df=pd.read_excel(name+\".xlsx\")\n",
        "    df.to_pickle(name+\".pkl\")\n",
        "  else:\n",
        "    print(\"using already read daframe\")\n",
        "\n",
        "def get_vocab(poses,vocab):\n",
        "  for pos in poses:\n",
        "    if pos not in vocab and not any(isinstance(n, float) and math.isnan(n) for n in pos):\n",
        "        vocab[pos]=len(vocab)+1\n",
        "  return vocab\n",
        "\n",
        "def get_fix_time_encoding(df):\n",
        "\n",
        "  df['month_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "  df['month_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "\n",
        "  df['day_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "  df['day_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "\n",
        "  df['hour_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "  df['hour_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "\n",
        "  df['minute_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "  df['minute_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "\n",
        "  df['second_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "  df['second_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "def get_time_data(df):\n",
        "  df['month'] =  df[\"start time\"].dt.month\n",
        "  df['day'] =  df[\"start time\"].dt.day\n",
        "  df['hour'] =  df[\"start time\"].dt.hour\n",
        "  df['minute'] = df[\"start time\"].dt.minute\n",
        "  df['second'] = df[\"start time\"].dt.second\n",
        "  return df\n",
        "\n",
        "\n",
        "def tokenize_pos(pos,vocab):\n",
        "\n",
        "  if math.isnan(pos[0]) and math.isnan(pos[1]):\n",
        "    return len(vocab)\n",
        "  else:\n",
        "    return vocab[pos]\n",
        "\n",
        "def get_coordinates(df,input_position,full_dataset):\n",
        "\n",
        "  if full_dataset:\n",
        "    df['x'] = df['latitude']\n",
        "    df['y'] = df['longitude']\n",
        "  else:\n",
        "    df['x'] = df['location(latitude/lontitude)'].apply(get_x)\n",
        "    df['y'] = df['location(latitude/lontitude)'].apply(get_y)\n",
        "\n",
        "  if input_position:\n",
        "    df['x_normalised']=(df['x']-df['x'].mean())/(df['x'].std())\n",
        "    df['y_normalised']=(df['y']-df['y'].mean())/df['y'].std()\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_joined_coordinates(df):\n",
        "  df['pos']= list(zip(df['x'],df['y']))\n",
        "  poses=df['pos'].unique()\n",
        "  return poses\n",
        "\n",
        "def get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset):\n",
        "  col_to_drop_in_df=['date', 'end time','pos']\n",
        "  col_to_drop_in_dict=['x','y', 'time_to_end', 'time_to_next','start time', 'user id']\n",
        "  col_to_add_to_dict=[]\n",
        "  col_in_input=[]\n",
        "  if not full_dataset:\n",
        "    col_to_drop_in_df+=['location(latitude/lontitude)']\n",
        "  else:\n",
        "    col_to_drop_in_df+=['latitude','longitude']\n",
        "  if fixed_time_encoding:\n",
        "    col_to_drop_in_df+=[]\n",
        "    col_to_drop_in_dict+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "    col_in_input+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "  else:\n",
        "    col_to_add_to_dict+=['month','day','hour','minute','second']\n",
        "  if input_position:\n",
        "    col_to_drop_in_dict += ['x_normalised', 'y_normalised']\n",
        "    col_in_input+=['x_normalised', 'y_normalised']\n",
        "  return col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict\n",
        "\n",
        "def replace_nan_with_previous_values(dict_user):\n",
        "  index_start_sequence=0\n",
        "  while index_start_sequence<dict_user[\"input\"].shape[0] and dict_user[\"input\"][index_start_sequence,-2:].isnan().any():\n",
        "    index_start_sequence+=1\n",
        "  for col in dict_user:\n",
        "    dict_user[col]=dict_user[col][index_start_sequence:]\n",
        "  for i in range(1,dict_user[\"input\"].shape[0]):\n",
        "    if dict_user[\"input\"][i,-2:].isnan().any():\n",
        "      dict_user[\"input\"][i,-2:]=dict_user[\"input\"][i-1,-2:]\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "\n",
        "def process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections,drop_nan):\n",
        "  #get the time to next connection\n",
        "  df_user[\"time_to_next\"] =  df_user[\"start time\"].diff(-1).dt.total_seconds()\n",
        "  dict_user=df_user.to_dict('list')\n",
        "  #create input\n",
        "  dict_user[\"pos_id\"],dict_user[\"pos_id_target\"]=torch.tensor(dict_user[\"pos_id\"][:-1]),torch.tensor(dict_user[\"pos_id\"][1:])\n",
        "\n",
        "  if col_in_input:\n",
        "    dict_user[\"input\"]=torch.tensor([dict_user[col] for col in col_in_input]).T\n",
        "    dict_user[\"input\"]=dict_user[\"input\"][:-1]\n",
        "  if col_to_add_to_dict:\n",
        "    for col in col_to_add_to_dict:\n",
        "      dict_user[col]=torch.tensor(dict_user[col])\n",
        "      dict_user[col]=dict_user[col][:-1]\n",
        "\n",
        "  dict_user[\"time_target\"]=torch.tensor([dict_user[\"time_to_end\"],dict_user[\"time_to_next\"]]).T\n",
        "  dict_user[\"time_target\"]=dict_user[\"time_target\"][:-1]\n",
        "  if not drop_nan and dict_user[\"input\"][:,-2:].isnan().any():\n",
        "    dict_user=replace_nan_with_previous_values(dict_user)\n",
        "  for e in col_to_drop_in_dict:\n",
        "    dict_user.pop(e)\n",
        "\n",
        "  if not with_repeated_connections:\n",
        "    dict_user=combine_repeated_connections_in_sequence_user(dict_user)\n",
        "    dict_user=delete_end_of_sequence_repeated_connections(dict_user)\n",
        "  return dict_user\n",
        "\n",
        "def delete_end_of_sequence_repeated_connections(dict_user):\n",
        "  if dict_user['pos_id'][-1]==dict_user[\"pos_id_target\"][-1]:\n",
        "    for key in dict_user:\n",
        "      dict_user[key]=dict_user[key][:-1]\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def combine_repeated_connections_in_sequence_user(dict_user):\n",
        "  index=0\n",
        "  while index < len(dict_user[\"pos_id\"])-1:\n",
        "    if dict_user[\"pos_id\"][index]==dict_user[\"pos_id_target\"][index]:\n",
        "      dict_user[\"pos_id_target\"][index]=dict_user[\"pos_id_target\"][index+1]\n",
        "      dict_user[\"time_target\"][index]=dict_user[\"time_target\"][index+1]\n",
        "      for key in dict_user:\n",
        "        dict_user[key]=torch.cat((dict_user[key][:index+1],dict_user[key][index+2:]))\n",
        "    else:\n",
        "      index+=1\n",
        "\n",
        "\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def normalize_output(list_users):\n",
        "  #get means and stds\n",
        "  time_targets=torch.cat([dict_user[\"time_target\"] for dict_user in list_users],dim=0)\n",
        "  time_targets_mean=time_targets.mean(dim=0)\n",
        "  time_targets_std=time_targets.std(dim=0)\n",
        "  #normalize\n",
        "  for i in range(len(list_users)):\n",
        "    list_users[i][\"time_target\"]=(list_users[i][\"time_target\"]-time_targets_mean)/time_targets_std\n",
        "  return list_users\n",
        "\n",
        "def keep_sequence(df_user,min_sequence_size,drop_nan):\n",
        "\n",
        "  return len(df_user)>=min_sequence_size and (not drop_nan or not df_user['x'].isnull().values.any()) and  (drop_nan or not df['latitude'].isnull().sum()==len(df['latitude']))\n",
        "\n",
        "def process_dataframe(name,vocab,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,min_sequence_size,drop_nan,format=\".pkl\"):\n",
        "  df= pd.read_pickle(name+format)\n",
        "  df=df.sort_values('start time')\n",
        "  df=df.drop(['month'],axis=1)\n",
        "\n",
        "  df=get_coordinates(df,input_position,full_dataset)\n",
        "\n",
        "  poses=get_joined_coordinates(df)\n",
        "  vocab=get_vocab(poses,vocab)\n",
        "  df['pos_id'] = df['pos'].apply(lambda pos: tokenize_pos(pos,vocab))\n",
        "\n",
        "  df['time_to_end']=df['end time']-df['start time']\n",
        "  df['time_to_end']=df['time_to_end'].dt.total_seconds()\n",
        "  if fixed_time_encoding:\n",
        "    df=get_fix_time_encoding(df)\n",
        "  else:\n",
        "    df=get_time_data(df)\n",
        "\n",
        "  col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict=get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset)\n",
        "  df=df.drop(col_to_drop_in_df, axis=1)\n",
        "\n",
        "  df_user_group = df.groupby('user id')\n",
        "  list_users=[]\n",
        "  for user, df_user in df_user_group:\n",
        "    if keep_sequence(df_user,min_sequence_size,drop_nan):\n",
        "        prossessed_user_data=process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections,drop_nan)\n",
        "        if prossessed_user_data[\"pos_id\"].shape[0]>=min_sequence_size-1:\n",
        "          list_users.append(prossessed_user_data)\n",
        "  list_users=normalize_output(list_users)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "def get_raw_data(directory,src_directory,download_raw_data):\n",
        "  if not download_raw_data:\n",
        "    shutil.copytree(src_directory,directory)#telecomDataset6mont\n",
        "  else:\n",
        "    runcmd('wget http://sguangwang.com/dataset/telecom.zip', verbose = False)\n",
        "    runcmd('unzip /content/telecom.zip')\n",
        "\n",
        "def get_processed_dataset(load_dataset_path):\n",
        "  saved_list_user_path = os.path.join(load_dataset_path,\"list_users\")\n",
        "  saved_vocab_path = os.path.join(load_dataset_path,\"vocab\")\n",
        "  print(\"loading already preprocessed data: \")\n",
        "  print(saved_list_user_path)\n",
        "  print(saved_vocab_path)\n",
        "  list_users=torch.load(saved_list_user_path)\n",
        "  vocab=torch.load(saved_vocab_path)\n",
        "  return list_users,vocab\n",
        "\n",
        "def get_and_process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,download_raw_data,with_repeated_connections,min_sequence_size,drop_nan):\n",
        "  list_users=[]\n",
        "  vocab={}\n",
        "  if not os.path.exists(directory_raw_data):\n",
        "    print('getting raw data at: '+src_directory_raw_data)\n",
        "    get_raw_data(directory_raw_data,src_directory_raw_data,download_raw_data)\n",
        "  for name in os.listdir(directory_raw_data):\n",
        "    if not name.endswith(\".pkl\"):\n",
        "      complete_name=os.path.join(directory_raw_data,\".\".join(name.split(\".\")[:-1]))\n",
        "      print(\"processing dataframe: \"+complete_name)\n",
        "      read_dataframe(complete_name)\n",
        "      new_list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections,min_sequence_size=min_sequence_size,drop_nan=drop_nan)\n",
        "      list_users+=new_list_users\n",
        "  return list_users,vocab\n",
        "\n",
        "def split_long_sequences(list_users,max_sequence_length):\n",
        "  new_list_users=[]\n",
        "  for i in range(len(list_users)):\n",
        "    seq_length=list_users[i][\"input\"].shape[0]\n",
        "    if seq_length>=max_sequence_length:\n",
        "      nb_of_seq=seq_length//max_sequence_length\n",
        "      rest=seq_length%max_sequence_length\n",
        "      list_splitted_seq=nb_of_seq*[{}]\n",
        "      rest_splitted={}\n",
        "      for key in list_users[i]:\n",
        "        for j in range(nb_of_seq):\n",
        "          list_splitted_seq[j][key]=list_users[i][key][max_sequence_length*j:max_sequence_length*(j+1)]\n",
        "        if rest>2:\n",
        "          rest_splitted[key]= list_users[i][key][-rest:]\n",
        "      new_list_users=new_list_users+list_splitted_seq\n",
        "      if len(rest_splitted)>0:\n",
        "        new_list_users+=[rest_splitted]\n",
        "    else:\n",
        "      new_list_users.append(list_users[i])\n",
        "\n",
        "  return new_list_users\n",
        "\n",
        "\n",
        "\n",
        "def save_processed_data(list_users,vocab,path_to_save_dataset):\n",
        "    print(\"creating directory: \"+path_to_save_dataset)\n",
        "    os.makedirs(path_to_save_dataset,exist_ok=True)\n",
        "    print(\"saving processed data at: \")\n",
        "    save_list_user_path = os.path.join(path_to_save_dataset,\"list_users\")\n",
        "    save_vocab_path = os.path.join(path_to_save_dataset,\"vocab\")\n",
        "    print(save_list_user_path)\n",
        "    print(save_vocab_path)\n",
        "    torch.save(list_users,save_list_user_path)\n",
        "    torch.save(vocab,save_vocab_path)\n",
        "\n",
        "def get_processed_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,spliting_long_sequences,with_repeated_connections,max_sequence_length=100,min_sequence_size=1,drop_nan=True,save=False,path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month\",process_raw_data=True,download_raw_data=False,processed_dataset_path=\"/content/drive/MyDrive/telecomDataset6month\"):\n",
        "  if not process_raw_data:\n",
        "    list_users,vocab = get_processed_dataset(processed_dataset_path)\n",
        "  else:\n",
        "    list_users,vocab=get_and_process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,download_raw_data,with_repeated_connections,min_sequence_size=min_sequence_size,drop_nan=drop_nan)\n",
        "  if spliting_long_sequences:\n",
        "    print(\"spliting sequences longuer than : \"+str(max_sequence_length)+ \" steps\")\n",
        "    list_users=split_long_sequences(list_users,max_sequence_length)\n",
        "  if save:\n",
        "    save_processed_data(list_users,vocab,path_to_save_dataset)\n",
        "  return list_users,vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeHcFERqHD4G"
      },
      "source": [
        "## preprocessing 1: replacing missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE27j0ba6_WI",
        "outputId": "e8feee36-5c5c-4ac3-979a-356f28884471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mLe flux de sortie a été tronqué et ne contient que les 5000 dernières lignes.\u001b[0m\n",
            "          4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,\n",
            "          7.6887e-02,  1.6537e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6537e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.4933e-01,  4.3836e-02,  4.3836e-02,  4.3836e-02,  4.7133e-02,\n",
            "          4.7133e-02,  4.7133e-02,  4.3836e-02,  4.3836e-02,  3.9823e-02,\n",
            "          4.3714e-02,  4.3836e-02,  4.5566e-02,  4.3714e-02,  1.6537e-01,\n",
            "          1.6537e-01,  1.6755e-01,  1.6755e-01,  1.6537e-01,  1.6920e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6537e-01,  6.0520e-02,\n",
            "          6.0520e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,\n",
            "          5.0682e-02,  5.0682e-02,  4.3714e-02,  3.9823e-02,  3.9823e-02,\n",
            "          5.9264e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,\n",
            "          5.0682e-02,  1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6537e-01,  1.6537e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6920e-01,  4.3836e-02,\n",
            "          4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,\n",
            "          4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,  4.3836e-02,\n",
            "          4.3836e-02,  4.3836e-02,  1.6755e-01,  1.6755e-01,  1.6537e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6537e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6537e-01,\n",
            "          1.6537e-01,  1.6755e-01,  1.6755e-01,  1.6537e-01,  1.6537e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6537e-01,  1.6537e-01,  1.6755e-01,\n",
            "          1.6755e-01,  1.6755e-01,  1.6755e-01,  1.6755e-01],\n",
            "        [ 1.3355e-01,  1.3355e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.3355e-01,  1.2995e-01,  1.3355e-01,  1.3355e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.3355e-01,  1.2995e-01,  1.2995e-01,  1.3355e-01,\n",
            "          1.3355e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.3355e-01,  1.3355e-01,  1.3355e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.3355e-01,  1.2995e-01,  1.3355e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.3435e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.3355e-01,  1.3355e-01,  1.3475e-01,  1.1993e-01,\n",
            "          9.3998e-02,  9.3998e-02,  9.3646e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.7877e-02,\n",
            "          9.3646e-02,  9.3998e-02,  1.3355e-01,  1.2995e-01,  1.3355e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.4442e-01,  1.5771e-01,\n",
            "          1.5771e-01,  1.1377e-01,  9.3998e-02,  9.3998e-02,  9.3646e-02,\n",
            "          9.6944e-02,  9.3998e-02,  9.3998e-02,  1.0085e-01,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3646e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  1.3355e-01,  1.2995e-01,  1.3355e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  9.9951e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3646e-02,  9.3646e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3646e-02,  9.3646e-02,  9.3646e-02,\n",
            "          9.3998e-02,  1.2995e-01,  1.2995e-01,  1.2995e-01,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          1.0085e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  9.2505e-02,\n",
            "         -8.9845e+00, -8.9845e+00,  1.2995e-01,  1.3355e-01,  1.3355e-01,\n",
            "          1.3355e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.3355e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.3355e-01,  1.3355e-01,  1.3355e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.3355e-01,  1.5766e-01,  1.5406e-01,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  1.2994e-01,\n",
            "          1.2994e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          9.7877e-02,  9.3998e-02,  9.3998e-02,  9.2250e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          1.1576e-01,  1.3355e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.3355e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.5766e-01,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.6944e-02,\n",
            "          9.6944e-02,  9.6944e-02,  9.3998e-02,  9.3998e-02,  9.3646e-02,\n",
            "          1.0085e-01,  9.3998e-02,  9.7877e-02,  1.0085e-01,  1.3355e-01,\n",
            "          1.3355e-01,  1.2995e-01,  1.2995e-01,  1.3355e-01,  1.3684e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.3355e-01,  1.0625e-01,\n",
            "          1.0625e-01,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          1.0220e-01,  1.0220e-01,  1.0085e-01,  9.3646e-02,  9.3646e-02,\n",
            "          9.8099e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          1.0220e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.3355e-01,  1.3355e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.3684e-01,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,  9.3998e-02,\n",
            "          9.3998e-02,  9.3998e-02,  1.2995e-01,  1.2995e-01,  1.3355e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.3355e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.3355e-01,\n",
            "          1.3355e-01,  1.2995e-01,  1.2995e-01,  1.3355e-01,  1.3355e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.3355e-01,  1.3355e-01,  1.2995e-01,\n",
            "          1.2995e-01,  1.2995e-01,  1.2995e-01,  1.2995e-01]])\n",
            "tensor([[    nan,     nan, -0.1058, -0.0831, -0.1058, -0.1058, -0.1058,     nan,\n",
            "          0.0161,     nan, -0.1058,     nan, -0.1058, -0.1058, -0.1058, -0.1058,\n",
            "         -0.1058,     nan,     nan,     nan,     nan,     nan, -0.0831, -0.1058,\n",
            "         -0.1058, -0.1058, -0.1058, -0.1058, -0.1058, -0.0831, -0.0831, -0.1058,\n",
            "         -0.1058, -0.1058, -0.1058, -0.0831, -0.1058, -0.0831, -0.1058, -0.1058,\n",
            "         -0.1058, -0.1058, -0.1058, -0.1058, -0.0831],\n",
            "        [    nan,     nan,  0.2003,  0.2038,  0.2003,  0.2003,  0.2003,     nan,\n",
            "          0.2273,     nan,  0.2003,     nan,  0.2003,  0.2003,  0.2003,  0.2003,\n",
            "          0.2003,     nan,     nan,     nan,     nan,     nan,  0.2038,  0.2003,\n",
            "          0.2003,  0.2003,  0.2003,  0.2003,  0.2003,  0.2038,  0.2038,  0.2003,\n",
            "          0.2003,  0.2003,  0.2003,  0.2038,  0.2003,  0.2038,  0.2003,  0.2003,\n",
            "          0.2003,  0.2003,  0.2003,  0.2003,  0.2038]])\n",
            "tensor([[-0.1058, -0.0831, -0.1058, -0.1058, -0.1058, -0.1058,  0.0161,  0.0161,\n",
            "         -0.1058, -0.1058, -0.1058, -0.1058, -0.1058, -0.1058, -0.1058, -0.1058,\n",
            "         -0.1058, -0.1058, -0.1058, -0.1058, -0.0831, -0.1058, -0.1058, -0.1058,\n",
            "         -0.1058, -0.1058, -0.1058, -0.0831, -0.0831, -0.1058, -0.1058, -0.1058,\n",
            "         -0.1058, -0.0831, -0.1058, -0.0831, -0.1058, -0.1058, -0.1058, -0.1058,\n",
            "         -0.1058, -0.1058, -0.0831],\n",
            "        [ 0.2003,  0.2038,  0.2003,  0.2003,  0.2003,  0.2003,  0.2273,  0.2273,\n",
            "          0.2003,  0.2003,  0.2003,  0.2003,  0.2003,  0.2003,  0.2003,  0.2003,\n",
            "          0.2003,  0.2003,  0.2003,  0.2003,  0.2038,  0.2003,  0.2003,  0.2003,\n",
            "          0.2003,  0.2003,  0.2003,  0.2038,  0.2038,  0.2003,  0.2003,  0.2003,\n",
            "          0.2003,  0.2038,  0.2003,  0.2038,  0.2003,  0.2003,  0.2003,  0.2003,\n",
            "          0.2003,  0.2003,  0.2038]])\n",
            "tensor([[ 0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.0557,\n",
            "          0.0557,  0.0504,  0.0504,  0.0504,  0.0504,  0.0619,  0.0619,  0.0619,\n",
            "          0.0619,  0.0619,  0.0619,  0.0511,  0.0511,  0.0511,  0.0511,  0.0546,\n",
            "          0.0546,  0.0546,  0.0546,  0.0511,  0.0511,  0.0511,  0.0546,  0.0511,\n",
            "          0.0511,  0.0546,  0.0511,  0.0511,  0.0511,  0.0511,  0.0546,  0.0511,\n",
            "          0.0511,  0.0511,  0.0546,  0.0546,  0.0619,  0.0592,  0.0592,  0.0592,\n",
            "          0.0592,  0.0592,  0.0592,  0.0619,  0.0619,  0.0605,  0.0605,  0.0605,\n",
            "          0.0557,  0.0557,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2044,  0.2044,  0.2044,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2039,  0.2039,     nan,\n",
            "             nan, -0.4434],\n",
            "        [-0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,  0.1083,\n",
            "          0.1083,  0.1098,  0.1098,  0.1098,  0.1098,  0.1037,  0.1037,  0.1037,\n",
            "          0.1037,  0.1037,  0.1037,  0.0923,  0.0923,  0.0923,  0.0923,  0.0962,\n",
            "          0.0962,  0.0962,  0.0962,  0.0923,  0.0923,  0.0923,  0.0962,  0.0923,\n",
            "          0.0923,  0.0962,  0.0923,  0.0923,  0.0923,  0.0923,  0.0962,  0.0923,\n",
            "          0.0923,  0.0923,  0.0962,  0.0962,  0.1037,  0.1020,  0.1020,  0.1020,\n",
            "          0.1020,  0.1020,  0.1020,  0.1037,  0.1037,  0.1063,  0.1063,  0.1063,\n",
            "          0.1083,  0.1083, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0262, -0.0262, -0.0262, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0131, -0.0131,     nan,\n",
            "             nan, -0.2380]])\n",
            "tensor([[ 0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.0557,\n",
            "          0.0557,  0.0504,  0.0504,  0.0504,  0.0504,  0.0619,  0.0619,  0.0619,\n",
            "          0.0619,  0.0619,  0.0619,  0.0511,  0.0511,  0.0511,  0.0511,  0.0546,\n",
            "          0.0546,  0.0546,  0.0546,  0.0511,  0.0511,  0.0511,  0.0546,  0.0511,\n",
            "          0.0511,  0.0546,  0.0511,  0.0511,  0.0511,  0.0511,  0.0546,  0.0511,\n",
            "          0.0511,  0.0511,  0.0546,  0.0546,  0.0619,  0.0592,  0.0592,  0.0592,\n",
            "          0.0592,  0.0592,  0.0592,  0.0619,  0.0619,  0.0605,  0.0605,  0.0605,\n",
            "          0.0557,  0.0557,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2044,  0.2044,  0.2044,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2059,\n",
            "          0.2059,  0.2059,  0.2059,  0.2059,  0.2059,  0.2039,  0.2039,  0.2039,\n",
            "          0.2039, -0.4434],\n",
            "        [-0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,  0.1083,\n",
            "          0.1083,  0.1098,  0.1098,  0.1098,  0.1098,  0.1037,  0.1037,  0.1037,\n",
            "          0.1037,  0.1037,  0.1037,  0.0923,  0.0923,  0.0923,  0.0923,  0.0962,\n",
            "          0.0962,  0.0962,  0.0962,  0.0923,  0.0923,  0.0923,  0.0962,  0.0923,\n",
            "          0.0923,  0.0962,  0.0923,  0.0923,  0.0923,  0.0923,  0.0962,  0.0923,\n",
            "          0.0923,  0.0923,  0.0962,  0.0962,  0.1037,  0.1020,  0.1020,  0.1020,\n",
            "          0.1020,  0.1020,  0.1020,  0.1037,  0.1037,  0.1063,  0.1063,  0.1063,\n",
            "          0.1083,  0.1083, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0262, -0.0262, -0.0262, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0018,\n",
            "         -0.0018, -0.0018, -0.0018, -0.0018, -0.0018, -0.0131, -0.0131, -0.0131,\n",
            "         -0.0131, -0.2380]])\n",
            "tensor([[        nan,  1.0021e-01,  1.2798e-01,  2.5131e-01,  1.2798e-01,\n",
            "          1.0021e-01,  1.2798e-01,  2.5131e-01,  2.5131e-01,  1.0021e-01,\n",
            "          1.2798e-01,  2.0890e-01,  2.5131e-01,  2.5131e-01,  2.4279e-01,\n",
            "          1.0021e-01,  2.5131e-01,  2.5131e-01,  6.0999e+00,  1.0021e-01,\n",
            "          6.0999e+00,  2.5131e-01,  2.5131e-01,  1.0021e-01,  3.3299e-03,\n",
            "          3.3299e-03,  3.3299e-03,  1.0021e-01,  6.4766e-02,  1.0021e-01,\n",
            "          2.5131e-01,  2.5131e-01,  1.2798e-01,  1.0021e-01,  1.0599e-01,\n",
            "          6.0999e+00,  2.5131e-01,  2.5131e-01,  6.0999e+00,  1.0021e-01,\n",
            "          6.0999e+00,  1.2798e-01,  2.5131e-01,  2.5131e-01,  2.5131e-01,\n",
            "          1.8256e-01,  6.0999e+00,  1.0021e-01,  1.0021e-01,  1.6733e-01,\n",
            "          1.8256e-01,  2.0890e-01,  2.5131e-01,  2.5131e-01,  2.3679e-01,\n",
            "          1.0021e-01,  1.2092e-01,  1.0021e-01],\n",
            "        [        nan,  2.1312e-01,  2.2396e-01,  1.9650e-01,  2.2396e-01,\n",
            "          2.1312e-01,  2.2396e-01,  1.9650e-01,  1.9650e-01,  2.1312e-01,\n",
            "          2.2396e-01,  2.2842e-01,  1.9650e-01,  1.9650e-01,  1.8453e-01,\n",
            "          2.1312e-01,  1.9650e-01,  1.9650e-01, -4.0746e+00,  2.1312e-01,\n",
            "         -4.0746e+00,  1.9650e-01,  1.9650e-01,  2.1312e-01,  9.3249e-02,\n",
            "          9.3249e-02,  9.3249e-02,  2.1312e-01,  1.3962e-01,  2.1312e-01,\n",
            "          1.9650e-01,  1.9650e-01,  2.2396e-01,  2.1312e-01,  2.0730e-01,\n",
            "         -4.0746e+00,  1.9650e-01,  1.9650e-01, -4.0746e+00,  2.1312e-01,\n",
            "         -4.0746e+00,  2.2396e-01,  1.9650e-01,  1.9650e-01,  1.9650e-01,\n",
            "          2.2733e-01, -4.0746e+00,  2.1312e-01,  2.1312e-01,  2.0902e-01,\n",
            "          2.2733e-01,  2.2842e-01,  1.9650e-01,  1.9650e-01,  2.0199e-01,\n",
            "          2.1312e-01, -1.1979e-01,  2.1312e-01]])\n",
            "tensor([[ 1.0021e-01,  1.2798e-01,  2.5131e-01,  1.2798e-01,  1.0021e-01,\n",
            "          1.2798e-01,  2.5131e-01,  2.5131e-01,  1.0021e-01,  1.2798e-01,\n",
            "          2.0890e-01,  2.5131e-01,  2.5131e-01,  2.4279e-01,  1.0021e-01,\n",
            "          2.5131e-01,  2.5131e-01,  6.0999e+00,  1.0021e-01,  6.0999e+00,\n",
            "          2.5131e-01,  2.5131e-01,  1.0021e-01,  3.3299e-03,  3.3299e-03,\n",
            "          3.3299e-03,  1.0021e-01,  6.4766e-02,  1.0021e-01,  2.5131e-01,\n",
            "          2.5131e-01,  1.2798e-01,  1.0021e-01,  1.0599e-01,  6.0999e+00,\n",
            "          2.5131e-01,  2.5131e-01,  6.0999e+00,  1.0021e-01,  6.0999e+00,\n",
            "          1.2798e-01,  2.5131e-01,  2.5131e-01,  2.5131e-01,  1.8256e-01,\n",
            "          6.0999e+00,  1.0021e-01,  1.0021e-01,  1.6733e-01,  1.8256e-01,\n",
            "          2.0890e-01,  2.5131e-01,  2.5131e-01,  2.3679e-01,  1.0021e-01,\n",
            "          1.2092e-01,  1.0021e-01],\n",
            "        [ 2.1312e-01,  2.2396e-01,  1.9650e-01,  2.2396e-01,  2.1312e-01,\n",
            "          2.2396e-01,  1.9650e-01,  1.9650e-01,  2.1312e-01,  2.2396e-01,\n",
            "          2.2842e-01,  1.9650e-01,  1.9650e-01,  1.8453e-01,  2.1312e-01,\n",
            "          1.9650e-01,  1.9650e-01, -4.0746e+00,  2.1312e-01, -4.0746e+00,\n",
            "          1.9650e-01,  1.9650e-01,  2.1312e-01,  9.3249e-02,  9.3249e-02,\n",
            "          9.3249e-02,  2.1312e-01,  1.3962e-01,  2.1312e-01,  1.9650e-01,\n",
            "          1.9650e-01,  2.2396e-01,  2.1312e-01,  2.0730e-01, -4.0746e+00,\n",
            "          1.9650e-01,  1.9650e-01, -4.0746e+00,  2.1312e-01, -4.0746e+00,\n",
            "          2.2396e-01,  1.9650e-01,  1.9650e-01,  1.9650e-01,  2.2733e-01,\n",
            "         -4.0746e+00,  2.1312e-01,  2.1312e-01,  2.0902e-01,  2.2733e-01,\n",
            "          2.2842e-01,  1.9650e-01,  1.9650e-01,  2.0199e-01,  2.1312e-01,\n",
            "         -1.1979e-01,  2.1312e-01]])\n",
            "tensor([[-0.0817, -0.0817, -0.0803, -0.0803, -0.0803, -0.0803, -0.0803, -0.0803,\n",
            "         -0.0978,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0049,  0.0045, -0.0114, -0.0697, -0.0705, -0.0817,\n",
            "         -0.0978, -0.0558,  0.0045,  0.0544,  0.0483,  0.0544,  0.0544,  0.0544,\n",
            "          0.0544,  0.0544,     nan,  0.0525,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0045,\n",
            "         -0.0697, -0.0978, -0.0817, -0.0012,  0.0114,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,\n",
            "          0.0544,  0.0544,  0.0544,  0.0483,  0.0458,  0.0544,  0.0544,  0.0036,\n",
            "         -0.0558, -0.0817, -0.0817, -0.0817, -0.0558,  0.0230,  0.0544,  0.0544,\n",
            "          0.0544,     nan,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0483,  0.0544,     nan,  0.0045, -0.0558,\n",
            "         -0.0978, -0.0778, -0.0817,     nan,     nan, -0.0778, -0.0803, -0.0817,\n",
            "         -0.0978,  0.0114,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0458,\n",
            "          0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,  0.0544,\n",
            "          0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544, -0.0697,\n",
            "         -0.0978,  0.0483,  0.0544,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0544, -0.0697, -0.0817, -0.0978,  0.0544,\n",
            "          0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,  0.0049,\n",
            "         -0.0978,  0.0339,  0.0483,  0.0483,  0.0483,  0.0041, -0.0817, -0.0705,\n",
            "         -0.0978,  0.0544,  0.0483,  0.0483,  0.0230, -0.0978, -0.0817, -0.1054,\n",
            "         -0.0803, -0.0803, -0.0803, -0.0803, -0.0803, -0.0803, -0.0978, -0.0978,\n",
            "         -0.0705, -0.0817, -0.0705, -0.0705, -0.0705, -0.0817],\n",
            "        [ 0.1057,  0.1057,  0.0934,  0.0934,  0.0934,  0.0934,  0.0934,  0.0934,\n",
            "          0.1154,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0860,  0.0893,  0.0930,  0.1013,  0.0897,  0.1057,\n",
            "          0.1154,  0.1040,  0.0893,  0.0461,  0.0609,  0.0461,  0.0461,  0.0461,\n",
            "          0.0461,  0.0461,     nan,  0.0720,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0893,\n",
            "          0.1013,  0.1154,  0.1057,  0.0925,  0.0698,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,\n",
            "          0.0461,  0.0461,  0.0461,  0.0609,  0.0347,  0.0461,  0.0461,  0.0960,\n",
            "          0.1040,  0.1057,  0.1057,  0.1057,  0.1040,  0.0732,  0.0461,  0.0461,\n",
            "          0.0461,     nan,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0609,  0.0461,     nan,  0.0893,  0.1040,\n",
            "          0.1154,  0.1201,  0.1057,     nan,     nan,  0.1201,  0.0934,  0.1057,\n",
            "          0.1154,  0.0698,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0347,\n",
            "          0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,  0.0461,\n",
            "          0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.1013,\n",
            "          0.1154,  0.0609,  0.0461,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0461,  0.1013,  0.1057,  0.1154,  0.0461,\n",
            "          0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,  0.0860,\n",
            "          0.1154,  0.0682,  0.0609,  0.0609,  0.0609,  0.0950,  0.1057,  0.0897,\n",
            "          0.1154,  0.0461,  0.0609,  0.0609,  0.0732,  0.1154,  0.1057,  0.1089,\n",
            "          0.0934,  0.0934,  0.0934,  0.0934,  0.0934,  0.0934,  0.1154,  0.1154,\n",
            "          0.0897,  0.1057,  0.0897,  0.0897,  0.0897,  0.1057]])\n",
            "tensor([[-0.0817, -0.0817, -0.0803, -0.0803, -0.0803, -0.0803, -0.0803, -0.0803,\n",
            "         -0.0978,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0049,  0.0045, -0.0114, -0.0697, -0.0705, -0.0817,\n",
            "         -0.0978, -0.0558,  0.0045,  0.0544,  0.0483,  0.0544,  0.0544,  0.0544,\n",
            "          0.0544,  0.0544,  0.0544,  0.0525,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0045,\n",
            "         -0.0697, -0.0978, -0.0817, -0.0012,  0.0114,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,\n",
            "          0.0544,  0.0544,  0.0544,  0.0483,  0.0458,  0.0544,  0.0544,  0.0036,\n",
            "         -0.0558, -0.0817, -0.0817, -0.0817, -0.0558,  0.0230,  0.0544,  0.0544,\n",
            "          0.0544,  0.0544,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0483,  0.0544,  0.0544,  0.0045, -0.0558,\n",
            "         -0.0978, -0.0778, -0.0817, -0.0817, -0.0817, -0.0778, -0.0803, -0.0817,\n",
            "         -0.0978,  0.0114,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0458,\n",
            "          0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,  0.0544,\n",
            "          0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544, -0.0697,\n",
            "         -0.0978,  0.0483,  0.0544,  0.0483,  0.0483,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0483,  0.0483,  0.0544, -0.0697, -0.0817, -0.0978,  0.0544,\n",
            "          0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,  0.0483,  0.0483,\n",
            "          0.0483,  0.0544,  0.0544,  0.0544,  0.0544,  0.0544,  0.0483,  0.0049,\n",
            "         -0.0978,  0.0339,  0.0483,  0.0483,  0.0483,  0.0041, -0.0817, -0.0705,\n",
            "         -0.0978,  0.0544,  0.0483,  0.0483,  0.0230, -0.0978, -0.0817, -0.1054,\n",
            "         -0.0803, -0.0803, -0.0803, -0.0803, -0.0803, -0.0803, -0.0978, -0.0978,\n",
            "         -0.0705, -0.0817, -0.0705, -0.0705, -0.0705, -0.0817],\n",
            "        [ 0.1057,  0.1057,  0.0934,  0.0934,  0.0934,  0.0934,  0.0934,  0.0934,\n",
            "          0.1154,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0860,  0.0893,  0.0930,  0.1013,  0.0897,  0.1057,\n",
            "          0.1154,  0.1040,  0.0893,  0.0461,  0.0609,  0.0461,  0.0461,  0.0461,\n",
            "          0.0461,  0.0461,  0.0461,  0.0720,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0893,\n",
            "          0.1013,  0.1154,  0.1057,  0.0925,  0.0698,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,\n",
            "          0.0461,  0.0461,  0.0461,  0.0609,  0.0347,  0.0461,  0.0461,  0.0960,\n",
            "          0.1040,  0.1057,  0.1057,  0.1057,  0.1040,  0.0732,  0.0461,  0.0461,\n",
            "          0.0461,  0.0461,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0609,  0.0461,  0.0461,  0.0893,  0.1040,\n",
            "          0.1154,  0.1201,  0.1057,  0.1057,  0.1057,  0.1201,  0.0934,  0.1057,\n",
            "          0.1154,  0.0698,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0347,\n",
            "          0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,  0.0461,\n",
            "          0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.1013,\n",
            "          0.1154,  0.0609,  0.0461,  0.0609,  0.0609,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0609,  0.0609,  0.0461,  0.1013,  0.1057,  0.1154,  0.0461,\n",
            "          0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,  0.0609,  0.0609,\n",
            "          0.0609,  0.0461,  0.0461,  0.0461,  0.0461,  0.0461,  0.0609,  0.0860,\n",
            "          0.1154,  0.0682,  0.0609,  0.0609,  0.0609,  0.0950,  0.1057,  0.0897,\n",
            "          0.1154,  0.0461,  0.0609,  0.0609,  0.0732,  0.1154,  0.1057,  0.1089,\n",
            "          0.0934,  0.0934,  0.0934,  0.0934,  0.0934,  0.0934,  0.1154,  0.1154,\n",
            "          0.0897,  0.1057,  0.0897,  0.0897,  0.0897,  0.1057]])\n",
            "tensor([[nan],\n",
            "        [nan]])\n",
            "tensor([], size=(2, 0))\n",
            "tensor([[ 0.3023,  0.3023,  0.1200,  0.1200,  0.3023,  0.3023,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.1200,  0.1200,  0.3023,  0.3023,  0.3023,  0.1200,\n",
            "          0.1200,  0.1200,  0.1200,  0.2692,  0.2692,  0.1200,  0.1200,  0.1200,\n",
            "          0.1200,  0.1200,  0.1200,  0.1200,  0.2692,  0.2692,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.2772,  0.2772,  0.0043,  0.0043,  0.0043,  0.3023,\n",
            "          0.3023,  0.3023,  0.3023,  0.1200,  0.3023,  0.3023,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.1200,  0.1200,  0.1200,  0.1200,  0.2772,  0.2772,\n",
            "          0.2772,  0.2772,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.1200,\n",
            "          0.1200,  0.1200,  0.1200,  0.3023,  0.3023,  0.1200,  0.1200,  0.3023,\n",
            "          0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,\n",
            "          0.1200,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.1200,\n",
            "          0.1200,  0.1200,  0.2692,  0.2692,  0.2692,  0.2692,  0.2692,  0.3023,\n",
            "          0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.3023,  0.3023,  0.2772,  0.2772,  0.3023,  0.3023,\n",
            "          0.0103,  0.0103,  0.0103,  0.0103,  0.3023,  0.3023,  0.1200,  0.1200,\n",
            "          0.3023,  0.3023,  0.3137,  0.3137,  0.3137,  0.3137,  0.2692,  0.3023,\n",
            "          0.3023,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.0702,  0.0702,\n",
            "          0.0777,  0.0777, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,     nan,\n",
            "             nan, -0.0030, -0.0030, -0.0030, -0.0030,     nan,     nan,  0.1200,\n",
            "          0.1200,  0.3023,  0.3023,  0.3023, -0.0030, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0095, -0.0095, -0.0095, -0.0030, -0.0030, -0.0095, -0.0095, -0.0095,\n",
            "         -0.0095, -0.0095, -0.0030, -0.0030, -0.0030,     nan,     nan, -0.0041,\n",
            "         -0.0041, -0.0095, -0.0030, -0.0030,     nan,     nan,     nan,     nan,\n",
            "         -0.0095, -0.0095, -0.0095, -0.0095, -0.0095,     nan, -0.0030, -0.0030,\n",
            "             nan,     nan, -0.0095, -0.0095,     nan,     nan, -0.0030],\n",
            "        [ 0.1420,  0.1420,  0.0318,  0.0318,  0.1420,  0.1420,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.0318,  0.0318,  0.1420,  0.1420,  0.1420,  0.0318,\n",
            "          0.0318,  0.0318,  0.0318,  0.1745,  0.1745,  0.0318,  0.0318,  0.0318,\n",
            "          0.0318,  0.0318,  0.0318,  0.0318,  0.1745,  0.1745,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.1331,  0.1331,  0.0953,  0.0953,  0.0953,  0.1420,\n",
            "          0.1420,  0.1420,  0.1420,  0.0318,  0.1420,  0.1420,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.0318,  0.0318,  0.0318,  0.0318,  0.1331,  0.1331,\n",
            "          0.1331,  0.1331,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.0318,\n",
            "          0.0318,  0.0318,  0.0318,  0.1420,  0.1420,  0.0318,  0.0318,  0.1420,\n",
            "          0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,\n",
            "          0.0318,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.0318,\n",
            "          0.0318,  0.0318,  0.1745,  0.1745,  0.1745,  0.1745,  0.1745,  0.1420,\n",
            "          0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.1420,  0.1420,  0.1331,  0.1331,  0.1420,  0.1420,\n",
            "          0.1145,  0.1145,  0.1145,  0.1145,  0.1420,  0.1420,  0.0318,  0.0318,\n",
            "          0.1420,  0.1420,  0.1340,  0.1340,  0.1340,  0.1340,  0.1745,  0.1420,\n",
            "          0.1420,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0931,  0.0931,\n",
            "          0.0814,  0.0814,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,     nan,\n",
            "             nan,  0.0675,  0.0675,  0.0675,  0.0675,     nan,     nan,  0.0318,\n",
            "          0.0318,  0.1420,  0.1420,  0.1420,  0.0675,  0.0675,  0.0675,  0.0738,\n",
            "          0.0681,  0.0681,  0.0681,  0.0675,  0.0675,  0.0681,  0.0681,  0.0681,\n",
            "          0.0681,  0.0681,  0.0675,  0.0675,  0.0675,     nan,     nan,  0.0738,\n",
            "          0.0738,  0.0681,  0.0675,  0.0675,     nan,     nan,     nan,     nan,\n",
            "          0.0681,  0.0681,  0.0681,  0.0681,  0.0681,     nan,  0.0675,  0.0675,\n",
            "             nan,     nan,  0.0681,  0.0681,     nan,     nan,  0.0675]])\n",
            "tensor([[ 0.3023,  0.3023,  0.1200,  0.1200,  0.3023,  0.3023,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.1200,  0.1200,  0.3023,  0.3023,  0.3023,  0.1200,\n",
            "          0.1200,  0.1200,  0.1200,  0.2692,  0.2692,  0.1200,  0.1200,  0.1200,\n",
            "          0.1200,  0.1200,  0.1200,  0.1200,  0.2692,  0.2692,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.2772,  0.2772,  0.0043,  0.0043,  0.0043,  0.3023,\n",
            "          0.3023,  0.3023,  0.3023,  0.1200,  0.3023,  0.3023,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.1200,  0.1200,  0.1200,  0.1200,  0.2772,  0.2772,\n",
            "          0.2772,  0.2772,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.1200,\n",
            "          0.1200,  0.1200,  0.1200,  0.3023,  0.3023,  0.1200,  0.1200,  0.3023,\n",
            "          0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,\n",
            "          0.1200,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.1200,\n",
            "          0.1200,  0.1200,  0.2692,  0.2692,  0.2692,  0.2692,  0.2692,  0.3023,\n",
            "          0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,  0.3023,\n",
            "          0.3023,  0.3023,  0.3023,  0.3023,  0.2772,  0.2772,  0.3023,  0.3023,\n",
            "          0.0103,  0.0103,  0.0103,  0.0103,  0.3023,  0.3023,  0.1200,  0.1200,\n",
            "          0.3023,  0.3023,  0.3137,  0.3137,  0.3137,  0.3137,  0.2692,  0.3023,\n",
            "          0.3023,  0.1200,  0.1200,  0.1200,  0.1200,  0.1200,  0.0702,  0.0702,\n",
            "          0.0777,  0.0777, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,  0.1200,\n",
            "          0.1200,  0.3023,  0.3023,  0.3023, -0.0030, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0095, -0.0095, -0.0095, -0.0030, -0.0030, -0.0095, -0.0095, -0.0095,\n",
            "         -0.0095, -0.0095, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0095, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030, -0.0030,\n",
            "         -0.0095, -0.0095, -0.0095, -0.0095, -0.0095, -0.0095, -0.0030, -0.0030,\n",
            "         -0.0030, -0.0030, -0.0095, -0.0095, -0.0095, -0.0095, -0.0030],\n",
            "        [ 0.1420,  0.1420,  0.0318,  0.0318,  0.1420,  0.1420,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.0318,  0.0318,  0.1420,  0.1420,  0.1420,  0.0318,\n",
            "          0.0318,  0.0318,  0.0318,  0.1745,  0.1745,  0.0318,  0.0318,  0.0318,\n",
            "          0.0318,  0.0318,  0.0318,  0.0318,  0.1745,  0.1745,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.1331,  0.1331,  0.0953,  0.0953,  0.0953,  0.1420,\n",
            "          0.1420,  0.1420,  0.1420,  0.0318,  0.1420,  0.1420,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.0318,  0.0318,  0.0318,  0.0318,  0.1331,  0.1331,\n",
            "          0.1331,  0.1331,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.0318,\n",
            "          0.0318,  0.0318,  0.0318,  0.1420,  0.1420,  0.0318,  0.0318,  0.1420,\n",
            "          0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,\n",
            "          0.0318,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.0318,\n",
            "          0.0318,  0.0318,  0.1745,  0.1745,  0.1745,  0.1745,  0.1745,  0.1420,\n",
            "          0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,  0.1420,\n",
            "          0.1420,  0.1420,  0.1420,  0.1420,  0.1331,  0.1331,  0.1420,  0.1420,\n",
            "          0.1145,  0.1145,  0.1145,  0.1145,  0.1420,  0.1420,  0.0318,  0.0318,\n",
            "          0.1420,  0.1420,  0.1340,  0.1340,  0.1340,  0.1340,  0.1745,  0.1420,\n",
            "          0.1420,  0.0318,  0.0318,  0.0318,  0.0318,  0.0318,  0.0931,  0.0931,\n",
            "          0.0814,  0.0814,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,\n",
            "          0.0675,  0.0675,  0.0675,  0.0675,  0.0675,  0.0675,  0.0675,  0.0318,\n",
            "          0.0318,  0.1420,  0.1420,  0.1420,  0.0675,  0.0675,  0.0675,  0.0738,\n",
            "          0.0681,  0.0681,  0.0681,  0.0675,  0.0675,  0.0681,  0.0681,  0.0681,\n",
            "          0.0681,  0.0681,  0.0675,  0.0675,  0.0675,  0.0675,  0.0675,  0.0738,\n",
            "          0.0738,  0.0681,  0.0675,  0.0675,  0.0675,  0.0675,  0.0675,  0.0675,\n",
            "          0.0681,  0.0681,  0.0681,  0.0681,  0.0681,  0.0681,  0.0675,  0.0675,\n",
            "          0.0675,  0.0675,  0.0681,  0.0681,  0.0681,  0.0681,  0.0675]])\n",
            "tensor([[        nan,         nan,  1.6102e-01,  1.6102e-01,  2.2666e-01,\n",
            "          1.2822e-01,  1.2822e-01, -5.2130e-01, -5.2130e-01, -5.2130e-01,\n",
            "         -5.2130e-01,  1.0972e-01, -1.2591e-02,  9.3107e-03,  4.7915e-03,\n",
            "          1.5141e-02,         nan,  2.3015e-01,  2.3230e-01,  2.3230e-01,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,  2.4605e-01,  3.2061e-02,\n",
            "         -6.7777e-03,  7.1722e-03,  7.1722e-03, -8.3997e-02, -2.3604e-02,\n",
            "         -2.3604e-02,  8.3169e-02,  8.3169e-02,  8.3169e-02,  1.2093e-01,\n",
            "         -1.3729e-01, -1.3729e-01, -1.0173e-01, -1.0173e-01,         nan,\n",
            "                 nan,  4.6888e-02,  4.6888e-02,  5.2524e-02,  5.2524e-02,\n",
            "          3.5905e-02,  3.5905e-02,  6.2591e-02,  6.2591e-02,  5.0896e-02,\n",
            "          5.0896e-02,  2.3230e-01,  2.2361e-01,  2.2361e-01,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,  1.5176e-01,\n",
            "          1.5176e-01,  1.0549e-01,  1.0549e-01,  9.0807e-02,  9.0807e-02,\n",
            "          5.2022e-02,  5.2022e-02,  5.2941e-02,  5.2941e-02,  5.2941e-02,\n",
            "          5.2941e-02, -6.2789e-02, -6.2789e-02, -1.2566e-01, -1.2566e-01,\n",
            "         -1.2566e-01, -1.2566e-01, -3.7919e-02, -4.9263e-02,         nan,\n",
            "         -2.1118e-01, -2.1118e-01,  6.7577e-02,  5.8331e-03, -6.2789e-02,\n",
            "         -6.2789e-02,         nan,         nan, -2.2749e-02,         nan,\n",
            "          2.4964e-01,  2.0313e-01,  2.0313e-01,         nan,         nan,\n",
            "         -6.6479e-03,  2.8460e-02,  2.8460e-02, -2.5528e-02,         nan,\n",
            "          2.4964e-01,  2.4964e-01,  2.2975e-01,  2.2975e-01,  2.4964e-01,\n",
            "          2.4964e-01,  2.4964e-01,  2.4964e-01,  2.5162e-01,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "          6.4766e-02,  6.0563e-03, -6.0615e-03, -2.9333e-02,  6.2013e-02,\n",
            "          4.5187e-03,  4.8929e-02,  4.8929e-02,         nan,  6.7577e-02,\n",
            "          8.5833e-02,  9.0807e-02,  3.9823e-02,  4.3714e-02, -6.7777e-03,\n",
            "          5.8212e-02,  4.1088e-03,  4.1088e-03,  5.2941e-02, -6.7777e-03,\n",
            "         -1.1269e-01, -6.7777e-03,  1.0972e-01,  1.4702e-01,  2.2975e-01,\n",
            "          2.4964e-01,  2.2899e-01,         nan,         nan,         nan,\n",
            "                 nan,  1.6183e-01,  8.9497e-02, -6.2789e-02,         nan,\n",
            "          2.5597e-03,  9.1678e-02,  8.4994e-02,  4.4870e-02,  7.8666e-02,\n",
            "         -6.2089e-02,  5.0926e-02, -4.0460e-02, -1.0273e-01, -6.2789e-02,\n",
            "          1.1434e-01,  1.3080e-01,  1.2032e-01,  2.4964e-01,  2.3230e-01,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  1.7566e-01,  9.0807e-02, -3.1970e-02,\n",
            "          1.5139e-01,  5.2524e-02,  5.2524e-02,  1.1446e-02,  4.7133e-02,\n",
            "          5.1120e-02,  2.3169e-02,         nan, -7.5351e-02,  8.4571e-02,\n",
            "          8.0854e-02,  3.0877e-02,  5.2941e-02, -1.2243e-01, -1.2433e-03,\n",
            "          7.0155e-02,  9.1300e-02,  5.5682e-02,  6.5460e-02,  1.5535e-01,\n",
            "          1.5118e-01,  1.6733e-01,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "          6.7577e-02,  6.5536e-02, -2.8214e+00,  1.6663e-01,  1.6663e-01,\n",
            "          1.7953e-01,  1.7953e-01,  2.1172e-01,  1.4396e-01,  1.4396e-01,\n",
            "          1.6079e-01,         nan,         nan],\n",
            "        [        nan,         nan,  2.5868e-01,  2.5868e-01,  9.1097e-02,\n",
            "          1.4228e-01,  1.4228e-01, -5.5831e-02, -5.5831e-02, -5.5831e-02,\n",
            "         -5.5831e-02,  1.2784e-01,  3.5562e-01,  2.8701e-01,  1.8456e-01,\n",
            "          1.7168e-01,         nan,  6.2108e-02,  9.0009e-02,  9.0009e-02,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,  8.6746e-02, -2.9470e-02,\n",
            "          8.2831e-02,  1.1845e-01,  1.1845e-01,  2.3055e-01,  1.5947e-01,\n",
            "          1.5947e-01,  2.3300e-01,  2.3300e-01,  2.3300e-01,  2.5862e-01,\n",
            "          3.7741e-01,  3.7741e-01,  1.0147e-01,  1.0147e-01,         nan,\n",
            "                 nan,  8.5941e-02,  8.5941e-02,  7.1987e-02,  7.1987e-02,\n",
            "          8.1445e-02,  8.1445e-02,  1.5088e-01,  1.5088e-01,  1.1771e-01,\n",
            "          1.1771e-01,  9.0009e-02,  1.5169e-01,  1.5169e-01,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,  2.6914e-01,\n",
            "          2.6914e-01,  1.6109e-01,  1.6109e-01,  1.5688e-01,  1.5688e-01,\n",
            "          1.6169e-01,  1.6169e-01,  1.5680e-01,  1.5680e-01,  1.5680e-01,\n",
            "          1.5680e-01,  2.8566e-02,  2.8566e-02,  4.7974e-02,  4.7974e-02,\n",
            "          4.7974e-02,  4.7974e-02, -1.4196e-01,  5.3286e-03,         nan,\n",
            "         -3.2287e-01, -3.2287e-01,  1.3317e-01,  9.9958e-02,  2.8566e-02,\n",
            "          2.8566e-02,         nan,         nan,  1.5736e-01,         nan,\n",
            "          7.1139e-02,  1.0983e-01,  1.0983e-01,         nan,         nan,\n",
            "          5.4510e-02,  1.3280e-01,  1.3280e-01,  9.1262e-02,         nan,\n",
            "          7.1139e-02,  7.1139e-02,  6.9238e-02,  6.9238e-02,  7.1139e-02,\n",
            "          7.1139e-02,  7.1139e-02,  7.1139e-02,  1.1389e-01,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "          1.3962e-01,  5.5474e-02,  3.8919e-02,  1.8096e-02,  1.5771e-01,\n",
            "          8.9282e-02,  1.9713e-01,  1.9713e-01,         nan,  1.3317e-01,\n",
            "          1.5124e-01,  1.5688e-01,  9.3646e-02,  1.0085e-01,  8.2831e-02,\n",
            "          1.1377e-01,  9.4956e-02,  9.4956e-02,  1.5680e-01,  8.2831e-02,\n",
            "          8.0150e-02,  8.2831e-02,  1.2784e-01,  1.3058e-01,  6.9238e-02,\n",
            "          7.1139e-02,  1.3441e-01,         nan,         nan,         nan,\n",
            "                 nan,  1.3841e-01,  1.3232e-01,  2.8566e-02,         nan,\n",
            "          1.6437e-03,  1.2935e-01,  1.3549e-01,  1.2346e-01,  1.0893e-01,\n",
            "          1.9614e-01,  1.2717e-01,  1.0691e-02,  5.0940e-02,  2.8566e-02,\n",
            "          5.5577e-02,  1.0483e-01,  1.2166e-01,  7.1139e-02,  9.0009e-02,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.5631e-01,  1.5688e-01,  8.3483e-02,\n",
            "          1.0380e-01,  7.1987e-02,  7.1987e-02,  6.9764e-02,  9.6944e-02,\n",
            "          9.2250e-02,  8.2750e-02,         nan,  5.9216e-02,  4.4830e-02,\n",
            "          5.1390e-02,  1.5613e-01,  1.5680e-01,  7.0968e-02,  9.2505e-02,\n",
            "          3.0317e-02,  6.7699e-02,  1.0829e-01,  5.2511e-02,  7.9875e-02,\n",
            "          1.2698e-01,  2.0902e-01,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "          1.3317e-01,  1.0999e-01, -4.8887e+00,  1.0306e-01,  1.0306e-01,\n",
            "          1.0109e-01,  1.0109e-01,  1.0246e-01,  1.2441e-01,  1.2441e-01,\n",
            "          2.6688e-01,         nan,         nan]])\n",
            "tensor([[ 1.6102e-01,  1.6102e-01,  2.2666e-01,  1.2822e-01,  1.2822e-01,\n",
            "         -5.2130e-01, -5.2130e-01, -5.2130e-01, -5.2130e-01,  1.0972e-01,\n",
            "         -1.2591e-02,  9.3107e-03,  4.7915e-03,  1.5141e-02,  1.5141e-02,\n",
            "          2.3015e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,\n",
            "          2.3230e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,\n",
            "          2.3230e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,\n",
            "          2.3230e-01,  2.4605e-01,  3.2061e-02, -6.7777e-03,  7.1722e-03,\n",
            "          7.1722e-03, -8.3997e-02, -2.3604e-02, -2.3604e-02,  8.3169e-02,\n",
            "          8.3169e-02,  8.3169e-02,  1.2093e-01, -1.3729e-01, -1.3729e-01,\n",
            "         -1.0173e-01, -1.0173e-01, -1.0173e-01, -1.0173e-01,  4.6888e-02,\n",
            "          4.6888e-02,  5.2524e-02,  5.2524e-02,  3.5905e-02,  3.5905e-02,\n",
            "          6.2591e-02,  6.2591e-02,  5.0896e-02,  5.0896e-02,  2.3230e-01,\n",
            "          2.2361e-01,  2.2361e-01,  2.2361e-01,  2.2361e-01,  2.2361e-01,\n",
            "          2.2361e-01,  2.2361e-01,  2.2361e-01,  2.2361e-01,  2.2361e-01,\n",
            "          2.2361e-01,  2.2361e-01,  1.5176e-01,  1.5176e-01,  1.0549e-01,\n",
            "          1.0549e-01,  9.0807e-02,  9.0807e-02,  5.2022e-02,  5.2022e-02,\n",
            "          5.2941e-02,  5.2941e-02,  5.2941e-02,  5.2941e-02, -6.2789e-02,\n",
            "         -6.2789e-02, -1.2566e-01, -1.2566e-01, -1.2566e-01, -1.2566e-01,\n",
            "         -3.7919e-02, -4.9263e-02, -4.9263e-02, -2.1118e-01, -2.1118e-01,\n",
            "          6.7577e-02,  5.8331e-03, -6.2789e-02, -6.2789e-02, -6.2789e-02,\n",
            "         -6.2789e-02, -2.2749e-02, -2.2749e-02,  2.4964e-01,  2.0313e-01,\n",
            "          2.0313e-01,  2.0313e-01,  2.0313e-01, -6.6479e-03,  2.8460e-02,\n",
            "          2.8460e-02, -2.5528e-02, -2.5528e-02,  2.4964e-01,  2.4964e-01,\n",
            "          2.2975e-01,  2.2975e-01,  2.4964e-01,  2.4964e-01,  2.4964e-01,\n",
            "          2.4964e-01,  2.5162e-01,  2.5162e-01,  2.5162e-01,  2.5162e-01,\n",
            "          2.5162e-01,  2.5162e-01,  2.5162e-01,  6.4766e-02,  6.0563e-03,\n",
            "         -6.0615e-03, -2.9333e-02,  6.2013e-02,  4.5187e-03,  4.8929e-02,\n",
            "          4.8929e-02,  4.8929e-02,  6.7577e-02,  8.5833e-02,  9.0807e-02,\n",
            "          3.9823e-02,  4.3714e-02, -6.7777e-03,  5.8212e-02,  4.1088e-03,\n",
            "          4.1088e-03,  5.2941e-02, -6.7777e-03, -1.1269e-01, -6.7777e-03,\n",
            "          1.0972e-01,  1.4702e-01,  2.2975e-01,  2.4964e-01,  2.2899e-01,\n",
            "          2.2899e-01,  2.2899e-01,  2.2899e-01,  2.2899e-01,  1.6183e-01,\n",
            "          8.9497e-02, -6.2789e-02, -6.2789e-02,  2.5597e-03,  9.1678e-02,\n",
            "          8.4994e-02,  4.4870e-02,  7.8666e-02, -6.2089e-02,  5.0926e-02,\n",
            "         -4.0460e-02, -1.0273e-01, -6.2789e-02,  1.1434e-01,  1.3080e-01,\n",
            "          1.2032e-01,  2.4964e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,\n",
            "          2.3230e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,  2.3230e-01,\n",
            "          1.7566e-01,  9.0807e-02, -3.1970e-02,  1.5139e-01,  5.2524e-02,\n",
            "          5.2524e-02,  1.1446e-02,  4.7133e-02,  5.1120e-02,  2.3169e-02,\n",
            "          2.3169e-02, -7.5351e-02,  8.4571e-02,  8.0854e-02,  3.0877e-02,\n",
            "          5.2941e-02, -1.2243e-01, -1.2433e-03,  7.0155e-02,  9.1300e-02,\n",
            "          5.5682e-02,  6.5460e-02,  1.5535e-01,  1.5118e-01,  1.6733e-01,\n",
            "          1.6733e-01,  1.6733e-01,  1.6733e-01,  1.6733e-01,  1.6733e-01,\n",
            "          1.6733e-01,  1.6733e-01,  1.6733e-01,  6.7577e-02,  6.5536e-02,\n",
            "         -2.8214e+00,  1.6663e-01,  1.6663e-01,  1.7953e-01,  1.7953e-01,\n",
            "          2.1172e-01,  1.4396e-01,  1.4396e-01,  1.6079e-01,  1.6079e-01,\n",
            "          1.6079e-01],\n",
            "        [ 2.5868e-01,  2.5868e-01,  9.1097e-02,  1.4228e-01,  1.4228e-01,\n",
            "         -5.5831e-02, -5.5831e-02, -5.5831e-02, -5.5831e-02,  1.2784e-01,\n",
            "          3.5562e-01,  2.8701e-01,  1.8456e-01,  1.7168e-01,  1.7168e-01,\n",
            "          6.2108e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,\n",
            "          9.0009e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,\n",
            "          9.0009e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,\n",
            "          9.0009e-02,  8.6746e-02, -2.9470e-02,  8.2831e-02,  1.1845e-01,\n",
            "          1.1845e-01,  2.3055e-01,  1.5947e-01,  1.5947e-01,  2.3300e-01,\n",
            "          2.3300e-01,  2.3300e-01,  2.5862e-01,  3.7741e-01,  3.7741e-01,\n",
            "          1.0147e-01,  1.0147e-01,  1.0147e-01,  1.0147e-01,  8.5941e-02,\n",
            "          8.5941e-02,  7.1987e-02,  7.1987e-02,  8.1445e-02,  8.1445e-02,\n",
            "          1.5088e-01,  1.5088e-01,  1.1771e-01,  1.1771e-01,  9.0009e-02,\n",
            "          1.5169e-01,  1.5169e-01,  1.5169e-01,  1.5169e-01,  1.5169e-01,\n",
            "          1.5169e-01,  1.5169e-01,  1.5169e-01,  1.5169e-01,  1.5169e-01,\n",
            "          1.5169e-01,  1.5169e-01,  2.6914e-01,  2.6914e-01,  1.6109e-01,\n",
            "          1.6109e-01,  1.5688e-01,  1.5688e-01,  1.6169e-01,  1.6169e-01,\n",
            "          1.5680e-01,  1.5680e-01,  1.5680e-01,  1.5680e-01,  2.8566e-02,\n",
            "          2.8566e-02,  4.7974e-02,  4.7974e-02,  4.7974e-02,  4.7974e-02,\n",
            "         -1.4196e-01,  5.3286e-03,  5.3286e-03, -3.2287e-01, -3.2287e-01,\n",
            "          1.3317e-01,  9.9958e-02,  2.8566e-02,  2.8566e-02,  2.8566e-02,\n",
            "          2.8566e-02,  1.5736e-01,  1.5736e-01,  7.1139e-02,  1.0983e-01,\n",
            "          1.0983e-01,  1.0983e-01,  1.0983e-01,  5.4510e-02,  1.3280e-01,\n",
            "          1.3280e-01,  9.1262e-02,  9.1262e-02,  7.1139e-02,  7.1139e-02,\n",
            "          6.9238e-02,  6.9238e-02,  7.1139e-02,  7.1139e-02,  7.1139e-02,\n",
            "          7.1139e-02,  1.1389e-01,  1.1389e-01,  1.1389e-01,  1.1389e-01,\n",
            "          1.1389e-01,  1.1389e-01,  1.1389e-01,  1.3962e-01,  5.5474e-02,\n",
            "          3.8919e-02,  1.8096e-02,  1.5771e-01,  8.9282e-02,  1.9713e-01,\n",
            "          1.9713e-01,  1.9713e-01,  1.3317e-01,  1.5124e-01,  1.5688e-01,\n",
            "          9.3646e-02,  1.0085e-01,  8.2831e-02,  1.1377e-01,  9.4956e-02,\n",
            "          9.4956e-02,  1.5680e-01,  8.2831e-02,  8.0150e-02,  8.2831e-02,\n",
            "          1.2784e-01,  1.3058e-01,  6.9238e-02,  7.1139e-02,  1.3441e-01,\n",
            "          1.3441e-01,  1.3441e-01,  1.3441e-01,  1.3441e-01,  1.3841e-01,\n",
            "          1.3232e-01,  2.8566e-02,  2.8566e-02,  1.6437e-03,  1.2935e-01,\n",
            "          1.3549e-01,  1.2346e-01,  1.0893e-01,  1.9614e-01,  1.2717e-01,\n",
            "          1.0691e-02,  5.0940e-02,  2.8566e-02,  5.5577e-02,  1.0483e-01,\n",
            "          1.2166e-01,  7.1139e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,\n",
            "          9.0009e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,  9.0009e-02,\n",
            "          2.5631e-01,  1.5688e-01,  8.3483e-02,  1.0380e-01,  7.1987e-02,\n",
            "          7.1987e-02,  6.9764e-02,  9.6944e-02,  9.2250e-02,  8.2750e-02,\n",
            "          8.2750e-02,  5.9216e-02,  4.4830e-02,  5.1390e-02,  1.5613e-01,\n",
            "          1.5680e-01,  7.0968e-02,  9.2505e-02,  3.0317e-02,  6.7699e-02,\n",
            "          1.0829e-01,  5.2511e-02,  7.9875e-02,  1.2698e-01,  2.0902e-01,\n",
            "          2.0902e-01,  2.0902e-01,  2.0902e-01,  2.0902e-01,  2.0902e-01,\n",
            "          2.0902e-01,  2.0902e-01,  2.0902e-01,  1.3317e-01,  1.0999e-01,\n",
            "         -4.8887e+00,  1.0306e-01,  1.0306e-01,  1.0109e-01,  1.0109e-01,\n",
            "          1.0246e-01,  1.2441e-01,  1.2441e-01,  2.6688e-01,  2.6688e-01,\n",
            "          2.6688e-01]])\n",
            "tensor([[ 0.0115,  0.0152,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,  0.0152,\n",
            "          0.0152,  0.0152,  0.0152,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0115,  0.0152,  0.0152,  0.0152,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0115, -0.0214, -0.0214,  0.0115,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0115,  0.0115,  0.0152,  0.0115,  0.0115,\n",
            "          0.0152,  0.0152,  0.0115,  0.0115,  0.0152,  0.0115,  0.0115,  0.0115,\n",
            "          0.0152,  0.0152,  0.0115,  0.0152,  0.0115,  0.0115,  0.0115,  0.0152,\n",
            "          0.0152,  0.0152,  0.0115,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,\n",
            "          0.0152,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115, -0.0006,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,  0.0152,\n",
            "          0.0152,  0.0115,  0.0152,  0.0152,  0.0115,  0.0115,  0.0152,  0.0115,\n",
            "          0.0152,  0.0115,  0.0152,  0.0115,  0.0152,  0.0115,  0.0152,  0.0152,\n",
            "          0.0152,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0152,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,  0.0115,  0.0115, -0.0214,  0.0152,  0.0115,  0.0152,\n",
            "          0.0152,  0.0115,  0.0115,  0.0115,  0.0152,  0.0115,  0.0152,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,  0.0115,\n",
            "          0.0152,  0.0152,  0.0115,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,\n",
            "          0.0152,  0.0152,  0.0058,  0.0120,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan],\n",
            "        [ 0.1074,  0.1107,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,  0.1107,\n",
            "          0.1107,  0.1107,  0.1107,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1074,  0.1107,  0.1107,  0.1107,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1074,  0.0964,  0.0964,  0.1074,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1107,  0.1074,  0.1074,\n",
            "          0.1107,  0.1107,  0.1074,  0.1074,  0.1107,  0.1074,  0.1074,  0.1074,\n",
            "          0.1107,  0.1107,  0.1074,  0.1107,  0.1074,  0.1074,  0.1074,  0.1107,\n",
            "          0.1107,  0.1107,  0.1074,  0.1107,  0.1107,  0.1107,  0.1107,  0.1107,\n",
            "          0.1107,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1012,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,  0.1107,\n",
            "          0.1107,  0.1074,  0.1107,  0.1107,  0.1074,  0.1074,  0.1107,  0.1074,\n",
            "          0.1107,  0.1074,  0.1107,  0.1074,  0.1107,  0.1074,  0.1107,  0.1107,\n",
            "          0.1107,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1107,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,  0.1074,  0.1074,  0.0964,  0.1107,  0.1074,  0.1107,\n",
            "          0.1107,  0.1074,  0.1074,  0.1074,  0.1107,  0.1074,  0.1107,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,  0.1074,\n",
            "          0.1107,  0.1107,  0.1074,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,\n",
            "          0.1107,  0.1107,  0.1000,  0.1031,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan]])\n",
            "tensor([[ 0.0115,  0.0152,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,  0.0152,\n",
            "          0.0152,  0.0152,  0.0152,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0115,  0.0152,  0.0152,  0.0152,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0115, -0.0214, -0.0214,  0.0115,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0115,  0.0115,  0.0152,  0.0115,  0.0115,\n",
            "          0.0152,  0.0152,  0.0115,  0.0115,  0.0152,  0.0115,  0.0115,  0.0115,\n",
            "          0.0152,  0.0152,  0.0115,  0.0152,  0.0115,  0.0115,  0.0115,  0.0152,\n",
            "          0.0152,  0.0152,  0.0115,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,\n",
            "          0.0152,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115, -0.0006,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,  0.0152,\n",
            "          0.0152,  0.0115,  0.0152,  0.0152,  0.0115,  0.0115,  0.0152,  0.0115,\n",
            "          0.0152,  0.0115,  0.0152,  0.0115,  0.0152,  0.0115,  0.0152,  0.0152,\n",
            "          0.0152,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,\n",
            "          0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,  0.0152,\n",
            "          0.0152,  0.0152,  0.0115,  0.0115, -0.0214,  0.0152,  0.0115,  0.0152,\n",
            "          0.0152,  0.0115,  0.0115,  0.0115,  0.0152,  0.0115,  0.0152,  0.0115,\n",
            "          0.0115,  0.0115,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,  0.0115,\n",
            "          0.0152,  0.0152,  0.0115,  0.0115,  0.0152,  0.0152,  0.0115,  0.0152,\n",
            "          0.0152,  0.0152,  0.0058,  0.0120,  0.0120,  0.0120,  0.0120,  0.0120,\n",
            "          0.0120,  0.0120,  0.0120,  0.0120,  0.0120,  0.0120,  0.0120,  0.0120,\n",
            "          0.0120,  0.0120,  0.0120,  0.0120,  0.0120,  0.0120,  0.0120,  0.0120,\n",
            "          0.0120,  0.0120,  0.0120],\n",
            "        [ 0.1074,  0.1107,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,  0.1107,\n",
            "          0.1107,  0.1107,  0.1107,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1074,  0.1107,  0.1107,  0.1107,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1074,  0.0964,  0.0964,  0.1074,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1107,  0.1074,  0.1074,\n",
            "          0.1107,  0.1107,  0.1074,  0.1074,  0.1107,  0.1074,  0.1074,  0.1074,\n",
            "          0.1107,  0.1107,  0.1074,  0.1107,  0.1074,  0.1074,  0.1074,  0.1107,\n",
            "          0.1107,  0.1107,  0.1074,  0.1107,  0.1107,  0.1107,  0.1107,  0.1107,\n",
            "          0.1107,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1012,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,  0.1107,\n",
            "          0.1107,  0.1074,  0.1107,  0.1107,  0.1074,  0.1074,  0.1107,  0.1074,\n",
            "          0.1107,  0.1074,  0.1107,  0.1074,  0.1107,  0.1074,  0.1107,  0.1107,\n",
            "          0.1107,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1107,  0.1107,  0.1107,  0.1107,  0.1107,\n",
            "          0.1107,  0.1107,  0.1107,  0.1107,  0.1107,  0.1107,  0.1107,  0.1107,\n",
            "          0.1107,  0.1107,  0.1074,  0.1074,  0.0964,  0.1107,  0.1074,  0.1107,\n",
            "          0.1107,  0.1074,  0.1074,  0.1074,  0.1107,  0.1074,  0.1107,  0.1074,\n",
            "          0.1074,  0.1074,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,  0.1074,\n",
            "          0.1107,  0.1107,  0.1074,  0.1074,  0.1107,  0.1107,  0.1074,  0.1107,\n",
            "          0.1107,  0.1107,  0.1000,  0.1031,  0.1031,  0.1031,  0.1031,  0.1031,\n",
            "          0.1031,  0.1031,  0.1031,  0.1031,  0.1031,  0.1031,  0.1031,  0.1031,\n",
            "          0.1031,  0.1031,  0.1031,  0.1031,  0.1031,  0.1031,  0.1031,  0.1031,\n",
            "          0.1031,  0.1031,  0.1031]])\n",
            "tensor([[ 0.1761,     nan,  0.0498, -0.3636,  0.1974, -0.3636, -0.3636, -0.3636,\n",
            "         -0.3636, -0.3636,  0.2357,  0.2357,  0.2357,  0.2357,  0.2357, -0.3636,\n",
            "         -0.3636,  0.2382,  0.0498,  0.0498, -0.3636, -0.3636, -0.3636,  0.2121,\n",
            "          0.2121,  0.2121,  0.2357,  0.2357,  0.2357, -0.3636, -0.3636,  0.1761,\n",
            "          0.1761,  0.1761, -0.3636, -0.3636, -0.3636, -0.3636, -0.3636,  0.0498,\n",
            "          0.0498,  0.0498,  0.0498,     nan,  0.2382,  0.1944,  0.1944,  0.2382,\n",
            "          0.2382,  0.2382,  0.0498,  0.0498,  0.0498,  0.2382],\n",
            "        [-0.1003,     nan, -0.0914,  0.0326, -0.1049,  0.0326,  0.0326,  0.0326,\n",
            "          0.0326,  0.0326, -0.1102, -0.1102, -0.1102, -0.1102, -0.1102,  0.0326,\n",
            "          0.0326,  0.0995, -0.0914, -0.0914,  0.0326,  0.0326,  0.0326, -0.0895,\n",
            "         -0.0895, -0.0895, -0.1102, -0.1102, -0.1102,  0.0326,  0.0326, -0.1003,\n",
            "         -0.1003, -0.1003,  0.0326,  0.0326,  0.0326,  0.0326,  0.0326, -0.0914,\n",
            "         -0.0914, -0.0914, -0.0914,     nan,  0.0995, -0.0758, -0.0758,  0.0995,\n",
            "          0.0995,  0.0995, -0.0914, -0.0914, -0.0914,  0.0995]])\n",
            "tensor([[ 0.1761,  0.1761,  0.0498, -0.3636,  0.1974, -0.3636, -0.3636, -0.3636,\n",
            "         -0.3636, -0.3636,  0.2357,  0.2357,  0.2357,  0.2357,  0.2357, -0.3636,\n",
            "         -0.3636,  0.2382,  0.0498,  0.0498, -0.3636, -0.3636, -0.3636,  0.2121,\n",
            "          0.2121,  0.2121,  0.2357,  0.2357,  0.2357, -0.3636, -0.3636,  0.1761,\n",
            "          0.1761,  0.1761, -0.3636, -0.3636, -0.3636, -0.3636, -0.3636,  0.0498,\n",
            "          0.0498,  0.0498,  0.0498,  0.0498,  0.2382,  0.1944,  0.1944,  0.2382,\n",
            "          0.2382,  0.2382,  0.0498,  0.0498,  0.0498,  0.2382],\n",
            "        [-0.1003, -0.1003, -0.0914,  0.0326, -0.1049,  0.0326,  0.0326,  0.0326,\n",
            "          0.0326,  0.0326, -0.1102, -0.1102, -0.1102, -0.1102, -0.1102,  0.0326,\n",
            "          0.0326,  0.0995, -0.0914, -0.0914,  0.0326,  0.0326,  0.0326, -0.0895,\n",
            "         -0.0895, -0.0895, -0.1102, -0.1102, -0.1102,  0.0326,  0.0326, -0.1003,\n",
            "         -0.1003, -0.1003,  0.0326,  0.0326,  0.0326,  0.0326,  0.0326, -0.0914,\n",
            "         -0.0914, -0.0914, -0.0914, -0.0914,  0.0995, -0.0758, -0.0758,  0.0995,\n",
            "          0.0995,  0.0995, -0.0914, -0.0914, -0.0914,  0.0995]])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "tensor([], size=(2, 0))\n",
            "tensor([[ 6.5711e-02,  5.6072e-02,  7.0223e-02,  7.0223e-02,  6.5412e-02,\n",
            "          7.1644e-02,  9.1300e-02,  9.1300e-02,  9.1300e-02,  9.1300e-02,\n",
            "          5.0935e+00,  5.0935e+00,  9.1300e-02,  9.1300e-02,  6.5412e-02,\n",
            "          9.1300e-02,  5.0935e+00,  6.5412e-02,  5.0935e+00,  6.5412e-02,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          6.7300e-02,  5.0935e+00,  7.7711e-02,  5.6072e-02, -7.2687e+00,\n",
            "         -7.2687e+00, -2.3002e-02,  6.5412e-02,  4.8032e-03, -7.2687e+00,\n",
            "         -7.2687e+00, -7.2687e+00, -7.2687e+00, -7.2687e+00, -7.2687e+00,\n",
            "          5.6072e-02,  9.8260e-02,  5.6072e-02,  6.7300e-02,  5.6072e-02,\n",
            "          5.0935e+00,  7.1644e-02,  6.9797e-02,  5.0935e+00,  5.0935e+00,\n",
            "          7.1644e-02,  7.1644e-02,  7.1644e-02,  7.1644e-02,  7.1644e-02,\n",
            "          5.4752e-02,  5.0935e+00,  7.1644e-02,  5.4752e-02,  9.1300e-02,\n",
            "          1.2915e-01,  1.2915e-01,  1.2915e-01,  1.2915e-01,  1.2915e-01,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  6.7300e-02,  5.0935e+00,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  6.5412e-02,  6.5412e-02,\n",
            "          5.6072e-02,  5.6072e-02,  5.6072e-02,  7.1644e-02,  7.1644e-02,\n",
            "          6.7300e-02,  6.7300e-02,  5.6072e-02,  7.1644e-02,  9.1300e-02,\n",
            "          6.7300e-02,  9.1300e-02,  9.8260e-02,  6.7300e-02,  6.7300e-02,\n",
            "                 nan,         nan,         nan,  6.7300e-02,  6.5412e-02,\n",
            "          5.0935e+00,  7.1644e-02,  7.1644e-02,  9.1300e-02,         nan,\n",
            "          6.7300e-02,  5.0935e+00,  9.8813e-02,  7.1644e-02,  5.6072e-02,\n",
            "          5.6072e-02,  6.5412e-02,  5.0935e+00,  5.6072e-02,  5.0935e+00,\n",
            "          6.7300e-02,  5.0935e+00,  6.7300e-02,  6.7300e-02,  6.7300e-02,\n",
            "          5.0682e-02,  5.0935e+00,  5.0935e+00,  5.0935e+00, -2.8214e+00,\n",
            "          5.0935e+00,  7.1644e-02,  7.1644e-02,  6.5412e-02,  5.0935e+00,\n",
            "          5.4752e-02, -2.3002e-02, -5.0433e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02,  2.2953e-02,  5.0935e+00,\n",
            "          7.7711e-02,  5.0682e-02,  6.7300e-02,  6.7300e-02,  9.1300e-02,\n",
            "          9.8260e-02,  6.7300e-02,  5.0935e+00,  6.7300e-02,  6.7300e-02,\n",
            "          5.0935e+00,  5.0935e+00,  5.6072e-02,  6.7300e-02,  5.6072e-02,\n",
            "          7.1644e-02,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          5.6072e-02,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          5.0935e+00,  6.7300e-02,  5.0935e+00,  6.7300e-02,  5.0935e+00,\n",
            "          5.0935e+00,  7.1644e-02,  7.1644e-02,  7.1644e-02,  7.1644e-02,\n",
            "          7.1644e-02,  6.5412e-02,  1.5601e-01,  1.1169e-01,  7.1644e-02,\n",
            "          7.1644e-02,  7.7711e-02,  7.7711e-02,  7.8650e-02,  6.7300e-02,\n",
            "          7.1644e-02,  6.7300e-02,  6.7300e-02,  5.0935e+00,  6.5412e-02,\n",
            "          5.0935e+00,  5.0935e+00,  5.6072e-02,  5.6072e-02,  5.6072e-02,\n",
            "          5.6072e-02,  5.6072e-02,  5.0935e+00,  5.6072e-02,  6.5412e-02,\n",
            "          7.1644e-02,  8.3559e-02,  7.8650e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -3.3869e-02,  4.3324e-02,  1.6733e-01,\n",
            "                 nan,  1.5176e-01,  6.7300e-02,  7.1644e-02,  6.7300e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -6.6479e-03,  4.6888e-02,\n",
            "          4.6888e-02,  4.3324e-02,  7.1644e-02,  5.0935e+00,  6.7300e-02,\n",
            "          6.7300e-02,  9.8260e-02,  6.7300e-02,  6.5412e-02,  6.7300e-02,\n",
            "          7.1644e-02,  6.5412e-02,  6.5412e-02,  7.1644e-02,  7.7711e-02,\n",
            "          7.1644e-02,  4.8032e-03, -2.3002e-02, -2.3015e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02,  6.7300e-02,  5.6072e-02,  6.7300e-02,  5.0935e+00,\n",
            "          7.1644e-02,  9.8260e-02,  5.6072e-02,  6.7300e-02,  6.7300e-02,\n",
            "          6.7300e-02,  5.6072e-02,  5.6072e-02,  6.7300e-02,  5.0682e-02,\n",
            "          5.6072e-02,  5.0935e+00,  5.6072e-02,  5.6072e-02,  5.6072e-02,\n",
            "          5.6072e-02,  6.7300e-02,  5.0935e+00,  5.0935e+00,  6.5711e-02,\n",
            "                 nan, -6.0555e-02, -5.2130e-01],\n",
            "        [ 7.1830e-02,  8.4755e-02,  9.3134e-02,  9.3134e-02,  7.5492e-02,\n",
            "          7.4405e-02,  6.7699e-02,  6.7699e-02,  6.7699e-02,  6.7699e-02,\n",
            "         -6.9186e+00, -6.9186e+00,  6.7699e-02,  6.7699e-02,  7.5492e-02,\n",
            "          6.7699e-02, -6.9186e+00,  7.5492e-02, -6.9186e+00,  7.5492e-02,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "          8.8541e-02, -6.9186e+00,  8.1429e-02,  8.4755e-02, -8.9845e+00,\n",
            "         -8.9845e+00,  5.1661e-02,  7.5492e-02,  6.0364e-02, -8.9845e+00,\n",
            "         -8.9845e+00, -8.9845e+00, -8.9845e+00, -8.9845e+00, -8.9845e+00,\n",
            "          8.4755e-02,  5.4953e-02,  8.4755e-02,  8.8541e-02,  8.4755e-02,\n",
            "         -6.9186e+00,  7.4405e-02,  6.8156e-02, -6.9186e+00, -6.9186e+00,\n",
            "          7.4405e-02,  7.4405e-02,  7.4405e-02,  7.4405e-02,  7.4405e-02,\n",
            "          7.2690e-02, -6.9186e+00,  7.4405e-02,  7.2690e-02,  6.7699e-02,\n",
            "          3.1133e-02,  3.1133e-02,  3.1133e-02,  3.1133e-02,  3.1133e-02,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00,  8.8541e-02, -6.9186e+00,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00,  7.5492e-02,  7.5492e-02,\n",
            "          8.4755e-02,  8.4755e-02,  8.4755e-02,  7.4405e-02,  7.4405e-02,\n",
            "          8.8541e-02,  8.8541e-02,  8.4755e-02,  7.4405e-02,  6.7699e-02,\n",
            "          8.8541e-02,  6.7699e-02,  5.4953e-02,  8.8541e-02,  8.8541e-02,\n",
            "                 nan,         nan,         nan,  8.8541e-02,  7.5492e-02,\n",
            "         -6.9186e+00,  7.4405e-02,  7.4405e-02,  6.7699e-02,         nan,\n",
            "          8.8541e-02, -6.9186e+00,  7.5517e-02,  7.4405e-02,  8.4755e-02,\n",
            "          8.4755e-02,  7.5492e-02, -6.9186e+00,  8.4755e-02, -6.9186e+00,\n",
            "          8.8541e-02, -6.9186e+00,  8.8541e-02,  8.8541e-02,  8.8541e-02,\n",
            "          1.0220e-01, -6.9186e+00, -6.9186e+00, -6.9186e+00, -4.8887e+00,\n",
            "         -6.9186e+00,  7.4405e-02,  7.4405e-02,  7.5492e-02, -6.9186e+00,\n",
            "          7.2690e-02,  5.1661e-02,  4.5082e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  7.3237e-02, -6.9186e+00,\n",
            "          8.1429e-02,  1.0220e-01,  8.8541e-02,  8.8541e-02,  6.7699e-02,\n",
            "          5.4953e-02,  8.8541e-02, -6.9186e+00,  8.8541e-02,  8.8541e-02,\n",
            "         -6.9186e+00, -6.9186e+00,  8.4755e-02,  8.8541e-02,  8.4755e-02,\n",
            "          7.4405e-02, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "          8.4755e-02, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "         -6.9186e+00,  8.8541e-02, -6.9186e+00,  8.8541e-02, -6.9186e+00,\n",
            "         -6.9186e+00,  7.4405e-02,  7.4405e-02,  7.4405e-02,  7.4405e-02,\n",
            "          7.4405e-02,  7.5492e-02,  1.3517e-01,  1.2159e-01,  7.4405e-02,\n",
            "          7.4405e-02,  8.1429e-02,  8.1429e-02,  7.3689e-02,  8.8541e-02,\n",
            "          7.4405e-02,  8.8541e-02,  8.8541e-02, -6.9186e+00,  7.5492e-02,\n",
            "         -6.9186e+00, -6.9186e+00,  8.4755e-02,  8.4755e-02,  8.4755e-02,\n",
            "          8.4755e-02,  8.4755e-02, -6.9186e+00,  8.4755e-02,  7.5492e-02,\n",
            "          7.4405e-02,  7.2047e-02,  7.3689e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.3412e-02,  8.3322e-02,  2.0902e-01,\n",
            "                 nan,  2.6914e-01,  8.8541e-02,  7.4405e-02,  8.8541e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.4510e-02,  8.5941e-02,\n",
            "          8.5941e-02,  8.3322e-02,  7.4405e-02, -6.9186e+00,  8.8541e-02,\n",
            "          8.8541e-02,  5.4953e-02,  8.8541e-02,  7.5492e-02,  8.8541e-02,\n",
            "          7.4405e-02,  7.5492e-02,  7.5492e-02,  7.4405e-02,  8.1429e-02,\n",
            "          7.4405e-02,  6.0364e-02,  5.1661e-02,  5.4667e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  8.8541e-02,  8.4755e-02,  8.8541e-02, -6.9186e+00,\n",
            "          7.4405e-02,  5.4953e-02,  8.4755e-02,  8.8541e-02,  8.8541e-02,\n",
            "          8.8541e-02,  8.4755e-02,  8.4755e-02,  8.8541e-02,  1.0220e-01,\n",
            "          8.4755e-02, -6.9186e+00,  8.4755e-02,  8.4755e-02,  8.4755e-02,\n",
            "          8.4755e-02,  8.8541e-02, -6.9186e+00, -6.9186e+00,  7.1830e-02,\n",
            "                 nan,  3.8150e-01, -5.5831e-02]])\n",
            "tensor([[ 6.5711e-02,  5.6072e-02,  7.0223e-02,  7.0223e-02,  6.5412e-02,\n",
            "          7.1644e-02,  9.1300e-02,  9.1300e-02,  9.1300e-02,  9.1300e-02,\n",
            "          5.0935e+00,  5.0935e+00,  9.1300e-02,  9.1300e-02,  6.5412e-02,\n",
            "          9.1300e-02,  5.0935e+00,  6.5412e-02,  5.0935e+00,  6.5412e-02,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          6.7300e-02,  5.0935e+00,  7.7711e-02,  5.6072e-02, -7.2687e+00,\n",
            "         -7.2687e+00, -2.3002e-02,  6.5412e-02,  4.8032e-03, -7.2687e+00,\n",
            "         -7.2687e+00, -7.2687e+00, -7.2687e+00, -7.2687e+00, -7.2687e+00,\n",
            "          5.6072e-02,  9.8260e-02,  5.6072e-02,  6.7300e-02,  5.6072e-02,\n",
            "          5.0935e+00,  7.1644e-02,  6.9797e-02,  5.0935e+00,  5.0935e+00,\n",
            "          7.1644e-02,  7.1644e-02,  7.1644e-02,  7.1644e-02,  7.1644e-02,\n",
            "          5.4752e-02,  5.0935e+00,  7.1644e-02,  5.4752e-02,  9.1300e-02,\n",
            "          1.2915e-01,  1.2915e-01,  1.2915e-01,  1.2915e-01,  1.2915e-01,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  6.7300e-02,  5.0935e+00,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  6.5412e-02,  6.5412e-02,\n",
            "          5.6072e-02,  5.6072e-02,  5.6072e-02,  7.1644e-02,  7.1644e-02,\n",
            "          6.7300e-02,  6.7300e-02,  5.6072e-02,  7.1644e-02,  9.1300e-02,\n",
            "          6.7300e-02,  9.1300e-02,  9.8260e-02,  6.7300e-02,  6.7300e-02,\n",
            "          6.7300e-02,  6.7300e-02,  6.7300e-02,  6.7300e-02,  6.5412e-02,\n",
            "          5.0935e+00,  7.1644e-02,  7.1644e-02,  9.1300e-02,  9.1300e-02,\n",
            "          6.7300e-02,  5.0935e+00,  9.8813e-02,  7.1644e-02,  5.6072e-02,\n",
            "          5.6072e-02,  6.5412e-02,  5.0935e+00,  5.6072e-02,  5.0935e+00,\n",
            "          6.7300e-02,  5.0935e+00,  6.7300e-02,  6.7300e-02,  6.7300e-02,\n",
            "          5.0682e-02,  5.0935e+00,  5.0935e+00,  5.0935e+00, -2.8214e+00,\n",
            "          5.0935e+00,  7.1644e-02,  7.1644e-02,  6.5412e-02,  5.0935e+00,\n",
            "          5.4752e-02, -2.3002e-02, -5.0433e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02,  2.2953e-02,  5.0935e+00,\n",
            "          7.7711e-02,  5.0682e-02,  6.7300e-02,  6.7300e-02,  9.1300e-02,\n",
            "          9.8260e-02,  6.7300e-02,  5.0935e+00,  6.7300e-02,  6.7300e-02,\n",
            "          5.0935e+00,  5.0935e+00,  5.6072e-02,  6.7300e-02,  5.6072e-02,\n",
            "          7.1644e-02,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          5.6072e-02,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,  5.0935e+00,\n",
            "          5.0935e+00,  6.7300e-02,  5.0935e+00,  6.7300e-02,  5.0935e+00,\n",
            "          5.0935e+00,  7.1644e-02,  7.1644e-02,  7.1644e-02,  7.1644e-02,\n",
            "          7.1644e-02,  6.5412e-02,  1.5601e-01,  1.1169e-01,  7.1644e-02,\n",
            "          7.1644e-02,  7.7711e-02,  7.7711e-02,  7.8650e-02,  6.7300e-02,\n",
            "          7.1644e-02,  6.7300e-02,  6.7300e-02,  5.0935e+00,  6.5412e-02,\n",
            "          5.0935e+00,  5.0935e+00,  5.6072e-02,  5.6072e-02,  5.6072e-02,\n",
            "          5.6072e-02,  5.6072e-02,  5.0935e+00,  5.6072e-02,  6.5412e-02,\n",
            "          7.1644e-02,  8.3559e-02,  7.8650e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -3.3869e-02,  4.3324e-02,  1.6733e-01,\n",
            "          1.6733e-01,  1.5176e-01,  6.7300e-02,  7.1644e-02,  6.7300e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -6.6479e-03,  4.6888e-02,\n",
            "          4.6888e-02,  4.3324e-02,  7.1644e-02,  5.0935e+00,  6.7300e-02,\n",
            "          6.7300e-02,  9.8260e-02,  6.7300e-02,  6.5412e-02,  6.7300e-02,\n",
            "          7.1644e-02,  6.5412e-02,  6.5412e-02,  7.1644e-02,  7.7711e-02,\n",
            "          7.1644e-02,  4.8032e-03, -2.3002e-02, -2.3015e-02, -2.3002e-02,\n",
            "         -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02, -2.3002e-02,\n",
            "         -2.3002e-02,  6.7300e-02,  5.6072e-02,  6.7300e-02,  5.0935e+00,\n",
            "          7.1644e-02,  9.8260e-02,  5.6072e-02,  6.7300e-02,  6.7300e-02,\n",
            "          6.7300e-02,  5.6072e-02,  5.6072e-02,  6.7300e-02,  5.0682e-02,\n",
            "          5.6072e-02,  5.0935e+00,  5.6072e-02,  5.6072e-02,  5.6072e-02,\n",
            "          5.6072e-02,  6.7300e-02,  5.0935e+00,  5.0935e+00,  6.5711e-02,\n",
            "          6.5711e-02, -6.0555e-02, -5.2130e-01],\n",
            "        [ 7.1830e-02,  8.4755e-02,  9.3134e-02,  9.3134e-02,  7.5492e-02,\n",
            "          7.4405e-02,  6.7699e-02,  6.7699e-02,  6.7699e-02,  6.7699e-02,\n",
            "         -6.9186e+00, -6.9186e+00,  6.7699e-02,  6.7699e-02,  7.5492e-02,\n",
            "          6.7699e-02, -6.9186e+00,  7.5492e-02, -6.9186e+00,  7.5492e-02,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "          8.8541e-02, -6.9186e+00,  8.1429e-02,  8.4755e-02, -8.9845e+00,\n",
            "         -8.9845e+00,  5.1661e-02,  7.5492e-02,  6.0364e-02, -8.9845e+00,\n",
            "         -8.9845e+00, -8.9845e+00, -8.9845e+00, -8.9845e+00, -8.9845e+00,\n",
            "          8.4755e-02,  5.4953e-02,  8.4755e-02,  8.8541e-02,  8.4755e-02,\n",
            "         -6.9186e+00,  7.4405e-02,  6.8156e-02, -6.9186e+00, -6.9186e+00,\n",
            "          7.4405e-02,  7.4405e-02,  7.4405e-02,  7.4405e-02,  7.4405e-02,\n",
            "          7.2690e-02, -6.9186e+00,  7.4405e-02,  7.2690e-02,  6.7699e-02,\n",
            "          3.1133e-02,  3.1133e-02,  3.1133e-02,  3.1133e-02,  3.1133e-02,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00,  8.8541e-02, -6.9186e+00,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00,  7.5492e-02,  7.5492e-02,\n",
            "          8.4755e-02,  8.4755e-02,  8.4755e-02,  7.4405e-02,  7.4405e-02,\n",
            "          8.8541e-02,  8.8541e-02,  8.4755e-02,  7.4405e-02,  6.7699e-02,\n",
            "          8.8541e-02,  6.7699e-02,  5.4953e-02,  8.8541e-02,  8.8541e-02,\n",
            "          8.8541e-02,  8.8541e-02,  8.8541e-02,  8.8541e-02,  7.5492e-02,\n",
            "         -6.9186e+00,  7.4405e-02,  7.4405e-02,  6.7699e-02,  6.7699e-02,\n",
            "          8.8541e-02, -6.9186e+00,  7.5517e-02,  7.4405e-02,  8.4755e-02,\n",
            "          8.4755e-02,  7.5492e-02, -6.9186e+00,  8.4755e-02, -6.9186e+00,\n",
            "          8.8541e-02, -6.9186e+00,  8.8541e-02,  8.8541e-02,  8.8541e-02,\n",
            "          1.0220e-01, -6.9186e+00, -6.9186e+00, -6.9186e+00, -4.8887e+00,\n",
            "         -6.9186e+00,  7.4405e-02,  7.4405e-02,  7.5492e-02, -6.9186e+00,\n",
            "          7.2690e-02,  5.1661e-02,  4.5082e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  7.3237e-02, -6.9186e+00,\n",
            "          8.1429e-02,  1.0220e-01,  8.8541e-02,  8.8541e-02,  6.7699e-02,\n",
            "          5.4953e-02,  8.8541e-02, -6.9186e+00,  8.8541e-02,  8.8541e-02,\n",
            "         -6.9186e+00, -6.9186e+00,  8.4755e-02,  8.8541e-02,  8.4755e-02,\n",
            "          7.4405e-02, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "          8.4755e-02, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "         -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00, -6.9186e+00,\n",
            "         -6.9186e+00,  8.8541e-02, -6.9186e+00,  8.8541e-02, -6.9186e+00,\n",
            "         -6.9186e+00,  7.4405e-02,  7.4405e-02,  7.4405e-02,  7.4405e-02,\n",
            "          7.4405e-02,  7.5492e-02,  1.3517e-01,  1.2159e-01,  7.4405e-02,\n",
            "          7.4405e-02,  8.1429e-02,  8.1429e-02,  7.3689e-02,  8.8541e-02,\n",
            "          7.4405e-02,  8.8541e-02,  8.8541e-02, -6.9186e+00,  7.5492e-02,\n",
            "         -6.9186e+00, -6.9186e+00,  8.4755e-02,  8.4755e-02,  8.4755e-02,\n",
            "          8.4755e-02,  8.4755e-02, -6.9186e+00,  8.4755e-02,  7.5492e-02,\n",
            "          7.4405e-02,  7.2047e-02,  7.3689e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.3412e-02,  8.3322e-02,  2.0902e-01,\n",
            "          2.0902e-01,  2.6914e-01,  8.8541e-02,  7.4405e-02,  8.8541e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.4510e-02,  8.5941e-02,\n",
            "          8.5941e-02,  8.3322e-02,  7.4405e-02, -6.9186e+00,  8.8541e-02,\n",
            "          8.8541e-02,  5.4953e-02,  8.8541e-02,  7.5492e-02,  8.8541e-02,\n",
            "          7.4405e-02,  7.5492e-02,  7.5492e-02,  7.4405e-02,  8.1429e-02,\n",
            "          7.4405e-02,  6.0364e-02,  5.1661e-02,  5.4667e-02,  5.1661e-02,\n",
            "          5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,  5.1661e-02,\n",
            "          5.1661e-02,  8.8541e-02,  8.4755e-02,  8.8541e-02, -6.9186e+00,\n",
            "          7.4405e-02,  5.4953e-02,  8.4755e-02,  8.8541e-02,  8.8541e-02,\n",
            "          8.8541e-02,  8.4755e-02,  8.4755e-02,  8.8541e-02,  1.0220e-01,\n",
            "          8.4755e-02, -6.9186e+00,  8.4755e-02,  8.4755e-02,  8.4755e-02,\n",
            "          8.4755e-02,  8.8541e-02, -6.9186e+00, -6.9186e+00,  7.1830e-02,\n",
            "          7.1830e-02,  3.8150e-01, -5.5831e-02]])\n",
            "tensor([[-0.1300, -0.1300, -0.1300, -0.0958, -0.1300, -0.0958, -0.1032, -0.0730,\n",
            "          0.1859, -0.1121,  0.1859,  0.0206, -0.0763, -0.1032, -0.1032, -0.0622,\n",
            "         -0.0958, -0.1032, -0.0833, -0.1032, -0.1300, -0.1300, -0.0622, -0.0958,\n",
            "         -0.1300, -0.0958, -0.0958, -0.0958, -0.1300, -0.1300, -0.1300, -0.0718,\n",
            "         -0.1208, -0.0564,     nan, -0.0732, -0.0833, -0.0763, -0.0958, -0.1032,\n",
            "         -0.1032, -0.1032, -0.1032, -0.1032, -0.1300, -0.1300, -0.1300, -0.1300,\n",
            "         -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.0958,\n",
            "         -0.0958, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0732,\n",
            "         -0.0732, -0.0732, -0.0833, -0.0833, -0.0833, -0.0732, -0.0833, -0.0732,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0958, -0.1300, -0.1032, -0.0958, -0.0958,\n",
            "         -0.1300, -0.0622, -0.1032, -0.1300, -0.0833, -0.0833, -0.0833, -0.0732,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0958, -0.0958, -0.0958,\n",
            "         -0.1450, -0.1300, -0.0958, -0.0958, -0.0958, -0.0958, -0.0730, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.1032, -0.1614, -0.1117, -0.0958, -0.1032,\n",
            "         -0.0958, -0.0718, -0.0958, -0.1032, -0.1032, -0.1032, -0.1032, -0.1032,\n",
            "         -0.1032, -0.0730, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0732, -0.0833, -0.0958, -0.0958, -0.0622, -0.0958, -0.1032,\n",
            "         -0.0622, -0.1300, -0.1032, -0.1450, -0.0763, -0.0730, -0.0730, -0.0763,\n",
            "         -0.0833, -0.1032, -0.0833, -0.0958, -0.1300, -0.1300, -0.0958, -0.0958,\n",
            "         -0.1300, -0.1032, -0.0958, -0.1300, -0.0958, -0.0958, -0.0958, -0.1300,\n",
            "         -0.0958, -0.0958, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1126,\n",
            "         -0.1126, -0.1126,  0.0832, -0.1300, -0.1032, -0.0958, -0.1300, -0.1032,\n",
            "         -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1032, -0.1032, -0.0718,\n",
            "         -0.1300, -0.0958, -0.1257, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0730, -0.0730, -0.0958, -0.0958, -0.1032,\n",
            "         -0.0504, -0.0504, -0.0504, -0.0614, -0.1300, -0.1300, -0.1300, -0.1300,\n",
            "         -0.1032, -0.1614, -0.2512, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833,  0.0041, -0.0321, -0.0763, -0.0730, -0.1614, -0.1032, -0.1032,\n",
            "         -0.0958, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300,\n",
            "         -0.1300, -0.1300, -0.0958, -0.1300, -0.1257, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0730, -0.0958, -0.0958, -0.1300, -0.0622, -0.1032, -0.0973,\n",
            "         -0.0763, -0.0833, -0.0958, -0.1032, -0.0958, -0.1614, -0.0718, -0.0718,\n",
            "         -0.0958, -0.0718, -0.0730, -0.1117, -0.0958, -0.0763, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0732, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0763, -0.1032, -0.0958, -0.0958,\n",
            "         -0.1032, -0.1032, -0.1300, -0.0958, -0.1032, -0.1300, -0.0718, -0.1300,\n",
            "         -0.1032, -0.1257, -0.0833, -0.0732, -0.0732, -0.0833, -0.0833, -0.0763,\n",
            "         -0.0833, -0.0718, -0.0833, -0.0833, -0.0763, -0.0958, -0.0958, -0.1032,\n",
            "         -0.0958, -0.0958, -0.1300, -0.1300, -0.0958, -0.0958, -0.1300, -0.0718,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0958, -0.1614, -0.0958, -0.1300,\n",
            "         -0.1300, -0.1300, -0.1300, -0.1032, -0.1300, -0.0958, -0.1300, -0.0958,\n",
            "         -0.0958, -0.1300, -0.0958, -0.0958, -0.2512, -0.1450, -0.2512, -0.0622,\n",
            "         -0.1032, -0.1300, -0.1300, -0.0958, -0.0958, -0.0958, -0.1300, -0.1300,\n",
            "         -0.0958, -0.1032, -0.0958, -0.1032, -0.0958, -0.1032, -0.1032, -0.0622,\n",
            "         -0.1300, -0.1300, -0.0958, -0.1300],\n",
            "        [ 0.0168,  0.0168,  0.0168,  0.0303,  0.0168,  0.0303,  0.0092,  0.0619,\n",
            "          0.1782, -0.0619,  0.1782,  0.0935,  0.0403,  0.0092,  0.0092,  0.0403,\n",
            "          0.0303,  0.0092,  0.0282,  0.0092,  0.0168,  0.0168,  0.0403,  0.0303,\n",
            "          0.0168,  0.0303,  0.0303,  0.0303,  0.0168,  0.0168,  0.0168,  0.0493,\n",
            "          0.0327,  0.0272,     nan,  0.0332,  0.0282,  0.0403,  0.0303,  0.0092,\n",
            "          0.0092,  0.0092,  0.0092,  0.0092,  0.0168,  0.0168,  0.0168,  0.0168,\n",
            "          0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0303,\n",
            "          0.0303,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0332,\n",
            "          0.0332,  0.0332,  0.0282,  0.0282,  0.0282,  0.0332,  0.0282,  0.0332,\n",
            "          0.0282,  0.0282,  0.0282,  0.0303,  0.0168,  0.0092,  0.0303,  0.0303,\n",
            "          0.0168,  0.0403,  0.0092,  0.0168,  0.0282,  0.0282,  0.0282,  0.0332,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0303,  0.0303,  0.0303,\n",
            "          0.0415,  0.0168,  0.0303,  0.0303,  0.0303,  0.0303,  0.0619,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0092, -0.0373,  0.0353,  0.0303,  0.0092,\n",
            "          0.0303,  0.0493,  0.0303,  0.0092,  0.0092,  0.0092,  0.0092,  0.0092,\n",
            "          0.0092,  0.0619,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0332,  0.0282,  0.0303,  0.0303,  0.0403,  0.0303,  0.0092,\n",
            "          0.0403,  0.0168,  0.0092,  0.0415,  0.0403,  0.0619,  0.0619,  0.0403,\n",
            "          0.0282,  0.0092,  0.0282,  0.0303,  0.0168,  0.0168,  0.0303,  0.0303,\n",
            "          0.0168,  0.0092,  0.0303,  0.0168,  0.0303,  0.0303,  0.0303,  0.0168,\n",
            "          0.0303,  0.0303,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0450,\n",
            "          0.0450,  0.0450, -0.0963,  0.0168,  0.0092,  0.0303,  0.0168,  0.0092,\n",
            "          0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0092,  0.0092,  0.0493,\n",
            "          0.0168,  0.0303,  0.0480,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0619,  0.0619,  0.0303,  0.0303,  0.0092,\n",
            "          0.0239,  0.0239,  0.0239,  0.0464,  0.0168,  0.0168,  0.0168,  0.0168,\n",
            "          0.0092, -0.0373,  0.1694,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0937,  0.0782,  0.0403,  0.0619, -0.0373,  0.0092,  0.0092,\n",
            "          0.0303,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,\n",
            "          0.0168,  0.0168,  0.0303,  0.0168,  0.0480,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0619,  0.0303,  0.0303,  0.0168,  0.0403,  0.0092,  0.0583,\n",
            "          0.0403,  0.0282,  0.0303,  0.0092,  0.0303, -0.0373,  0.0493,  0.0493,\n",
            "          0.0303,  0.0493,  0.0619,  0.0353,  0.0303,  0.0403,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0332,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0403,  0.0092,  0.0303,  0.0303,\n",
            "          0.0092,  0.0092,  0.0168,  0.0303,  0.0092,  0.0168,  0.0493,  0.0168,\n",
            "          0.0092,  0.0480,  0.0282,  0.0332,  0.0332,  0.0282,  0.0282,  0.0403,\n",
            "          0.0282,  0.0493,  0.0282,  0.0282,  0.0403,  0.0303,  0.0303,  0.0092,\n",
            "          0.0303,  0.0303,  0.0168,  0.0168,  0.0303,  0.0303,  0.0168,  0.0493,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0303, -0.0373,  0.0303,  0.0168,\n",
            "          0.0168,  0.0168,  0.0168,  0.0092,  0.0168,  0.0303,  0.0168,  0.0303,\n",
            "          0.0303,  0.0168,  0.0303,  0.0303,  0.1694,  0.0415,  0.1694,  0.0403,\n",
            "          0.0092,  0.0168,  0.0168,  0.0303,  0.0303,  0.0303,  0.0168,  0.0168,\n",
            "          0.0303,  0.0092,  0.0303,  0.0092,  0.0303,  0.0092,  0.0092,  0.0403,\n",
            "          0.0168,  0.0168,  0.0303,  0.0168]])\n",
            "tensor([[-0.1300, -0.1300, -0.1300, -0.0958, -0.1300, -0.0958, -0.1032, -0.0730,\n",
            "          0.1859, -0.1121,  0.1859,  0.0206, -0.0763, -0.1032, -0.1032, -0.0622,\n",
            "         -0.0958, -0.1032, -0.0833, -0.1032, -0.1300, -0.1300, -0.0622, -0.0958,\n",
            "         -0.1300, -0.0958, -0.0958, -0.0958, -0.1300, -0.1300, -0.1300, -0.0718,\n",
            "         -0.1208, -0.0564, -0.0564, -0.0732, -0.0833, -0.0763, -0.0958, -0.1032,\n",
            "         -0.1032, -0.1032, -0.1032, -0.1032, -0.1300, -0.1300, -0.1300, -0.1300,\n",
            "         -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.0958,\n",
            "         -0.0958, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0732,\n",
            "         -0.0732, -0.0732, -0.0833, -0.0833, -0.0833, -0.0732, -0.0833, -0.0732,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0958, -0.1300, -0.1032, -0.0958, -0.0958,\n",
            "         -0.1300, -0.0622, -0.1032, -0.1300, -0.0833, -0.0833, -0.0833, -0.0732,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0958, -0.0958, -0.0958,\n",
            "         -0.1450, -0.1300, -0.0958, -0.0958, -0.0958, -0.0958, -0.0730, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.1032, -0.1614, -0.1117, -0.0958, -0.1032,\n",
            "         -0.0958, -0.0718, -0.0958, -0.1032, -0.1032, -0.1032, -0.1032, -0.1032,\n",
            "         -0.1032, -0.0730, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0732, -0.0833, -0.0958, -0.0958, -0.0622, -0.0958, -0.1032,\n",
            "         -0.0622, -0.1300, -0.1032, -0.1450, -0.0763, -0.0730, -0.0730, -0.0763,\n",
            "         -0.0833, -0.1032, -0.0833, -0.0958, -0.1300, -0.1300, -0.0958, -0.0958,\n",
            "         -0.1300, -0.1032, -0.0958, -0.1300, -0.0958, -0.0958, -0.0958, -0.1300,\n",
            "         -0.0958, -0.0958, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1126,\n",
            "         -0.1126, -0.1126,  0.0832, -0.1300, -0.1032, -0.0958, -0.1300, -0.1032,\n",
            "         -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1032, -0.1032, -0.0718,\n",
            "         -0.1300, -0.0958, -0.1257, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0730, -0.0730, -0.0958, -0.0958, -0.1032,\n",
            "         -0.0504, -0.0504, -0.0504, -0.0614, -0.1300, -0.1300, -0.1300, -0.1300,\n",
            "         -0.1032, -0.1614, -0.2512, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833,  0.0041, -0.0321, -0.0763, -0.0730, -0.1614, -0.1032, -0.1032,\n",
            "         -0.0958, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300, -0.1300,\n",
            "         -0.1300, -0.1300, -0.0958, -0.1300, -0.1257, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0730, -0.0958, -0.0958, -0.1300, -0.0622, -0.1032, -0.0973,\n",
            "         -0.0763, -0.0833, -0.0958, -0.1032, -0.0958, -0.1614, -0.0718, -0.0718,\n",
            "         -0.0958, -0.0718, -0.0730, -0.1117, -0.0958, -0.0763, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0732, -0.0833, -0.0833, -0.0833, -0.0833, -0.0833,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0763, -0.1032, -0.0958, -0.0958,\n",
            "         -0.1032, -0.1032, -0.1300, -0.0958, -0.1032, -0.1300, -0.0718, -0.1300,\n",
            "         -0.1032, -0.1257, -0.0833, -0.0732, -0.0732, -0.0833, -0.0833, -0.0763,\n",
            "         -0.0833, -0.0718, -0.0833, -0.0833, -0.0763, -0.0958, -0.0958, -0.1032,\n",
            "         -0.0958, -0.0958, -0.1300, -0.1300, -0.0958, -0.0958, -0.1300, -0.0718,\n",
            "         -0.0833, -0.0833, -0.0833, -0.0833, -0.0958, -0.1614, -0.0958, -0.1300,\n",
            "         -0.1300, -0.1300, -0.1300, -0.1032, -0.1300, -0.0958, -0.1300, -0.0958,\n",
            "         -0.0958, -0.1300, -0.0958, -0.0958, -0.2512, -0.1450, -0.2512, -0.0622,\n",
            "         -0.1032, -0.1300, -0.1300, -0.0958, -0.0958, -0.0958, -0.1300, -0.1300,\n",
            "         -0.0958, -0.1032, -0.0958, -0.1032, -0.0958, -0.1032, -0.1032, -0.0622,\n",
            "         -0.1300, -0.1300, -0.0958, -0.1300],\n",
            "        [ 0.0168,  0.0168,  0.0168,  0.0303,  0.0168,  0.0303,  0.0092,  0.0619,\n",
            "          0.1782, -0.0619,  0.1782,  0.0935,  0.0403,  0.0092,  0.0092,  0.0403,\n",
            "          0.0303,  0.0092,  0.0282,  0.0092,  0.0168,  0.0168,  0.0403,  0.0303,\n",
            "          0.0168,  0.0303,  0.0303,  0.0303,  0.0168,  0.0168,  0.0168,  0.0493,\n",
            "          0.0327,  0.0272,  0.0272,  0.0332,  0.0282,  0.0403,  0.0303,  0.0092,\n",
            "          0.0092,  0.0092,  0.0092,  0.0092,  0.0168,  0.0168,  0.0168,  0.0168,\n",
            "          0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0303,\n",
            "          0.0303,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0332,\n",
            "          0.0332,  0.0332,  0.0282,  0.0282,  0.0282,  0.0332,  0.0282,  0.0332,\n",
            "          0.0282,  0.0282,  0.0282,  0.0303,  0.0168,  0.0092,  0.0303,  0.0303,\n",
            "          0.0168,  0.0403,  0.0092,  0.0168,  0.0282,  0.0282,  0.0282,  0.0332,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0303,  0.0303,  0.0303,\n",
            "          0.0415,  0.0168,  0.0303,  0.0303,  0.0303,  0.0303,  0.0619,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0092, -0.0373,  0.0353,  0.0303,  0.0092,\n",
            "          0.0303,  0.0493,  0.0303,  0.0092,  0.0092,  0.0092,  0.0092,  0.0092,\n",
            "          0.0092,  0.0619,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0332,  0.0282,  0.0303,  0.0303,  0.0403,  0.0303,  0.0092,\n",
            "          0.0403,  0.0168,  0.0092,  0.0415,  0.0403,  0.0619,  0.0619,  0.0403,\n",
            "          0.0282,  0.0092,  0.0282,  0.0303,  0.0168,  0.0168,  0.0303,  0.0303,\n",
            "          0.0168,  0.0092,  0.0303,  0.0168,  0.0303,  0.0303,  0.0303,  0.0168,\n",
            "          0.0303,  0.0303,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0450,\n",
            "          0.0450,  0.0450, -0.0963,  0.0168,  0.0092,  0.0303,  0.0168,  0.0092,\n",
            "          0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0092,  0.0092,  0.0493,\n",
            "          0.0168,  0.0303,  0.0480,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0619,  0.0619,  0.0303,  0.0303,  0.0092,\n",
            "          0.0239,  0.0239,  0.0239,  0.0464,  0.0168,  0.0168,  0.0168,  0.0168,\n",
            "          0.0092, -0.0373,  0.1694,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0937,  0.0782,  0.0403,  0.0619, -0.0373,  0.0092,  0.0092,\n",
            "          0.0303,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,  0.0168,\n",
            "          0.0168,  0.0168,  0.0303,  0.0168,  0.0480,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0619,  0.0303,  0.0303,  0.0168,  0.0403,  0.0092,  0.0583,\n",
            "          0.0403,  0.0282,  0.0303,  0.0092,  0.0303, -0.0373,  0.0493,  0.0493,\n",
            "          0.0303,  0.0493,  0.0619,  0.0353,  0.0303,  0.0403,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0332,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0403,  0.0092,  0.0303,  0.0303,\n",
            "          0.0092,  0.0092,  0.0168,  0.0303,  0.0092,  0.0168,  0.0493,  0.0168,\n",
            "          0.0092,  0.0480,  0.0282,  0.0332,  0.0332,  0.0282,  0.0282,  0.0403,\n",
            "          0.0282,  0.0493,  0.0282,  0.0282,  0.0403,  0.0303,  0.0303,  0.0092,\n",
            "          0.0303,  0.0303,  0.0168,  0.0168,  0.0303,  0.0303,  0.0168,  0.0493,\n",
            "          0.0282,  0.0282,  0.0282,  0.0282,  0.0303, -0.0373,  0.0303,  0.0168,\n",
            "          0.0168,  0.0168,  0.0168,  0.0092,  0.0168,  0.0303,  0.0168,  0.0303,\n",
            "          0.0303,  0.0168,  0.0303,  0.0303,  0.1694,  0.0415,  0.1694,  0.0403,\n",
            "          0.0092,  0.0168,  0.0168,  0.0303,  0.0303,  0.0303,  0.0168,  0.0168,\n",
            "          0.0303,  0.0092,  0.0303,  0.0092,  0.0303,  0.0092,  0.0092,  0.0403,\n",
            "          0.0168,  0.0168,  0.0303,  0.0168]])\n",
            "tensor([[ 0.1679,  0.1225,  0.1153,  0.1153,  0.1413,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1403,  0.1229,  0.1225,  0.1229,  0.1413,\n",
            "          0.1413,  0.1229,  0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.0716,\n",
            "          0.0716,  0.0673,  0.0673,  0.1229,  0.1349,  0.1349,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.0716,  0.0716,  0.1225,  0.1229,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1322,  0.1114,  0.0716,  0.0988,  0.1229,\n",
            "          0.1283,  0.1283,  0.1283,  0.1413,  0.1679,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.0716, -0.0925, -0.1422, -0.3425, -0.0827,     nan,  0.1349,\n",
            "          0.1229,  0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1413,  0.1283,\n",
            "          0.1283,  0.1349,  0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1349,\n",
            "          0.1349,  0.1283,  0.1229,  0.1349,  0.1679,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.0716,  0.0777,  0.0724,  0.1349,  0.1349,  0.1229,\n",
            "          0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.0716,  0.0716,  0.0716,\n",
            "          0.1349,  0.1229,  0.1225,  0.1679,  0.1679,  0.1679,  0.1679,  0.0550,\n",
            "          0.0114,  0.0333,  0.1349,  0.1349,  0.1679,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679, -2.8214,  0.0716,  0.0716,     nan,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1349,  0.0716,  0.0716,     nan,  0.1349,\n",
            "          0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1229,     nan,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1229,  0.1225,  0.1322,\n",
            "          0.1229],\n",
            "        [ 0.0732,  0.0871,  0.1063,  0.1063,  0.0837,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0733,  0.0755,  0.0871,  0.0755,  0.0837,\n",
            "          0.0837,  0.0755,  0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0744,\n",
            "          0.0744,  0.0885,  0.0885,  0.0755,  0.0787,  0.0787,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0744,  0.0744,  0.0871,  0.0755,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0815,  0.0970,  0.0744,  0.0755,  0.0755,\n",
            "          0.0937,  0.0937,  0.0937,  0.0837,  0.0732,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0744,  0.0522,  0.0608,  0.0588,  0.1372,     nan,  0.0787,\n",
            "          0.0755,  0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0837,  0.0937,\n",
            "          0.0937,  0.0787,  0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0787,\n",
            "          0.0787,  0.0937,  0.0755,  0.0787,  0.0732,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0744,  0.0814,  0.0715,  0.0787,  0.0787,  0.0755,\n",
            "          0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0744,  0.0744,  0.0744,\n",
            "          0.0787,  0.0755,  0.0871,  0.0732,  0.0732,  0.0732,  0.0732,  0.1274,\n",
            "         -0.0469, -0.0142,  0.0787,  0.0787,  0.0732,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732, -4.8887,  0.0744,  0.0744,     nan,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0787,  0.0744,  0.0744,     nan,  0.0787,\n",
            "          0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0755,     nan,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0755,  0.0871,  0.0815,\n",
            "          0.0755]])\n",
            "tensor([[ 0.1679,  0.1225,  0.1153,  0.1153,  0.1413,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1403,  0.1229,  0.1225,  0.1229,  0.1413,\n",
            "          0.1413,  0.1229,  0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.0716,\n",
            "          0.0716,  0.0673,  0.0673,  0.1229,  0.1349,  0.1349,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.0716,  0.0716,  0.1225,  0.1229,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1322,  0.1114,  0.0716,  0.0988,  0.1229,\n",
            "          0.1283,  0.1283,  0.1283,  0.1413,  0.1679,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.0716, -0.0925, -0.1422, -0.3425, -0.0827, -0.0827,  0.1349,\n",
            "          0.1229,  0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1413,  0.1283,\n",
            "          0.1283,  0.1349,  0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1349,\n",
            "          0.1349,  0.1283,  0.1229,  0.1349,  0.1679,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.0716,  0.0777,  0.0724,  0.1349,  0.1349,  0.1229,\n",
            "          0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.0716,  0.0716,  0.0716,\n",
            "          0.1349,  0.1229,  0.1225,  0.1679,  0.1679,  0.1679,  0.1679,  0.0550,\n",
            "          0.0114,  0.0333,  0.1349,  0.1349,  0.1679,  0.1679,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679, -2.8214,  0.0716,  0.0716,  0.0716,  0.1679,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1349,  0.0716,  0.0716,  0.0716,  0.1349,\n",
            "          0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1229,  0.1229,  0.1679,\n",
            "          0.1679,  0.1679,  0.1679,  0.1679,  0.1679,  0.1229,  0.1225,  0.1322,\n",
            "          0.1229],\n",
            "        [ 0.0732,  0.0871,  0.1063,  0.1063,  0.0837,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0733,  0.0755,  0.0871,  0.0755,  0.0837,\n",
            "          0.0837,  0.0755,  0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0744,\n",
            "          0.0744,  0.0885,  0.0885,  0.0755,  0.0787,  0.0787,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0744,  0.0744,  0.0871,  0.0755,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0815,  0.0970,  0.0744,  0.0755,  0.0755,\n",
            "          0.0937,  0.0937,  0.0937,  0.0837,  0.0732,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0744,  0.0522,  0.0608,  0.0588,  0.1372,  0.1372,  0.0787,\n",
            "          0.0755,  0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0837,  0.0937,\n",
            "          0.0937,  0.0787,  0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0787,\n",
            "          0.0787,  0.0937,  0.0755,  0.0787,  0.0732,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0744,  0.0814,  0.0715,  0.0787,  0.0787,  0.0755,\n",
            "          0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0744,  0.0744,  0.0744,\n",
            "          0.0787,  0.0755,  0.0871,  0.0732,  0.0732,  0.0732,  0.0732,  0.1274,\n",
            "         -0.0469, -0.0142,  0.0787,  0.0787,  0.0732,  0.0732,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732, -4.8887,  0.0744,  0.0744,  0.0744,  0.0732,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0787,  0.0744,  0.0744,  0.0744,  0.0787,\n",
            "          0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0755,  0.0755,  0.0732,\n",
            "          0.0732,  0.0732,  0.0732,  0.0732,  0.0732,  0.0755,  0.0871,  0.0815,\n",
            "          0.0755]])\n",
            "tensor([[-1.2371e-01, -1.2371e-01, -3.5171e-02, -3.5171e-02, -5.6398e-02,\n",
            "         -5.6398e-02, -5.6398e-02,         nan,         nan, -6.7725e-02,\n",
            "         -1.2229e-01, -1.1247e-01, -1.3305e-01, -1.2542e-01, -1.2542e-01,\n",
            "          9.8253e-02,  9.8253e-02,  1.1676e-01,  1.1676e-01,  4.7258e-02,\n",
            "          4.7258e-02,  5.5682e-02,  5.5682e-02,  5.5682e-02,  5.5682e-02,\n",
            "         -6.7725e-02, -6.7725e-02, -1.2542e-01, -6.7725e-02, -6.7725e-02,\n",
            "         -6.7725e-02, -6.7725e-02, -6.7725e-02, -6.7725e-02, -5.9407e-02,\n",
            "         -4.6098e-02,  1.2694e-02,  1.2694e-02,  1.9948e-02,  1.9948e-02,\n",
            "          1.9948e-02,  1.9948e-02,  3.5968e-03,  9.6973e-03,  9.6973e-03,\n",
            "          9.6973e-03,  9.6973e-03,  9.6973e-03, -3.0199e-01, -3.0199e-01,\n",
            "         -4.4572e-01, -1.1247e-01, -3.8336e-02, -1.2546e-02, -1.2433e-03,\n",
            "          4.1088e-03,  9.9991e+00,  9.9991e+00,  8.0328e-03,  3.3299e-03,\n",
            "          3.3299e-03,  3.3299e-03,  3.3299e-03,  3.3299e-03,  3.3299e-03,\n",
            "         -1.1351e-02, -5.9407e-02, -1.2883e-01, -1.2883e-01, -8.8191e-02,\n",
            "         -6.7725e-02, -6.7725e-02, -6.7725e-02, -6.7725e-02, -5.9407e-02,\n",
            "         -5.9407e-02],\n",
            "        [-2.0130e-01, -2.0130e-01,  2.0863e-02,  2.0863e-02,  2.7222e-02,\n",
            "          2.7222e-02,  2.7222e-02,         nan,         nan,  6.7860e-02,\n",
            "          5.4967e-02,  5.0829e-02,  5.8444e-02,  4.7778e-02,  4.7778e-02,\n",
            "          1.3943e-01,  1.3943e-01,  1.3068e-01,  1.3068e-01,  1.4119e-01,\n",
            "          1.4119e-01,  1.0829e-01,  1.0829e-01,  1.0829e-01,  1.0829e-01,\n",
            "          6.7860e-02,  6.7860e-02,  4.7778e-02,  6.7860e-02,  6.7860e-02,\n",
            "          6.7860e-02,  6.7860e-02,  6.7860e-02,  6.7860e-02,  7.3934e-02,\n",
            "          7.4612e-02,  9.7801e-02,  9.7801e-02,  1.1709e-01,  1.1709e-01,\n",
            "          1.1709e-01,  1.1709e-01,  9.6019e-02,  8.4401e-02,  8.4401e-02,\n",
            "          8.4401e-02,  8.4401e-02,  8.4401e-02, -1.5381e-01, -1.5381e-01,\n",
            "         -2.3284e-01,  5.0829e-02,  9.1239e-02,  9.2994e-02,  9.2505e-02,\n",
            "          9.4956e-02, -5.3070e+00, -5.3070e+00,  9.4935e-02,  9.3249e-02,\n",
            "          9.3249e-02,  9.3249e-02,  9.3249e-02,  9.3249e-02,  9.3249e-02,\n",
            "          9.2988e-02,  7.3934e-02,  6.1148e-02,  6.1148e-02,  6.5214e-02,\n",
            "          6.7860e-02,  6.7860e-02,  6.7860e-02,  6.7860e-02,  7.3934e-02,\n",
            "          7.3934e-02]])\n",
            "tensor([[-1.2371e-01, -1.2371e-01, -3.5171e-02, -3.5171e-02, -5.6398e-02,\n",
            "         -5.6398e-02, -5.6398e-02, -5.6398e-02, -5.6398e-02, -6.7725e-02,\n",
            "         -1.2229e-01, -1.1247e-01, -1.3305e-01, -1.2542e-01, -1.2542e-01,\n",
            "          9.8253e-02,  9.8253e-02,  1.1676e-01,  1.1676e-01,  4.7258e-02,\n",
            "          4.7258e-02,  5.5682e-02,  5.5682e-02,  5.5682e-02,  5.5682e-02,\n",
            "         -6.7725e-02, -6.7725e-02, -1.2542e-01, -6.7725e-02, -6.7725e-02,\n",
            "         -6.7725e-02, -6.7725e-02, -6.7725e-02, -6.7725e-02, -5.9407e-02,\n",
            "         -4.6098e-02,  1.2694e-02,  1.2694e-02,  1.9948e-02,  1.9948e-02,\n",
            "          1.9948e-02,  1.9948e-02,  3.5968e-03,  9.6973e-03,  9.6973e-03,\n",
            "          9.6973e-03,  9.6973e-03,  9.6973e-03, -3.0199e-01, -3.0199e-01,\n",
            "         -4.4572e-01, -1.1247e-01, -3.8336e-02, -1.2546e-02, -1.2433e-03,\n",
            "          4.1088e-03,  9.9991e+00,  9.9991e+00,  8.0328e-03,  3.3299e-03,\n",
            "          3.3299e-03,  3.3299e-03,  3.3299e-03,  3.3299e-03,  3.3299e-03,\n",
            "         -1.1351e-02, -5.9407e-02, -1.2883e-01, -1.2883e-01, -8.8191e-02,\n",
            "         -6.7725e-02, -6.7725e-02, -6.7725e-02, -6.7725e-02, -5.9407e-02,\n",
            "         -5.9407e-02],\n",
            "        [-2.0130e-01, -2.0130e-01,  2.0863e-02,  2.0863e-02,  2.7222e-02,\n",
            "          2.7222e-02,  2.7222e-02,  2.7222e-02,  2.7222e-02,  6.7860e-02,\n",
            "          5.4967e-02,  5.0829e-02,  5.8444e-02,  4.7778e-02,  4.7778e-02,\n",
            "          1.3943e-01,  1.3943e-01,  1.3068e-01,  1.3068e-01,  1.4119e-01,\n",
            "          1.4119e-01,  1.0829e-01,  1.0829e-01,  1.0829e-01,  1.0829e-01,\n",
            "          6.7860e-02,  6.7860e-02,  4.7778e-02,  6.7860e-02,  6.7860e-02,\n",
            "          6.7860e-02,  6.7860e-02,  6.7860e-02,  6.7860e-02,  7.3934e-02,\n",
            "          7.4612e-02,  9.7801e-02,  9.7801e-02,  1.1709e-01,  1.1709e-01,\n",
            "          1.1709e-01,  1.1709e-01,  9.6019e-02,  8.4401e-02,  8.4401e-02,\n",
            "          8.4401e-02,  8.4401e-02,  8.4401e-02, -1.5381e-01, -1.5381e-01,\n",
            "         -2.3284e-01,  5.0829e-02,  9.1239e-02,  9.2994e-02,  9.2505e-02,\n",
            "          9.4956e-02, -5.3070e+00, -5.3070e+00,  9.4935e-02,  9.3249e-02,\n",
            "          9.3249e-02,  9.3249e-02,  9.3249e-02,  9.3249e-02,  9.3249e-02,\n",
            "          9.2988e-02,  7.3934e-02,  6.1148e-02,  6.1148e-02,  6.5214e-02,\n",
            "          6.7860e-02,  6.7860e-02,  6.7860e-02,  6.7860e-02,  7.3934e-02,\n",
            "          7.3934e-02]])\n",
            "tensor([[0.1593, 0.1693, 0.1693, 0.1593, 0.1693, 0.1593, 0.1593, 0.1693, 0.1693,\n",
            "         0.1594, 0.1555, 0.1555, 0.1693, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1593, 0.1593,\n",
            "         0.1693,    nan,    nan,    nan, 0.1455, 0.1593, 0.1693, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1693, 0.1693, 0.1593, 0.1593, 0.1693,\n",
            "         0.1693, 0.1593, 0.1693, 0.1693, 0.1693, 0.1693, 0.1593, 0.1593, 0.1593,\n",
            "         0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1693, 0.1693, 0.1593, 0.1593, 0.1593, 0.1593,    nan,\n",
            "         0.1693, 0.1593, 0.1693, 0.1693, 0.1593, 0.1693, 0.1693, 0.1693, 0.1693,\n",
            "         0.1693, 0.1593, 0.1593, 0.1569, 0.1583, 0.1583, 0.1583, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1693, 0.1593, 0.1593, 0.1512,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1693, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1593],\n",
            "        [0.1461, 0.1444, 0.1444, 0.1461, 0.1444, 0.1461, 0.1461, 0.1444, 0.1444,\n",
            "         0.1539, 0.1541, 0.1541, 0.1444, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1461, 0.1461,\n",
            "         0.1444,    nan,    nan,    nan, 0.0960, 0.1461, 0.1444, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1444, 0.1444, 0.1461, 0.1461, 0.1444,\n",
            "         0.1444, 0.1461, 0.1444, 0.1444, 0.1444, 0.1444, 0.1461, 0.1461, 0.1461,\n",
            "         0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1444, 0.1444, 0.1461, 0.1461, 0.1461, 0.1461,    nan,\n",
            "         0.1444, 0.1461, 0.1444, 0.1444, 0.1461, 0.1444, 0.1444, 0.1444, 0.1444,\n",
            "         0.1444, 0.1461, 0.1461, 0.1238, 0.1314, 0.1314, 0.1314, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1444, 0.1461, 0.1461, 0.1270,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1444, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1461]])\n",
            "tensor([[0.1593, 0.1693, 0.1693, 0.1593, 0.1693, 0.1593, 0.1593, 0.1693, 0.1693,\n",
            "         0.1594, 0.1555, 0.1555, 0.1693, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1593, 0.1593,\n",
            "         0.1693, 0.1693, 0.1693, 0.1693, 0.1455, 0.1593, 0.1693, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1693, 0.1693, 0.1593, 0.1593, 0.1693,\n",
            "         0.1693, 0.1593, 0.1693, 0.1693, 0.1693, 0.1693, 0.1593, 0.1593, 0.1593,\n",
            "         0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1693, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1693, 0.1693, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1693, 0.1593, 0.1693, 0.1693, 0.1593, 0.1693, 0.1693, 0.1693, 0.1693,\n",
            "         0.1693, 0.1593, 0.1593, 0.1569, 0.1583, 0.1583, 0.1583, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1693, 0.1593, 0.1593, 0.1512,\n",
            "         0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1593, 0.1593, 0.1693, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593, 0.1593,\n",
            "         0.1593],\n",
            "        [0.1461, 0.1444, 0.1444, 0.1461, 0.1444, 0.1461, 0.1461, 0.1444, 0.1444,\n",
            "         0.1539, 0.1541, 0.1541, 0.1444, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1461, 0.1461,\n",
            "         0.1444, 0.1444, 0.1444, 0.1444, 0.0960, 0.1461, 0.1444, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1444, 0.1444, 0.1461, 0.1461, 0.1444,\n",
            "         0.1444, 0.1461, 0.1444, 0.1444, 0.1444, 0.1444, 0.1461, 0.1461, 0.1461,\n",
            "         0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1444, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1444, 0.1444, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1444, 0.1461, 0.1444, 0.1444, 0.1461, 0.1444, 0.1444, 0.1444, 0.1444,\n",
            "         0.1444, 0.1461, 0.1461, 0.1238, 0.1314, 0.1314, 0.1314, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1444, 0.1461, 0.1461, 0.1270,\n",
            "         0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1461, 0.1461, 0.1444, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461, 0.1461,\n",
            "         0.1461]])\n",
            "tensor([[    nan,  0.0537, -0.0500, -0.0500, -0.0500,  0.0293,     nan, -0.0833,\n",
            "         -0.2300, -0.4953, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500,\n",
            "         -0.0500,  0.0599,  0.0451,  0.0482,  0.0620,  0.0553,  0.0553,  0.0620,\n",
            "          0.0346,  0.0451,  0.0599,  0.0423,  0.0423, -0.0500, -0.0500, -0.0098,\n",
            "          0.0451,  0.0451,  0.0451,  0.0451,  0.0451,  0.0451, -0.0559, -0.0500,\n",
            "         -0.0500, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500,  0.0451,\n",
            "          0.0451,  0.0489,  0.0451,  0.0451,  0.0451,  0.0451,  0.0537,  0.0644,\n",
            "         -0.0500, -0.0500, -0.0293],\n",
            "        [    nan,  0.1628,  0.1398,  0.1398,  0.1398,  0.1400,     nan,  0.0282,\n",
            "         -0.0117, -0.2238,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,\n",
            "          0.1398,  0.1769,  0.1692,  0.1697,  0.1577,  0.1649,  0.1649,  0.1577,\n",
            "          0.1678,  0.1692,  0.1769,  0.1615,  0.1615,  0.1398,  0.1398,  0.1430,\n",
            "          0.1692,  0.1692,  0.1692,  0.1692,  0.1692,  0.1692,  0.1485,  0.1398,\n",
            "          0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1692,\n",
            "          0.1692,  0.1657,  0.1692,  0.1692,  0.1692,  0.1692,  0.1628,  0.1280,\n",
            "          0.1398,  0.1398,  0.0181]])\n",
            "tensor([[ 0.0537, -0.0500, -0.0500, -0.0500,  0.0293,  0.0293, -0.0833, -0.2300,\n",
            "         -0.4953, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500,\n",
            "          0.0599,  0.0451,  0.0482,  0.0620,  0.0553,  0.0553,  0.0620,  0.0346,\n",
            "          0.0451,  0.0599,  0.0423,  0.0423, -0.0500, -0.0500, -0.0098,  0.0451,\n",
            "          0.0451,  0.0451,  0.0451,  0.0451,  0.0451, -0.0559, -0.0500, -0.0500,\n",
            "         -0.0500, -0.0500, -0.0500, -0.0500, -0.0500, -0.0500,  0.0451,  0.0451,\n",
            "          0.0489,  0.0451,  0.0451,  0.0451,  0.0451,  0.0537,  0.0644, -0.0500,\n",
            "         -0.0500, -0.0293],\n",
            "        [ 0.1628,  0.1398,  0.1398,  0.1398,  0.1400,  0.1400,  0.0282, -0.0117,\n",
            "         -0.2238,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,\n",
            "          0.1769,  0.1692,  0.1697,  0.1577,  0.1649,  0.1649,  0.1577,  0.1678,\n",
            "          0.1692,  0.1769,  0.1615,  0.1615,  0.1398,  0.1398,  0.1430,  0.1692,\n",
            "          0.1692,  0.1692,  0.1692,  0.1692,  0.1692,  0.1485,  0.1398,  0.1398,\n",
            "          0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1398,  0.1692,  0.1692,\n",
            "          0.1657,  0.1692,  0.1692,  0.1692,  0.1692,  0.1628,  0.1280,  0.1398,\n",
            "          0.1398,  0.0181]])\n",
            "tensor([[-0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0594, -0.0594, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0594,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0306, -0.0306, -0.0306, -0.0306, -0.0503,     nan,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0594,     nan, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,     nan, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,     nan, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0503,     nan,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,     nan,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0461,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503,     nan, -0.0503, -0.0594, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0503, -0.0503, -0.0594,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503],\n",
            "        [ 0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0739,  0.0739,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0739,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0632,  0.0776,  0.0776,  0.0776,  0.0776,  0.0632,     nan,\n",
            "          0.0632,  0.0632,  0.0632,  0.0739,     nan,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,     nan,  0.0632,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,\n",
            "          0.0632,  0.0632,  0.0632,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,     nan,  0.0632,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0632,     nan,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,     nan,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0746,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,     nan,  0.0632,  0.0739,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0632,  0.0632,  0.0739,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632]])\n",
            "tensor([[-0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0594, -0.0594, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0594,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0306, -0.0306, -0.0306, -0.0306, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0594, -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0461,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0594, -0.0503, -0.0503, -0.0594,\n",
            "         -0.0594, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0594, -0.0594, -0.0503, -0.0503, -0.0503,\n",
            "         -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503, -0.0503],\n",
            "        [ 0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0739,  0.0739,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0739,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0632,  0.0776,  0.0776,  0.0776,  0.0776,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,\n",
            "          0.0632,  0.0632,  0.0632,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0739,  0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0746,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0739,  0.0632,  0.0632,  0.0739,\n",
            "          0.0739,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0739,  0.0739,  0.0632,  0.0632,  0.0632,\n",
            "          0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632,  0.0632]])\n",
            "tensor([[nan],\n",
            "        [nan]])\n",
            "tensor([], size=(2, 0))\n",
            "tensor([[   nan, 0.0648, 0.0161, 0.0161, 0.0737, 0.0737, 0.0760, 0.0760, 0.0599,\n",
            "         0.0599, 0.0599, 0.0760, 0.0599, 0.0737, 0.0737, 0.0737, 0.0737, 0.0737,\n",
            "         0.0760, 0.0760, 0.0599, 0.0599, 0.0599, 0.0599, 0.0707, 0.0570,    nan,\n",
            "            nan, 0.0760, 0.0599, 0.0599, 0.0760, 0.0599, 0.0599, 0.0599, 0.0599,\n",
            "         0.0599, 0.0599, 0.0599, 0.0599, 0.0599, 0.0599, 0.0599, 0.0599, 0.0760,\n",
            "         0.0760,    nan,    nan,    nan, 0.0760, 0.0737, 0.0760, 0.0760, 0.0760,\n",
            "         0.0760, 0.0760, 0.0760, 0.0737, 0.0737, 0.0760, 0.0760, 0.0760, 0.0599,\n",
            "         0.0599, 0.0599, 0.0599, 0.0599, 0.0760, 0.0760, 0.0737, 0.0737],\n",
            "        [   nan, 0.1396, 0.2273, 0.2273, 0.1676, 0.1676, 0.1732, 0.1732, 0.1769,\n",
            "         0.1769, 0.1769, 0.1732, 0.1769, 0.1676, 0.1676, 0.1676, 0.1676, 0.1676,\n",
            "         0.1732, 0.1732, 0.1769, 0.1769, 0.1769, 0.1769, 0.1745, 0.1727,    nan,\n",
            "            nan, 0.1732, 0.1769, 0.1769, 0.1732, 0.1769, 0.1769, 0.1769, 0.1769,\n",
            "         0.1769, 0.1769, 0.1769, 0.1769, 0.1769, 0.1769, 0.1769, 0.1769, 0.1732,\n",
            "         0.1732,    nan,    nan,    nan, 0.1732, 0.1676, 0.1732, 0.1732, 0.1732,\n",
            "         0.1732, 0.1732, 0.1732, 0.1676, 0.1676, 0.1732, 0.1732, 0.1732, 0.1769,\n",
            "         0.1769, 0.1769, 0.1769, 0.1769, 0.1732, 0.1732, 0.1676, 0.1676]])\n",
            "tensor([[0.0648, 0.0161, 0.0161, 0.0737, 0.0737, 0.0760, 0.0760, 0.0599, 0.0599,\n",
            "         0.0599, 0.0760, 0.0599, 0.0737, 0.0737, 0.0737, 0.0737, 0.0737, 0.0760,\n",
            "         0.0760, 0.0599, 0.0599, 0.0599, 0.0599, 0.0707, 0.0570, 0.0570, 0.0570,\n",
            "         0.0760, 0.0599, 0.0599, 0.0760, 0.0599, 0.0599, 0.0599, 0.0599, 0.0599,\n",
            "         0.0599, 0.0599, 0.0599, 0.0599, 0.0599, 0.0599, 0.0599, 0.0760, 0.0760,\n",
            "         0.0760, 0.0760, 0.0760, 0.0760, 0.0737, 0.0760, 0.0760, 0.0760, 0.0760,\n",
            "         0.0760, 0.0760, 0.0737, 0.0737, 0.0760, 0.0760, 0.0760, 0.0599, 0.0599,\n",
            "         0.0599, 0.0599, 0.0599, 0.0760, 0.0760, 0.0737, 0.0737],\n",
            "        [0.1396, 0.2273, 0.2273, 0.1676, 0.1676, 0.1732, 0.1732, 0.1769, 0.1769,\n",
            "         0.1769, 0.1732, 0.1769, 0.1676, 0.1676, 0.1676, 0.1676, 0.1676, 0.1732,\n",
            "         0.1732, 0.1769, 0.1769, 0.1769, 0.1769, 0.1745, 0.1727, 0.1727, 0.1727,\n",
            "         0.1732, 0.1769, 0.1769, 0.1732, 0.1769, 0.1769, 0.1769, 0.1769, 0.1769,\n",
            "         0.1769, 0.1769, 0.1769, 0.1769, 0.1769, 0.1769, 0.1769, 0.1732, 0.1732,\n",
            "         0.1732, 0.1732, 0.1732, 0.1732, 0.1676, 0.1732, 0.1732, 0.1732, 0.1732,\n",
            "         0.1732, 0.1732, 0.1676, 0.1676, 0.1732, 0.1732, 0.1732, 0.1769, 0.1769,\n",
            "         0.1769, 0.1769, 0.1769, 0.1732, 0.1732, 0.1676, 0.1676]])\n",
            "tensor([[ 1.3945e-01,  1.3945e-01,  2.4742e-01,  1.3945e-01,  1.5926e-01,\n",
            "          1.3559e-01,  1.5926e-01,  1.3945e-01,  1.3945e-01,  1.2915e-01,\n",
            "          1.3718e-01,  1.3945e-01,  1.2915e-01,  1.3945e-01,  1.2915e-01,\n",
            "          1.2915e-01,  1.3945e-01,  1.3718e-01,  1.3945e-01,  1.3945e-01,\n",
            "          3.8306e-02,  1.5926e-01,  1.3321e-01, -2.9770e-01, -2.1803e-01,\n",
            "          1.1488e-01,  1.3945e-01,  1.3945e-01,  1.5926e-01,  1.3945e-01,\n",
            "          7.9206e-02,  1.2915e-01,  1.3718e-01,  1.2593e-01, -1.0224e-01,\n",
            "          1.3718e-01,  1.2915e-01,  1.2915e-01,  2.0436e-01,  1.0822e-01,\n",
            "          1.0822e-01,  1.2915e-01,  1.3945e-01,  1.3945e-01,  9.1573e-02,\n",
            "         -7.9081e-02, -5.5739e-02, -7.6623e-02,  1.5926e-01, -3.7076e-02,\n",
            "         -2.2685e-02, -1.1674e-01, -1.1674e-01,  2.2992e-02,  1.5158e-01,\n",
            "          1.1600e-01,  1.5926e-01,  1.2915e-01,  1.1140e-01,  1.5926e-01,\n",
            "          3.3088e-02, -1.3934e-01, -1.0173e-01,  1.5926e-01,  1.4130e-01,\n",
            "          1.5926e-01,  1.5926e-01,  1.3945e-01,  1.1488e-01,  1.3945e-01,\n",
            "          1.5926e-01,  1.3945e-01,  1.3945e-01,  1.2915e-01,  2.4742e-01,\n",
            "          1.3945e-01,  1.5926e-01,  1.3718e-01,  1.5926e-01,  3.3088e-02,\n",
            "         -3.8336e-02,  8.3450e-02,  1.8252e-01,  1.3321e-01,  1.6918e-01,\n",
            "          1.5926e-01,  1.3945e-01,  1.3945e-01,  1.3945e-01,  1.3945e-01,\n",
            "          1.3945e-01,  1.3945e-01,  1.2915e-01,  1.3321e-01,  1.3321e-01,\n",
            "          1.6918e-01, -1.0173e-01, -6.6479e-03,  1.5926e-01,  2.4742e-01,\n",
            "          1.5075e-01,  1.2016e-01,  1.1488e-01,  1.6918e-01,  1.3106e-01,\n",
            "          1.2915e-01,  1.3945e-01,  1.2915e-01,  1.4523e-01,  1.1488e-01,\n",
            "         -6.9266e-02,  1.2915e-01,  1.5926e-01,  1.5926e-01,  1.5926e-01,\n",
            "          1.1488e-01,  1.1488e-01,  1.0948e-01,  4.5757e-02, -3.2928e-03,\n",
            "          1.2915e-01,  1.3718e-01,  1.5926e-01,  7.8650e-02,  4.5054e-02,\n",
            "          6.2013e-02,  1.3395e-01,  1.3395e-01,  1.5926e-01,  1.3945e-01,\n",
            "          1.5926e-01,  1.5926e-01,  1.3247e-01,  8.6921e-02,  1.5926e-01,\n",
            "          1.4523e-01,  1.5926e-01,  1.3945e-01,  1.3945e-01,  1.3945e-01,\n",
            "          1.3945e-01,  1.1488e-01,  1.3247e-01,  1.3974e-01,  5.6972e-02,\n",
            "          4.4913e-02,  1.4523e-01,         nan, -4.0578e-01, -7.1760e-02,\n",
            "          1.5926e-01,  1.1488e-01,  1.5535e-01,  1.5926e-01,  1.5926e-01,\n",
            "          1.5926e-01,  1.2915e-01,  1.0822e-01,  1.2016e-01,  2.4742e-01,\n",
            "         -1.2655e+01,  1.5926e-01,  1.2016e-01,  1.8252e-01,  1.8252e-01,\n",
            "          2.5144e-01,  2.4269e-01, -7.3029e-02,  1.3822e-01,  1.5926e-01,\n",
            "          1.5926e-01,  1.3247e-01,  1.5158e-01,  1.3945e-01,  1.3945e-01,\n",
            "          1.3945e-01,  1.6733e-01,  1.5926e-01,  1.0109e-01,  1.5926e-01,\n",
            "          4.9190e-02,  1.2915e-01,  1.3945e-01,  1.3945e-01,  1.5926e-01,\n",
            "          1.5926e-01,  2.4742e-01,  1.3945e-01,  1.3945e-01,  1.1793e-01,\n",
            "          1.1793e-01,  1.1511e-01,  1.3559e-01,  1.2915e-01,  1.3945e-01],\n",
            "        [ 3.0058e-02,  3.0058e-02, -3.9220e-03,  3.0058e-02,  2.9801e-02,\n",
            "          2.7793e-02,  2.9801e-02,  3.0058e-02,  3.0058e-02,  3.1133e-02,\n",
            "          3.5519e-02,  3.0058e-02,  3.1133e-02,  3.0058e-02,  3.1133e-02,\n",
            "          3.1133e-02,  3.0058e-02,  3.5519e-02,  3.0058e-02,  3.0058e-02,\n",
            "          8.9042e-02,  2.9801e-02,  4.2647e-02, -1.5044e-01, -1.4104e-01,\n",
            "          3.7292e-02,  3.0058e-02,  3.0058e-02,  2.9801e-02,  3.0058e-02,\n",
            "          4.5823e-02,  3.1133e-02,  3.5519e-02, -3.8896e-02,  1.0017e-01,\n",
            "          3.5519e-02,  3.1133e-02,  3.1133e-02, -2.6150e-02,  4.0488e-02,\n",
            "          4.0488e-02,  3.1133e-02,  3.0058e-02,  3.0058e-02,  5.8624e-02,\n",
            "          8.5913e-02,  1.5658e-01,  7.2781e-02,  2.9801e-02,  3.1123e-02,\n",
            "          4.4723e-02,  1.0385e-01,  1.0385e-01,  4.8471e-02,  2.2945e-02,\n",
            "          4.5941e-02,  2.9801e-02,  3.1133e-02,  5.4040e-02,  2.9801e-02,\n",
            "          2.8171e-02,  7.0048e-02,  1.0147e-01,  2.9801e-02,  8.3738e-02,\n",
            "          2.9801e-02,  2.9801e-02,  3.0058e-02,  3.7292e-02,  3.0058e-02,\n",
            "          2.9801e-02,  3.0058e-02,  3.0058e-02,  3.1133e-02, -3.9220e-03,\n",
            "          3.0058e-02,  2.9801e-02,  3.5519e-02,  2.9801e-02,  2.8171e-02,\n",
            "          9.1239e-02,  6.2024e-02,  3.1553e-02,  4.2647e-02,  4.5918e-02,\n",
            "          2.9801e-02,  3.0058e-02,  3.0058e-02,  3.0058e-02,  3.0058e-02,\n",
            "          3.0058e-02,  3.0058e-02,  3.1133e-02,  4.2647e-02,  4.2647e-02,\n",
            "          4.5918e-02,  1.0147e-01,  5.4510e-02,  2.9801e-02, -3.9220e-03,\n",
            "          5.9013e-03,  6.1549e-02,  3.7292e-02,  4.5918e-02,  2.5025e-02,\n",
            "          3.1133e-02,  3.0058e-02,  3.1133e-02,  4.0632e-02,  3.7292e-02,\n",
            "          1.3018e-01,  3.1133e-02,  2.9801e-02,  2.9801e-02,  2.9801e-02,\n",
            "          3.7292e-02,  3.7292e-02, -1.2429e-02,  3.4650e-02,  7.7949e-02,\n",
            "          3.1133e-02,  3.5519e-02,  2.9801e-02,  7.3689e-02,  1.6916e-01,\n",
            "          1.5771e-01,  1.0194e-01,  1.0194e-01,  2.9801e-02,  3.0058e-02,\n",
            "          2.9801e-02,  2.9801e-02,  5.1556e-02, -9.2519e-03,  2.9801e-02,\n",
            "          4.0632e-02,  2.9801e-02,  3.0058e-02,  3.0058e-02,  3.0058e-02,\n",
            "          3.0058e-02,  3.7292e-02,  5.1556e-02,  9.2316e-02,  1.7268e-01,\n",
            "          1.5518e-01,  4.0632e-02,         nan,  1.0552e-01,  4.9290e-02,\n",
            "          2.9801e-02,  3.7292e-02,  7.9875e-02,  2.9801e-02,  2.9801e-02,\n",
            "          2.9801e-02,  3.1133e-02,  4.0488e-02,  6.1549e-02, -3.9220e-03,\n",
            "         -5.5129e+00,  2.9801e-02,  6.1549e-02,  3.1553e-02,  3.1553e-02,\n",
            "         -4.5103e-02, -5.6349e-02,  6.1876e-02,  6.7240e-02,  2.9801e-02,\n",
            "          2.9801e-02,  5.1556e-02,  2.2945e-02,  3.0058e-02,  3.0058e-02,\n",
            "          3.0058e-02,  2.0902e-01,  2.9801e-02,  4.7842e-02,  2.9801e-02,\n",
            "          1.6492e-03,  3.1133e-02,  3.0058e-02,  3.0058e-02,  2.9801e-02,\n",
            "          2.9801e-02, -3.9220e-03,  3.0058e-02,  3.0058e-02,  7.4557e-02,\n",
            "          7.4557e-02,  7.5439e-02,  2.7793e-02,  3.1133e-02,  3.0058e-02]])\n",
            "tensor([[ 1.3945e-01,  1.3945e-01,  2.4742e-01,  1.3945e-01,  1.5926e-01,\n",
            "          1.3559e-01,  1.5926e-01,  1.3945e-01,  1.3945e-01,  1.2915e-01,\n",
            "          1.3718e-01,  1.3945e-01,  1.2915e-01,  1.3945e-01,  1.2915e-01,\n",
            "          1.2915e-01,  1.3945e-01,  1.3718e-01,  1.3945e-01,  1.3945e-01,\n",
            "          3.8306e-02,  1.5926e-01,  1.3321e-01, -2.9770e-01, -2.1803e-01,\n",
            "          1.1488e-01,  1.3945e-01,  1.3945e-01,  1.5926e-01,  1.3945e-01,\n",
            "          7.9206e-02,  1.2915e-01,  1.3718e-01,  1.2593e-01, -1.0224e-01,\n",
            "          1.3718e-01,  1.2915e-01,  1.2915e-01,  2.0436e-01,  1.0822e-01,\n",
            "          1.0822e-01,  1.2915e-01,  1.3945e-01,  1.3945e-01,  9.1573e-02,\n",
            "         -7.9081e-02, -5.5739e-02, -7.6623e-02,  1.5926e-01, -3.7076e-02,\n",
            "         -2.2685e-02, -1.1674e-01, -1.1674e-01,  2.2992e-02,  1.5158e-01,\n",
            "          1.1600e-01,  1.5926e-01,  1.2915e-01,  1.1140e-01,  1.5926e-01,\n",
            "          3.3088e-02, -1.3934e-01, -1.0173e-01,  1.5926e-01,  1.4130e-01,\n",
            "          1.5926e-01,  1.5926e-01,  1.3945e-01,  1.1488e-01,  1.3945e-01,\n",
            "          1.5926e-01,  1.3945e-01,  1.3945e-01,  1.2915e-01,  2.4742e-01,\n",
            "          1.3945e-01,  1.5926e-01,  1.3718e-01,  1.5926e-01,  3.3088e-02,\n",
            "         -3.8336e-02,  8.3450e-02,  1.8252e-01,  1.3321e-01,  1.6918e-01,\n",
            "          1.5926e-01,  1.3945e-01,  1.3945e-01,  1.3945e-01,  1.3945e-01,\n",
            "          1.3945e-01,  1.3945e-01,  1.2915e-01,  1.3321e-01,  1.3321e-01,\n",
            "          1.6918e-01, -1.0173e-01, -6.6479e-03,  1.5926e-01,  2.4742e-01,\n",
            "          1.5075e-01,  1.2016e-01,  1.1488e-01,  1.6918e-01,  1.3106e-01,\n",
            "          1.2915e-01,  1.3945e-01,  1.2915e-01,  1.4523e-01,  1.1488e-01,\n",
            "         -6.9266e-02,  1.2915e-01,  1.5926e-01,  1.5926e-01,  1.5926e-01,\n",
            "          1.1488e-01,  1.1488e-01,  1.0948e-01,  4.5757e-02, -3.2928e-03,\n",
            "          1.2915e-01,  1.3718e-01,  1.5926e-01,  7.8650e-02,  4.5054e-02,\n",
            "          6.2013e-02,  1.3395e-01,  1.3395e-01,  1.5926e-01,  1.3945e-01,\n",
            "          1.5926e-01,  1.5926e-01,  1.3247e-01,  8.6921e-02,  1.5926e-01,\n",
            "          1.4523e-01,  1.5926e-01,  1.3945e-01,  1.3945e-01,  1.3945e-01,\n",
            "          1.3945e-01,  1.1488e-01,  1.3247e-01,  1.3974e-01,  5.6972e-02,\n",
            "          4.4913e-02,  1.4523e-01,  1.4523e-01, -4.0578e-01, -7.1760e-02,\n",
            "          1.5926e-01,  1.1488e-01,  1.5535e-01,  1.5926e-01,  1.5926e-01,\n",
            "          1.5926e-01,  1.2915e-01,  1.0822e-01,  1.2016e-01,  2.4742e-01,\n",
            "         -1.2655e+01,  1.5926e-01,  1.2016e-01,  1.8252e-01,  1.8252e-01,\n",
            "          2.5144e-01,  2.4269e-01, -7.3029e-02,  1.3822e-01,  1.5926e-01,\n",
            "          1.5926e-01,  1.3247e-01,  1.5158e-01,  1.3945e-01,  1.3945e-01,\n",
            "          1.3945e-01,  1.6733e-01,  1.5926e-01,  1.0109e-01,  1.5926e-01,\n",
            "          4.9190e-02,  1.2915e-01,  1.3945e-01,  1.3945e-01,  1.5926e-01,\n",
            "          1.5926e-01,  2.4742e-01,  1.3945e-01,  1.3945e-01,  1.1793e-01,\n",
            "          1.1793e-01,  1.1511e-01,  1.3559e-01,  1.2915e-01,  1.3945e-01],\n",
            "        [ 3.0058e-02,  3.0058e-02, -3.9220e-03,  3.0058e-02,  2.9801e-02,\n",
            "          2.7793e-02,  2.9801e-02,  3.0058e-02,  3.0058e-02,  3.1133e-02,\n",
            "          3.5519e-02,  3.0058e-02,  3.1133e-02,  3.0058e-02,  3.1133e-02,\n",
            "          3.1133e-02,  3.0058e-02,  3.5519e-02,  3.0058e-02,  3.0058e-02,\n",
            "          8.9042e-02,  2.9801e-02,  4.2647e-02, -1.5044e-01, -1.4104e-01,\n",
            "          3.7292e-02,  3.0058e-02,  3.0058e-02,  2.9801e-02,  3.0058e-02,\n",
            "          4.5823e-02,  3.1133e-02,  3.5519e-02, -3.8896e-02,  1.0017e-01,\n",
            "          3.5519e-02,  3.1133e-02,  3.1133e-02, -2.6150e-02,  4.0488e-02,\n",
            "          4.0488e-02,  3.1133e-02,  3.0058e-02,  3.0058e-02,  5.8624e-02,\n",
            "          8.5913e-02,  1.5658e-01,  7.2781e-02,  2.9801e-02,  3.1123e-02,\n",
            "          4.4723e-02,  1.0385e-01,  1.0385e-01,  4.8471e-02,  2.2945e-02,\n",
            "          4.5941e-02,  2.9801e-02,  3.1133e-02,  5.4040e-02,  2.9801e-02,\n",
            "          2.8171e-02,  7.0048e-02,  1.0147e-01,  2.9801e-02,  8.3738e-02,\n",
            "          2.9801e-02,  2.9801e-02,  3.0058e-02,  3.7292e-02,  3.0058e-02,\n",
            "          2.9801e-02,  3.0058e-02,  3.0058e-02,  3.1133e-02, -3.9220e-03,\n",
            "          3.0058e-02,  2.9801e-02,  3.5519e-02,  2.9801e-02,  2.8171e-02,\n",
            "          9.1239e-02,  6.2024e-02,  3.1553e-02,  4.2647e-02,  4.5918e-02,\n",
            "          2.9801e-02,  3.0058e-02,  3.0058e-02,  3.0058e-02,  3.0058e-02,\n",
            "          3.0058e-02,  3.0058e-02,  3.1133e-02,  4.2647e-02,  4.2647e-02,\n",
            "          4.5918e-02,  1.0147e-01,  5.4510e-02,  2.9801e-02, -3.9220e-03,\n",
            "          5.9013e-03,  6.1549e-02,  3.7292e-02,  4.5918e-02,  2.5025e-02,\n",
            "          3.1133e-02,  3.0058e-02,  3.1133e-02,  4.0632e-02,  3.7292e-02,\n",
            "          1.3018e-01,  3.1133e-02,  2.9801e-02,  2.9801e-02,  2.9801e-02,\n",
            "          3.7292e-02,  3.7292e-02, -1.2429e-02,  3.4650e-02,  7.7949e-02,\n",
            "          3.1133e-02,  3.5519e-02,  2.9801e-02,  7.3689e-02,  1.6916e-01,\n",
            "          1.5771e-01,  1.0194e-01,  1.0194e-01,  2.9801e-02,  3.0058e-02,\n",
            "          2.9801e-02,  2.9801e-02,  5.1556e-02, -9.2519e-03,  2.9801e-02,\n",
            "          4.0632e-02,  2.9801e-02,  3.0058e-02,  3.0058e-02,  3.0058e-02,\n",
            "          3.0058e-02,  3.7292e-02,  5.1556e-02,  9.2316e-02,  1.7268e-01,\n",
            "          1.5518e-01,  4.0632e-02,  4.0632e-02,  1.0552e-01,  4.9290e-02,\n",
            "          2.9801e-02,  3.7292e-02,  7.9875e-02,  2.9801e-02,  2.9801e-02,\n",
            "          2.9801e-02,  3.1133e-02,  4.0488e-02,  6.1549e-02, -3.9220e-03,\n",
            "         -5.5129e+00,  2.9801e-02,  6.1549e-02,  3.1553e-02,  3.1553e-02,\n",
            "         -4.5103e-02, -5.6349e-02,  6.1876e-02,  6.7240e-02,  2.9801e-02,\n",
            "          2.9801e-02,  5.1556e-02,  2.2945e-02,  3.0058e-02,  3.0058e-02,\n",
            "          3.0058e-02,  2.0902e-01,  2.9801e-02,  4.7842e-02,  2.9801e-02,\n",
            "          1.6492e-03,  3.1133e-02,  3.0058e-02,  3.0058e-02,  2.9801e-02,\n",
            "          2.9801e-02, -3.9220e-03,  3.0058e-02,  3.0058e-02,  7.4557e-02,\n",
            "          7.4557e-02,  7.5439e-02,  2.7793e-02,  3.1133e-02,  3.0058e-02]])\n",
            "tensor([[-0.0324, -0.0324, -0.0693, -0.0693,  0.1180,  0.1180, -0.0324, -0.0324,\n",
            "          0.0680,  0.0971,  0.0971,  0.1357,  0.1357, -0.0324, -0.0693, -0.0693,\n",
            "         -0.0827, -0.0827, -0.0693, -0.0504, -0.0827, -0.0436, -0.0827,  0.0279,\n",
            "          0.0279,  0.0279, -0.0469, -0.0469, -0.0693, -0.0827,  0.1357,  0.1357,\n",
            "          0.1357,  0.1357, -0.0693, -0.0693,  0.1357,  0.1180,  0.0971, -0.2375,\n",
            "         -0.2375, -0.0668, -0.0668, -0.0693, -0.0693,  0.0529,  0.0529,  0.0529,\n",
            "         -0.0693, -0.0693,  0.1180,  0.1180,  0.1180,  0.1180,  0.1180,  0.1180,\n",
            "          0.1180, -0.0693, -0.0693, -0.0693, -0.0693,  0.0971,  0.0971,  0.1180,\n",
            "          0.1180,  0.1357,  0.1357, -0.0827, -0.0827, -0.0827, -0.0827, -0.0693,\n",
            "         -0.0693, -0.0693, -0.0827, -0.0827, -0.0827, -0.0827,  0.2608,  0.2608,\n",
            "          0.2608, -0.0693, -0.0693, -0.0559, -0.0693, -0.1559, -0.0778, -0.0827,\n",
            "         -0.0827, -0.0693, -0.0693,  0.1357,  0.1357,  0.1180,  0.1180,  0.1180,\n",
            "          0.1357, -0.0693,  0.1180,  0.1180,  0.1357,  0.1357,  0.1357,  0.1357,\n",
            "         -0.0693, -0.0693, -0.0693, -0.0827,  0.1357,  0.1180,  0.1357,  0.1357,\n",
            "          0.0971, -0.0693, -0.0827,  0.1357,  0.1180,  0.1357,  0.1357,  0.1357,\n",
            "          0.1357,  0.1357, -0.0693, -0.0693, -0.0693,     nan,     nan,  0.1357,\n",
            "          0.1357,  0.1357,  0.1357,  0.1357,  0.1357, -0.0693, -0.0693, -0.0693,\n",
            "         -0.2375, -0.2375,  0.1180,  0.1180,  0.1357,  0.1357,  0.1357, -0.0693,\n",
            "         -0.0693, -0.0693,  0.0547,  0.0547,  0.1180,  0.1180,  0.1357,  0.1357,\n",
            "          0.1357],\n",
            "        [ 0.2480,  0.2480,  0.1302,  0.1302,  0.3003,  0.3003,  0.2480,  0.2480,\n",
            "          0.3114,  0.3043,  0.3043,  0.2901,  0.2901,  0.2480,  0.1302,  0.1302,\n",
            "          0.1372,  0.1372,  0.1302,  0.0239,  0.1372,  0.1280,  0.1372,  0.1559,\n",
            "          0.1559,  0.1559,  0.1607,  0.1607,  0.1302,  0.1372,  0.2901,  0.2901,\n",
            "          0.2901,  0.2901,  0.1302,  0.1302,  0.2901,  0.3003,  0.3043,  0.1715,\n",
            "          0.1715,  0.1484,  0.1484,  0.1302,  0.1302,  0.1568,  0.1568,  0.1568,\n",
            "          0.1302,  0.1302,  0.3003,  0.3003,  0.3003,  0.3003,  0.3003,  0.3003,\n",
            "          0.3003,  0.1302,  0.1302,  0.1302,  0.1302,  0.3043,  0.3043,  0.3003,\n",
            "          0.3003,  0.2901,  0.2901,  0.1372,  0.1372,  0.1372,  0.1372,  0.1302,\n",
            "          0.1302,  0.1302,  0.1372,  0.1372,  0.1372,  0.1372,  0.1674,  0.1674,\n",
            "          0.1674,  0.1302,  0.1302,  0.1485,  0.1302,  0.1756,  0.1201,  0.1372,\n",
            "          0.1372,  0.1302,  0.1302,  0.2901,  0.2901,  0.3003,  0.3003,  0.3003,\n",
            "          0.2901,  0.1302,  0.3003,  0.3003,  0.2901,  0.2901,  0.2901,  0.2901,\n",
            "          0.1302,  0.1302,  0.1302,  0.1372,  0.2901,  0.3003,  0.2901,  0.2901,\n",
            "          0.3043,  0.1302,  0.1372,  0.2901,  0.3003,  0.2901,  0.2901,  0.2901,\n",
            "          0.2901,  0.2901,  0.1302,  0.1302,  0.1302,     nan,     nan,  0.2901,\n",
            "          0.2901,  0.2901,  0.2901,  0.2901,  0.2901,  0.1302,  0.1302,  0.1302,\n",
            "          0.1715,  0.1715,  0.3003,  0.3003,  0.2901,  0.2901,  0.2901,  0.1302,\n",
            "          0.1302,  0.1302,  0.2560,  0.2560,  0.3003,  0.3003,  0.2901,  0.2901,\n",
            "          0.2901]])\n",
            "tensor([[-0.0324, -0.0324, -0.0693, -0.0693,  0.1180,  0.1180, -0.0324, -0.0324,\n",
            "          0.0680,  0.0971,  0.0971,  0.1357,  0.1357, -0.0324, -0.0693, -0.0693,\n",
            "         -0.0827, -0.0827, -0.0693, -0.0504, -0.0827, -0.0436, -0.0827,  0.0279,\n",
            "          0.0279,  0.0279, -0.0469, -0.0469, -0.0693, -0.0827,  0.1357,  0.1357,\n",
            "          0.1357,  0.1357, -0.0693, -0.0693,  0.1357,  0.1180,  0.0971, -0.2375,\n",
            "         -0.2375, -0.0668, -0.0668, -0.0693, -0.0693,  0.0529,  0.0529,  0.0529,\n",
            "         -0.0693, -0.0693,  0.1180,  0.1180,  0.1180,  0.1180,  0.1180,  0.1180,\n",
            "          0.1180, -0.0693, -0.0693, -0.0693, -0.0693,  0.0971,  0.0971,  0.1180,\n",
            "          0.1180,  0.1357,  0.1357, -0.0827, -0.0827, -0.0827, -0.0827, -0.0693,\n",
            "         -0.0693, -0.0693, -0.0827, -0.0827, -0.0827, -0.0827,  0.2608,  0.2608,\n",
            "          0.2608, -0.0693, -0.0693, -0.0559, -0.0693, -0.1559, -0.0778, -0.0827,\n",
            "         -0.0827, -0.0693, -0.0693,  0.1357,  0.1357,  0.1180,  0.1180,  0.1180,\n",
            "          0.1357, -0.0693,  0.1180,  0.1180,  0.1357,  0.1357,  0.1357,  0.1357,\n",
            "         -0.0693, -0.0693, -0.0693, -0.0827,  0.1357,  0.1180,  0.1357,  0.1357,\n",
            "          0.0971, -0.0693, -0.0827,  0.1357,  0.1180,  0.1357,  0.1357,  0.1357,\n",
            "          0.1357,  0.1357, -0.0693, -0.0693, -0.0693, -0.0693, -0.0693,  0.1357,\n",
            "          0.1357,  0.1357,  0.1357,  0.1357,  0.1357, -0.0693, -0.0693, -0.0693,\n",
            "         -0.2375, -0.2375,  0.1180,  0.1180,  0.1357,  0.1357,  0.1357, -0.0693,\n",
            "         -0.0693, -0.0693,  0.0547,  0.0547,  0.1180,  0.1180,  0.1357,  0.1357,\n",
            "          0.1357],\n",
            "        [ 0.2480,  0.2480,  0.1302,  0.1302,  0.3003,  0.3003,  0.2480,  0.2480,\n",
            "          0.3114,  0.3043,  0.3043,  0.2901,  0.2901,  0.2480,  0.1302,  0.1302,\n",
            "          0.1372,  0.1372,  0.1302,  0.0239,  0.1372,  0.1280,  0.1372,  0.1559,\n",
            "          0.1559,  0.1559,  0.1607,  0.1607,  0.1302,  0.1372,  0.2901,  0.2901,\n",
            "          0.2901,  0.2901,  0.1302,  0.1302,  0.2901,  0.3003,  0.3043,  0.1715,\n",
            "          0.1715,  0.1484,  0.1484,  0.1302,  0.1302,  0.1568,  0.1568,  0.1568,\n",
            "          0.1302,  0.1302,  0.3003,  0.3003,  0.3003,  0.3003,  0.3003,  0.3003,\n",
            "          0.3003,  0.1302,  0.1302,  0.1302,  0.1302,  0.3043,  0.3043,  0.3003,\n",
            "          0.3003,  0.2901,  0.2901,  0.1372,  0.1372,  0.1372,  0.1372,  0.1302,\n",
            "          0.1302,  0.1302,  0.1372,  0.1372,  0.1372,  0.1372,  0.1674,  0.1674,\n",
            "          0.1674,  0.1302,  0.1302,  0.1485,  0.1302,  0.1756,  0.1201,  0.1372,\n",
            "          0.1372,  0.1302,  0.1302,  0.2901,  0.2901,  0.3003,  0.3003,  0.3003,\n",
            "          0.2901,  0.1302,  0.3003,  0.3003,  0.2901,  0.2901,  0.2901,  0.2901,\n",
            "          0.1302,  0.1302,  0.1302,  0.1372,  0.2901,  0.3003,  0.2901,  0.2901,\n",
            "          0.3043,  0.1302,  0.1372,  0.2901,  0.3003,  0.2901,  0.2901,  0.2901,\n",
            "          0.2901,  0.2901,  0.1302,  0.1302,  0.1302,  0.1302,  0.1302,  0.2901,\n",
            "          0.2901,  0.2901,  0.2901,  0.2901,  0.2901,  0.1302,  0.1302,  0.1302,\n",
            "          0.1715,  0.1715,  0.3003,  0.3003,  0.2901,  0.2901,  0.2901,  0.1302,\n",
            "          0.1302,  0.1302,  0.2560,  0.2560,  0.3003,  0.3003,  0.2901,  0.2901,\n",
            "          0.2901]])\n",
            "tensor([[0.1200, 0.1200, 0.1200,  ..., 0.1200, 0.1200, 0.1200],\n",
            "        [0.0318, 0.0318, 0.0318,  ..., 0.0318, 0.0318, 0.0318]])\n",
            "tensor([[0.1200, 0.1200, 0.1200,  ..., 0.1200, 0.1200, 0.1200],\n",
            "        [0.0318, 0.0318, 0.0318,  ..., 0.0318, 0.0318, 0.0318]])\n",
            "tensor([[        nan,         nan,         nan,  2.5207e-02,  2.5207e-02,\n",
            "                 nan,         nan,         nan,         nan,  2.5207e-02,\n",
            "          9.0627e-03,  1.6132e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  9.0627e-03,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,  5.0710e-02,  5.0710e-02,  3.7117e-02,  3.5905e-02,\n",
            "          3.5905e-02,  3.5905e-02,  3.5905e-02,  3.5905e-02,  3.5905e-02,\n",
            "          3.5905e-02,  3.5905e-02,  3.5905e-02,  3.5905e-02,  2.5207e-02,\n",
            "                 nan,  1.1919e-04,  1.1919e-04,         nan,         nan,\n",
            "                 nan,         nan,  2.5207e-02,  2.5207e-02,         nan,\n",
            "                 nan,         nan,  2.5207e-02,         nan,         nan,\n",
            "          1.1919e-04,  1.1919e-04,  9.0627e-03,         nan,         nan,\n",
            "                 nan,  9.0627e-03,  1.1919e-04,         nan,         nan,\n",
            "                 nan,  1.1919e-04,  1.1919e-04,  2.5207e-02,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "          2.8308e-02,  2.8518e-02,  2.8308e-02,  2.8518e-02,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.5207e-02,         nan,  1.1919e-04,\n",
            "                 nan,  2.5207e-02,         nan,  2.5207e-02,  1.1919e-04,\n",
            "                 nan,         nan,         nan,  2.5207e-02,         nan,\n",
            "                 nan,  2.5207e-02,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.5207e-02,  2.5207e-02,  1.6132e-02,\n",
            "                 nan,  2.6848e-02,         nan,  1.1919e-04,         nan,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          2.5207e-02,         nan,  1.1919e-04,         nan,  4.2596e-02,\n",
            "         -9.2511e-02,         nan,         nan,  4.5374e-02,         nan,\n",
            "                 nan,         nan,  1.6132e-02,  5.0710e-02,  5.0710e-02,\n",
            "         -6.0483e-03,  4.5164e-02,  1.3538e-02,         nan,  1.1919e-04,\n",
            "          2.5207e-02,         nan,         nan,  1.1919e-04,  2.6848e-02,\n",
            "                 nan,         nan, -1.9197e-01,  1.1504e+01, -2.0404e-01,\n",
            "         -2.0404e-01, -1.9197e-01,  5.0222e-02,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.6848e-02,  2.5207e-02,  2.6848e-02,\n",
            "          1.1919e-04,         nan,  4.2596e-02,  4.5374e-02, -6.3151e-02,\n",
            "         -5.0713e-02, -8.8085e-02,  2.3247e-02,  2.3247e-02,  5.3292e-02,\n",
            "                 nan,         nan,  2.6848e-02,  2.5207e-02,  2.6848e-02,\n",
            "          1.1919e-04,         nan,         nan,  1.1919e-04,         nan,\n",
            "                 nan,         nan,         nan,  2.6848e-02,  2.5207e-02,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.5207e-02,  2.6848e-02,  2.5207e-02,\n",
            "                 nan,  9.0627e-03,         nan,         nan,         nan,\n",
            "          1.1919e-04,  1.1919e-04,         nan,         nan,         nan,\n",
            "                 nan,  8.2188e-02,  1.7606e-01,  3.4734e-02,  2.6848e-02,\n",
            "          1.1919e-04,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,  2.6848e-02,\n",
            "                 nan,  2.5207e-02,  4.2596e-02, -9.1379e-03,  1.5552e-01,\n",
            "          1.5944e-01,         nan,         nan,         nan,         nan,\n",
            "          2.5207e-02,  2.6848e-02,  1.1919e-04,  1.1919e-04,         nan,\n",
            "          1.1919e-04,         nan,         nan,         nan,  1.1919e-04,\n",
            "          2.5207e-02,  1.1919e-04,         nan,  2.6848e-02,  1.1919e-04,\n",
            "          1.1919e-04,  2.6848e-02,  2.6848e-02,  9.0627e-03,  2.5207e-02,\n",
            "                 nan,         nan,  2.6848e-02,         nan],\n",
            "        [        nan,         nan,         nan,  2.1780e-01,  2.1780e-01,\n",
            "                 nan,         nan,         nan,         nan,  2.1780e-01,\n",
            "          1.6679e-01,  2.2725e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  1.6679e-01,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,  1.8878e-01,  1.8878e-01,  8.1414e-02,  8.1445e-02,\n",
            "          8.1445e-02,  8.1445e-02,  8.1445e-02,  8.1445e-02,  8.1445e-02,\n",
            "          8.1445e-02,  8.1445e-02,  8.1445e-02,  8.1445e-02,  2.1780e-01,\n",
            "                 nan,  2.1758e-01,  2.1758e-01,         nan,         nan,\n",
            "                 nan,         nan,  2.1780e-01,  2.1780e-01,         nan,\n",
            "                 nan,         nan,  2.1780e-01,         nan,         nan,\n",
            "          2.1758e-01,  2.1758e-01,  1.6679e-01,         nan,         nan,\n",
            "                 nan,  1.6679e-01,  2.1758e-01,         nan,         nan,\n",
            "                 nan,  2.1758e-01,  2.1758e-01,  2.1780e-01,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "          1.0637e-01,  1.0028e-01,  1.0637e-01,  1.0028e-01,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.1780e-01,         nan,  2.1758e-01,\n",
            "                 nan,  2.1780e-01,         nan,  2.1780e-01,  2.1758e-01,\n",
            "                 nan,         nan,         nan,  2.1780e-01,         nan,\n",
            "                 nan,  2.1780e-01,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.1780e-01,  2.1780e-01,  2.2725e-01,\n",
            "                 nan,  2.1594e-01,         nan,  2.1758e-01,         nan,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1780e-01,         nan,  2.1758e-01,         nan,  2.4028e-01,\n",
            "          5.2232e-02,         nan,         nan,  9.9951e-02,         nan,\n",
            "                 nan,         nan,  2.2725e-01,  1.8878e-01,  1.8878e-01,\n",
            "          1.3315e-01,  1.2668e-01,  1.9344e-01,         nan,  2.1758e-01,\n",
            "          2.1780e-01,         nan,         nan,  2.1758e-01,  2.1594e-01,\n",
            "                 nan,         nan, -8.9569e-02, -3.1716e+00, -6.8026e-02,\n",
            "         -6.8026e-02, -8.9569e-02,  1.8703e-01,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.1594e-01,  2.1780e-01,  2.1594e-01,\n",
            "          2.1758e-01,         nan,  2.4028e-01,  9.9951e-02,  3.8029e-01,\n",
            "         -1.6469e-01, -1.8601e-01,  6.7167e-02,  6.7167e-02,  1.0820e-01,\n",
            "                 nan,         nan,  2.1594e-01,  2.1780e-01,  2.1594e-01,\n",
            "          2.1758e-01,         nan,         nan,  2.1758e-01,         nan,\n",
            "                 nan,         nan,         nan,  2.1594e-01,  2.1780e-01,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  2.1780e-01,  2.1594e-01,  2.1780e-01,\n",
            "                 nan,  1.6679e-01,         nan,         nan,         nan,\n",
            "          2.1758e-01,  2.1758e-01,         nan,         nan,         nan,\n",
            "                 nan,  1.0526e-01, -1.0034e-01,  1.8225e-01,  2.1594e-01,\n",
            "          2.1758e-01,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,  2.1594e-01,\n",
            "                 nan,  2.1780e-01,  2.4028e-01,  9.7386e-02,  1.5406e-01,\n",
            "          1.5385e-01,         nan,         nan,         nan,         nan,\n",
            "          2.1780e-01,  2.1594e-01,  2.1758e-01,  2.1758e-01,         nan,\n",
            "          2.1758e-01,         nan,         nan,         nan,  2.1758e-01,\n",
            "          2.1780e-01,  2.1758e-01,         nan,  2.1594e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1594e-01,  2.1594e-01,  1.6679e-01,  2.1780e-01,\n",
            "                 nan,         nan,  2.1594e-01,         nan]])\n",
            "tensor([[ 2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  9.0627e-03,  1.6132e-02,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  9.0627e-03,\n",
            "          9.0627e-03,  9.0627e-03,  9.0627e-03,  9.0627e-03,  9.0627e-03,\n",
            "          9.0627e-03,  9.0627e-03,  9.0627e-03,  5.0710e-02,  5.0710e-02,\n",
            "          3.7117e-02,  3.5905e-02,  3.5905e-02,  3.5905e-02,  3.5905e-02,\n",
            "          3.5905e-02,  3.5905e-02,  3.5905e-02,  3.5905e-02,  3.5905e-02,\n",
            "          3.5905e-02,  2.5207e-02,  2.5207e-02,  1.1919e-04,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  1.1919e-04,  1.1919e-04,  9.0627e-03,\n",
            "          9.0627e-03,  9.0627e-03,  9.0627e-03,  9.0627e-03,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  2.8308e-02,  2.8518e-02,  2.8308e-02,\n",
            "          2.8518e-02,  2.8518e-02,  2.8518e-02,  2.8518e-02,  2.8518e-02,\n",
            "          2.8518e-02,  2.8518e-02,  2.8518e-02,  2.8518e-02,  2.8518e-02,\n",
            "          2.8518e-02,  2.8518e-02,  2.8518e-02,  2.8518e-02,  2.8518e-02,\n",
            "          2.8518e-02,  2.8518e-02,  2.8518e-02,  2.8518e-02,  2.5207e-02,\n",
            "          2.5207e-02,  1.1919e-04,  1.1919e-04,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  1.6132e-02,  1.6132e-02,  2.6848e-02,  2.6848e-02,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  2.5207e-02,  2.5207e-02,  1.1919e-04,\n",
            "          1.1919e-04,  4.2596e-02, -9.2511e-02, -9.2511e-02, -9.2511e-02,\n",
            "          4.5374e-02,  4.5374e-02,  4.5374e-02,  4.5374e-02,  1.6132e-02,\n",
            "          5.0710e-02,  5.0710e-02, -6.0483e-03,  4.5164e-02,  1.3538e-02,\n",
            "          1.3538e-02,  1.1919e-04,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          1.1919e-04,  2.6848e-02,  2.6848e-02,  2.6848e-02, -1.9197e-01,\n",
            "          1.1504e+01, -2.0404e-01, -2.0404e-01, -1.9197e-01,  5.0222e-02,\n",
            "          5.0222e-02,  5.0222e-02,  5.0222e-02,  5.0222e-02,  5.0222e-02,\n",
            "          5.0222e-02,  5.0222e-02,  5.0222e-02,  5.0222e-02,  2.6848e-02,\n",
            "          2.5207e-02,  2.6848e-02,  1.1919e-04,  1.1919e-04,  4.2596e-02,\n",
            "          4.5374e-02, -6.3151e-02, -5.0713e-02, -8.8085e-02,  2.3247e-02,\n",
            "          2.3247e-02,  5.3292e-02,  5.3292e-02,  5.3292e-02,  2.6848e-02,\n",
            "          2.5207e-02,  2.6848e-02,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          2.6848e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.5207e-02,\n",
            "          2.6848e-02,  2.5207e-02,  2.5207e-02,  9.0627e-03,  9.0627e-03,\n",
            "          9.0627e-03,  9.0627e-03,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  8.2188e-02,  1.7606e-01,\n",
            "          3.4734e-02,  2.6848e-02,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          1.1919e-04,  2.6848e-02,  2.6848e-02,  2.5207e-02,  4.2596e-02,\n",
            "         -9.1379e-03,  1.5552e-01,  1.5944e-01,  1.5944e-01,  1.5944e-01,\n",
            "          1.5944e-01,  1.5944e-01,  2.5207e-02,  2.6848e-02,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,  1.1919e-04,\n",
            "          1.1919e-04,  1.1919e-04,  2.5207e-02,  1.1919e-04,  1.1919e-04,\n",
            "          2.6848e-02,  1.1919e-04,  1.1919e-04,  2.6848e-02,  2.6848e-02,\n",
            "          9.0627e-03,  2.5207e-02,  2.5207e-02,  2.5207e-02,  2.6848e-02,\n",
            "          2.6848e-02],\n",
            "        [ 2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  1.6679e-01,  2.2725e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  1.6679e-01,\n",
            "          1.6679e-01,  1.6679e-01,  1.6679e-01,  1.6679e-01,  1.6679e-01,\n",
            "          1.6679e-01,  1.6679e-01,  1.6679e-01,  1.8878e-01,  1.8878e-01,\n",
            "          8.1414e-02,  8.1445e-02,  8.1445e-02,  8.1445e-02,  8.1445e-02,\n",
            "          8.1445e-02,  8.1445e-02,  8.1445e-02,  8.1445e-02,  8.1445e-02,\n",
            "          8.1445e-02,  2.1780e-01,  2.1780e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  2.1758e-01,  2.1758e-01,  1.6679e-01,\n",
            "          1.6679e-01,  1.6679e-01,  1.6679e-01,  1.6679e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  1.0637e-01,  1.0028e-01,  1.0637e-01,\n",
            "          1.0028e-01,  1.0028e-01,  1.0028e-01,  1.0028e-01,  1.0028e-01,\n",
            "          1.0028e-01,  1.0028e-01,  1.0028e-01,  1.0028e-01,  1.0028e-01,\n",
            "          1.0028e-01,  1.0028e-01,  1.0028e-01,  1.0028e-01,  1.0028e-01,\n",
            "          1.0028e-01,  1.0028e-01,  1.0028e-01,  1.0028e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1758e-01,  2.1758e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.2725e-01,  2.2725e-01,  2.1594e-01,  2.1594e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1780e-01,  2.1780e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.4028e-01,  5.2232e-02,  5.2232e-02,  5.2232e-02,\n",
            "          9.9951e-02,  9.9951e-02,  9.9951e-02,  9.9951e-02,  2.2725e-01,\n",
            "          1.8878e-01,  1.8878e-01,  1.3315e-01,  1.2668e-01,  1.9344e-01,\n",
            "          1.9344e-01,  2.1758e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1758e-01,  2.1594e-01,  2.1594e-01,  2.1594e-01, -8.9569e-02,\n",
            "         -3.1716e+00, -6.8026e-02, -6.8026e-02, -8.9569e-02,  1.8703e-01,\n",
            "          1.8703e-01,  1.8703e-01,  1.8703e-01,  1.8703e-01,  1.8703e-01,\n",
            "          1.8703e-01,  1.8703e-01,  1.8703e-01,  1.8703e-01,  2.1594e-01,\n",
            "          2.1780e-01,  2.1594e-01,  2.1758e-01,  2.1758e-01,  2.4028e-01,\n",
            "          9.9951e-02,  3.8029e-01, -1.6469e-01, -1.8601e-01,  6.7167e-02,\n",
            "          6.7167e-02,  1.0820e-01,  1.0820e-01,  1.0820e-01,  2.1594e-01,\n",
            "          2.1780e-01,  2.1594e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1594e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,\n",
            "          2.1594e-01,  2.1780e-01,  2.1780e-01,  1.6679e-01,  1.6679e-01,\n",
            "          1.6679e-01,  1.6679e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  1.0526e-01, -1.0034e-01,\n",
            "          1.8225e-01,  2.1594e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1594e-01,  2.1594e-01,  2.1780e-01,  2.4028e-01,\n",
            "          9.7386e-02,  1.5406e-01,  1.5385e-01,  1.5385e-01,  1.5385e-01,\n",
            "          1.5385e-01,  1.5385e-01,  2.1780e-01,  2.1594e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1758e-01,  2.1758e-01,  2.1780e-01,  2.1758e-01,  2.1758e-01,\n",
            "          2.1594e-01,  2.1758e-01,  2.1758e-01,  2.1594e-01,  2.1594e-01,\n",
            "          1.6679e-01,  2.1780e-01,  2.1780e-01,  2.1780e-01,  2.1594e-01,\n",
            "          2.1594e-01]])\n",
            "tensor([[    nan,  0.0161,  0.0161,  0.0161,  0.0161,  0.0161,  0.0161,     nan,\n",
            "             nan,     nan, -0.0098, -0.0098, -0.0704, -0.0704,     nan,     nan,\n",
            "         -0.0696,     nan,  0.0131,  0.0083,  0.0161,  0.0268, -0.0261,     nan,\n",
            "          0.0514, -0.0060, -0.0060, -0.0138, -0.0060,     nan,     nan],\n",
            "        [    nan,  0.2273,  0.2273,  0.2273,  0.2273,  0.2273,  0.2273,     nan,\n",
            "             nan,     nan,  0.1430,  0.1430,  0.1611,  0.1611,     nan,     nan,\n",
            "          0.1150,     nan,  0.2076,  0.1710,  0.2273,  0.2159,  0.2214,     nan,\n",
            "          0.1132,  0.1332,  0.1332,  0.1356,  0.1332,     nan,     nan]])\n",
            "tensor([[ 0.0161,  0.0161,  0.0161,  0.0161,  0.0161,  0.0161,  0.0161,  0.0161,\n",
            "          0.0161, -0.0098, -0.0098, -0.0704, -0.0704, -0.0704, -0.0704, -0.0696,\n",
            "         -0.0696,  0.0131,  0.0083,  0.0161,  0.0268, -0.0261, -0.0261,  0.0514,\n",
            "         -0.0060, -0.0060, -0.0138, -0.0060, -0.0060, -0.0060],\n",
            "        [ 0.2273,  0.2273,  0.2273,  0.2273,  0.2273,  0.2273,  0.2273,  0.2273,\n",
            "          0.2273,  0.1430,  0.1430,  0.1611,  0.1611,  0.1611,  0.1611,  0.1150,\n",
            "          0.1150,  0.2076,  0.1710,  0.2273,  0.2159,  0.2214,  0.2214,  0.1132,\n",
            "          0.1332,  0.1332,  0.1356,  0.1332,  0.1332,  0.1332]])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "tensor([], size=(2, 0))\n",
            "tensor([[-0.0976, -0.0976,  0.0134,  0.0120,     nan,  0.0152,  0.0045, -0.0881,\n",
            "         -0.0881,  0.0152,  0.0152,  0.0283, -0.1821, -0.0881, -0.0804, -0.0881,\n",
            "         -0.0881, -0.0881, -0.0881, -0.0881, -0.0881, -0.0953, -0.0953, -0.0953,\n",
            "         -0.0953, -0.0953, -0.0504, -0.0504, -0.0804, -0.0804, -0.0804, -0.0804,\n",
            "         -0.0804, -0.0804, -0.0804, -0.0804, -0.0881, -0.0804],\n",
            "        [-0.0195, -0.0195,  0.0977,  0.1031,     nan,  0.1107,  0.0893,  0.0119,\n",
            "          0.0119,  0.1107,  0.1107,  0.1064, -0.0045,  0.0119,  0.0056,  0.0119,\n",
            "          0.0119,  0.0119,  0.0119,  0.0119,  0.0119,  0.0006,  0.0006,  0.0006,\n",
            "          0.0006,  0.0006,  0.0239,  0.0239,  0.0056,  0.0056,  0.0056,  0.0056,\n",
            "          0.0056,  0.0056,  0.0056,  0.0056,  0.0119,  0.0056]])\n",
            "tensor([[-0.0976, -0.0976,  0.0134,  0.0120,  0.0120,  0.0152,  0.0045, -0.0881,\n",
            "         -0.0881,  0.0152,  0.0152,  0.0283, -0.1821, -0.0881, -0.0804, -0.0881,\n",
            "         -0.0881, -0.0881, -0.0881, -0.0881, -0.0881, -0.0953, -0.0953, -0.0953,\n",
            "         -0.0953, -0.0953, -0.0504, -0.0504, -0.0804, -0.0804, -0.0804, -0.0804,\n",
            "         -0.0804, -0.0804, -0.0804, -0.0804, -0.0881, -0.0804],\n",
            "        [-0.0195, -0.0195,  0.0977,  0.1031,  0.1031,  0.1107,  0.0893,  0.0119,\n",
            "          0.0119,  0.1107,  0.1107,  0.1064, -0.0045,  0.0119,  0.0056,  0.0119,\n",
            "          0.0119,  0.0119,  0.0119,  0.0119,  0.0119,  0.0006,  0.0006,  0.0006,\n",
            "          0.0006,  0.0006,  0.0239,  0.0239,  0.0056,  0.0056,  0.0056,  0.0056,\n",
            "          0.0056,  0.0056,  0.0056,  0.0056,  0.0119,  0.0056]])\n",
            "tensor([[-1.3564e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.4501e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.2566e-01, -8.8191e-02, -8.8191e-02, -8.8191e-02, -8.8191e-02,\n",
            "         -8.8191e-02, -8.8191e-02, -8.8191e-02, -8.8191e-02, -8.8191e-02,\n",
            "         -1.9217e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -3.7919e-02, -1.8945e-02,  1.1446e-02,  1.1446e-02,\n",
            "          1.1446e-02,  6.4766e-02,  6.4766e-02,  6.3886e-02,  6.3886e-02,\n",
            "          6.3886e-02,  3.9823e-02, -7.9748e-02, -1.4891e-01, -1.3564e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -3.0338e-02,  2.8111e-02,\n",
            "          2.9982e-02,  2.9982e-02,  6.5412e-02,  5.4752e-02,  7.9410e-02,\n",
            "                 nan,  8.8891e-02,  5.0935e+00,  4.6888e-02,  4.3324e-02,\n",
            "          2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,\n",
            "          2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,\n",
            "          4.6888e-02,  5.4752e-02,  1.7367e-02,  1.1446e-02,  1.4794e-02,\n",
            "          1.1446e-02,  1.1446e-02,  1.1446e-02,  1.1446e-02, -5.6079e-02,\n",
            "         -1.2566e-01, -1.3564e-01, -1.4891e-01, -1.9217e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.9217e-01, -1.8945e-02,\n",
            "          1.1446e-02,  1.7367e-02,         nan, -5.7371e-04, -1.4891e-01,\n",
            "         -1.9217e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.2628e-01,  1.7367e-02,  1.1446e-02,  1.1446e-02, -5.7371e-04,\n",
            "         -7.3029e-02, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.9217e-01, -1.9217e-01, -1.4891e-01,\n",
            "         -1.9217e-01, -1.9217e-01, -1.3564e-01, -1.9217e-01, -1.3564e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.2628e-01,  1.9948e-02,\n",
            "          2.7403e-02,  2.5816e-02,  2.7403e-02,  1.5197e-02,  4.1088e-03,\n",
            "         -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.9217e-01, -1.4891e-01,\n",
            "         -1.3564e-01, -1.3564e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.6199e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.4891e-01,  1.1446e-02,\n",
            "          1.4794e-02,  1.1446e-02,  1.1446e-02,  2.8518e-02, -1.6199e-01,\n",
            "         -1.9217e-01, -1.3564e-01, -1.3564e-01, -1.9217e-01, -1.3564e-01,\n",
            "          1.1446e-02,  1.1446e-02,  1.1446e-02,  1.1446e-02,  1.2147e-03,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.3564e-01,\n",
            "         -1.3564e-01, -1.4891e-01, -2.7155e-02,  1.7367e-02, -5.7371e-04,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.9217e-01, -1.4891e-01, -1.3564e-01,\n",
            "         -1.3564e-01, -1.3564e-01, -1.6199e-01, -1.4891e-01,  1.7367e-02,\n",
            "                 nan, -1.4891e-01, -1.9217e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -6.0615e-03,         nan,  1.1446e-02, -1.9217e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.1247e-01,  5.1375e-02, -1.4891e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -1.9217e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.3564e-01,  2.6830e-02,\n",
            "         -6.6479e-03, -1.9217e-01, -1.3564e-01],\n",
            "        [ 2.8945e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02, -1.3203e-03,\n",
            "          4.1522e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.0885e-02,  2.0885e-02,  2.8945e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.8945e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          4.7974e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,\n",
            "          6.5214e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,\n",
            "         -1.3203e-03,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          2.0885e-02, -1.4196e-01, -1.0223e-02,  6.9764e-02,  6.9764e-02,\n",
            "          6.9764e-02,  1.3962e-01,  1.3962e-01,  1.4070e-01,  1.4070e-01,\n",
            "          1.4070e-01,  9.3646e-02, -1.4801e-01,  2.0885e-02,  2.8945e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  9.4573e-02,  8.4724e-02,\n",
            "          9.0439e-02,  9.0439e-02,  7.5492e-02,  7.2690e-02,  8.6439e-02,\n",
            "                 nan,  9.1816e-02, -6.9186e+00,  8.5941e-02,  8.3322e-02,\n",
            "          9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,\n",
            "          9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,\n",
            "          8.5941e-02,  7.2690e-02,  6.6724e-02,  6.9764e-02,  7.0762e-02,\n",
            "          6.9764e-02,  6.9764e-02,  6.9764e-02,  6.9764e-02,  8.7013e-02,\n",
            "          4.7974e-02,  2.8945e-02,  2.0885e-02, -1.3203e-03, -1.3203e-03,\n",
            "          2.0885e-02,  2.8945e-02,  2.0885e-02, -1.3203e-03, -1.0223e-02,\n",
            "          6.9764e-02,  6.6724e-02,         nan,  4.0950e-02,  2.0885e-02,\n",
            "         -1.3203e-03,  2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          4.3234e-02,  6.6724e-02,  6.9764e-02,  6.9764e-02,  4.0950e-02,\n",
            "          6.1876e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02, -1.3203e-03, -1.3203e-03,  2.0885e-02,\n",
            "         -1.3203e-03, -1.3203e-03,  2.8945e-02, -1.3203e-03,  2.8945e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  4.3234e-02,  1.1709e-01,\n",
            "          1.1854e-01,  1.1224e-01,  1.1854e-01,  1.1075e-01,  9.4956e-02,\n",
            "          2.0885e-02,  2.8945e-02,  2.0885e-02, -1.3203e-03,  2.0885e-02,\n",
            "          2.8945e-02,  2.8945e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          4.9582e-02,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.0885e-02,  6.9764e-02,\n",
            "          7.0762e-02,  6.9764e-02,  6.9764e-02,  1.0028e-01,  4.9582e-02,\n",
            "         -1.3203e-03,  2.8945e-02,  2.8945e-02, -1.3203e-03,  2.8945e-02,\n",
            "          6.9764e-02,  6.9764e-02,  6.9764e-02,  6.9764e-02,  4.6607e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.8945e-02,\n",
            "          2.8945e-02,  2.0885e-02, -8.3604e-03,  6.6724e-02,  4.0950e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02, -1.3203e-03,  2.0885e-02,  2.8945e-02,\n",
            "          2.8945e-02,  2.8945e-02,  4.9582e-02,  2.0885e-02,  6.6724e-02,\n",
            "                 nan,  2.0885e-02, -1.3203e-03,  2.0885e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          2.0885e-02,  3.8919e-02,         nan,  6.9764e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,  2.0885e-02,\n",
            "          5.0829e-02,  1.2284e-01,  2.0885e-02,  2.0885e-02, -1.3203e-03,\n",
            "         -1.3203e-03, -1.3203e-03,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.8945e-02,  2.8945e-02,  2.8945e-02,  5.7103e-02,\n",
            "          5.4510e-02, -1.3203e-03,  2.8945e-02]])\n",
            "tensor([[-1.3564e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.4501e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.2566e-01, -8.8191e-02, -8.8191e-02, -8.8191e-02, -8.8191e-02,\n",
            "         -8.8191e-02, -8.8191e-02, -8.8191e-02, -8.8191e-02, -8.8191e-02,\n",
            "         -1.9217e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -3.7919e-02, -1.8945e-02,  1.1446e-02,  1.1446e-02,\n",
            "          1.1446e-02,  6.4766e-02,  6.4766e-02,  6.3886e-02,  6.3886e-02,\n",
            "          6.3886e-02,  3.9823e-02, -7.9748e-02, -1.4891e-01, -1.3564e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -3.0338e-02,  2.8111e-02,\n",
            "          2.9982e-02,  2.9982e-02,  6.5412e-02,  5.4752e-02,  7.9410e-02,\n",
            "          7.9410e-02,  8.8891e-02,  5.0935e+00,  4.6888e-02,  4.3324e-02,\n",
            "          2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,\n",
            "          2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,  2.9982e-02,\n",
            "          4.6888e-02,  5.4752e-02,  1.7367e-02,  1.1446e-02,  1.4794e-02,\n",
            "          1.1446e-02,  1.1446e-02,  1.1446e-02,  1.1446e-02, -5.6079e-02,\n",
            "         -1.2566e-01, -1.3564e-01, -1.4891e-01, -1.9217e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.9217e-01, -1.8945e-02,\n",
            "          1.1446e-02,  1.7367e-02,  1.7367e-02, -5.7371e-04, -1.4891e-01,\n",
            "         -1.9217e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.2628e-01,  1.7367e-02,  1.1446e-02,  1.1446e-02, -5.7371e-04,\n",
            "         -7.3029e-02, -1.4891e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.9217e-01, -1.9217e-01, -1.4891e-01,\n",
            "         -1.9217e-01, -1.9217e-01, -1.3564e-01, -1.9217e-01, -1.3564e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.2628e-01,  1.9948e-02,\n",
            "          2.7403e-02,  2.5816e-02,  2.7403e-02,  1.5197e-02,  4.1088e-03,\n",
            "         -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.9217e-01, -1.4891e-01,\n",
            "         -1.3564e-01, -1.3564e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.6199e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.4891e-01,  1.1446e-02,\n",
            "          1.4794e-02,  1.1446e-02,  1.1446e-02,  2.8518e-02, -1.6199e-01,\n",
            "         -1.9217e-01, -1.3564e-01, -1.3564e-01, -1.9217e-01, -1.3564e-01,\n",
            "          1.1446e-02,  1.1446e-02,  1.1446e-02,  1.1446e-02,  1.2147e-03,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.3564e-01,\n",
            "         -1.3564e-01, -1.4891e-01, -2.7155e-02,  1.7367e-02, -5.7371e-04,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.9217e-01, -1.4891e-01, -1.3564e-01,\n",
            "         -1.3564e-01, -1.3564e-01, -1.6199e-01, -1.4891e-01,  1.7367e-02,\n",
            "          1.7367e-02, -1.4891e-01, -1.9217e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.4891e-01, -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -6.0615e-03, -6.0615e-03,  1.1446e-02, -1.9217e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.1247e-01,  5.1375e-02, -1.4891e-01, -1.4891e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -1.9217e-01, -1.4891e-01, -1.4891e-01, -1.4891e-01,\n",
            "         -1.4891e-01, -1.3564e-01, -1.3564e-01, -1.3564e-01,  2.6830e-02,\n",
            "         -6.6479e-03, -1.9217e-01, -1.3564e-01],\n",
            "        [ 2.8945e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02, -1.3203e-03,\n",
            "          4.1522e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.0885e-02,  2.0885e-02,  2.8945e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.8945e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          4.7974e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,\n",
            "          6.5214e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,  6.5214e-02,\n",
            "         -1.3203e-03,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          2.0885e-02, -1.4196e-01, -1.0223e-02,  6.9764e-02,  6.9764e-02,\n",
            "          6.9764e-02,  1.3962e-01,  1.3962e-01,  1.4070e-01,  1.4070e-01,\n",
            "          1.4070e-01,  9.3646e-02, -1.4801e-01,  2.0885e-02,  2.8945e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  9.4573e-02,  8.4724e-02,\n",
            "          9.0439e-02,  9.0439e-02,  7.5492e-02,  7.2690e-02,  8.6439e-02,\n",
            "          8.6439e-02,  9.1816e-02, -6.9186e+00,  8.5941e-02,  8.3322e-02,\n",
            "          9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,\n",
            "          9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,  9.0439e-02,\n",
            "          8.5941e-02,  7.2690e-02,  6.6724e-02,  6.9764e-02,  7.0762e-02,\n",
            "          6.9764e-02,  6.9764e-02,  6.9764e-02,  6.9764e-02,  8.7013e-02,\n",
            "          4.7974e-02,  2.8945e-02,  2.0885e-02, -1.3203e-03, -1.3203e-03,\n",
            "          2.0885e-02,  2.8945e-02,  2.0885e-02, -1.3203e-03, -1.0223e-02,\n",
            "          6.9764e-02,  6.6724e-02,  6.6724e-02,  4.0950e-02,  2.0885e-02,\n",
            "         -1.3203e-03,  2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          4.3234e-02,  6.6724e-02,  6.9764e-02,  6.9764e-02,  4.0950e-02,\n",
            "          6.1876e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02, -1.3203e-03, -1.3203e-03,  2.0885e-02,\n",
            "         -1.3203e-03, -1.3203e-03,  2.8945e-02, -1.3203e-03,  2.8945e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  4.3234e-02,  1.1709e-01,\n",
            "          1.1854e-01,  1.1224e-01,  1.1854e-01,  1.1075e-01,  9.4956e-02,\n",
            "          2.0885e-02,  2.8945e-02,  2.0885e-02, -1.3203e-03,  2.0885e-02,\n",
            "          2.8945e-02,  2.8945e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          4.9582e-02,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.0885e-02,  6.9764e-02,\n",
            "          7.0762e-02,  6.9764e-02,  6.9764e-02,  1.0028e-01,  4.9582e-02,\n",
            "         -1.3203e-03,  2.8945e-02,  2.8945e-02, -1.3203e-03,  2.8945e-02,\n",
            "          6.9764e-02,  6.9764e-02,  6.9764e-02,  6.9764e-02,  4.6607e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.8945e-02,\n",
            "          2.8945e-02,  2.0885e-02, -8.3604e-03,  6.6724e-02,  4.0950e-02,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.0885e-02, -1.3203e-03,  2.0885e-02,  2.8945e-02,\n",
            "          2.8945e-02,  2.8945e-02,  4.9582e-02,  2.0885e-02,  6.6724e-02,\n",
            "          6.6724e-02,  2.0885e-02, -1.3203e-03,  2.0885e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,\n",
            "          2.0885e-02,  3.8919e-02,  3.8919e-02,  6.9764e-02, -1.3203e-03,\n",
            "          2.0885e-02,  2.8945e-02,  2.8945e-02,  2.0885e-02,  2.0885e-02,\n",
            "          5.0829e-02,  1.2284e-01,  2.0885e-02,  2.0885e-02, -1.3203e-03,\n",
            "         -1.3203e-03, -1.3203e-03,  2.0885e-02,  2.0885e-02,  2.0885e-02,\n",
            "          2.0885e-02,  2.8945e-02,  2.8945e-02,  2.8945e-02,  5.7103e-02,\n",
            "          5.4510e-02, -1.3203e-03,  2.8945e-02]])\n",
            "tensor([[-0.1246,     nan, -0.1053, -0.1053, -0.1053,     nan],\n",
            "        [ 0.1670,     nan,  0.1850,  0.1850,  0.1850,     nan]])\n",
            "tensor([[-0.1246, -0.1246, -0.1053, -0.1053, -0.1053, -0.1053],\n",
            "        [ 0.1670,  0.1670,  0.1850,  0.1850,  0.1850,  0.1850]])\n",
            "tensor([[ 0.0268,  0.0197,  0.0197,  0.0452,  0.0295,  0.0295,  0.0300,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197, -0.2827,\n",
            "         -0.2827, -0.2827, -0.0530,  0.0268,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0268,\n",
            "          0.0268,  0.0197,  0.0197,  0.0230,  0.0197,  0.0197,  0.0555,  0.0555,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0454,  0.0197,\n",
            "          0.0197,  0.0197,     nan,  0.0197,  0.0555,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,     nan,\n",
            "          0.0426,  0.0303,  0.0555,  0.0341,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0268,  0.0197],\n",
            "        [ 0.0571,  0.0618,  0.0618,  0.1267,  0.1269,  0.1269,  0.0904,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618, -0.1990,\n",
            "         -0.1990, -0.1990, -0.0287,  0.0571,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0571,\n",
            "          0.0571,  0.0618,  0.0618,  0.0485,  0.1925,  0.1925,  0.1205,  0.1205,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.1000,  0.1925,\n",
            "          0.1925,  0.1925,     nan,  0.1925,  0.1205,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,     nan,\n",
            "          0.2403,  0.1901,  0.1205,  0.0525,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0571,  0.0618]])\n",
            "tensor([[ 0.0268,  0.0197,  0.0197,  0.0452,  0.0295,  0.0295,  0.0300,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197, -0.2827,\n",
            "         -0.2827, -0.2827, -0.0530,  0.0268,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0268,\n",
            "          0.0268,  0.0197,  0.0197,  0.0230,  0.0197,  0.0197,  0.0555,  0.0555,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0454,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0555,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0426,  0.0303,  0.0555,  0.0341,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,  0.0197,\n",
            "          0.0268,  0.0197],\n",
            "        [ 0.0571,  0.0618,  0.0618,  0.1267,  0.1269,  0.1269,  0.0904,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618, -0.1990,\n",
            "         -0.1990, -0.1990, -0.0287,  0.0571,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0571,\n",
            "          0.0571,  0.0618,  0.0618,  0.0485,  0.1925,  0.1925,  0.1205,  0.1205,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.1000,  0.1925,\n",
            "          0.1925,  0.1925,  0.1925,  0.1925,  0.1205,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.2403,  0.1901,  0.1205,  0.0525,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,  0.0618,\n",
            "          0.0571,  0.0618]])\n",
            "tensor([[    nan,     nan, -0.2012, -0.2012, -0.2012, -0.2012,  0.0258,  0.0258,\n",
            "          0.0258,  0.0258,  0.0258,  0.0258,  0.0258,  0.0258,  0.0295,  0.0258,\n",
            "          0.0258,  0.0258,     nan,     nan,  0.0258],\n",
            "        [    nan,     nan, -0.0996, -0.0996, -0.0996, -0.0996,  0.1122,  0.1122,\n",
            "          0.1122,  0.1122,  0.1122,  0.1122,  0.1122,  0.1122,  0.1269,  0.1122,\n",
            "          0.1122,  0.1122,     nan,     nan,  0.1122]])\n",
            "tensor([[-0.2012, -0.2012, -0.2012, -0.2012,  0.0258,  0.0258,  0.0258,  0.0258,\n",
            "          0.0258,  0.0258,  0.0258,  0.0258,  0.0295,  0.0258,  0.0258,  0.0258,\n",
            "          0.0258,  0.0258,  0.0258],\n",
            "        [-0.0996, -0.0996, -0.0996, -0.0996,  0.1122,  0.1122,  0.1122,  0.1122,\n",
            "          0.1122,  0.1122,  0.1122,  0.1122,  0.1269,  0.1122,  0.1122,  0.1122,\n",
            "          0.1122,  0.1122,  0.1122]])\n",
            "tensor([[-0.0514, -0.1109, -0.1109, -0.0628,  0.0747, -0.0293,     nan,     nan,\n",
            "         -0.0750,  0.0026, -0.0405, -0.1257, -0.1032, -0.0881, -0.0881, -0.0881,\n",
            "         -0.0881, -0.0881, -0.0881, -0.0881, -0.0881, -0.0953, -0.0953, -0.0405,\n",
            "          0.0747, -0.0750, -0.0797, -0.0797,     nan, -0.0405, -0.0405, -0.0881,\n",
            "             nan, -0.0953, -0.0953, -0.0953, -0.0405,  0.0747, -0.0766, -0.0405,\n",
            "         -0.0405, -0.0405, -0.0827, -0.0881, -0.0881, -0.0851],\n",
            "        [ 0.3481,  0.0087,  0.0087,  0.0880,  0.1275,  0.0181,     nan,     nan,\n",
            "          0.0224,  0.0016,  0.0107,  0.0480,  0.0092,  0.0119,  0.0119,  0.0119,\n",
            "          0.0119,  0.0119,  0.0119,  0.0119,  0.0119,  0.0006,  0.0006,  0.0107,\n",
            "          0.1275,  0.0224, -0.1480, -0.1480,     nan,  0.0107,  0.0107,  0.0119,\n",
            "             nan,  0.0006,  0.0006,  0.0006,  0.0107,  0.1275,  0.0728,  0.0107,\n",
            "          0.0107,  0.0107,  0.1372,  0.0119,  0.0119, -0.0192]])\n",
            "tensor([[-0.0514, -0.1109, -0.1109, -0.0628,  0.0747, -0.0293, -0.0293, -0.0293,\n",
            "         -0.0750,  0.0026, -0.0405, -0.1257, -0.1032, -0.0881, -0.0881, -0.0881,\n",
            "         -0.0881, -0.0881, -0.0881, -0.0881, -0.0881, -0.0953, -0.0953, -0.0405,\n",
            "          0.0747, -0.0750, -0.0797, -0.0797, -0.0797, -0.0405, -0.0405, -0.0881,\n",
            "         -0.0881, -0.0953, -0.0953, -0.0953, -0.0405,  0.0747, -0.0766, -0.0405,\n",
            "         -0.0405, -0.0405, -0.0827, -0.0881, -0.0881, -0.0851],\n",
            "        [ 0.3481,  0.0087,  0.0087,  0.0880,  0.1275,  0.0181,  0.0181,  0.0181,\n",
            "          0.0224,  0.0016,  0.0107,  0.0480,  0.0092,  0.0119,  0.0119,  0.0119,\n",
            "          0.0119,  0.0119,  0.0119,  0.0119,  0.0119,  0.0006,  0.0006,  0.0107,\n",
            "          0.1275,  0.0224, -0.1480, -0.1480, -0.1480,  0.0107,  0.0107,  0.0119,\n",
            "          0.0119,  0.0006,  0.0006,  0.0006,  0.0107,  0.1275,  0.0728,  0.0107,\n",
            "          0.0107,  0.0107,  0.1372,  0.0119,  0.0119, -0.0192]])\n",
            "tensor([[nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan]])\n",
            "tensor([], size=(2, 0))\n",
            "tensor([[        nan,         nan,  5.6072e-02,         nan,         nan,\n",
            "          8.0328e-03,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  8.0328e-03,         nan,         nan,\n",
            "          2.1401e-01,  2.1172e-01,  2.2548e-01,  2.2548e-01,  2.2548e-01,\n",
            "          2.2548e-01,  2.0603e-01,  1.2851e-01,  3.8782e-02,  3.6674e-02,\n",
            "          2.1401e-01,  2.2548e-01,  2.2548e-01,  2.2548e-01,  2.1401e-01,\n",
            "          7.3612e-02,  4.1497e-03,  2.1401e-01,  2.1401e-01,  2.0313e-01,\n",
            "          2.1172e-01,  8.4759e-02,  1.2694e-02,  4.1088e-03,         nan,\n",
            "         -1.2655e+01,         nan,  2.1401e-01, -1.2655e+01,  4.1088e-03,\n",
            "                 nan, -4.1243e-03, -7.5001e-02,  4.1088e-03,  1.1930e-01,\n",
            "          1.1930e-01,  4.1497e-03,  4.1088e-03,         nan,  1.9504e-01,\n",
            "          2.1401e-01,  2.1401e-01,  2.2899e-01,  2.2548e-01,  1.1903e-01,\n",
            "          3.8782e-02,  8.0328e-03,         nan,  2.1401e-01,  2.2548e-01,\n",
            "          2.2548e-01,  2.1401e-01,  6.0344e-03,  2.1401e-01,  2.0320e-01,\n",
            "          3.2064e-02,  4.1088e-03,         nan,  6.0344e-03,  6.0344e-03,\n",
            "          2.1401e-01,  2.0320e-01,  2.1401e-01,  1.8869e-01,  4.1497e-03,\n",
            "                 nan,  6.0344e-03,  1.9504e-01,         nan,  2.0603e-01,\n",
            "          8.0328e-03,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan],\n",
            "        [        nan,         nan,  8.4755e-02,         nan,         nan,\n",
            "          9.4935e-02,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan,  9.4935e-02,         nan,         nan,\n",
            "          9.3367e-02,  1.0246e-01,  1.2930e-01,  1.2930e-01,  1.2930e-01,\n",
            "          1.2930e-01,  1.0052e-01,  1.1692e-01,  1.2330e-01,  1.0141e-01,\n",
            "          9.3367e-02,  1.2930e-01,  1.2930e-01,  1.2930e-01,  9.3367e-02,\n",
            "          1.0868e-01,  9.3707e-02,  9.3367e-02,  9.3367e-02,  1.0983e-01,\n",
            "          1.0246e-01,  1.0744e-01,  9.7801e-02,  9.4956e-02,         nan,\n",
            "         -5.5129e+00,         nan,  9.3367e-02, -5.5129e+00,  9.4956e-02,\n",
            "                 nan,  4.8370e-02, -6.8469e-02,  9.4956e-02,  1.3513e-01,\n",
            "          1.3513e-01,  9.3707e-02,  9.4956e-02,         nan,  9.0874e-02,\n",
            "          9.3367e-02,  9.3367e-02,  1.3441e-01,  1.2930e-01,  1.1132e-01,\n",
            "          1.2330e-01,  9.4935e-02,         nan,  9.3367e-02,  1.2930e-01,\n",
            "          1.2930e-01,  9.3367e-02,  9.4879e-02,  9.3367e-02,  1.1662e-01,\n",
            "          1.1126e-01,  9.4956e-02,         nan,  9.4879e-02,  9.4879e-02,\n",
            "          9.3367e-02,  1.1662e-01,  9.3367e-02,  1.0740e-01,  9.3707e-02,\n",
            "                 nan,  9.4879e-02,  9.0874e-02,         nan,  1.0052e-01,\n",
            "          9.4935e-02,         nan,         nan,         nan,         nan,\n",
            "                 nan,         nan]])\n",
            "tensor([[ 5.6072e-02,  5.6072e-02,  5.6072e-02,  8.0328e-03,  8.0328e-03,\n",
            "          8.0328e-03,  8.0328e-03,  8.0328e-03,  8.0328e-03,  8.0328e-03,\n",
            "          8.0328e-03,  8.0328e-03,  8.0328e-03,  2.1401e-01,  2.1172e-01,\n",
            "          2.2548e-01,  2.2548e-01,  2.2548e-01,  2.2548e-01,  2.0603e-01,\n",
            "          1.2851e-01,  3.8782e-02,  3.6674e-02,  2.1401e-01,  2.2548e-01,\n",
            "          2.2548e-01,  2.2548e-01,  2.1401e-01,  7.3612e-02,  4.1497e-03,\n",
            "          2.1401e-01,  2.1401e-01,  2.0313e-01,  2.1172e-01,  8.4759e-02,\n",
            "          1.2694e-02,  4.1088e-03,  4.1088e-03, -1.2655e+01, -1.2655e+01,\n",
            "          2.1401e-01, -1.2655e+01,  4.1088e-03,  4.1088e-03, -4.1243e-03,\n",
            "         -7.5001e-02,  4.1088e-03,  1.1930e-01,  1.1930e-01,  4.1497e-03,\n",
            "          4.1088e-03,  4.1088e-03,  1.9504e-01,  2.1401e-01,  2.1401e-01,\n",
            "          2.2899e-01,  2.2548e-01,  1.1903e-01,  3.8782e-02,  8.0328e-03,\n",
            "          8.0328e-03,  2.1401e-01,  2.2548e-01,  2.2548e-01,  2.1401e-01,\n",
            "          6.0344e-03,  2.1401e-01,  2.0320e-01,  3.2064e-02,  4.1088e-03,\n",
            "          4.1088e-03,  6.0344e-03,  6.0344e-03,  2.1401e-01,  2.0320e-01,\n",
            "          2.1401e-01,  1.8869e-01,  4.1497e-03,  4.1497e-03,  6.0344e-03,\n",
            "          1.9504e-01,  1.9504e-01,  2.0603e-01,  8.0328e-03,  8.0328e-03,\n",
            "          8.0328e-03,  8.0328e-03,  8.0328e-03,  8.0328e-03,  8.0328e-03],\n",
            "        [ 8.4755e-02,  8.4755e-02,  8.4755e-02,  9.4935e-02,  9.4935e-02,\n",
            "          9.4935e-02,  9.4935e-02,  9.4935e-02,  9.4935e-02,  9.4935e-02,\n",
            "          9.4935e-02,  9.4935e-02,  9.4935e-02,  9.3367e-02,  1.0246e-01,\n",
            "          1.2930e-01,  1.2930e-01,  1.2930e-01,  1.2930e-01,  1.0052e-01,\n",
            "          1.1692e-01,  1.2330e-01,  1.0141e-01,  9.3367e-02,  1.2930e-01,\n",
            "          1.2930e-01,  1.2930e-01,  9.3367e-02,  1.0868e-01,  9.3707e-02,\n",
            "          9.3367e-02,  9.3367e-02,  1.0983e-01,  1.0246e-01,  1.0744e-01,\n",
            "          9.7801e-02,  9.4956e-02,  9.4956e-02, -5.5129e+00, -5.5129e+00,\n",
            "          9.3367e-02, -5.5129e+00,  9.4956e-02,  9.4956e-02,  4.8370e-02,\n",
            "         -6.8469e-02,  9.4956e-02,  1.3513e-01,  1.3513e-01,  9.3707e-02,\n",
            "          9.4956e-02,  9.4956e-02,  9.0874e-02,  9.3367e-02,  9.3367e-02,\n",
            "          1.3441e-01,  1.2930e-01,  1.1132e-01,  1.2330e-01,  9.4935e-02,\n",
            "          9.4935e-02,  9.3367e-02,  1.2930e-01,  1.2930e-01,  9.3367e-02,\n",
            "          9.4879e-02,  9.3367e-02,  1.1662e-01,  1.1126e-01,  9.4956e-02,\n",
            "          9.4956e-02,  9.4879e-02,  9.4879e-02,  9.3367e-02,  1.1662e-01,\n",
            "          9.3367e-02,  1.0740e-01,  9.3707e-02,  9.3707e-02,  9.4879e-02,\n",
            "          9.0874e-02,  9.0874e-02,  1.0052e-01,  9.4935e-02,  9.4935e-02,\n",
            "          9.4935e-02,  9.4935e-02,  9.4935e-02,  9.4935e-02,  9.4935e-02]])\n",
            "tensor([[ 0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0554,\n",
            "         -0.0679,  0.0662,  0.1106,  0.0820,  0.1112,  0.1106,  0.0820,  0.0820,\n",
            "          0.0820,  0.0820,  0.1112,  0.1112,  0.1112,  0.1106,  0.1106,  0.1112,\n",
            "          0.1112,  0.1106, -0.0679, -0.0679, -0.0679,  0.0554, -0.0679,  0.1106,\n",
            "          0.0368,  0.0718, -0.1181,  0.0718, -0.1340,  0.0662,  0.0662,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.1112,  0.1106,  0.1106,  0.1106,\n",
            "          0.0718,  0.0554,  0.0718,  0.0718, -0.0679,  0.1106,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.1106,\n",
            "         -0.0679, -0.0679,  0.1106,  0.0554,  0.1106,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0662, -0.0679, -0.0679,  0.0718, -0.0679,  0.0718,  0.1106,\n",
            "          0.0718,  0.1115,  0.1106,  0.0554,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0603,  0.0718,  0.0718,  0.0718,  0.0554,  0.0718,  0.0718,\n",
            "          0.1106,  0.0718,  0.1115,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0554, -0.0679, -0.0679,  0.0554,  0.0718,\n",
            "          0.0832,  0.0820,  0.0718,  0.0718,  0.0718, -0.0679, -0.0679,  0.0554,\n",
            "          0.0718,  0.1115,  0.0718,  0.0718,  0.1115,  0.0718,  0.0718,  0.1115,\n",
            "          0.0718,  0.0718, -0.1181, -0.0679,  0.1106,  0.1106,  0.0718,  0.1106,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718, -0.0679,  0.1115,  0.1115,  0.1106,\n",
            "          0.0554,  0.0718,  0.1115,  0.0718,  0.0820,  0.0820,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718, -0.0679, -0.0679,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0662,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718, -0.0679, -0.0679,  0.0718,  0.0718,\n",
            "          0.0718,  0.1106,  0.0662,  0.1115,  0.1106,  0.0718,  0.0554,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0554,  0.0554,  0.1106,  0.0603,  0.1106,\n",
            "          0.0554,  0.0820,  0.1106,     nan,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0554,  0.0718, -0.0679, -0.0679,  0.0554,\n",
            "          0.1115,  0.1106,  0.1106,  0.0718,  0.0718,  0.0718,  0.0662,  0.0435,\n",
            "          0.0662,  0.1115,  0.1115,  0.0820,  0.0820,  0.0718,  0.1944,  0.0718,\n",
            "          0.0718,  0.1106,  0.0718,  0.1106,  0.1115,  0.0718,  0.1115,  0.0718,\n",
            "          0.0718,  0.1115,  0.0718,  0.1106,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0662,  0.0718, -0.0679,  0.0341,  0.0341,  0.0341,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0341,  0.0662,  0.0554,  0.0662,  0.0341,  0.0662,\n",
            "          0.0718,  0.1115,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.1944, -0.3636,  0.0820,  0.0820,  0.1106,  0.1106,\n",
            "          0.0820,  0.0820,  0.0820,  0.0718, -0.1340,  0.0554,  0.0718,  0.0718,\n",
            "          0.0368,  0.1106, -0.0679, -0.0679,  0.0174, -0.0679,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0554,  0.1106,  0.1106,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0341,  0.0718,  0.0554,  0.0554,  0.0662,  0.1106,\n",
            "          0.0718,  0.1106, -0.0309,  0.1115,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.1801,  0.2529,  0.0820,  0.0718,  0.0718,  0.1106,  0.0718,  0.1106,\n",
            "          0.0662,  0.0718,  0.0662,  0.1115,  0.0718, -0.0309,  0.0603,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0662,  0.1106,  0.0662],\n",
            "        [-0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0804,\n",
            "         -0.0343, -0.0541, -0.0788, -0.0769, -0.0794, -0.0788, -0.0769, -0.0769,\n",
            "         -0.0769, -0.0769, -0.0794, -0.0794, -0.0794, -0.0788, -0.0788, -0.0794,\n",
            "         -0.0794, -0.0788, -0.0343, -0.0343, -0.0343, -0.0804, -0.0343, -0.0788,\n",
            "         -0.1078, -0.0718, -0.0407, -0.0718, -0.2202, -0.0541, -0.0541, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0794, -0.0788, -0.0788, -0.0788,\n",
            "         -0.0718, -0.0804, -0.0718, -0.0718, -0.0343, -0.0788, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0788,\n",
            "         -0.0343, -0.0343, -0.0788, -0.0804, -0.0788, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0541, -0.0343, -0.0343, -0.0718, -0.0343, -0.0718, -0.0788,\n",
            "         -0.0718, -0.0516, -0.0788, -0.0804, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0452, -0.0718, -0.0718, -0.0718, -0.0804, -0.0718, -0.0718,\n",
            "         -0.0788, -0.0718, -0.0516, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0804, -0.0343, -0.0343, -0.0804, -0.0718,\n",
            "         -0.0963, -0.0769, -0.0718, -0.0718, -0.0718, -0.0343, -0.0343, -0.0804,\n",
            "         -0.0718, -0.0516, -0.0718, -0.0718, -0.0516, -0.0718, -0.0718, -0.0516,\n",
            "         -0.0718, -0.0718, -0.0407, -0.0343, -0.0788, -0.0788, -0.0718, -0.0788,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0343, -0.0516, -0.0516, -0.0788,\n",
            "         -0.0804, -0.0718, -0.0516, -0.0718, -0.0769, -0.0769, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0343, -0.0343, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0541, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0343, -0.0343, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0788, -0.0541, -0.0516, -0.0788, -0.0718, -0.0804, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0804, -0.0804, -0.0788, -0.0452, -0.0788,\n",
            "         -0.0804, -0.0769, -0.0788,     nan, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0804, -0.0718, -0.0343, -0.0343, -0.0804,\n",
            "         -0.0516, -0.0788, -0.0788, -0.0718, -0.0718, -0.0718, -0.0541, -0.0539,\n",
            "         -0.0541, -0.0516, -0.0516, -0.0769, -0.0769, -0.0718, -0.0758, -0.0718,\n",
            "         -0.0718, -0.0788, -0.0718, -0.0788, -0.0516, -0.0718, -0.0516, -0.0718,\n",
            "         -0.0718, -0.0516, -0.0718, -0.0788, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0541, -0.0718, -0.0343, -0.0948, -0.0948, -0.0948, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0948, -0.0541, -0.0804, -0.0541, -0.0948, -0.0541,\n",
            "         -0.0718, -0.0516, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0758,  0.0326, -0.0769, -0.0769, -0.0788, -0.0788,\n",
            "         -0.0769, -0.0769, -0.0769, -0.0718, -0.2202, -0.0804, -0.0718, -0.0718,\n",
            "         -0.1078, -0.0788, -0.0343, -0.0343, -0.0694, -0.0343, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0804, -0.0788, -0.0788, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0948, -0.0718, -0.0804, -0.0804, -0.0541, -0.0788,\n",
            "         -0.0718, -0.0788, -0.0563, -0.0516, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0843, -0.0678, -0.0769, -0.0718, -0.0718, -0.0788, -0.0718, -0.0788,\n",
            "         -0.0541, -0.0718, -0.0541, -0.0516, -0.0718, -0.0563, -0.0452, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0541, -0.0788, -0.0541]])\n",
            "tensor([[ 0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0554,\n",
            "         -0.0679,  0.0662,  0.1106,  0.0820,  0.1112,  0.1106,  0.0820,  0.0820,\n",
            "          0.0820,  0.0820,  0.1112,  0.1112,  0.1112,  0.1106,  0.1106,  0.1112,\n",
            "          0.1112,  0.1106, -0.0679, -0.0679, -0.0679,  0.0554, -0.0679,  0.1106,\n",
            "          0.0368,  0.0718, -0.1181,  0.0718, -0.1340,  0.0662,  0.0662,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.1112,  0.1106,  0.1106,  0.1106,\n",
            "          0.0718,  0.0554,  0.0718,  0.0718, -0.0679,  0.1106,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.1106,\n",
            "         -0.0679, -0.0679,  0.1106,  0.0554,  0.1106,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0662, -0.0679, -0.0679,  0.0718, -0.0679,  0.0718,  0.1106,\n",
            "          0.0718,  0.1115,  0.1106,  0.0554,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0603,  0.0718,  0.0718,  0.0718,  0.0554,  0.0718,  0.0718,\n",
            "          0.1106,  0.0718,  0.1115,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0554, -0.0679, -0.0679,  0.0554,  0.0718,\n",
            "          0.0832,  0.0820,  0.0718,  0.0718,  0.0718, -0.0679, -0.0679,  0.0554,\n",
            "          0.0718,  0.1115,  0.0718,  0.0718,  0.1115,  0.0718,  0.0718,  0.1115,\n",
            "          0.0718,  0.0718, -0.1181, -0.0679,  0.1106,  0.1106,  0.0718,  0.1106,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718, -0.0679,  0.1115,  0.1115,  0.1106,\n",
            "          0.0554,  0.0718,  0.1115,  0.0718,  0.0820,  0.0820,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718, -0.0679, -0.0679,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0662,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718, -0.0679, -0.0679,  0.0718,  0.0718,\n",
            "          0.0718,  0.1106,  0.0662,  0.1115,  0.1106,  0.0718,  0.0554,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0554,  0.0554,  0.1106,  0.0603,  0.1106,\n",
            "          0.0554,  0.0820,  0.1106,  0.1106,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0554,  0.0718, -0.0679, -0.0679,  0.0554,\n",
            "          0.1115,  0.1106,  0.1106,  0.0718,  0.0718,  0.0718,  0.0662,  0.0435,\n",
            "          0.0662,  0.1115,  0.1115,  0.0820,  0.0820,  0.0718,  0.1944,  0.0718,\n",
            "          0.0718,  0.1106,  0.0718,  0.1106,  0.1115,  0.0718,  0.1115,  0.0718,\n",
            "          0.0718,  0.1115,  0.0718,  0.1106,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0662,  0.0718, -0.0679,  0.0341,  0.0341,  0.0341,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0341,  0.0662,  0.0554,  0.0662,  0.0341,  0.0662,\n",
            "          0.0718,  0.1115,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.1944, -0.3636,  0.0820,  0.0820,  0.1106,  0.1106,\n",
            "          0.0820,  0.0820,  0.0820,  0.0718, -0.1340,  0.0554,  0.0718,  0.0718,\n",
            "          0.0368,  0.1106, -0.0679, -0.0679,  0.0174, -0.0679,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0554,  0.1106,  0.1106,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0341,  0.0718,  0.0554,  0.0554,  0.0662,  0.1106,\n",
            "          0.0718,  0.1106, -0.0309,  0.1115,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.1801,  0.2529,  0.0820,  0.0718,  0.0718,  0.1106,  0.0718,  0.1106,\n",
            "          0.0662,  0.0718,  0.0662,  0.1115,  0.0718, -0.0309,  0.0603,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0718,\n",
            "          0.0718,  0.0718,  0.0718,  0.0718,  0.0718,  0.0662,  0.1106,  0.0662],\n",
            "        [-0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0804,\n",
            "         -0.0343, -0.0541, -0.0788, -0.0769, -0.0794, -0.0788, -0.0769, -0.0769,\n",
            "         -0.0769, -0.0769, -0.0794, -0.0794, -0.0794, -0.0788, -0.0788, -0.0794,\n",
            "         -0.0794, -0.0788, -0.0343, -0.0343, -0.0343, -0.0804, -0.0343, -0.0788,\n",
            "         -0.1078, -0.0718, -0.0407, -0.0718, -0.2202, -0.0541, -0.0541, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0794, -0.0788, -0.0788, -0.0788,\n",
            "         -0.0718, -0.0804, -0.0718, -0.0718, -0.0343, -0.0788, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0788,\n",
            "         -0.0343, -0.0343, -0.0788, -0.0804, -0.0788, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0541, -0.0343, -0.0343, -0.0718, -0.0343, -0.0718, -0.0788,\n",
            "         -0.0718, -0.0516, -0.0788, -0.0804, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0452, -0.0718, -0.0718, -0.0718, -0.0804, -0.0718, -0.0718,\n",
            "         -0.0788, -0.0718, -0.0516, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0804, -0.0343, -0.0343, -0.0804, -0.0718,\n",
            "         -0.0963, -0.0769, -0.0718, -0.0718, -0.0718, -0.0343, -0.0343, -0.0804,\n",
            "         -0.0718, -0.0516, -0.0718, -0.0718, -0.0516, -0.0718, -0.0718, -0.0516,\n",
            "         -0.0718, -0.0718, -0.0407, -0.0343, -0.0788, -0.0788, -0.0718, -0.0788,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0343, -0.0516, -0.0516, -0.0788,\n",
            "         -0.0804, -0.0718, -0.0516, -0.0718, -0.0769, -0.0769, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0343, -0.0343, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0541, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0343, -0.0343, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0788, -0.0541, -0.0516, -0.0788, -0.0718, -0.0804, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0804, -0.0804, -0.0788, -0.0452, -0.0788,\n",
            "         -0.0804, -0.0769, -0.0788, -0.0788, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0804, -0.0718, -0.0343, -0.0343, -0.0804,\n",
            "         -0.0516, -0.0788, -0.0788, -0.0718, -0.0718, -0.0718, -0.0541, -0.0539,\n",
            "         -0.0541, -0.0516, -0.0516, -0.0769, -0.0769, -0.0718, -0.0758, -0.0718,\n",
            "         -0.0718, -0.0788, -0.0718, -0.0788, -0.0516, -0.0718, -0.0516, -0.0718,\n",
            "         -0.0718, -0.0516, -0.0718, -0.0788, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0541, -0.0718, -0.0343, -0.0948, -0.0948, -0.0948, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0948, -0.0541, -0.0804, -0.0541, -0.0948, -0.0541,\n",
            "         -0.0718, -0.0516, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0758,  0.0326, -0.0769, -0.0769, -0.0788, -0.0788,\n",
            "         -0.0769, -0.0769, -0.0769, -0.0718, -0.2202, -0.0804, -0.0718, -0.0718,\n",
            "         -0.1078, -0.0788, -0.0343, -0.0343, -0.0694, -0.0343, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0804, -0.0788, -0.0788, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0948, -0.0718, -0.0804, -0.0804, -0.0541, -0.0788,\n",
            "         -0.0718, -0.0788, -0.0563, -0.0516, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0843, -0.0678, -0.0769, -0.0718, -0.0718, -0.0788, -0.0718, -0.0788,\n",
            "         -0.0541, -0.0718, -0.0541, -0.0516, -0.0718, -0.0563, -0.0452, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0718,\n",
            "         -0.0718, -0.0718, -0.0718, -0.0718, -0.0718, -0.0541, -0.0788, -0.0541]])\n",
            "tensor([[ 5.5378e-02,  5.5378e-02,  5.5378e-02, -2.7144e-01, -2.7144e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.3568e-01, -2.3568e-01, -2.0358e-01, -2.0358e-01,\n",
            "                 nan,         nan,  1.1504e+01, -1.9217e-01, -1.9217e-01,\n",
            "                 nan,         nan, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -2.1415e-01, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -1.4827e-01, -1.7525e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01, -1.9197e-01,\n",
            "         -1.9197e-01, -1.4827e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -3.7919e-02, -3.7919e-02,  1.1504e+01,  1.1504e+01, -3.7919e-02,\n",
            "         -3.7919e-02, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -2.0161e-01, -2.0161e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -2.3568e-01, -2.3568e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -3.7919e-02,         nan,\n",
            "                 nan, -1.4827e-01, -1.4827e-01, -1.4827e-01, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -1.9217e-01, -1.9217e-01, -1.7525e-01,\n",
            "         -1.7525e-01,         nan,         nan, -3.7919e-02, -3.7919e-02,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -2.0358e-01, -2.0358e-01, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -3.7919e-02,         nan, -3.7919e-02,\n",
            "         -1.8210e-01, -1.8210e-01, -1.8210e-01, -2.1415e-01, -2.1415e-01,\n",
            "                 nan,         nan, -2.1415e-01, -2.1415e-01, -2.1415e-01,\n",
            "         -3.7919e-02, -3.7919e-02, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -3.7919e-02, -3.7919e-02,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.8210e-01, -1.8210e-01, -1.8210e-01,         nan,         nan,\n",
            "                 nan,         nan, -1.9217e-01, -1.9217e-01, -1.9217e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -2.6550e-01, -2.6550e-01, -2.7144e-01,\n",
            "         -2.7144e-01, -2.7144e-01, -2.7144e-01, -2.7144e-01, -2.7144e-01,\n",
            "         -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01,\n",
            "         -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01,\n",
            "         -2.6550e-01, -2.1415e-01, -2.1415e-01, -2.1415e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.9081e-01,\n",
            "         -2.9081e-01, -1.9217e-01, -1.9217e-01, -1.9217e-01,         nan,\n",
            "                 nan, -3.7919e-02, -3.7919e-02, -3.7919e-02, -1.8210e-01,\n",
            "         -1.8210e-01, -1.8210e-01, -2.0358e-01, -2.0914e-01, -2.0914e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01,         nan,         nan, -2.1415e-01,\n",
            "                 nan, -3.7919e-02, -3.7919e-02, -1.8210e-01,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan, -2.0358e-01, -2.0358e-01,         nan,         nan,\n",
            "                 nan,         nan,         nan, -1.9217e-01, -1.9217e-01,\n",
            "                 nan,         nan,         nan, -1.7525e-01,         nan,\n",
            "                 nan, -1.7525e-01, -1.7525e-01,         nan, -1.9217e-01,\n",
            "         -1.9217e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -1.8210e-01,         nan,         nan, -3.7919e-02, -3.7919e-02,\n",
            "                 nan,         nan,         nan, -3.7919e-02, -1.7525e-01,\n",
            "         -1.7525e-01,         nan,         nan, -1.9217e-01, -2.1415e-01,\n",
            "         -2.1415e-01, -1.4827e-01, -1.4827e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.4827e-01, -1.4827e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -2.0358e-01, -2.0358e-01,         nan,         nan,\n",
            "                 nan,         nan, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -3.7919e-02,         nan,         nan, -3.7919e-02,\n",
            "                 nan,         nan,         nan, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01,         nan, -1.7525e-01, -1.4827e-01,         nan,\n",
            "                 nan, -2.0358e-01, -2.0358e-01, -2.0914e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01],\n",
            "        [ 9.7781e-02,  9.7781e-02,  9.7781e-02, -6.8797e-02, -6.8797e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02, -3.9797e-02, -3.9797e-02,  5.5009e-03,  5.5009e-03,\n",
            "                 nan,         nan, -3.1716e+00, -1.3203e-03, -1.3203e-03,\n",
            "                 nan,         nan, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01, -9.1177e-03, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "          1.3637e-02,  1.2551e-02,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02, -8.9569e-02,\n",
            "         -8.9569e-02,  1.3637e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "         -1.4196e-01, -1.4196e-01, -3.1716e+00, -3.1716e+00, -1.4196e-01,\n",
            "         -1.4196e-01,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "          5.1280e-02,  5.1280e-02,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02, -3.9797e-02, -3.9797e-02,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03, -1.4196e-01,         nan,\n",
            "                 nan,  1.3637e-02,  1.3637e-02,  1.3637e-02, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.3203e-03, -1.3203e-03,  1.2551e-02,\n",
            "          1.2551e-02,         nan,         nan, -1.4196e-01, -1.4196e-01,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  5.5009e-03,  5.5009e-03, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.4196e-01,         nan, -1.4196e-01,\n",
            "         -4.5164e-03, -4.5164e-03, -4.5164e-03, -9.1177e-03, -9.1177e-03,\n",
            "                 nan,         nan, -9.1177e-03, -9.1177e-03, -9.1177e-03,\n",
            "         -1.4196e-01, -1.4196e-01,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "          5.5009e-03, -1.4196e-01, -1.4196e-01,         nan,         nan,\n",
            "                 nan,         nan,         nan,         nan,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "         -4.5164e-03, -4.5164e-03, -4.5164e-03,         nan,         nan,\n",
            "                 nan,         nan, -1.3203e-03, -1.3203e-03, -1.3203e-03,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02, -7.4587e-02, -7.4587e-02, -6.8797e-02,\n",
            "         -6.8797e-02, -6.8797e-02, -6.8797e-02, -6.8797e-02, -6.8797e-02,\n",
            "         -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02,\n",
            "         -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02,\n",
            "         -7.4587e-02, -9.1177e-03, -9.1177e-03, -9.1177e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02, -8.3353e-02,\n",
            "         -8.3353e-02, -1.3203e-03, -1.3203e-03, -1.3203e-03,         nan,\n",
            "                 nan, -1.4196e-01, -1.4196e-01, -1.4196e-01, -4.5164e-03,\n",
            "         -4.5164e-03, -4.5164e-03,  5.5009e-03,  1.2026e-02,  1.2026e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,         nan,         nan, -9.1177e-03,\n",
            "                 nan, -1.4196e-01, -1.4196e-01, -4.5164e-03,         nan,\n",
            "                 nan,         nan,         nan,         nan,         nan,\n",
            "                 nan,  5.5009e-03,  5.5009e-03,         nan,         nan,\n",
            "                 nan,         nan,         nan, -1.3203e-03, -1.3203e-03,\n",
            "                 nan,         nan,         nan,  1.2551e-02,         nan,\n",
            "                 nan,  1.2551e-02,  1.2551e-02,         nan, -1.3203e-03,\n",
            "         -1.3203e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "         -4.5164e-03,         nan,         nan, -1.4196e-01, -1.4196e-01,\n",
            "                 nan,         nan,         nan, -1.4196e-01,  1.2551e-02,\n",
            "          1.2551e-02,         nan,         nan, -1.3203e-03, -9.1177e-03,\n",
            "         -9.1177e-03,  1.3637e-02,  1.3637e-02,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.3637e-02,  1.3637e-02, -1.3203e-03,\n",
            "         -1.3203e-03,  5.5009e-03,  5.5009e-03,         nan,         nan,\n",
            "                 nan,         nan,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03, -1.4196e-01,         nan,         nan, -1.4196e-01,\n",
            "                 nan,         nan,         nan, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01,  1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,         nan,  1.2551e-02,  1.3637e-02,         nan,\n",
            "                 nan,  5.5009e-03,  5.5009e-03,  1.2026e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02]])\n",
            "tensor([[ 5.5378e-02,  5.5378e-02,  5.5378e-02, -2.7144e-01, -2.7144e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.3568e-01, -2.3568e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01,  1.1504e+01, -1.9217e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -1.9217e-01, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -2.1415e-01, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -1.4827e-01, -1.7525e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01, -1.9197e-01,\n",
            "         -1.9197e-01, -1.4827e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -3.7919e-02, -3.7919e-02,  1.1504e+01,  1.1504e+01, -3.7919e-02,\n",
            "         -3.7919e-02, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -2.0161e-01, -2.0161e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -2.3568e-01, -2.3568e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -1.4827e-01, -1.4827e-01, -1.4827e-01, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -1.9217e-01, -1.9217e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -3.7919e-02, -3.7919e-02,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -2.0358e-01, -2.0358e-01, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -1.8210e-01, -1.8210e-01, -1.8210e-01, -2.1415e-01, -2.1415e-01,\n",
            "         -2.1415e-01, -2.1415e-01, -2.1415e-01, -2.1415e-01, -2.1415e-01,\n",
            "         -3.7919e-02, -3.7919e-02, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.8210e-01, -1.8210e-01, -1.8210e-01, -1.8210e-01, -1.8210e-01,\n",
            "         -1.8210e-01, -1.8210e-01, -1.9217e-01, -1.9217e-01, -1.9217e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -2.6550e-01, -2.6550e-01, -2.7144e-01,\n",
            "         -2.7144e-01, -2.7144e-01, -2.7144e-01, -2.7144e-01, -2.7144e-01,\n",
            "         -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01,\n",
            "         -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01,\n",
            "         -2.6550e-01, -2.1415e-01, -2.1415e-01, -2.1415e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -2.6550e-01, -2.6550e-01, -2.6550e-01, -2.9081e-01,\n",
            "         -2.9081e-01, -1.9217e-01, -1.9217e-01, -1.9217e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -3.7919e-02, -3.7919e-02, -3.7919e-02, -1.8210e-01,\n",
            "         -1.8210e-01, -1.8210e-01, -2.0358e-01, -2.0914e-01, -2.0914e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -2.1415e-01,\n",
            "         -2.1415e-01, -3.7919e-02, -3.7919e-02, -1.8210e-01, -1.8210e-01,\n",
            "         -1.8210e-01, -1.8210e-01, -1.8210e-01, -1.8210e-01, -1.8210e-01,\n",
            "         -1.8210e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -1.9217e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -1.9217e-01, -1.9217e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01, -2.0358e-01,\n",
            "         -1.8210e-01, -1.8210e-01, -1.8210e-01, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.9217e-01, -2.1415e-01,\n",
            "         -2.1415e-01, -1.4827e-01, -1.4827e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.4827e-01, -1.4827e-01, -1.9217e-01,\n",
            "         -1.9217e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01, -2.0358e-01,\n",
            "         -2.0358e-01, -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02, -3.7919e-02,\n",
            "         -3.7919e-02, -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -2.0358e-01, -2.0358e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01, -1.4827e-01, -1.4827e-01,\n",
            "         -1.4827e-01, -2.0358e-01, -2.0358e-01, -2.0914e-01, -1.7525e-01,\n",
            "         -1.7525e-01, -1.7525e-01, -1.7525e-01],\n",
            "        [ 9.7781e-02,  9.7781e-02,  9.7781e-02, -6.8797e-02, -6.8797e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02, -3.9797e-02, -3.9797e-02,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03, -3.1716e+00, -1.3203e-03, -1.3203e-03,\n",
            "         -1.3203e-03, -1.3203e-03, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01, -9.1177e-03, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "          1.3637e-02,  1.2551e-02,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02, -8.9569e-02,\n",
            "         -8.9569e-02,  1.3637e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "         -1.4196e-01, -1.4196e-01, -3.1716e+00, -3.1716e+00, -1.4196e-01,\n",
            "         -1.4196e-01,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "          5.1280e-02,  5.1280e-02,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02, -3.9797e-02, -3.9797e-02,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01,  1.3637e-02,  1.3637e-02,  1.3637e-02, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.3203e-03, -1.3203e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02, -1.4196e-01, -1.4196e-01,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  5.5009e-03,  5.5009e-03, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -4.5164e-03, -4.5164e-03, -4.5164e-03, -9.1177e-03, -9.1177e-03,\n",
            "         -9.1177e-03, -9.1177e-03, -9.1177e-03, -9.1177e-03, -9.1177e-03,\n",
            "         -1.4196e-01, -1.4196e-01,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "          5.5009e-03, -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "         -4.5164e-03, -4.5164e-03, -4.5164e-03, -4.5164e-03, -4.5164e-03,\n",
            "         -4.5164e-03, -4.5164e-03, -1.3203e-03, -1.3203e-03, -1.3203e-03,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02, -7.4587e-02, -7.4587e-02, -6.8797e-02,\n",
            "         -6.8797e-02, -6.8797e-02, -6.8797e-02, -6.8797e-02, -6.8797e-02,\n",
            "         -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02,\n",
            "         -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02,\n",
            "         -7.4587e-02, -9.1177e-03, -9.1177e-03, -9.1177e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02, -7.4587e-02, -7.4587e-02, -7.4587e-02, -8.3353e-02,\n",
            "         -8.3353e-02, -1.3203e-03, -1.3203e-03, -1.3203e-03, -1.3203e-03,\n",
            "         -1.3203e-03, -1.4196e-01, -1.4196e-01, -1.4196e-01, -4.5164e-03,\n",
            "         -4.5164e-03, -4.5164e-03,  5.5009e-03,  1.2026e-02,  1.2026e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02, -9.1177e-03,\n",
            "         -9.1177e-03, -1.4196e-01, -1.4196e-01, -4.5164e-03, -4.5164e-03,\n",
            "         -4.5164e-03, -4.5164e-03, -4.5164e-03, -4.5164e-03, -4.5164e-03,\n",
            "         -4.5164e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03, -1.3203e-03, -1.3203e-03,\n",
            "         -1.3203e-03, -1.3203e-03, -1.3203e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02, -1.3203e-03,\n",
            "         -1.3203e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,  5.5009e-03,\n",
            "         -4.5164e-03, -4.5164e-03, -4.5164e-03, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02, -1.3203e-03, -9.1177e-03,\n",
            "         -9.1177e-03,  1.3637e-02,  1.3637e-02,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.3637e-02,  1.3637e-02, -1.3203e-03,\n",
            "         -1.3203e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,  5.5009e-03,\n",
            "          5.5009e-03, -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01, -1.4196e-01,\n",
            "         -1.4196e-01,  1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  5.5009e-03,  5.5009e-03,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02,  1.3637e-02,  1.3637e-02,\n",
            "          1.3637e-02,  5.5009e-03,  5.5009e-03,  1.2026e-02,  1.2551e-02,\n",
            "          1.2551e-02,  1.2551e-02,  1.2551e-02]])\n",
            "tensor([[0.0091, 0.0248, 0.0248, 0.0489, 0.0091, 0.0248, 0.0248, 0.0426, 0.0567,\n",
            "         0.0680, 0.0646, 0.0135, 0.1056, 0.1056, 0.0853, 0.0853, 0.0248, 0.0248,\n",
            "         0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0764, 0.0764, 0.0853, 0.0853,\n",
            "         0.0900,    nan,    nan, 0.0737, 0.0737, 0.0639, 0.0639, 0.0578, 0.0578,\n",
            "         0.0578, 0.0578, 0.0578, 0.0578, 0.0578, 0.0248, 0.0248, 0.0248, 0.0279,\n",
            "         0.0764, 0.0872],\n",
            "        [0.1668, 0.2042, 0.2042, 0.1971, 0.1668, 0.2042, 0.2042, 0.2403, 0.1479,\n",
            "         0.1445, 0.1430, 0.1934, 0.2244, 0.2244, 0.2222, 0.2222, 0.2042, 0.2042,\n",
            "         0.2042, 0.2042, 0.2042, 0.2042, 0.2042, 0.2102, 0.2102, 0.2222, 0.2222,\n",
            "         0.2102,    nan,    nan, 0.1676, 0.1676, 0.1407, 0.1407, 0.1280, 0.1280,\n",
            "         0.1280, 0.1280, 0.1280, 0.1280, 0.1280, 0.1578, 0.1578, 0.1578, 0.1447,\n",
            "         0.2102, 0.2157]])\n",
            "tensor([[0.0091, 0.0248, 0.0248, 0.0489, 0.0091, 0.0248, 0.0248, 0.0426, 0.0567,\n",
            "         0.0680, 0.0646, 0.0135, 0.1056, 0.1056, 0.0853, 0.0853, 0.0248, 0.0248,\n",
            "         0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0764, 0.0764, 0.0853, 0.0853,\n",
            "         0.0900, 0.0900, 0.0900, 0.0737, 0.0737, 0.0639, 0.0639, 0.0578, 0.0578,\n",
            "         0.0578, 0.0578, 0.0578, 0.0578, 0.0578, 0.0248, 0.0248, 0.0248, 0.0279,\n",
            "         0.0764, 0.0872],\n",
            "        [0.1668, 0.2042, 0.2042, 0.1971, 0.1668, 0.2042, 0.2042, 0.2403, 0.1479,\n",
            "         0.1445, 0.1430, 0.1934, 0.2244, 0.2244, 0.2222, 0.2222, 0.2042, 0.2042,\n",
            "         0.2042, 0.2042, 0.2042, 0.2042, 0.2042, 0.2102, 0.2102, 0.2222, 0.2222,\n",
            "         0.2102, 0.2102, 0.2102, 0.1676, 0.1676, 0.1407, 0.1407, 0.1280, 0.1280,\n",
            "         0.1280, 0.1280, 0.1280, 0.1280, 0.1280, 0.1578, 0.1578, 0.1578, 0.1447,\n",
            "         0.2102, 0.2157]])\n",
            "tensor([[ 1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04, -3.3038e-02,\n",
            "         -1.1205e-01,  2.5354e-02, -8.2365e-03,  1.2687e-03, -1.1810e-01,\n",
            "          1.8484e-04,  1.8484e-04, -6.7897e-02, -9.9211e-02,  2.5597e-03,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03, -1.1810e-01,\n",
            "          1.8484e-04, -1.1810e-01, -1.1810e-01, -1.8582e-02,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04, -1.1209e-01,\n",
            "          1.7414e-02, -3.7919e-02, -1.6432e-01,  1.8484e-04,  1.8484e-04,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03, -7.9748e-02,\n",
            "                 nan, -9.2939e-02, -9.2939e-02,  1.8484e-04,  1.8484e-04,\n",
            "          5.9935e-03, -1.0991e-01, -1.8775e-01, -2.0462e-01,  1.8484e-04,\n",
            "          1.8484e-04, -1.3564e-01, -1.9197e-01, -1.6432e-01,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04, -8.2365e-03, -1.2832e-01, -2.3166e-01,\n",
            "         -1.9197e-01, -1.1810e-01,  1.8484e-04, -1.0878e-01,  1.2687e-03,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.8484e-04,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04,  1.8484e-04,  1.2687e-03,  1.2687e-03,\n",
            "         -1.1205e-01, -2.3306e-01, -2.8044e-01, -1.9197e-01,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04],\n",
            "        [-1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.1044e-01,\n",
            "         -1.4094e-01, -1.4661e-01, -1.4025e-01, -8.4242e-02, -4.0690e-02,\n",
            "         -1.5322e-01, -1.5322e-01, -3.4308e-02,  1.0253e-02,  1.6437e-03,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -4.0690e-02,\n",
            "         -1.5322e-01, -4.0690e-02, -4.0690e-02, -1.1944e-01, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01, -6.1924e-02,\n",
            "         -6.9393e-02, -1.4196e-01, -6.7221e-02, -1.5322e-01, -1.5322e-01,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -1.4801e-01,\n",
            "                 nan, -3.3444e-02, -3.3444e-02, -1.5322e-01, -1.5322e-01,\n",
            "         -1.0574e-01,  8.5491e-02,  4.0232e-02, -9.0100e-02, -1.5322e-01,\n",
            "         -1.5322e-01,  2.8945e-02, -8.9569e-02, -6.7221e-02, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.4025e-01, -1.2321e-01, -6.5180e-02,\n",
            "         -8.9569e-02, -4.0690e-02, -1.5322e-01, -3.5398e-02, -8.4242e-02,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -1.5322e-01, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.5322e-01, -8.4242e-02, -8.4242e-02,\n",
            "         -1.4094e-01, -8.3154e-02, -9.4369e-02, -8.9569e-02, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01]])\n",
            "tensor([[ 1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04, -3.3038e-02,\n",
            "         -1.1205e-01,  2.5354e-02, -8.2365e-03,  1.2687e-03, -1.1810e-01,\n",
            "          1.8484e-04,  1.8484e-04, -6.7897e-02, -9.9211e-02,  2.5597e-03,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03, -1.1810e-01,\n",
            "          1.8484e-04, -1.1810e-01, -1.1810e-01, -1.8582e-02,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04, -1.1209e-01,\n",
            "          1.7414e-02, -3.7919e-02, -1.6432e-01,  1.8484e-04,  1.8484e-04,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03, -7.9748e-02,\n",
            "         -7.9748e-02, -9.2939e-02, -9.2939e-02,  1.8484e-04,  1.8484e-04,\n",
            "          5.9935e-03, -1.0991e-01, -1.8775e-01, -2.0462e-01,  1.8484e-04,\n",
            "          1.8484e-04, -1.3564e-01, -1.9197e-01, -1.6432e-01,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04, -8.2365e-03, -1.2832e-01, -2.3166e-01,\n",
            "         -1.9197e-01, -1.1810e-01,  1.8484e-04, -1.0878e-01,  1.2687e-03,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.8484e-04,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04,  1.8484e-04,  1.2687e-03,  1.2687e-03,\n",
            "         -1.1205e-01, -2.3306e-01, -2.8044e-01, -1.9197e-01,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04,  1.8484e-04,\n",
            "          1.2687e-03,  1.2687e-03,  1.2687e-03,  1.2687e-03,  1.8484e-04,\n",
            "          1.8484e-04,  1.8484e-04],\n",
            "        [-1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.1044e-01,\n",
            "         -1.4094e-01, -1.4661e-01, -1.4025e-01, -8.4242e-02, -4.0690e-02,\n",
            "         -1.5322e-01, -1.5322e-01, -3.4308e-02,  1.0253e-02,  1.6437e-03,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -4.0690e-02,\n",
            "         -1.5322e-01, -4.0690e-02, -4.0690e-02, -1.1944e-01, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01, -6.1924e-02,\n",
            "         -6.9393e-02, -1.4196e-01, -6.7221e-02, -1.5322e-01, -1.5322e-01,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -1.4801e-01,\n",
            "         -1.4801e-01, -3.3444e-02, -3.3444e-02, -1.5322e-01, -1.5322e-01,\n",
            "         -1.0574e-01,  8.5491e-02,  4.0232e-02, -9.0100e-02, -1.5322e-01,\n",
            "         -1.5322e-01,  2.8945e-02, -8.9569e-02, -6.7221e-02, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.4025e-01, -1.2321e-01, -6.5180e-02,\n",
            "         -8.9569e-02, -4.0690e-02, -1.5322e-01, -3.5398e-02, -8.4242e-02,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -1.5322e-01, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.5322e-01, -8.4242e-02, -8.4242e-02,\n",
            "         -1.4094e-01, -8.3154e-02, -9.4369e-02, -8.9569e-02, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01, -1.5322e-01,\n",
            "         -8.4242e-02, -8.4242e-02, -8.4242e-02, -8.4242e-02, -1.5322e-01,\n",
            "         -1.5322e-01, -1.5322e-01]])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
            "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]])\n",
            "tensor([], size=(2, 0))\n",
            "tensor([[-0.1232, -0.1232, -0.1232, -0.2901, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.2497, -0.2497, -0.2520, -0.2520, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.2520, -0.1288,\n",
            "         -0.1257, -0.1300, -0.2520, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.2901, -0.2497, -0.2520, -0.2497, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.2402, -0.2641, -0.2402, -0.2520, -0.4053,\n",
            "         -0.6660, -0.6230, -0.6077, -0.2497, -0.2497, -0.2497, -0.2012, -0.2012,\n",
            "         -0.2012, -0.2012, -0.2012, -0.2012, -0.2012, -0.2224, -0.2497, -0.2520,\n",
            "         -0.3201, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.2497,\n",
            "         -0.2402, -0.2520, -0.2497, -0.2520, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.2497, -0.2641, -0.0718, -0.2497, -0.2901,\n",
            "         -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3461, -0.2497, -0.2520,\n",
            "         -0.2520, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.2901, -0.1232, -0.0833, -0.0732, -0.0732, -0.1047, -0.2402, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.3358, -0.2497, -0.2299,     nan,     nan,\n",
            "         -0.2012, -0.0920,  0.0041, -0.3358, -0.3358],\n",
            "        [ 0.0539,  0.0539,  0.0539,  0.0964,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0795,  0.0795,  0.0922,  0.0922,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0922,  0.0611,\n",
            "          0.0480,  0.0522,  0.0922,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0964,  0.0795,  0.0922,  0.0795,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0805,  0.0823,  0.0805,  0.0922,  0.1283,\n",
            "          0.0438,  0.0650,  0.0740,  0.0795,  0.0795,  0.0795, -0.0996, -0.0996,\n",
            "         -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0829,  0.0795,  0.0922,\n",
            "          0.0963,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0795,\n",
            "          0.0805,  0.0922,  0.0795,  0.0922,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0795,  0.0823,  0.0493,  0.0795,  0.0964,\n",
            "          0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0959,  0.0795,  0.0922,\n",
            "          0.0922,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0964,  0.0539,  0.0282,  0.0332,  0.0332,  0.0422,  0.0805,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0961,  0.0795, -0.0528,     nan,     nan,\n",
            "         -0.0996,  0.0083,  0.0937,  0.0961,  0.0961]])\n",
            "tensor([[-0.1232, -0.1232, -0.1232, -0.2901, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.2497, -0.2497, -0.2520, -0.2520, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.2520, -0.1288,\n",
            "         -0.1257, -0.1300, -0.2520, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.2901, -0.2497, -0.2520, -0.2497, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.2402, -0.2641, -0.2402, -0.2520, -0.4053,\n",
            "         -0.6660, -0.6230, -0.6077, -0.2497, -0.2497, -0.2497, -0.2012, -0.2012,\n",
            "         -0.2012, -0.2012, -0.2012, -0.2012, -0.2012, -0.2224, -0.2497, -0.2520,\n",
            "         -0.3201, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.2497,\n",
            "         -0.2402, -0.2520, -0.2497, -0.2520, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.2497, -0.2641, -0.0718, -0.2497, -0.2901,\n",
            "         -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3461, -0.2497, -0.2520,\n",
            "         -0.2520, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358, -0.3358,\n",
            "         -0.2901, -0.1232, -0.0833, -0.0732, -0.0732, -0.1047, -0.2402, -0.3358,\n",
            "         -0.3358, -0.3358, -0.3358, -0.3358, -0.2497, -0.2299, -0.2299, -0.2299,\n",
            "         -0.2012, -0.0920,  0.0041, -0.3358, -0.3358],\n",
            "        [ 0.0539,  0.0539,  0.0539,  0.0964,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0795,  0.0795,  0.0922,  0.0922,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0922,  0.0611,\n",
            "          0.0480,  0.0522,  0.0922,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0964,  0.0795,  0.0922,  0.0795,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0805,  0.0823,  0.0805,  0.0922,  0.1283,\n",
            "          0.0438,  0.0650,  0.0740,  0.0795,  0.0795,  0.0795, -0.0996, -0.0996,\n",
            "         -0.0996, -0.0996, -0.0996, -0.0996, -0.0996, -0.0829,  0.0795,  0.0922,\n",
            "          0.0963,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0795,\n",
            "          0.0805,  0.0922,  0.0795,  0.0922,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0795,  0.0823,  0.0493,  0.0795,  0.0964,\n",
            "          0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0959,  0.0795,  0.0922,\n",
            "          0.0922,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,  0.0961,\n",
            "          0.0964,  0.0539,  0.0282,  0.0332,  0.0332,  0.0422,  0.0805,  0.0961,\n",
            "          0.0961,  0.0961,  0.0961,  0.0961,  0.0795, -0.0528, -0.0528, -0.0528,\n",
            "         -0.0996,  0.0083,  0.0937,  0.0961,  0.0961]])\n",
            "tensor([[-0.0383, -0.0383,  0.0979,  0.0979,  0.0979,  0.0912,  0.0912,  0.0912,\n",
            "          0.1082,  0.0979,  0.0979,  0.0979,  0.0979,  0.0979,  0.0979,  0.0912,\n",
            "          0.0910,  0.0910,  0.0787,  0.0787,  0.0787,  0.0787,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,  0.0818,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,  0.0818,  0.0822,  0.0655,\n",
            "             nan,     nan,     nan,     nan,  0.0818,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,  0.0818,     nan,     nan,     nan,  0.0818,\n",
            "             nan,     nan,  0.0818,     nan,  0.0818,  0.0818,     nan,  0.0818,\n",
            "             nan,     nan,     nan,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0769,     nan,     nan,     nan,     nan,     nan,  0.0769,\n",
            "             nan,     nan,     nan,     nan,  0.0769,     nan,     nan,     nan,\n",
            "             nan,  0.0769,     nan,  0.0818,  0.0818,     nan,     nan,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0769,  0.0818,     nan,  0.0818,     nan,\n",
            "             nan,     nan,     nan,  0.0818,  0.0818,  0.0818,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,  0.0818,     nan,  0.0818,\n",
            "          0.0818,  0.0818,     nan,  0.0818,  0.0818,     nan,     nan,  0.0818],\n",
            "        [ 0.0912,  0.0912,  0.1030,  0.1030,  0.1030,  0.1068,  0.1068,  0.1068,\n",
            "          0.1053,  0.1030,  0.1030,  0.1030,  0.1030,  0.1030,  0.1030,  0.1068,\n",
            "          0.1014,  0.1014,  0.1089,  0.1089,  0.1089,  0.1089,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,  0.1095,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,  0.1095,  0.1053,  0.1100,\n",
            "             nan,     nan,     nan,     nan,  0.1095,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,  0.1095,     nan,     nan,     nan,  0.1095,\n",
            "             nan,     nan,  0.1095,     nan,  0.1095,  0.1095,     nan,  0.1095,\n",
            "             nan,     nan,     nan,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1158,     nan,     nan,     nan,     nan,     nan,  0.1158,\n",
            "             nan,     nan,     nan,     nan,  0.1158,     nan,     nan,     nan,\n",
            "             nan,  0.1158,     nan,  0.1095,  0.1095,     nan,     nan,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1158,  0.1095,     nan,  0.1095,     nan,\n",
            "             nan,     nan,     nan,  0.1095,  0.1095,  0.1095,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,     nan,     nan,     nan,  0.1095,     nan,  0.1095,\n",
            "          0.1095,  0.1095,     nan,  0.1095,  0.1095,     nan,     nan,  0.1095]])\n",
            "tensor([[-0.0383, -0.0383,  0.0979,  0.0979,  0.0979,  0.0912,  0.0912,  0.0912,\n",
            "          0.1082,  0.0979,  0.0979,  0.0979,  0.0979,  0.0979,  0.0979,  0.0912,\n",
            "          0.0910,  0.0910,  0.0787,  0.0787,  0.0787,  0.0787,  0.0787,  0.0787,\n",
            "          0.0787,  0.0787,  0.0787,  0.0787,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0822,  0.0655,\n",
            "          0.0655,  0.0655,  0.0655,  0.0655,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0769,  0.0769,  0.0769,  0.0769,  0.0769,  0.0769,  0.0769,\n",
            "          0.0769,  0.0769,  0.0769,  0.0769,  0.0769,  0.0769,  0.0769,  0.0769,\n",
            "          0.0769,  0.0769,  0.0769,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0769,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,\n",
            "          0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818,  0.0818],\n",
            "        [ 0.0912,  0.0912,  0.1030,  0.1030,  0.1030,  0.1068,  0.1068,  0.1068,\n",
            "          0.1053,  0.1030,  0.1030,  0.1030,  0.1030,  0.1030,  0.1030,  0.1068,\n",
            "          0.1014,  0.1014,  0.1089,  0.1089,  0.1089,  0.1089,  0.1089,  0.1089,\n",
            "          0.1089,  0.1089,  0.1089,  0.1089,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1053,  0.1100,\n",
            "          0.1100,  0.1100,  0.1100,  0.1100,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1158,  0.1158,  0.1158,  0.1158,  0.1158,  0.1158,  0.1158,\n",
            "          0.1158,  0.1158,  0.1158,  0.1158,  0.1158,  0.1158,  0.1158,  0.1158,\n",
            "          0.1158,  0.1158,  0.1158,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1158,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,\n",
            "          0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095,  0.1095]])\n",
            "tensor([[    nan,     nan, -0.0690, -0.0971, -0.1393, -0.0690, -0.1351, -0.0697,\n",
            "         -0.0922, -0.0512, -0.0697],\n",
            "        [    nan,     nan,  0.0864,  0.0823,  0.0700,  0.0864,  0.0892,  0.1013,\n",
            "          0.0941,  0.0968,  0.1013]])\n",
            "tensor([[-0.0690, -0.0971, -0.1393, -0.0690, -0.1351, -0.0697, -0.0922, -0.0512,\n",
            "         -0.0697],\n",
            "        [ 0.0864,  0.0823,  0.0700,  0.0864,  0.0892,  0.1013,  0.0941,  0.0968,\n",
            "          0.1013]])\n",
            "tensor([[-0.7000, -0.7000, -0.6662, -0.7000, -0.7000, -0.7000, -0.7000, -0.6799,\n",
            "         -0.7000, -0.7000, -0.7000, -0.6662, -0.7000, -0.7000, -0.6997, -0.6767,\n",
            "         -0.6767, -0.6767, -0.6767, -0.6767, -0.6767, -0.6767, -0.6767, -0.0766,\n",
            "          0.0339,     nan,     nan,  0.0339,     nan,     nan,     nan,  0.0339,\n",
            "          0.0339,     nan,  0.0339,     nan,  0.0339,  0.0339,  0.0339,     nan,\n",
            "          0.0339,  0.0339,     nan,  0.0339,     nan, -0.0476, -0.6767, -0.7000,\n",
            "         -0.7000, -0.7000, -0.7000, -0.7000, -0.7000],\n",
            "        [-0.0314, -0.0314,  0.0087, -0.0314, -0.0314, -0.0314, -0.0314,  0.0082,\n",
            "         -0.0314, -0.0314, -0.0314,  0.0087, -0.0314, -0.0314, -0.0070,  0.0210,\n",
            "          0.0210,  0.0210,  0.0210,  0.0210,  0.0210,  0.0210,  0.0210,  0.0728,\n",
            "          0.1348,     nan,     nan,  0.1348,     nan,     nan,     nan,  0.1348,\n",
            "          0.1348,     nan,  0.1348,     nan,  0.1348,  0.1348,  0.1348,     nan,\n",
            "          0.1348,  0.1348,     nan,  0.1348,     nan,  0.0808,  0.0210, -0.0314,\n",
            "         -0.0314, -0.0314, -0.0314, -0.0314, -0.0314]])\n",
            "tensor([[-0.7000, -0.7000, -0.6662, -0.7000, -0.7000, -0.7000, -0.7000, -0.6799,\n",
            "         -0.7000, -0.7000, -0.7000, -0.6662, -0.7000, -0.7000, -0.6997, -0.6767,\n",
            "         -0.6767, -0.6767, -0.6767, -0.6767, -0.6767, -0.6767, -0.6767, -0.0766,\n",
            "          0.0339,  0.0339,  0.0339,  0.0339,  0.0339,  0.0339,  0.0339,  0.0339,\n",
            "          0.0339,  0.0339,  0.0339,  0.0339,  0.0339,  0.0339,  0.0339,  0.0339,\n",
            "          0.0339,  0.0339,  0.0339,  0.0339,  0.0339, -0.0476, -0.6767, -0.7000,\n",
            "         -0.7000, -0.7000, -0.7000, -0.7000, -0.7000],\n",
            "        [-0.0314, -0.0314,  0.0087, -0.0314, -0.0314, -0.0314, -0.0314,  0.0082,\n",
            "         -0.0314, -0.0314, -0.0314,  0.0087, -0.0314, -0.0314, -0.0070,  0.0210,\n",
            "          0.0210,  0.0210,  0.0210,  0.0210,  0.0210,  0.0210,  0.0210,  0.0728,\n",
            "          0.1348,  0.1348,  0.1348,  0.1348,  0.1348,  0.1348,  0.1348,  0.1348,\n",
            "          0.1348,  0.1348,  0.1348,  0.1348,  0.1348,  0.1348,  0.1348,  0.1348,\n",
            "          0.1348,  0.1348,  0.1348,  0.1348,  0.1348,  0.0808,  0.0210, -0.0314,\n",
            "         -0.0314, -0.0314, -0.0314, -0.0314, -0.0314]])\n",
            "tensor([[-1.2883e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01,\n",
            "         -1.1688e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01,\n",
            "         -1.1688e-01, -1.1688e-01, -1.2883e-01, -1.1688e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.1688e-01,\n",
            "         -1.1688e-01, -1.1688e-01, -1.2883e-01, -1.1688e-01, -1.2883e-01,\n",
            "         -1.7447e-01, -1.1688e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.1688e-01,\n",
            "          5.0935e+00,  6.5412e-02, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.4223e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -6.6987e-02, -6.6987e-02,  3.3299e-03,  3.3299e-03,\n",
            "          4.5187e-03,  4.5187e-03,  3.3299e-03,  3.3299e-03, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.1688e-01, -6.1439e-02,\n",
            "          8.3234e-02, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.2883e-01, -1.3757e-01, -1.2883e-01,\n",
            "         -9.7845e-02, -6.0483e-03, -6.0483e-03, -6.0483e-03, -6.0483e-03,\n",
            "         -6.0483e-03, -6.0483e-03, -6.0483e-03, -6.0483e-03,         nan,\n",
            "         -9.5082e-02, -1.2883e-01, -1.2883e-01, -1.3757e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.0273e-01, -1.0273e-01,\n",
            "         -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.1269e-01, -1.2883e-01, -1.2883e-01, -1.3757e-01,\n",
            "         -8.8725e-02,  1.3106e-01,  1.3106e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.7385e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,  9.1573e-02,\n",
            "         -5.6398e-02, -5.6398e-02, -7.6310e-02, -1.2628e-01, -1.2628e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01,  3.3299e-03,  3.3299e-03, -1.1878e-03, -1.2433e-03,\n",
            "          3.3299e-03, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.3757e-01,\n",
            "          2.4207e-02,  3.2061e-02,  3.2061e-02, -5.0423e-02, -7.3204e-02,\n",
            "         -6.1439e-02, -8.8725e-02, -7.3029e-02, -8.8725e-02, -8.8725e-02,\n",
            "          1.1368e-02, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01,\n",
            "         -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01,\n",
            "         -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.2883e-01,\n",
            "         -1.3757e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -3.6774e-02, -3.6774e-02, -3.6774e-02,  5.6072e-02,\n",
            "          5.6072e-02,  5.6072e-02,  4.8032e-03, -6.0615e-03, -1.3757e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.2883e-01,\n",
            "         -1.3757e-01, -1.2883e-01, -8.3572e-05, -2.3015e-02,         nan,\n",
            "         -3.0487e-02,  5.2524e-02,  6.7851e-02,  6.7851e-02, -9.7349e-02,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.3757e-01],\n",
            "        [ 6.1148e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,\n",
            "          6.0864e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,\n",
            "          6.0864e-02,  6.0864e-02,  6.1148e-02,  6.0864e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.0864e-02,\n",
            "          6.0864e-02,  6.0864e-02,  6.1148e-02,  6.0864e-02,  6.1148e-02,\n",
            "          6.2860e-02,  6.0864e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.0864e-02,\n",
            "         -6.9186e+00,  7.5492e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.0753e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4786e-02,  7.4786e-02,  9.3249e-02,  9.3249e-02,\n",
            "          8.9282e-02,  8.9282e-02,  9.3249e-02,  9.3249e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.0864e-02,  4.6375e-02,\n",
            "         -9.6278e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4941e-02,  6.1148e-02,  7.4941e-02,  6.1148e-02,\n",
            "          1.1542e-01,  1.3315e-01,  1.3315e-01,  1.3315e-01,  1.3315e-01,\n",
            "          1.3315e-01,  1.3315e-01,  1.3315e-01,  1.3315e-01,         nan,\n",
            "          6.9696e-02,  6.1148e-02,  6.1148e-02,  7.4941e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,\n",
            "          6.1148e-02,  7.4941e-02,  7.4941e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  5.0940e-02,  5.0940e-02,\n",
            "          5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  8.0150e-02,  6.1148e-02,  6.1148e-02,  7.4941e-02,\n",
            "          1.7232e-01,  2.5025e-02,  2.5025e-02,  6.1148e-02,  6.1148e-02,\n",
            "          7.6389e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  5.8624e-02,\n",
            "          2.7222e-02,  2.7222e-02,  4.0320e-02,  4.3234e-02,  4.3234e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  9.3249e-02,  9.3249e-02,  9.4171e-02,  9.2505e-02,\n",
            "          9.3249e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  7.4941e-02,\n",
            "         -1.9189e-02, -2.9470e-02, -2.9470e-02,  5.6475e-02,  3.3241e-02,\n",
            "          4.6375e-02,  1.7232e-01,  6.1876e-02,  1.7232e-01,  1.7232e-01,\n",
            "         -4.6924e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,\n",
            "          5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,\n",
            "          5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  6.1148e-02,\n",
            "          7.4941e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.9402e-02,  7.9402e-02,  7.9402e-02,  8.4755e-02,\n",
            "          8.4755e-02,  8.4755e-02,  6.0364e-02,  3.8919e-02,  7.4941e-02,\n",
            "          6.1148e-02,  6.1148e-02,  7.4941e-02,  7.4941e-02,  6.1148e-02,\n",
            "          7.4941e-02,  6.1148e-02,  7.9849e-02,  5.4667e-02,         nan,\n",
            "          3.6631e-02,  7.1987e-02,  3.6458e-02,  3.6458e-02,  5.8291e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  7.4941e-02]])\n",
            "tensor([[-1.2883e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01,\n",
            "         -1.1688e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01, -1.1688e-01,\n",
            "         -1.1688e-01, -1.1688e-01, -1.2883e-01, -1.1688e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.1688e-01,\n",
            "         -1.1688e-01, -1.1688e-01, -1.2883e-01, -1.1688e-01, -1.2883e-01,\n",
            "         -1.7447e-01, -1.1688e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.1688e-01,\n",
            "          5.0935e+00,  6.5412e-02, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.4223e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -6.6987e-02, -6.6987e-02,  3.3299e-03,  3.3299e-03,\n",
            "          4.5187e-03,  4.5187e-03,  3.3299e-03,  3.3299e-03, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.1688e-01, -6.1439e-02,\n",
            "          8.3234e-02, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.2883e-01, -1.3757e-01, -1.2883e-01,\n",
            "         -9.7845e-02, -6.0483e-03, -6.0483e-03, -6.0483e-03, -6.0483e-03,\n",
            "         -6.0483e-03, -6.0483e-03, -6.0483e-03, -6.0483e-03, -6.0483e-03,\n",
            "         -9.5082e-02, -1.2883e-01, -1.2883e-01, -1.3757e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01, -1.3757e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.0273e-01, -1.0273e-01,\n",
            "         -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.1269e-01, -1.2883e-01, -1.2883e-01, -1.3757e-01,\n",
            "         -8.8725e-02,  1.3106e-01,  1.3106e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.7385e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,  9.1573e-02,\n",
            "         -5.6398e-02, -5.6398e-02, -7.6310e-02, -1.2628e-01, -1.2628e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01,  3.3299e-03,  3.3299e-03, -1.1878e-03, -1.2433e-03,\n",
            "          3.3299e-03, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.3757e-01,\n",
            "          2.4207e-02,  3.2061e-02,  3.2061e-02, -5.0423e-02, -7.3204e-02,\n",
            "         -6.1439e-02, -8.8725e-02, -7.3029e-02, -8.8725e-02, -8.8725e-02,\n",
            "          1.1368e-02, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01,\n",
            "         -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01,\n",
            "         -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.0273e-01, -1.2883e-01,\n",
            "         -1.3757e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -3.6774e-02, -3.6774e-02, -3.6774e-02,  5.6072e-02,\n",
            "          5.6072e-02,  5.6072e-02,  4.8032e-03, -6.0615e-03, -1.3757e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.3757e-01, -1.3757e-01, -1.2883e-01,\n",
            "         -1.3757e-01, -1.2883e-01, -8.3572e-05, -2.3015e-02, -2.3015e-02,\n",
            "         -3.0487e-02,  5.2524e-02,  6.7851e-02,  6.7851e-02, -9.7349e-02,\n",
            "         -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01, -1.2883e-01,\n",
            "         -1.2883e-01, -1.2883e-01, -1.3757e-01],\n",
            "        [ 6.1148e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,\n",
            "          6.0864e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,  6.0864e-02,\n",
            "          6.0864e-02,  6.0864e-02,  6.1148e-02,  6.0864e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.0864e-02,\n",
            "          6.0864e-02,  6.0864e-02,  6.1148e-02,  6.0864e-02,  6.1148e-02,\n",
            "          6.2860e-02,  6.0864e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.0864e-02,\n",
            "         -6.9186e+00,  7.5492e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.0753e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4786e-02,  7.4786e-02,  9.3249e-02,  9.3249e-02,\n",
            "          8.9282e-02,  8.9282e-02,  9.3249e-02,  9.3249e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.0864e-02,  4.6375e-02,\n",
            "         -9.6278e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4941e-02,  6.1148e-02,  7.4941e-02,  6.1148e-02,\n",
            "          1.1542e-01,  1.3315e-01,  1.3315e-01,  1.3315e-01,  1.3315e-01,\n",
            "          1.3315e-01,  1.3315e-01,  1.3315e-01,  1.3315e-01,  1.3315e-01,\n",
            "          6.9696e-02,  6.1148e-02,  6.1148e-02,  7.4941e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,\n",
            "          6.1148e-02,  7.4941e-02,  7.4941e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,  7.4941e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  5.0940e-02,  5.0940e-02,\n",
            "          5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  8.0150e-02,  6.1148e-02,  6.1148e-02,  7.4941e-02,\n",
            "          1.7232e-01,  2.5025e-02,  2.5025e-02,  6.1148e-02,  6.1148e-02,\n",
            "          7.6389e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  5.8624e-02,\n",
            "          2.7222e-02,  2.7222e-02,  4.0320e-02,  4.3234e-02,  4.3234e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  9.3249e-02,  9.3249e-02,  9.4171e-02,  9.2505e-02,\n",
            "          9.3249e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  7.4941e-02,\n",
            "         -1.9189e-02, -2.9470e-02, -2.9470e-02,  5.6475e-02,  3.3241e-02,\n",
            "          4.6375e-02,  1.7232e-01,  6.1876e-02,  1.7232e-01,  1.7232e-01,\n",
            "         -4.6924e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,\n",
            "          5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,\n",
            "          5.0940e-02,  5.0940e-02,  5.0940e-02,  5.0940e-02,  6.1148e-02,\n",
            "          7.4941e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  7.9402e-02,  7.9402e-02,  7.9402e-02,  8.4755e-02,\n",
            "          8.4755e-02,  8.4755e-02,  6.0364e-02,  3.8919e-02,  7.4941e-02,\n",
            "          6.1148e-02,  6.1148e-02,  7.4941e-02,  7.4941e-02,  6.1148e-02,\n",
            "          7.4941e-02,  6.1148e-02,  7.9849e-02,  5.4667e-02,  5.4667e-02,\n",
            "          3.6631e-02,  7.1987e-02,  3.6458e-02,  3.6458e-02,  5.8291e-02,\n",
            "          6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,  6.1148e-02,\n",
            "          6.1148e-02,  6.1148e-02,  7.4941e-02]])\n",
            "tensor([[ 0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280,  0.1230,  0.1230,  0.1230,  0.1594,  0.1594,  0.1588,\n",
            "          0.1588,  0.1588,  0.1588,  0.1588,  0.1588,  0.1588,  0.1588,  0.1014,\n",
            "          0.1014,  0.1014,  0.1014,  6.0999,  6.0999,  6.0999,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280,  0.0492,  0.0492,  0.0853,  0.0620,  0.0620,  0.0598,\n",
            "          0.0648,  0.0648,  0.1280,  0.1280,  0.1280,  6.0999,  0.0948,  6.0999,\n",
            "          6.0999,  6.0999,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,\n",
            "          0.1280,  0.0492,     nan,     nan,     nan, -0.4927,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280, -0.4927, -0.4927, -0.4927, -0.4927,     nan,     nan,\n",
            "             nan, -0.4927, -0.4927,     nan, -0.4927,  0.1280,  0.1280,  0.1280,\n",
            "         -0.4927, -0.4927,     nan,     nan,     nan,     nan, -0.4927, -0.4927,\n",
            "             nan, -0.4927, -0.4927, -0.4927,  0.1280,  0.1280,  0.1280, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,     nan,     nan,\n",
            "          0.0492,  0.1280,  0.1143,  0.1143,  0.1230,  0.1230,  0.1230,  0.1230,\n",
            "          0.1230,  0.1230,  0.1230,  0.1230,  0.1280,  0.1280,  6.0999,  6.0999,\n",
            "          0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280, -0.5060,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,     nan,     nan,     nan,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927,     nan,     nan, -0.4927, -0.4927,\n",
            "         -0.4927,     nan, -0.4927, -0.4927,     nan,     nan,  0.0492,  0.0492,\n",
            "             nan, -0.4927, -0.4927, -0.5060, -0.5060,  0.1280,  0.1280,  0.1280,\n",
            "          0.1230,  0.1230,  0.1230,  0.1230,  0.1230,  0.1230,  0.1117,  0.1117,\n",
            "          0.1230,  0.1230,  0.1117,  0.1117,  0.1117,  0.1117,  0.1280,  0.1280,\n",
            "             nan,     nan,     nan, -0.4927, -0.4927,     nan,     nan,     nan,\n",
            "         -0.4927,     nan, -0.4927, -0.4927,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan, -0.4927, -0.1184,     nan, -0.4927, -0.4927,     nan,\n",
            "         -0.4927, -0.4927,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "         -0.4927,     nan, -0.4927,  0.1673, -0.4927, -0.4927, -0.4927,     nan,\n",
            "             nan, -0.4927, -0.4927,     nan, -0.5060, -0.5060,  0.1280,  0.1280,\n",
            "             nan,     nan,     nan,  0.1280,  0.1280, -0.1058, -0.3105,  0.1280,\n",
            "             nan,  0.1014,  0.1014],\n",
            "        [ 0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,\n",
            "          0.2348,  0.2348,  0.1559,  0.1559,  0.1559,  0.1539,  0.1539,  0.1520,\n",
            "          0.1520,  0.1520,  0.1520,  0.1520,  0.1520,  0.1520,  0.1520,  0.1978,\n",
            "          0.1978,  0.1978,  0.1978, -4.0746, -4.0746, -4.0746,  0.2348,  0.2348,\n",
            "          0.2348,  0.2348,  0.1729,  0.1729,  0.2222,  0.1577,  0.1577,  0.1357,\n",
            "          0.1396,  0.1396,  0.2348,  0.2348,  0.2348, -4.0746,  0.2167, -4.0746,\n",
            "         -4.0746, -4.0746,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,\n",
            "          0.2348,  0.1729,     nan,     nan,     nan,  0.3649,  0.2348,  0.2240,\n",
            "          0.2240,  0.2240,  0.3649,  0.3649,  0.3649,  0.3649,     nan,     nan,\n",
            "             nan,  0.3649,  0.3649,     nan,  0.3649,  0.2240,  0.2240,  0.2240,\n",
            "          0.3649,  0.3649,     nan,     nan,     nan,     nan,  0.3649,  0.3649,\n",
            "             nan,  0.3649,  0.3649,  0.3649,  0.2348,  0.2240,  0.2240,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,     nan,     nan,\n",
            "          0.1729,  0.2348,  0.1561,  0.1561,  0.1559,  0.1559,  0.1559,  0.1559,\n",
            "          0.1559,  0.1559,  0.1559,  0.1559,  0.2348,  0.2348, -4.0746, -4.0746,\n",
            "          0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2240,  0.4131,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,     nan,     nan,     nan,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,     nan,     nan,  0.3649,  0.3649,\n",
            "          0.3649,     nan,  0.3649,  0.3649,     nan,     nan,  0.1729,  0.1729,\n",
            "             nan,  0.3649,  0.3649,  0.4131,  0.4131,  0.2348,  0.2348,  0.2348,\n",
            "          0.1559,  0.1559,  0.1559,  0.1559,  0.1559,  0.1559,  0.1522,  0.1522,\n",
            "          0.1559,  0.1559,  0.1522,  0.1522,  0.1522,  0.1522,  0.2348,  0.2348,\n",
            "             nan,     nan,     nan,  0.3649,  0.3649,     nan,     nan,     nan,\n",
            "          0.3649,     nan,  0.3649,  0.3649,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,  0.3649, -0.1219,     nan,  0.3649,  0.3649,     nan,\n",
            "          0.3649,  0.3649,     nan,     nan,     nan,     nan,     nan,     nan,\n",
            "          0.3649,     nan,  0.3649,  0.2090,  0.3649,  0.3649,  0.3649,     nan,\n",
            "             nan,  0.3649,  0.3649,     nan,  0.4131,  0.4131,  0.2240,  0.2240,\n",
            "             nan,     nan,     nan,  0.2348,  0.2348,  0.2003,  0.2852,  0.2348,\n",
            "             nan,  0.1978,  0.1978]])\n",
            "tensor([[ 0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280,  0.1230,  0.1230,  0.1230,  0.1594,  0.1594,  0.1588,\n",
            "          0.1588,  0.1588,  0.1588,  0.1588,  0.1588,  0.1588,  0.1588,  0.1014,\n",
            "          0.1014,  0.1014,  0.1014,  6.0999,  6.0999,  6.0999,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280,  0.0492,  0.0492,  0.0853,  0.0620,  0.0620,  0.0598,\n",
            "          0.0648,  0.0648,  0.1280,  0.1280,  0.1280,  6.0999,  0.0948,  6.0999,\n",
            "          6.0999,  6.0999,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,\n",
            "          0.1280,  0.0492,  0.0492,  0.0492,  0.0492, -0.4927,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,  0.1280,  0.1280,  0.1280,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927,  0.1280,  0.1280,  0.1280, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "          0.0492,  0.1280,  0.1143,  0.1143,  0.1230,  0.1230,  0.1230,  0.1230,\n",
            "          0.1230,  0.1230,  0.1230,  0.1230,  0.1280,  0.1280,  6.0999,  6.0999,\n",
            "          0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280,  0.1280, -0.5060,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,  0.0492,  0.0492,\n",
            "          0.0492, -0.4927, -0.4927, -0.5060, -0.5060,  0.1280,  0.1280,  0.1280,\n",
            "          0.1230,  0.1230,  0.1230,  0.1230,  0.1230,  0.1230,  0.1117,  0.1117,\n",
            "          0.1230,  0.1230,  0.1117,  0.1117,  0.1117,  0.1117,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280,  0.1280, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.1184, -0.1184, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927,  0.1673, -0.4927, -0.4927, -0.4927, -0.4927,\n",
            "         -0.4927, -0.4927, -0.4927, -0.4927, -0.5060, -0.5060,  0.1280,  0.1280,\n",
            "          0.1280,  0.1280,  0.1280,  0.1280,  0.1280, -0.1058, -0.3105,  0.1280,\n",
            "          0.1280,  0.1014,  0.1014],\n",
            "        [ 0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,\n",
            "          0.2348,  0.2348,  0.1559,  0.1559,  0.1559,  0.1539,  0.1539,  0.1520,\n",
            "          0.1520,  0.1520,  0.1520,  0.1520,  0.1520,  0.1520,  0.1520,  0.1978,\n",
            "          0.1978,  0.1978,  0.1978, -4.0746, -4.0746, -4.0746,  0.2348,  0.2348,\n",
            "          0.2348,  0.2348,  0.1729,  0.1729,  0.2222,  0.1577,  0.1577,  0.1357,\n",
            "          0.1396,  0.1396,  0.2348,  0.2348,  0.2348, -4.0746,  0.2167, -4.0746,\n",
            "         -4.0746, -4.0746,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,\n",
            "          0.2348,  0.1729,  0.1729,  0.1729,  0.1729,  0.3649,  0.2348,  0.2240,\n",
            "          0.2240,  0.2240,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.2240,  0.2240,  0.2240,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.2348,  0.2240,  0.2240,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.1729,  0.2348,  0.1561,  0.1561,  0.1559,  0.1559,  0.1559,  0.1559,\n",
            "          0.1559,  0.1559,  0.1559,  0.1559,  0.2348,  0.2348, -4.0746, -4.0746,\n",
            "          0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2348,  0.2240,  0.4131,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.1729,  0.1729,\n",
            "          0.1729,  0.3649,  0.3649,  0.4131,  0.4131,  0.2348,  0.2348,  0.2348,\n",
            "          0.1559,  0.1559,  0.1559,  0.1559,  0.1559,  0.1559,  0.1522,  0.1522,\n",
            "          0.1559,  0.1559,  0.1522,  0.1522,  0.1522,  0.1522,  0.2348,  0.2348,\n",
            "          0.2348,  0.2348,  0.2348,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649, -0.1219, -0.1219,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.2090,  0.3649,  0.3649,  0.3649,  0.3649,\n",
            "          0.3649,  0.3649,  0.3649,  0.3649,  0.4131,  0.4131,  0.2240,  0.2240,\n",
            "          0.2240,  0.2240,  0.2240,  0.2348,  0.2348,  0.2003,  0.2852,  0.2348,\n",
            "          0.2348,  0.1978,  0.1978]])\n",
            "tensor([[0.1772, 0.1772, 0.1772, 0.0792, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1742, 0.1772,\n",
            "            nan, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772],\n",
            "        [0.1131, 0.1131, 0.1131, 0.1919, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1243, 0.1131,\n",
            "            nan, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131]])\n",
            "tensor([[0.1772, 0.1772, 0.1772, 0.0792, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1742, 0.1772,\n",
            "         0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772, 0.1772,\n",
            "         0.1772, 0.1772],\n",
            "        [0.1131, 0.1131, 0.1131, 0.1919, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1243, 0.1131,\n",
            "         0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131, 0.1131,\n",
            "         0.1131, 0.1131]])\n",
            "tensor([[    nan,     nan, -0.0493, -0.4295, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4295, -0.3790, -0.1922, -0.2091, -0.2036,\n",
            "         -0.2036, -0.2036, -0.2036, -0.2141, -0.4295, -0.4494, -0.3790, -0.3643,\n",
            "         -0.3550, -0.3550, -0.3550, -0.3550, -0.3790, -0.3790, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4295, -0.3790, -0.3790, -0.3790, -0.4505, -0.3790, -0.4505,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3643, -0.4494, -0.4710,\n",
            "         -0.3790, -0.3790, -0.3790, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4421, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3550, -0.3550, -0.3643, -0.3643, -0.3643,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4710, -0.4505, -0.3790, -0.3643, -0.3550, -0.3790,     nan, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3643, -0.3643, -0.3643, -0.3643,\n",
            "         -0.3643, -0.4421, -0.3643, -0.3643, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.3790, -0.3550, -0.3550, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3550, -0.3790, -0.3790, -0.3643, -0.4295, -0.4494, -0.4494,\n",
            "         -0.3643, -0.3643, -0.3643, -0.3643, -0.3643, -0.3643, -0.3643, -0.4494,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3248, -0.1753, -0.0561, -0.0561, -0.0561, -0.0086,     nan,\n",
            "         -0.3550, -0.4494, -0.4295, -0.3790, -0.3790, -0.3643, -0.3643, -0.3643,\n",
            "         -0.3643, -0.3643, -0.3643, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.3790, -0.3643,     nan, -0.3790, -0.3790, -0.3550,\n",
            "         -0.3550, -0.3790, -0.3790, -0.3790, -0.3790, -0.3643, -0.3643, -0.4494,\n",
            "         -0.4494, -0.3643, -0.3790, -0.3790, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.4505,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3550, -0.3550,\n",
            "         -0.4494, -0.3550, -0.3643, -0.3790, -0.3550, -0.3790, -0.3790, -0.3790,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790,     nan, -0.3643,\n",
            "         -0.3790, -0.3643, -0.3790, -0.4524, -0.3790, -0.3790, -0.3643, -0.3643,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4421,     nan, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.4295, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4295, -0.3790,\n",
            "         -0.3790, -0.3643, -0.3550, -0.4991, -0.3790, -0.4295, -0.3790, -0.3550,\n",
            "         -0.4494, -0.4494, -0.4505, -0.3790, -0.3790, -0.4295, -0.3790],\n",
            "        [    nan,     nan,  0.0053,  0.0059, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025, -0.0025,  0.0059,  0.0031, -0.0013,  0.0120,  0.0055,\n",
            "          0.0055,  0.0055,  0.0055, -0.0091,  0.0059, -0.0025,  0.0031,  0.0006,\n",
            "          0.0079,  0.0079,  0.0079,  0.0079,  0.0031,  0.0031, -0.0025, -0.0025,\n",
            "         -0.0025,  0.0059,  0.0031,  0.0031,  0.0031, -0.0051,  0.0031, -0.0051,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0006, -0.0025,  0.0008,\n",
            "          0.0031,  0.0031,  0.0031, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0032,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0079,  0.0079,  0.0006,  0.0006,  0.0006,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031, -0.0025, -0.0025, -0.0025,\n",
            "          0.0008, -0.0051,  0.0031,  0.0006,  0.0079,  0.0031,     nan,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0006,  0.0006,  0.0006,  0.0006,\n",
            "          0.0006, -0.0032,  0.0006,  0.0006, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025,  0.0031,  0.0079,  0.0079,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0079,  0.0031,  0.0031,  0.0006,  0.0059, -0.0025, -0.0025,\n",
            "          0.0006,  0.0006,  0.0006,  0.0006,  0.0006,  0.0006,  0.0006, -0.0025,\n",
            "         -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0203,  0.0126,  0.0870,  0.0870,  0.0870,  0.0891,     nan,\n",
            "          0.0079, -0.0025,  0.0059,  0.0031,  0.0031,  0.0006,  0.0006,  0.0006,\n",
            "          0.0006,  0.0006,  0.0006, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025,  0.0031,  0.0006,     nan,  0.0031,  0.0031,  0.0079,\n",
            "          0.0079,  0.0031,  0.0031,  0.0031,  0.0031,  0.0006,  0.0006, -0.0025,\n",
            "         -0.0025,  0.0006,  0.0031,  0.0031, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031, -0.0051,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0079,  0.0079,\n",
            "         -0.0025,  0.0079,  0.0006,  0.0031,  0.0079,  0.0031,  0.0031,  0.0031,\n",
            "         -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,     nan,  0.0006,\n",
            "          0.0031,  0.0006,  0.0031,  0.0177,  0.0031,  0.0031,  0.0006,  0.0006,\n",
            "         -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0032,     nan,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0059, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,  0.0059,  0.0031,\n",
            "          0.0031,  0.0006,  0.0079, -0.0091,  0.0031,  0.0059,  0.0031,  0.0079,\n",
            "         -0.0025, -0.0025, -0.0051,  0.0031,  0.0031,  0.0059,  0.0031]])\n",
            "tensor([[-0.0493, -0.4295, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4295, -0.3790, -0.1922, -0.2091, -0.2036, -0.2036, -0.2036,\n",
            "         -0.2036, -0.2141, -0.4295, -0.4494, -0.3790, -0.3643, -0.3550, -0.3550,\n",
            "         -0.3550, -0.3550, -0.3790, -0.3790, -0.4494, -0.4494, -0.4494, -0.4295,\n",
            "         -0.3790, -0.3790, -0.3790, -0.4505, -0.3790, -0.4505, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3643, -0.4494, -0.4710, -0.3790, -0.3790,\n",
            "         -0.3790, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4421, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3550, -0.3550, -0.3643, -0.3643, -0.3643, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.4494, -0.4494, -0.4494, -0.4710, -0.4505,\n",
            "         -0.3790, -0.3643, -0.3550, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3643, -0.3643, -0.3643, -0.3643, -0.3643, -0.4421,\n",
            "         -0.3643, -0.3643, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.3790, -0.3550, -0.3550, -0.3790, -0.3790, -0.3790, -0.3790, -0.3550,\n",
            "         -0.3790, -0.3790, -0.3643, -0.4295, -0.4494, -0.4494, -0.3643, -0.3643,\n",
            "         -0.3643, -0.3643, -0.3643, -0.3643, -0.3643, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.4494, -0.3790, -0.3790, -0.3790, -0.3790, -0.3248,\n",
            "         -0.1753, -0.0561, -0.0561, -0.0561, -0.0086, -0.0086, -0.3550, -0.4494,\n",
            "         -0.4295, -0.3790, -0.3790, -0.3643, -0.3643, -0.3643, -0.3643, -0.3643,\n",
            "         -0.3643, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.3790, -0.3643, -0.3643, -0.3790, -0.3790, -0.3550, -0.3550, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3643, -0.3643, -0.4494, -0.4494, -0.3643,\n",
            "         -0.3790, -0.3790, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.4505, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3550, -0.3550, -0.4494, -0.3550,\n",
            "         -0.3643, -0.3790, -0.3550, -0.3790, -0.3790, -0.3790, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4494, -0.3790, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3643, -0.3790, -0.3643,\n",
            "         -0.3790, -0.4524, -0.3790, -0.3790, -0.3643, -0.3643, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4421, -0.4421, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790, -0.3790,\n",
            "         -0.3790, -0.3790, -0.4295, -0.4494, -0.4494, -0.4494, -0.4494, -0.4494,\n",
            "         -0.4494, -0.4494, -0.4494, -0.4494, -0.4295, -0.3790, -0.3790, -0.3643,\n",
            "         -0.3550, -0.4991, -0.3790, -0.4295, -0.3790, -0.3550, -0.4494, -0.4494,\n",
            "         -0.4505, -0.3790, -0.3790, -0.4295, -0.3790],\n",
            "        [ 0.0053,  0.0059, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025,  0.0059,  0.0031, -0.0013,  0.0120,  0.0055,  0.0055,  0.0055,\n",
            "          0.0055, -0.0091,  0.0059, -0.0025,  0.0031,  0.0006,  0.0079,  0.0079,\n",
            "          0.0079,  0.0079,  0.0031,  0.0031, -0.0025, -0.0025, -0.0025,  0.0059,\n",
            "          0.0031,  0.0031,  0.0031, -0.0051,  0.0031, -0.0051,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0006, -0.0025,  0.0008,  0.0031,  0.0031,\n",
            "          0.0031, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0032,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0079,  0.0079,  0.0006,  0.0006,  0.0006,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031, -0.0025, -0.0025, -0.0025,  0.0008, -0.0051,\n",
            "          0.0031,  0.0006,  0.0079,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0006,  0.0006,  0.0006,  0.0006,  0.0006, -0.0032,\n",
            "          0.0006,  0.0006, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "          0.0031,  0.0079,  0.0079,  0.0031,  0.0031,  0.0031,  0.0031,  0.0079,\n",
            "          0.0031,  0.0031,  0.0006,  0.0059, -0.0025, -0.0025,  0.0006,  0.0006,\n",
            "          0.0006,  0.0006,  0.0006,  0.0006,  0.0006, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025, -0.0025,  0.0031,  0.0031,  0.0031,  0.0031,  0.0203,\n",
            "          0.0126,  0.0870,  0.0870,  0.0870,  0.0891,  0.0891,  0.0079, -0.0025,\n",
            "          0.0059,  0.0031,  0.0031,  0.0006,  0.0006,  0.0006,  0.0006,  0.0006,\n",
            "          0.0006, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "          0.0031,  0.0006,  0.0006,  0.0031,  0.0031,  0.0079,  0.0079,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0006,  0.0006, -0.0025, -0.0025,  0.0006,\n",
            "          0.0031,  0.0031, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031, -0.0051,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0079,  0.0079, -0.0025,  0.0079,\n",
            "          0.0006,  0.0031,  0.0079,  0.0031,  0.0031,  0.0031, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025, -0.0025, -0.0025,  0.0031,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0006,  0.0031,  0.0006,\n",
            "          0.0031,  0.0177,  0.0031,  0.0031,  0.0006,  0.0006, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025, -0.0025, -0.0032, -0.0032,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,  0.0031,\n",
            "          0.0031,  0.0031,  0.0059, -0.0025, -0.0025, -0.0025, -0.0025, -0.0025,\n",
            "         -0.0025, -0.0025, -0.0025, -0.0025,  0.0059,  0.0031,  0.0031,  0.0006,\n",
            "          0.0079, -0.0091,  0.0031,  0.0059,  0.0031,  0.0079, -0.0025, -0.0025,\n",
            "         -0.0051,  0.0031,  0.0031,  0.0059,  0.0031]])\n",
            "tensor([[0.1408, 0.0907, 0.0999, 0.1408, 0.1408, 0.1408, 0.0999, 0.0999, 0.1408,\n",
            "         0.1469, 0.1408, 0.1408, 0.1469, 0.1408, 0.0849, 0.0849, 0.0907, 0.0999,\n",
            "         0.0849, 0.1408, 0.0999, 0.0849, 0.0849, 0.1408, 0.0999, 0.0849, 0.0999,\n",
            "         0.1408, 0.1281,    nan, 0.0849, 0.0849, 0.1014, 0.1408, 0.0999, 0.0849,\n",
            "         0.0849, 0.0849, 0.1408, 0.0907, 0.1408, 0.1469, 0.1408, 0.1517, 0.1014,\n",
            "         0.0999, 0.0999, 0.0907, 0.0999, 0.0999, 0.0999, 0.0999, 0.0999],\n",
            "        [0.1661, 0.1690, 0.1720, 0.1661, 0.1661, 0.1661, 0.1720, 0.1720, 0.1661,\n",
            "         0.1694, 0.1661, 0.1661, 0.1694, 0.1661, 0.1817, 0.1817, 0.1690, 0.1720,\n",
            "         0.1817, 0.1661, 0.1720, 0.1817, 0.1817, 0.1661, 0.1720, 0.1817, 0.1720,\n",
            "         0.1661, 0.1729,    nan, 0.1817, 0.1817, 0.1732, 0.1661, 0.1720, 0.1817,\n",
            "         0.1817, 0.1817, 0.1661, 0.1690, 0.1661, 0.1694, 0.1661, 0.1737, 0.1732,\n",
            "         0.1720, 0.1720, 0.1690, 0.1720, 0.1720, 0.1720, 0.1720, 0.1720]])\n",
            "tensor([[0.1408, 0.0907, 0.0999, 0.1408, 0.1408, 0.1408, 0.0999, 0.0999, 0.1408,\n",
            "         0.1469, 0.1408, 0.1408, 0.1469, 0.1408, 0.0849, 0.0849, 0.0907, 0.0999,\n",
            "         0.0849, 0.1408, 0.0999, 0.0849, 0.0849, 0.1408, 0.0999, 0.0849, 0.0999,\n",
            "         0.1408, 0.1281, 0.1281, 0.0849, 0.0849, 0.1014, 0.1408, 0.0999, 0.0849,\n",
            "         0.0849, 0.0849, 0.1408, 0.0907, 0.1408, 0.1469, 0.1408, 0.1517, 0.1014,\n",
            "         0.0999, 0.0999, 0.0907, 0.0999, 0.0999, 0.0999, 0.0999, 0.0999],\n",
            "        [0.1661, 0.1690, 0.1720, 0.1661, 0.1661, 0.1661, 0.1720, 0.1720, 0.1661,\n",
            "         0.1694, 0.1661, 0.1661, 0.1694, 0.1661, 0.1817, 0.1817, 0.1690, 0.1720,\n",
            "         0.1817, 0.1661, 0.1720, 0.1817, 0.1817, 0.1661, 0.1720, 0.1817, 0.1720,\n",
            "         0.1661, 0.1729, 0.1729, 0.1817, 0.1817, 0.1732, 0.1661, 0.1720, 0.1817,\n",
            "         0.1817, 0.1817, 0.1661, 0.1690, 0.1661, 0.1694, 0.1661, 0.1737, 0.1732,\n",
            "         0.1720, 0.1720, 0.1690, 0.1720, 0.1720, 0.1720, 0.1720, 0.1720]])\n",
            "tensor([[-0.1257, -0.1230, -0.1257, -0.1257, -0.1230, -0.1257, -0.1230, -0.1230,\n",
            "         -0.1230, -0.1230, -0.1611,  0.0244,  0.0244,  0.0244,  0.0375,  0.0504,\n",
            "         -0.1169, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1230, -0.1230,\n",
            "         -0.1230, -0.1230,  0.0655,  0.0605,  0.1179,  0.1179,  0.1179,  0.1179,\n",
            "         -0.1545, -0.1422, -0.1422, -0.1422, -0.1422, -0.1611, -0.1422, -0.1288,\n",
            "          0.0917,  0.0832,  0.1209,  0.1485,  0.0832,  0.0331, -0.1257, -0.1257,\n",
            "         -0.1257, -0.0813, -0.1422, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1422, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1637, -0.0622, -0.1422, -0.1422, -0.1422, -0.1422, -0.1422, -0.1422,\n",
            "         -0.1422, -0.1422, -0.1422, -0.1422, -0.1422, -0.1545, -0.1422, -0.1422,\n",
            "         -0.1545, -0.1545, -0.1545, -0.1230, -0.1230, -0.1230, -0.1230, -0.1230,\n",
            "         -0.1230, -0.1230, -0.1230, -0.1230, -0.1230, -0.1257, -0.1230, -0.1257,\n",
            "         -0.1257, -0.1257, -0.1257, -0.1257, -0.1230, -0.1257, -0.1230, -0.0813,\n",
            "         -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1545, -0.1300, -0.1257, -0.0230, -0.0230, -0.0371, -0.0371, -0.0371,\n",
            "         -0.0371, -0.1574, -0.1738, -0.1738, -0.1738, -0.1738,     nan,     nan,\n",
            "             nan,     nan,  0.0232, -0.1422, -0.1422, -0.1422, -0.1422, -0.1422,\n",
            "         -0.1545, -0.1545,  0.0679,  0.1257,  0.1359,  0.1361,     nan,  0.0879,\n",
            "          0.1285,  0.1269,  0.0509, -0.1611, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1032, -0.1032, -0.1032, -0.1260, -0.2827,     nan, -0.1541, -0.1541,\n",
            "         -0.0750, -0.0979, -0.0979, -0.0979, -0.1545, -0.1545],\n",
            "        [ 0.0480,  0.0466,  0.0480,  0.0480,  0.0466,  0.0480,  0.0466,  0.0466,\n",
            "          0.0466,  0.0466,  0.0583,  0.0799,  0.0799,  0.0799,  0.1049,  0.1098,\n",
            "          0.0609,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0466,  0.0466,\n",
            "          0.0466,  0.0466,  0.1100,  0.1063,  0.0746,  0.0746,  0.0746,  0.0746,\n",
            "          0.0496,  0.0608,  0.0608,  0.0608,  0.0608,  0.0583,  0.0608,  0.0611,\n",
            "          0.0336, -0.0963, -0.1198, -0.1190, -0.0963,  0.0282,  0.0480,  0.0480,\n",
            "          0.0480,  0.0699,  0.0608,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "          0.0608,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "         -0.2773, -0.0940,  0.0608,  0.0608,  0.0608,  0.0608,  0.0608,  0.0608,\n",
            "          0.0608,  0.0608,  0.0608,  0.0608,  0.0608,  0.0496,  0.0608,  0.0608,\n",
            "          0.0496,  0.0496,  0.0496,  0.0466,  0.0466,  0.0466,  0.0466,  0.0466,\n",
            "          0.0466,  0.0466,  0.0466,  0.0466,  0.0466,  0.0480,  0.0466,  0.0480,\n",
            "          0.0480,  0.0480,  0.0480,  0.0480,  0.0466,  0.0480,  0.0466,  0.0699,\n",
            "          0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "          0.0496,  0.0522,  0.0480,  0.0547,  0.0517,  0.0311,  0.0311,  0.0311,\n",
            "          0.0311,  0.0647,  0.0764,  0.0764,  0.0764,  0.0764,     nan,     nan,\n",
            "             nan,     nan,  0.0827,  0.0608,  0.0608,  0.0608,  0.0608,  0.0608,\n",
            "          0.0496,  0.0496,  0.0365,  0.1222,  0.1151,  0.1289,     nan,  0.1208,\n",
            "          0.1349,  0.1284,  0.1177,  0.0583,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "          0.0092,  0.0092,  0.0092,  0.0009, -0.1990,     nan, -0.3193, -0.3193,\n",
            "         -0.0685, -0.1802, -0.1802, -0.1802,  0.0496,  0.0496]])\n",
            "tensor([[-0.1257, -0.1230, -0.1257, -0.1257, -0.1230, -0.1257, -0.1230, -0.1230,\n",
            "         -0.1230, -0.1230, -0.1611,  0.0244,  0.0244,  0.0244,  0.0375,  0.0504,\n",
            "         -0.1169, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1230, -0.1230,\n",
            "         -0.1230, -0.1230,  0.0655,  0.0605,  0.1179,  0.1179,  0.1179,  0.1179,\n",
            "         -0.1545, -0.1422, -0.1422, -0.1422, -0.1422, -0.1611, -0.1422, -0.1288,\n",
            "          0.0917,  0.0832,  0.1209,  0.1485,  0.0832,  0.0331, -0.1257, -0.1257,\n",
            "         -0.1257, -0.0813, -0.1422, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1422, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1637, -0.0622, -0.1422, -0.1422, -0.1422, -0.1422, -0.1422, -0.1422,\n",
            "         -0.1422, -0.1422, -0.1422, -0.1422, -0.1422, -0.1545, -0.1422, -0.1422,\n",
            "         -0.1545, -0.1545, -0.1545, -0.1230, -0.1230, -0.1230, -0.1230, -0.1230,\n",
            "         -0.1230, -0.1230, -0.1230, -0.1230, -0.1230, -0.1257, -0.1230, -0.1257,\n",
            "         -0.1257, -0.1257, -0.1257, -0.1257, -0.1230, -0.1257, -0.1230, -0.0813,\n",
            "         -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1545, -0.1300, -0.1257, -0.0230, -0.0230, -0.0371, -0.0371, -0.0371,\n",
            "         -0.0371, -0.1574, -0.1738, -0.1738, -0.1738, -0.1738, -0.1738, -0.1738,\n",
            "         -0.1738, -0.1738,  0.0232, -0.1422, -0.1422, -0.1422, -0.1422, -0.1422,\n",
            "         -0.1545, -0.1545,  0.0679,  0.1257,  0.1359,  0.1361,  0.1361,  0.0879,\n",
            "          0.1285,  0.1269,  0.0509, -0.1611, -0.1545, -0.1545, -0.1545, -0.1545,\n",
            "         -0.1032, -0.1032, -0.1032, -0.1260, -0.2827, -0.2827, -0.1541, -0.1541,\n",
            "         -0.0750, -0.0979, -0.0979, -0.0979, -0.1545, -0.1545],\n",
            "        [ 0.0480,  0.0466,  0.0480,  0.0480,  0.0466,  0.0480,  0.0466,  0.0466,\n",
            "          0.0466,  0.0466,  0.0583,  0.0799,  0.0799,  0.0799,  0.1049,  0.1098,\n",
            "          0.0609,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0466,  0.0466,\n",
            "          0.0466,  0.0466,  0.1100,  0.1063,  0.0746,  0.0746,  0.0746,  0.0746,\n",
            "          0.0496,  0.0608,  0.0608,  0.0608,  0.0608,  0.0583,  0.0608,  0.0611,\n",
            "          0.0336, -0.0963, -0.1198, -0.1190, -0.0963,  0.0282,  0.0480,  0.0480,\n",
            "          0.0480,  0.0699,  0.0608,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "          0.0608,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "         -0.2773, -0.0940,  0.0608,  0.0608,  0.0608,  0.0608,  0.0608,  0.0608,\n",
            "          0.0608,  0.0608,  0.0608,  0.0608,  0.0608,  0.0496,  0.0608,  0.0608,\n",
            "          0.0496,  0.0496,  0.0496,  0.0466,  0.0466,  0.0466,  0.0466,  0.0466,\n",
            "          0.0466,  0.0466,  0.0466,  0.0466,  0.0466,  0.0480,  0.0466,  0.0480,\n",
            "          0.0480,  0.0480,  0.0480,  0.0480,  0.0466,  0.0480,  0.0466,  0.0699,\n",
            "          0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "          0.0496,  0.0522,  0.0480,  0.0547,  0.0517,  0.0311,  0.0311,  0.0311,\n",
            "          0.0311,  0.0647,  0.0764,  0.0764,  0.0764,  0.0764,  0.0764,  0.0764,\n",
            "          0.0764,  0.0764,  0.0827,  0.0608,  0.0608,  0.0608,  0.0608,  0.0608,\n",
            "          0.0496,  0.0496,  0.0365,  0.1222,  0.1151,  0.1289,  0.1289,  0.1208,\n",
            "          0.1349,  0.1284,  0.1177,  0.0583,  0.0496,  0.0496,  0.0496,  0.0496,\n",
            "          0.0092,  0.0092,  0.0092,  0.0009, -0.1990, -0.1990, -0.3193, -0.3193,\n",
            "         -0.0685, -0.1802, -0.1802, -0.1802,  0.0496,  0.0496]])\n",
            "tensor([[ 0.0345,  0.0345,  0.0345,  0.0345,  0.0345,  0.0345,  0.0345, -0.0493,\n",
            "          0.0331,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,     nan, -0.0199, -0.0199,     nan,  0.0331,\n",
            "          0.0345,  0.0345,  0.0331,  0.0345,  0.0345,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345, -0.0293,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0048,  0.0353,     nan,  0.0345,  0.0331,\n",
            "          0.0331,  0.0331,  0.0345,  0.0331,  0.0331,  0.0331,  0.0345,  0.0345,\n",
            "          0.0345,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0174,  0.0174,  0.0345, -0.0493,  0.0345,  0.0345,  0.0026,  0.0345,\n",
            "         -0.0405,  0.0345, -0.0405, -0.0405, -0.0405, -0.0405,  0.0825,  0.0300,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0237,  0.0230,  0.0230,  0.0026,  0.0331,     nan,  0.0331,  0.0331,\n",
            "          0.0026,  0.0450,  0.0450, -0.0405,  0.0026, -0.0405,  0.0331, -0.0405,\n",
            "             nan,  0.0199,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0300,  0.0174,  0.0174,  0.0174,  0.0174,  0.0174,\n",
            "          0.0174,  0.0174,  0.0331,  0.0331,  0.0026,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345,  0.0345, -0.0293, -0.0405,  0.0331,  0.0331,  0.0331,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,     nan,  0.0345,  0.0345,  0.0331,\n",
            "          0.0331,  0.0345,  0.0331,  0.0345,  0.0345, -0.0493, -0.0405,  0.0331,\n",
            "          0.0345,  0.0345,     nan,  0.0341,  0.0341,  0.0285,  0.0285,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,  0.0174,  0.0237,  0.0331,  0.0345,\n",
            "          0.0345,  0.0331, -0.0405,  0.0345,  0.0026,  0.0345,  0.0345,  0.0345,\n",
            "         -0.0405,  0.0345,  0.0345,  0.0345,  0.0345, -0.0804,  0.0331,  0.0174,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341, -0.0061,  0.0345,\n",
            "          0.0345,  0.0345,  0.0331, -0.0493,  0.0345,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345,  0.0345,  0.0331,  0.0174,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0199,  0.0174,     nan,  0.0331,  0.0345,\n",
            "          0.0026,  0.0026,  0.0026, -0.0405,  0.0331,  0.0026,  0.0026, -0.0293,\n",
            "          0.0345,  0.0345,  0.0345,  0.0174,  0.0341,  0.0375,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341, -0.0006,  0.0199,  0.0199,  0.0331,  0.0331,  0.0331,\n",
            "          0.0331,  0.0331,  0.0825, -0.0405,  0.0345,  0.0331,  0.0331,  0.0331,\n",
            "          0.0345, -0.0405,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0174,  0.0174,  0.0174, -0.0114, -0.0114,\n",
            "          0.0345,  0.0331,  0.0331,  0.0331,  0.0331,  0.0331, -0.0405,  0.0345,\n",
            "          0.0331, -0.0405,  0.0006,  0.0341,  0.0341,  0.0341,  0.0341,  0.0315,\n",
            "          0.0341,  0.0341,  0.0174,  0.0237,  0.0237,  0.0237,  0.0237,  0.0237,\n",
            "          0.0331,  0.0331,  0.0331, -0.0405,  0.0345,  0.0331,  0.0331,  0.0331,\n",
            "          0.0331, -0.0405,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "         -0.0006,     nan,     nan,  0.0313,  0.0331,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345, -0.0405,  0.0331,  0.0331,  0.0345,  0.0345,  0.0331,\n",
            "          0.0345,  0.0345,  0.0345,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0174,  0.0174,  0.0174,  0.0174,  0.0331,  0.0345, -0.0405,\n",
            "          0.0026, -0.0405,  0.0345, -0.0405,  0.0345, -0.0405,  0.0331,  0.0331,\n",
            "          0.0345,  0.0345,  0.0341,  0.0375, -0.0191, -0.0060, -0.0060, -0.0436,\n",
            "         -0.0306, -0.0306,  0.0375,     nan,  0.0174,  0.0174,  0.0174,  0.0004,\n",
            "          0.0345,  0.0345, -0.0293,  0.0331, -0.0293],\n",
            "        [ 0.0219,  0.0219,  0.0219,  0.0219,  0.0219,  0.0219,  0.0219,  0.0053,\n",
            "          0.0282,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,     nan,  0.0914,  0.0914,     nan,  0.0282,\n",
            "          0.0219,  0.0219,  0.0282,  0.0219,  0.0219,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0181,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0604,  0.0429,     nan,  0.0219,  0.0282,\n",
            "          0.0282,  0.0282,  0.0219,  0.0282,  0.0282,  0.0282,  0.0219,  0.0219,\n",
            "          0.0219,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0396,  0.0396,  0.0219,  0.0053,  0.0219,  0.0219,  0.0016,  0.0219,\n",
            "          0.0107,  0.0219,  0.0107,  0.0107,  0.0107,  0.0107,  0.0187,  0.0904,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0414,  0.0485,  0.0485,  0.0016,  0.0282,     nan,  0.0282,  0.0282,\n",
            "          0.0016,  0.0117,  0.0117,  0.0107,  0.0016,  0.0107,  0.0282,  0.0107,\n",
            "             nan,  0.0330,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0904,  0.0396,  0.0396,  0.0396,  0.0396,  0.0396,\n",
            "          0.0396,  0.0396,  0.0282,  0.0282,  0.0016,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0219,  0.0181,  0.0107,  0.0282,  0.0282,  0.0282,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,     nan,  0.0219,  0.0219,  0.0282,\n",
            "          0.0282,  0.0219,  0.0282,  0.0219,  0.0219,  0.0053,  0.0107,  0.0282,\n",
            "          0.0219,  0.0219,     nan,  0.0972,  0.0972,  0.1003,  0.1003,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0396,  0.0414,  0.0282,  0.0219,\n",
            "          0.0219,  0.0282,  0.0107,  0.0219,  0.0016,  0.0219,  0.0219,  0.0219,\n",
            "          0.0107,  0.0219,  0.0219,  0.0219,  0.0219,  0.0056,  0.0282,  0.0396,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0389,  0.0219,\n",
            "          0.0219,  0.0219,  0.0282,  0.0053,  0.0219,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0219,  0.0282,  0.0396,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0330,  0.0396,     nan,  0.0282,  0.0219,\n",
            "          0.0016,  0.0016,  0.0016,  0.0107,  0.0282,  0.0016,  0.0016,  0.0181,\n",
            "          0.0219,  0.0219,  0.0219,  0.0396,  0.0972,  0.1049,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0410,  0.0330,  0.0330,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0187,  0.0107,  0.0219,  0.0282,  0.0282,  0.0282,\n",
            "          0.0219,  0.0107,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0396,  0.0396,  0.0396,  0.0930,  0.0930,\n",
            "          0.0219,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0107,  0.0219,\n",
            "          0.0282,  0.0107,  0.0249,  0.0972,  0.0972,  0.0972,  0.0972,  0.1022,\n",
            "          0.0972,  0.0972,  0.0396,  0.0414,  0.0414,  0.0414,  0.0414,  0.0414,\n",
            "          0.0282,  0.0282,  0.0282,  0.0107,  0.0219,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0107,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0410,     nan,     nan,  0.0395,  0.0282,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0107,  0.0282,  0.0282,  0.0219,  0.0219,  0.0282,\n",
            "          0.0219,  0.0219,  0.0219,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0396,  0.0396,  0.0396,  0.0396,  0.0282,  0.0219,  0.0107,\n",
            "          0.0016,  0.0107,  0.0219,  0.0107,  0.0219,  0.0107,  0.0282,  0.0282,\n",
            "          0.0219,  0.0219,  0.0972,  0.1049,  0.1448,  0.1332,  0.1332,  0.1280,\n",
            "          0.1365,  0.1365,  0.1049,     nan,  0.0396,  0.0396,  0.0396,  0.0465,\n",
            "          0.0219,  0.0219,  0.0181,  0.0282,  0.0181]])\n",
            "tensor([[ 0.0345,  0.0345,  0.0345,  0.0345,  0.0345,  0.0345,  0.0345, -0.0493,\n",
            "          0.0331,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341, -0.0199, -0.0199, -0.0199,  0.0331,\n",
            "          0.0345,  0.0345,  0.0331,  0.0345,  0.0345,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345, -0.0293,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0048,  0.0353,  0.0353,  0.0345,  0.0331,\n",
            "          0.0331,  0.0331,  0.0345,  0.0331,  0.0331,  0.0331,  0.0345,  0.0345,\n",
            "          0.0345,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0174,  0.0174,  0.0345, -0.0493,  0.0345,  0.0345,  0.0026,  0.0345,\n",
            "         -0.0405,  0.0345, -0.0405, -0.0405, -0.0405, -0.0405,  0.0825,  0.0300,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0237,  0.0230,  0.0230,  0.0026,  0.0331,  0.0331,  0.0331,  0.0331,\n",
            "          0.0026,  0.0450,  0.0450, -0.0405,  0.0026, -0.0405,  0.0331, -0.0405,\n",
            "         -0.0405,  0.0199,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0300,  0.0174,  0.0174,  0.0174,  0.0174,  0.0174,\n",
            "          0.0174,  0.0174,  0.0331,  0.0331,  0.0026,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345,  0.0345, -0.0293, -0.0405,  0.0331,  0.0331,  0.0331,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0345,  0.0345,  0.0331,\n",
            "          0.0331,  0.0345,  0.0331,  0.0345,  0.0345, -0.0493, -0.0405,  0.0331,\n",
            "          0.0345,  0.0345,  0.0345,  0.0341,  0.0341,  0.0285,  0.0285,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,  0.0174,  0.0237,  0.0331,  0.0345,\n",
            "          0.0345,  0.0331, -0.0405,  0.0345,  0.0026,  0.0345,  0.0345,  0.0345,\n",
            "         -0.0405,  0.0345,  0.0345,  0.0345,  0.0345, -0.0804,  0.0331,  0.0174,\n",
            "          0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341, -0.0061,  0.0345,\n",
            "          0.0345,  0.0345,  0.0331, -0.0493,  0.0345,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345,  0.0345,  0.0331,  0.0174,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0199,  0.0174,  0.0174,  0.0331,  0.0345,\n",
            "          0.0026,  0.0026,  0.0026, -0.0405,  0.0331,  0.0026,  0.0026, -0.0293,\n",
            "          0.0345,  0.0345,  0.0345,  0.0174,  0.0341,  0.0375,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341, -0.0006,  0.0199,  0.0199,  0.0331,  0.0331,  0.0331,\n",
            "          0.0331,  0.0331,  0.0825, -0.0405,  0.0345,  0.0331,  0.0331,  0.0331,\n",
            "          0.0345, -0.0405,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0341,  0.0341,  0.0174,  0.0174,  0.0174, -0.0114, -0.0114,\n",
            "          0.0345,  0.0331,  0.0331,  0.0331,  0.0331,  0.0331, -0.0405,  0.0345,\n",
            "          0.0331, -0.0405,  0.0006,  0.0341,  0.0341,  0.0341,  0.0341,  0.0315,\n",
            "          0.0341,  0.0341,  0.0174,  0.0237,  0.0237,  0.0237,  0.0237,  0.0237,\n",
            "          0.0331,  0.0331,  0.0331, -0.0405,  0.0345,  0.0331,  0.0331,  0.0331,\n",
            "          0.0331, -0.0405,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "         -0.0006, -0.0006, -0.0006,  0.0313,  0.0331,  0.0345,  0.0345,  0.0345,\n",
            "          0.0345,  0.0345, -0.0405,  0.0331,  0.0331,  0.0345,  0.0345,  0.0331,\n",
            "          0.0345,  0.0345,  0.0345,  0.0331,  0.0341,  0.0341,  0.0341,  0.0341,\n",
            "          0.0341,  0.0174,  0.0174,  0.0174,  0.0174,  0.0331,  0.0345, -0.0405,\n",
            "          0.0026, -0.0405,  0.0345, -0.0405,  0.0345, -0.0405,  0.0331,  0.0331,\n",
            "          0.0345,  0.0345,  0.0341,  0.0375, -0.0191, -0.0060, -0.0060, -0.0436,\n",
            "         -0.0306, -0.0306,  0.0375,  0.0375,  0.0174,  0.0174,  0.0174,  0.0004,\n",
            "          0.0345,  0.0345, -0.0293,  0.0331, -0.0293],\n",
            "        [ 0.0219,  0.0219,  0.0219,  0.0219,  0.0219,  0.0219,  0.0219,  0.0053,\n",
            "          0.0282,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0914,  0.0914,  0.0914,  0.0282,\n",
            "          0.0219,  0.0219,  0.0282,  0.0219,  0.0219,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0181,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0604,  0.0429,  0.0429,  0.0219,  0.0282,\n",
            "          0.0282,  0.0282,  0.0219,  0.0282,  0.0282,  0.0282,  0.0219,  0.0219,\n",
            "          0.0219,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0396,  0.0396,  0.0219,  0.0053,  0.0219,  0.0219,  0.0016,  0.0219,\n",
            "          0.0107,  0.0219,  0.0107,  0.0107,  0.0107,  0.0107,  0.0187,  0.0904,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0414,  0.0485,  0.0485,  0.0016,  0.0282,  0.0282,  0.0282,  0.0282,\n",
            "          0.0016,  0.0117,  0.0117,  0.0107,  0.0016,  0.0107,  0.0282,  0.0107,\n",
            "          0.0107,  0.0330,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0904,  0.0396,  0.0396,  0.0396,  0.0396,  0.0396,\n",
            "          0.0396,  0.0396,  0.0282,  0.0282,  0.0016,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0219,  0.0181,  0.0107,  0.0282,  0.0282,  0.0282,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0219,  0.0219,  0.0282,\n",
            "          0.0282,  0.0219,  0.0282,  0.0219,  0.0219,  0.0053,  0.0107,  0.0282,\n",
            "          0.0219,  0.0219,  0.0219,  0.0972,  0.0972,  0.1003,  0.1003,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0396,  0.0414,  0.0282,  0.0219,\n",
            "          0.0219,  0.0282,  0.0107,  0.0219,  0.0016,  0.0219,  0.0219,  0.0219,\n",
            "          0.0107,  0.0219,  0.0219,  0.0219,  0.0219,  0.0056,  0.0282,  0.0396,\n",
            "          0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0389,  0.0219,\n",
            "          0.0219,  0.0219,  0.0282,  0.0053,  0.0219,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0219,  0.0282,  0.0396,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0330,  0.0396,  0.0396,  0.0282,  0.0219,\n",
            "          0.0016,  0.0016,  0.0016,  0.0107,  0.0282,  0.0016,  0.0016,  0.0181,\n",
            "          0.0219,  0.0219,  0.0219,  0.0396,  0.0972,  0.1049,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0410,  0.0330,  0.0330,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0282,  0.0187,  0.0107,  0.0219,  0.0282,  0.0282,  0.0282,\n",
            "          0.0219,  0.0107,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0972,  0.0972,  0.0396,  0.0396,  0.0396,  0.0930,  0.0930,\n",
            "          0.0219,  0.0282,  0.0282,  0.0282,  0.0282,  0.0282,  0.0107,  0.0219,\n",
            "          0.0282,  0.0107,  0.0249,  0.0972,  0.0972,  0.0972,  0.0972,  0.1022,\n",
            "          0.0972,  0.0972,  0.0396,  0.0414,  0.0414,  0.0414,  0.0414,  0.0414,\n",
            "          0.0282,  0.0282,  0.0282,  0.0107,  0.0219,  0.0282,  0.0282,  0.0282,\n",
            "          0.0282,  0.0107,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0410,  0.0410,  0.0410,  0.0395,  0.0282,  0.0219,  0.0219,  0.0219,\n",
            "          0.0219,  0.0219,  0.0107,  0.0282,  0.0282,  0.0219,  0.0219,  0.0282,\n",
            "          0.0219,  0.0219,  0.0219,  0.0282,  0.0972,  0.0972,  0.0972,  0.0972,\n",
            "          0.0972,  0.0396,  0.0396,  0.0396,  0.0396,  0.0282,  0.0219,  0.0107,\n",
            "          0.0016,  0.0107,  0.0219,  0.0107,  0.0219,  0.0107,  0.0282,  0.0282,\n",
            "          0.0219,  0.0219,  0.0972,  0.1049,  0.1448,  0.1332,  0.1332,  0.1280,\n",
            "          0.1365,  0.1365,  0.1049,  0.1049,  0.0396,  0.0396,  0.0396,  0.0465,\n",
            "          0.0219,  0.0219,  0.0181,  0.0282,  0.0181]])\n",
            "tensor([[0.0846, 0.0846, 0.0846,  ..., 0.0548, 0.0996, 0.1011],\n",
            "        [0.0448, 0.0448, 0.0448,  ..., 0.0727, 0.0615, 0.0478]])\n",
            "tensor([[0.0846, 0.0846, 0.0846,  ..., 0.0548, 0.0996, 0.1011],\n",
            "        [0.0448, 0.0448, 0.0448,  ..., 0.0727, 0.0615, 0.0478]])\n",
            "tensor([[nan],\n",
            "        [nan]])\n",
            "tensor([], size=(2, 0))\n",
            "tensor([[0.1777, 0.1703, 0.1777, 0.1760, 0.1703, 0.1703, 0.1703, 0.1777, 0.1777,\n",
            "         0.1777, 0.1703, 0.0285, 0.0243, 0.1777, 0.1703, 0.1193, 0.1703, 0.0500,\n",
            "         0.1777, 0.1594, 0.1594, 0.1777,    nan,    nan, 0.1703, 0.1703,    nan,\n",
            "         0.1594, 0.1594, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703,\n",
            "         0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703,\n",
            "         0.0500,    nan, 0.1098,    nan,    nan, 0.0243, 0.0243, 0.0248, 0.1594,\n",
            "            nan,    nan,    nan,    nan,    nan, 0.0243, 0.0243, 0.0243, 0.0243,\n",
            "         0.0243, 0.0080, 0.1703,    nan,    nan,    nan,    nan, 0.0732, 0.0732,\n",
            "         0.1193, 0.1193, 0.1703, 0.1193, 0.1147, 0.1703,    nan,    nan,    nan,\n",
            "         0.1193, 0.0555,    nan],\n",
            "        [0.1572, 0.1545, 0.1572, 0.1671, 0.1545, 0.1545, 0.1545, 0.1572, 0.1572,\n",
            "         0.1572, 0.1545, 0.1631, 0.1570, 0.1572, 0.1545, 0.1351, 0.1545, 0.1694,\n",
            "         0.1572, 0.1539, 0.1539, 0.1572,    nan,    nan, 0.1545, 0.1545,    nan,\n",
            "         0.1539, 0.1539, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545,\n",
            "         0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545,\n",
            "         0.1694,    nan, 0.1365,    nan,    nan, 0.1570, 0.1570, 0.1578, 0.1539,\n",
            "            nan,    nan,    nan,    nan,    nan, 0.1570, 0.1570, 0.1570, 0.1570,\n",
            "         0.1570, 0.0949, 0.1545,    nan,    nan,    nan,    nan, 0.0913, 0.0913,\n",
            "         0.1351, 0.1351, 0.1545, 0.1351, 0.1240, 0.1545,    nan,    nan,    nan,\n",
            "         0.1351, 0.1205,    nan]])\n",
            "tensor([[0.1777, 0.1703, 0.1777, 0.1760, 0.1703, 0.1703, 0.1703, 0.1777, 0.1777,\n",
            "         0.1777, 0.1703, 0.0285, 0.0243, 0.1777, 0.1703, 0.1193, 0.1703, 0.0500,\n",
            "         0.1777, 0.1594, 0.1594, 0.1777, 0.1777, 0.1777, 0.1703, 0.1703, 0.1703,\n",
            "         0.1594, 0.1594, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703,\n",
            "         0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703,\n",
            "         0.0500, 0.0500, 0.1098, 0.1098, 0.1098, 0.0243, 0.0243, 0.0248, 0.1594,\n",
            "         0.1594, 0.1594, 0.1594, 0.1594, 0.1594, 0.0243, 0.0243, 0.0243, 0.0243,\n",
            "         0.0243, 0.0080, 0.1703, 0.1703, 0.1703, 0.1703, 0.1703, 0.0732, 0.0732,\n",
            "         0.1193, 0.1193, 0.1703, 0.1193, 0.1147, 0.1703, 0.1703, 0.1703, 0.1703,\n",
            "         0.1193, 0.0555, 0.0555],\n",
            "        [0.1572, 0.1545, 0.1572, 0.1671, 0.1545, 0.1545, 0.1545, 0.1572, 0.1572,\n",
            "         0.1572, 0.1545, 0.1631, 0.1570, 0.1572, 0.1545, 0.1351, 0.1545, 0.1694,\n",
            "         0.1572, 0.1539, 0.1539, 0.1572, 0.1572, 0.1572, 0.1545, 0.1545, 0.1545,\n",
            "         0.1539, 0.1539, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545,\n",
            "         0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545,\n",
            "         0.1694, 0.1694, 0.1365, 0.1365, 0.1365, 0.1570, 0.1570, 0.1578, 0.1539,\n",
            "         0.1539, 0.1539, 0.1539, 0.1539, 0.1539, 0.1570, 0.1570, 0.1570, 0.1570,\n",
            "         0.1570, 0.0949, 0.1545, 0.1545, 0.1545, 0.1545, 0.1545, 0.0913, 0.0913,\n",
            "         0.1351, 0.1351, 0.1545, 0.1351, 0.1240, 0.1545, 0.1545, 0.1545, 0.1545,\n",
            "         0.1351, 0.1205, 0.1205]])\n",
            "tensor([[    nan, -0.0766, -0.0754, -0.0882, -0.0882, -0.1127, -0.0882, -0.0882,\n",
            "         -0.0882, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973],\n",
            "        [    nan,  0.0728,  0.0592,  0.0652,  0.0652,  0.0802,  0.0652,  0.0652,\n",
            "          0.0652,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583]])\n",
            "tensor([[-0.0766, -0.0754, -0.0882, -0.0882, -0.1127, -0.0882, -0.0882, -0.0882,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973, -0.0973,\n",
            "         -0.0973],\n",
            "        [ 0.0728,  0.0592,  0.0652,  0.0652,  0.0802,  0.0652,  0.0652,  0.0652,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,  0.0583,\n",
            "          0.0583]])\n",
            "tensor([[ 0.0723,  0.0723,  0.0723, -0.4736, -0.4736, -0.4736,     nan, -0.4053],\n",
            "        [-0.0011, -0.0011, -0.0011, -0.0305, -0.0305, -0.0305,     nan,  0.1283]])\n",
            "tensor([[ 0.0723,  0.0723,  0.0723, -0.4736, -0.4736, -0.4736, -0.4736, -0.4053],\n",
            "        [-0.0011, -0.0011, -0.0011, -0.0305, -0.0305, -0.0305, -0.0305,  0.1283]])\n",
            "tensor([[-0.1167, -0.1099, -0.1099, -0.1249, -0.1249, -0.1167, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1393, -0.1393, -0.1232, -0.1232, -0.1232, -0.1232, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1054, -0.0503, -0.0693, -0.0046,\n",
            "         -0.0046, -0.1032, -0.1229, -0.0813, -0.1257, -0.0813, -0.0813, -0.1249,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1249, -0.1117, -0.1249, -0.1167, -0.1249,\n",
            "         -0.1249, -0.1249, -0.0971, -0.0971, -0.1194, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1167, -0.1167, -0.1249, -0.1249, -0.1249, -0.1167,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1099, -0.1127, -0.1127, -0.1169, -0.1127, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1167, -0.1249,\n",
            "         -0.1249, -0.1099, -0.1167, -0.1167, -0.1167, -0.1099, -0.1167, -0.0971,\n",
            "         -0.0971, -0.0971, -0.1194, -0.1249, -0.1249, -0.1054, -0.0214,  0.0060,\n",
            "          0.0060,  0.0080,  0.0060,  0.0080,  0.0041,  0.0080, -0.1167, -0.1167,\n",
            "         -0.1249, -0.1249, -0.1099, -0.1167, -0.1167, -0.1167, -0.1099, -0.1169,\n",
            "         -0.1169, -0.1249, -0.1127, -0.1249, -0.1249, -0.0827, -0.1054, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1167, -0.1017, -0.1167, -0.1167, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1099, -0.1249, -0.1249, -0.1249, -0.1099,\n",
            "         -0.1099, -0.1099, -0.1167, -0.1099, -0.1099, -0.1249, -0.1167, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1167, -0.1167, -0.1167, -0.1167, -0.1167, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1054, -0.1167, -0.1054, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1167, -0.1249, -0.1249, -0.0971, -0.1017,\n",
            "         -0.1249, -0.1249, -0.0383, -0.0303, -0.0214, -0.0177,  0.0397,  0.0437,\n",
            "          0.0437,  0.0706,     nan, -0.0926, -0.1249, -0.1054, -0.1167, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1249, -0.1167, -0.1099, -0.1249, -0.1249,  0.0049,\n",
            "          0.0097, -0.0813, -0.1125, -0.1099, -0.1099, -0.1127, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1099, -0.1017, -0.1249, -0.1249, -0.0750, -0.0797,\n",
            "         -0.1121, -0.1121, -0.1121, -0.0797, -0.1167, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1167, -0.1249, -0.1249, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1249, -0.1167, -0.1249, -0.1249, -0.1167,\n",
            "         -0.1099, -0.1167, -0.1099, -0.1099, -0.1099, -0.1099, -0.1099, -0.1099,\n",
            "         -0.1249, -0.0978, -0.0817, -0.0817, -0.1017, -0.0696, -0.0803, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1167, -0.1167, -0.1167, -0.1125, -0.1257, -0.1224,\n",
            "         -0.1127, -0.1127, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1167, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1249, -0.1249, -0.1167, -0.1167, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1167, -0.1249, -0.1167, -0.1249, -0.1167],\n",
            "        [ 0.1038,  0.0855,  0.0855,  0.1215,  0.1215,  0.1038,  0.1215,  0.1215,\n",
            "          0.1215,  0.0700,  0.0700,  0.0539,  0.0539,  0.0539,  0.0539,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1089,  0.1943,  0.1302,  0.1112,\n",
            "          0.1112,  0.0092,  0.0142,  0.0699,  0.0480,  0.0699,  0.0699,  0.1215,\n",
            "          0.1038,  0.1038,  0.1038,  0.1215,  0.0353,  0.1215,  0.1038,  0.1215,\n",
            "          0.1215,  0.1215,  0.0823,  0.0823,  0.0870,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.1038,  0.1038,  0.1215,  0.1215,  0.1215,  0.1038,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.0855,  0.0802,  0.0802,  0.0609,  0.0802,  0.1215,  0.1215,\n",
            "          0.1038,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1038,  0.1215,\n",
            "          0.1215,  0.0855,  0.1038,  0.1038,  0.1038,  0.0855,  0.1038,  0.0823,\n",
            "          0.0823,  0.0823,  0.0870,  0.1215,  0.1215,  0.1089,  0.0964,  0.0949,\n",
            "          0.0949,  0.0949,  0.0949,  0.0949,  0.0950,  0.0949,  0.1038,  0.1038,\n",
            "          0.1215,  0.1215,  0.0855,  0.1038,  0.1038,  0.1038,  0.0855,  0.0609,\n",
            "          0.0609,  0.1215,  0.0802,  0.1215,  0.1215,  0.1372,  0.1089,  0.1038,\n",
            "          0.1038,  0.1038,  0.1038,  0.1038,  0.1015,  0.1038,  0.1038,  0.1038,\n",
            "          0.1038,  0.1038,  0.1038,  0.0855,  0.1215,  0.1215,  0.1215,  0.0855,\n",
            "          0.0855,  0.0855,  0.1038,  0.0855,  0.0855,  0.1215,  0.1038,  0.1215,\n",
            "          0.1038,  0.1215,  0.1038,  0.1038,  0.1038,  0.1038,  0.1038,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1089,  0.1038,  0.1089,  0.1215,  0.1215,\n",
            "          0.1038,  0.1038,  0.1038,  0.1038,  0.1215,  0.1215,  0.0823,  0.1015,\n",
            "          0.1215,  0.1215,  0.0912,  0.0946,  0.0964,  0.1042,  0.1003,  0.1009,\n",
            "          0.1009,  0.1009,     nan,  0.0807,  0.1215,  0.1089,  0.1038,  0.1215,\n",
            "          0.1038,  0.1215,  0.1215,  0.1038,  0.0855,  0.1215,  0.1215,  0.0860,\n",
            "          0.0844,  0.0699,  0.0508,  0.0855,  0.0855,  0.0802,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.0855,  0.1015,  0.1215,  0.1215, -0.0685, -0.1480,\n",
            "         -0.1409, -0.1409, -0.1409, -0.1480,  0.1038,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1038,  0.1215,  0.1215,  0.1038,\n",
            "          0.1038,  0.1038,  0.1038,  0.1215,  0.1038,  0.1215,  0.1215,  0.1038,\n",
            "          0.0855,  0.1038,  0.0855,  0.0855,  0.0855,  0.0855,  0.0855,  0.0855,\n",
            "          0.1215,  0.1154,  0.1057,  0.1057,  0.1015,  0.1150,  0.0934,  0.1038,\n",
            "          0.1038,  0.1038,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,\n",
            "          0.1038,  0.1215,  0.1038,  0.1038,  0.1038,  0.0508,  0.0480,  0.0710,\n",
            "          0.0802,  0.0802,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1038,  0.1038,\n",
            "          0.1038,  0.1038,  0.1215,  0.1215,  0.1038,  0.1038,  0.1215,  0.1215,\n",
            "          0.1038,  0.1038,  0.1215,  0.1038,  0.1215,  0.1038]])\n",
            "tensor([[-0.1167, -0.1099, -0.1099, -0.1249, -0.1249, -0.1167, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1393, -0.1393, -0.1232, -0.1232, -0.1232, -0.1232, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1054, -0.0503, -0.0693, -0.0046,\n",
            "         -0.0046, -0.1032, -0.1229, -0.0813, -0.1257, -0.0813, -0.0813, -0.1249,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1249, -0.1117, -0.1249, -0.1167, -0.1249,\n",
            "         -0.1249, -0.1249, -0.0971, -0.0971, -0.1194, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1167, -0.1167, -0.1249, -0.1249, -0.1249, -0.1167,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1099, -0.1127, -0.1127, -0.1169, -0.1127, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1167, -0.1249,\n",
            "         -0.1249, -0.1099, -0.1167, -0.1167, -0.1167, -0.1099, -0.1167, -0.0971,\n",
            "         -0.0971, -0.0971, -0.1194, -0.1249, -0.1249, -0.1054, -0.0214,  0.0060,\n",
            "          0.0060,  0.0080,  0.0060,  0.0080,  0.0041,  0.0080, -0.1167, -0.1167,\n",
            "         -0.1249, -0.1249, -0.1099, -0.1167, -0.1167, -0.1167, -0.1099, -0.1169,\n",
            "         -0.1169, -0.1249, -0.1127, -0.1249, -0.1249, -0.0827, -0.1054, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1167, -0.1017, -0.1167, -0.1167, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1099, -0.1249, -0.1249, -0.1249, -0.1099,\n",
            "         -0.1099, -0.1099, -0.1167, -0.1099, -0.1099, -0.1249, -0.1167, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1167, -0.1167, -0.1167, -0.1167, -0.1167, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1054, -0.1167, -0.1054, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1167, -0.1249, -0.1249, -0.0971, -0.1017,\n",
            "         -0.1249, -0.1249, -0.0383, -0.0303, -0.0214, -0.0177,  0.0397,  0.0437,\n",
            "          0.0437,  0.0706,  0.0706, -0.0926, -0.1249, -0.1054, -0.1167, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1249, -0.1167, -0.1099, -0.1249, -0.1249,  0.0049,\n",
            "          0.0097, -0.0813, -0.1125, -0.1099, -0.1099, -0.1127, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1099, -0.1017, -0.1249, -0.1249, -0.0750, -0.0797,\n",
            "         -0.1121, -0.1121, -0.1121, -0.0797, -0.1167, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1167, -0.1249, -0.1249, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1167, -0.1249, -0.1167, -0.1249, -0.1249, -0.1167,\n",
            "         -0.1099, -0.1167, -0.1099, -0.1099, -0.1099, -0.1099, -0.1099, -0.1099,\n",
            "         -0.1249, -0.0978, -0.0817, -0.0817, -0.1017, -0.0696, -0.0803, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1249, -0.1167, -0.1167, -0.1167, -0.1125, -0.1257, -0.1224,\n",
            "         -0.1127, -0.1127, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249,\n",
            "         -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1249, -0.1167, -0.1167,\n",
            "         -0.1167, -0.1167, -0.1249, -0.1249, -0.1167, -0.1167, -0.1249, -0.1249,\n",
            "         -0.1167, -0.1167, -0.1249, -0.1167, -0.1249, -0.1167],\n",
            "        [ 0.1038,  0.0855,  0.0855,  0.1215,  0.1215,  0.1038,  0.1215,  0.1215,\n",
            "          0.1215,  0.0700,  0.0700,  0.0539,  0.0539,  0.0539,  0.0539,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1089,  0.1943,  0.1302,  0.1112,\n",
            "          0.1112,  0.0092,  0.0142,  0.0699,  0.0480,  0.0699,  0.0699,  0.1215,\n",
            "          0.1038,  0.1038,  0.1038,  0.1215,  0.0353,  0.1215,  0.1038,  0.1215,\n",
            "          0.1215,  0.1215,  0.0823,  0.0823,  0.0870,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.1038,  0.1038,  0.1215,  0.1215,  0.1215,  0.1038,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.0855,  0.0802,  0.0802,  0.0609,  0.0802,  0.1215,  0.1215,\n",
            "          0.1038,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1038,  0.1215,\n",
            "          0.1215,  0.0855,  0.1038,  0.1038,  0.1038,  0.0855,  0.1038,  0.0823,\n",
            "          0.0823,  0.0823,  0.0870,  0.1215,  0.1215,  0.1089,  0.0964,  0.0949,\n",
            "          0.0949,  0.0949,  0.0949,  0.0949,  0.0950,  0.0949,  0.1038,  0.1038,\n",
            "          0.1215,  0.1215,  0.0855,  0.1038,  0.1038,  0.1038,  0.0855,  0.0609,\n",
            "          0.0609,  0.1215,  0.0802,  0.1215,  0.1215,  0.1372,  0.1089,  0.1038,\n",
            "          0.1038,  0.1038,  0.1038,  0.1038,  0.1015,  0.1038,  0.1038,  0.1038,\n",
            "          0.1038,  0.1038,  0.1038,  0.0855,  0.1215,  0.1215,  0.1215,  0.0855,\n",
            "          0.0855,  0.0855,  0.1038,  0.0855,  0.0855,  0.1215,  0.1038,  0.1215,\n",
            "          0.1038,  0.1215,  0.1038,  0.1038,  0.1038,  0.1038,  0.1038,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1089,  0.1038,  0.1089,  0.1215,  0.1215,\n",
            "          0.1038,  0.1038,  0.1038,  0.1038,  0.1215,  0.1215,  0.0823,  0.1015,\n",
            "          0.1215,  0.1215,  0.0912,  0.0946,  0.0964,  0.1042,  0.1003,  0.1009,\n",
            "          0.1009,  0.1009,  0.1009,  0.0807,  0.1215,  0.1089,  0.1038,  0.1215,\n",
            "          0.1038,  0.1215,  0.1215,  0.1038,  0.0855,  0.1215,  0.1215,  0.0860,\n",
            "          0.0844,  0.0699,  0.0508,  0.0855,  0.0855,  0.0802,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.0855,  0.1015,  0.1215,  0.1215, -0.0685, -0.1480,\n",
            "         -0.1409, -0.1409, -0.1409, -0.1480,  0.1038,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1038,  0.1215,  0.1215,  0.1038,\n",
            "          0.1038,  0.1038,  0.1038,  0.1215,  0.1038,  0.1215,  0.1215,  0.1038,\n",
            "          0.0855,  0.1038,  0.0855,  0.0855,  0.0855,  0.0855,  0.0855,  0.0855,\n",
            "          0.1215,  0.1154,  0.1057,  0.1057,  0.1015,  0.1150,  0.0934,  0.1038,\n",
            "          0.1038,  0.1038,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,\n",
            "          0.1038,  0.1215,  0.1038,  0.1038,  0.1038,  0.0508,  0.0480,  0.0710,\n",
            "          0.0802,  0.0802,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,\n",
            "          0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1215,  0.1038,  0.1038,\n",
            "          0.1038,  0.1038,  0.1215,  0.1215,  0.1038,  0.1038,  0.1215,  0.1215,\n",
            "          0.1038,  0.1038,  0.1215,  0.1038,  0.1215,  0.1038]])\n",
            "tensor([[ 0.0809,  0.0809,  0.0792,  0.0792,  0.1381,  0.1414,  0.1414,  0.1381,\n",
            "          0.1381,  0.1404,  0.2515,  0.1496,  0.1496,  0.1496,  0.0698,  0.0698,\n",
            "          0.1381,  0.1381,  0.1381,  0.1381,  0.2515,  0.2515,  0.1381,  0.1381,\n",
            "          0.1381,  0.0763,  0.0763,  0.0809,  0.0809,  0.2031,  0.2031,  0.1381,\n",
            "          0.1381,  0.1381,  0.1381,  0.0750,  0.0750, -0.1022, -0.1022,  0.1223,\n",
            "          0.1223,  0.1095,  0.1095,  0.2515,  0.2515,  0.0795,  0.0795,  0.0795,\n",
            "          0.0795,  0.0795,  0.0763,  0.0763,  0.1381,  0.1381,  0.1106,  0.1106,\n",
            "          0.1368,  0.1368,  0.1259,  0.1259,  0.1259,  0.1095,  0.1095,  0.1095,\n",
            "          0.1381,  0.1381,  0.2515,  0.2515,  0.1381,  0.1031,  0.1031,  0.1381,\n",
            "          0.1381,  0.1095,  0.1095,  0.1404,  0.1404,  0.1160,  0.1160,  0.1410,\n",
            "          0.1410,  0.1095,  0.2515,  0.2515,  0.2515,  0.2515,  0.1381,  0.1381,\n",
            "          0.1381,  0.1381,  0.1496,  0.1413,  0.1413,  0.1795,  0.1795,  0.1381,\n",
            "          0.1381,  0.1381,  0.1381,  0.1381,  0.1381,  0.1381,  0.1223,  0.1381,\n",
            "          0.1381,  0.0792,  0.1381,  0.1381,  0.1381,  0.1381,  0.1381,  0.0792,\n",
            "          0.0792,  0.0655,  0.0655,  0.0809,  0.0809,  0.0792,  0.0792,  0.0792,\n",
            "          0.0792,  0.0792,  0.1381,  0.1031,     nan],\n",
            "        [ 0.0514,  0.0514,  0.0458,  0.0458, -0.0138, -0.0613, -0.0613, -0.0138,\n",
            "         -0.0138, -0.0053, -0.1053, -0.0005, -0.0005, -0.0005,  0.0682,  0.0682,\n",
            "         -0.0138, -0.0138, -0.0138, -0.0138, -0.1053, -0.1053, -0.0138, -0.0138,\n",
            "         -0.0138,  0.0643,  0.0643,  0.0514,  0.0514,  0.1098,  0.1098, -0.0138,\n",
            "         -0.0138, -0.0138, -0.0138,  0.0672,  0.0672,  0.1002,  0.1002,  0.0238,\n",
            "          0.0238, -0.0124, -0.0124, -0.1053, -0.1053,  0.0516,  0.0516,  0.0516,\n",
            "          0.0516,  0.0516,  0.0643,  0.0643, -0.0138, -0.0138, -0.0788, -0.0788,\n",
            "         -0.0935, -0.0935, -0.0389, -0.0389, -0.0389, -0.0124, -0.0124, -0.0124,\n",
            "         -0.0138, -0.0138, -0.1053, -0.1053, -0.0138,  0.0069,  0.0069, -0.0138,\n",
            "         -0.0138, -0.0124, -0.0124, -0.0053, -0.0053, -0.0277, -0.0277, -0.0912,\n",
            "         -0.0912, -0.0124, -0.1053, -0.1053, -0.1053, -0.1053, -0.0138, -0.0138,\n",
            "         -0.0138, -0.0138, -0.1191,  0.0837,  0.0837,  0.1011,  0.1011, -0.0138,\n",
            "         -0.0138, -0.0138, -0.0138, -0.0138, -0.0138, -0.0138,  0.0238, -0.0138,\n",
            "         -0.0138,  0.0458, -0.0138, -0.0138, -0.0138, -0.0138, -0.0138,  0.0458,\n",
            "          0.0458,  0.0525,  0.0525,  0.0514,  0.0514,  0.0458,  0.0458,  0.0458,\n",
            "          0.0458,  0.0458, -0.0138,  0.0069,     nan]])\n",
            "tensor([[ 0.0809,  0.0809,  0.0792,  0.0792,  0.1381,  0.1414,  0.1414,  0.1381,\n",
            "          0.1381,  0.1404,  0.2515,  0.1496,  0.1496,  0.1496,  0.0698,  0.0698,\n",
            "          0.1381,  0.1381,  0.1381,  0.1381,  0.2515,  0.2515,  0.1381,  0.1381,\n",
            "          0.1381,  0.0763,  0.0763,  0.0809,  0.0809,  0.2031,  0.2031,  0.1381,\n",
            "          0.1381,  0.1381,  0.1381,  0.0750,  0.0750, -0.1022, -0.1022,  0.1223,\n",
            "          0.1223,  0.1095,  0.1095,  0.2515,  0.2515,  0.0795,  0.0795,  0.0795,\n",
            "          0.0795,  0.0795,  0.0763,  0.0763,  0.1381,  0.1381,  0.1106,  0.1106,\n",
            "          0.1368,  0.1368,  0.1259,  0.1259,  0.1259,  0.1095,  0.1095,  0.1095,\n",
            "          0.1381,  0.1381,  0.2515,  0.2515,  0.1381,  0.1031,  0.1031,  0.1381,\n",
            "          0.1381,  0.1095,  0.1095,  0.1404,  0.1404,  0.1160,  0.1160,  0.1410,\n",
            "          0.1410,  0.1095,  0.2515,  0.2515,  0.2515,  0.2515,  0.1381,  0.1381,\n",
            "          0.1381,  0.1381,  0.1496,  0.1413,  0.1413,  0.1795,  0.1795,  0.1381,\n",
            "          0.1381,  0.1381,  0.1381,  0.1381,  0.1381,  0.1381,  0.1223,  0.1381,\n",
            "          0.1381,  0.0792,  0.1381,  0.1381,  0.1381,  0.1381,  0.1381,  0.0792,\n",
            "          0.0792,  0.0655,  0.0655,  0.0809,  0.0809,  0.0792,  0.0792,  0.0792,\n",
            "          0.0792,  0.0792,  0.1381,  0.1031,  0.1031],\n",
            "        [ 0.0514,  0.0514,  0.0458,  0.0458, -0.0138, -0.0613, -0.0613, -0.0138,\n",
            "         -0.0138, -0.0053, -0.1053, -0.0005, -0.0005, -0.0005,  0.0682,  0.0682,\n",
            "         -0.0138, -0.0138, -0.0138, -0.0138, -0.1053, -0.1053, -0.0138, -0.0138,\n",
            "         -0.0138,  0.0643,  0.0643,  0.0514,  0.0514,  0.1098,  0.1098, -0.0138,\n",
            "         -0.0138, -0.0138, -0.0138,  0.0672,  0.0672,  0.1002,  0.1002,  0.0238,\n",
            "          0.0238, -0.0124, -0.0124, -0.1053, -0.1053,  0.0516,  0.0516,  0.0516,\n",
            "          0.0516,  0.0516,  0.0643,  0.0643, -0.0138, -0.0138, -0.0788, -0.0788,\n",
            "         -0.0935, -0.0935, -0.0389, -0.0389, -0.0389, -0.0124, -0.0124, -0.0124,\n",
            "         -0.0138, -0.0138, -0.1053, -0.1053, -0.0138,  0.0069,  0.0069, -0.0138,\n",
            "         -0.0138, -0.0124, -0.0124, -0.0053, -0.0053, -0.0277, -0.0277, -0.0912,\n",
            "         -0.0912, -0.0124, -0.1053, -0.1053, -0.1053, -0.1053, -0.0138, -0.0138,\n",
            "         -0.0138, -0.0138, -0.1191,  0.0837,  0.0837,  0.1011,  0.1011, -0.0138,\n",
            "         -0.0138, -0.0138, -0.0138, -0.0138, -0.0138, -0.0138,  0.0238, -0.0138,\n",
            "         -0.0138,  0.0458, -0.0138, -0.0138, -0.0138, -0.0138, -0.0138,  0.0458,\n",
            "          0.0458,  0.0525,  0.0525,  0.0514,  0.0514,  0.0458,  0.0458,  0.0458,\n",
            "          0.0458,  0.0458, -0.0138,  0.0069,  0.0069]])\n",
            "tensor([[-0.3827, -0.4163, -0.4057, -0.4133, -0.4118, -0.3201, -0.3827, -0.4057,\n",
            "         -0.4057, -0.5329, -0.5158, -0.5329,     nan, -0.3954, -0.3639, -0.4118,\n",
            "         -0.3810, -0.1574, -0.4163, -0.4065, -0.4199, -0.4199, -0.4199, -0.3639,\n",
            "         -0.4118, -0.5451, -0.3827,     nan, -0.5451, -0.3917,  0.3289, -0.5329,\n",
            "         -0.5158, -0.5329, -0.4057, -0.4118, -0.3954,  0.1142, -0.2781, -0.5329,\n",
            "         -0.5158, -0.5153, -0.4118, -0.4221, -0.4065, -0.4062, -0.3827, -0.4118,\n",
            "         -0.3810, -0.4080,  0.1142, -0.4057,  0.2345, -0.3639, -0.3762, -0.4017,\n",
            "         -0.4053, -0.1125, -0.1263, -0.4053, -0.5158],\n",
            "        [ 0.1502,  0.1098,  0.1091,  0.1148,  0.1074,  0.0963,  0.1502,  0.1091,\n",
            "          0.1091,  0.1228,  0.1592,  0.1228,     nan,  0.1118,  0.1101,  0.1074,\n",
            "          0.1142,  0.0647,  0.1098,  0.1179,  0.1407,  0.1407,  0.1407,  0.1101,\n",
            "          0.1074,  0.0313,  0.1502,     nan,  0.0313,  0.1061,  0.0852,  0.1228,\n",
            "          0.1592,  0.1228,  0.1091,  0.1074,  0.1118,  0.2790,  0.0834,  0.1228,\n",
            "          0.1592,  0.1588,  0.1074,  0.2580,  0.1179,  0.1053,  0.1502,  0.1074,\n",
            "          0.1142,  0.2448,  0.2790,  0.1091,  0.1433,  0.1101,  0.1744,  0.1163,\n",
            "          0.1283,  0.0508,  0.0432,  0.1283,  0.1592]])\n",
            "tensor([[-0.3827, -0.4163, -0.4057, -0.4133, -0.4118, -0.3201, -0.3827, -0.4057,\n",
            "         -0.4057, -0.5329, -0.5158, -0.5329, -0.5329, -0.3954, -0.3639, -0.4118,\n",
            "         -0.3810, -0.1574, -0.4163, -0.4065, -0.4199, -0.4199, -0.4199, -0.3639,\n",
            "         -0.4118, -0.5451, -0.3827, -0.3827, -0.5451, -0.3917,  0.3289, -0.5329,\n",
            "         -0.5158, -0.5329, -0.4057, -0.4118, -0.3954,  0.1142, -0.2781, -0.5329,\n",
            "         -0.5158, -0.5153, -0.4118, -0.4221, -0.4065, -0.4062, -0.3827, -0.4118,\n",
            "         -0.3810, -0.4080,  0.1142, -0.4057,  0.2345, -0.3639, -0.3762, -0.4017,\n",
            "         -0.4053, -0.1125, -0.1263, -0.4053, -0.5158],\n",
            "        [ 0.1502,  0.1098,  0.1091,  0.1148,  0.1074,  0.0963,  0.1502,  0.1091,\n",
            "          0.1091,  0.1228,  0.1592,  0.1228,  0.1228,  0.1118,  0.1101,  0.1074,\n",
            "          0.1142,  0.0647,  0.1098,  0.1179,  0.1407,  0.1407,  0.1407,  0.1101,\n",
            "          0.1074,  0.0313,  0.1502,  0.1502,  0.0313,  0.1061,  0.0852,  0.1228,\n",
            "          0.1592,  0.1228,  0.1091,  0.1074,  0.1118,  0.2790,  0.0834,  0.1228,\n",
            "          0.1592,  0.1588,  0.1074,  0.2580,  0.1179,  0.1053,  0.1502,  0.1074,\n",
            "          0.1142,  0.2448,  0.2790,  0.1091,  0.1433,  0.1101,  0.1744,  0.1163,\n",
            "          0.1283,  0.0508,  0.0432,  0.1283,  0.1592]])\n",
            "tensor([[ 0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.4169,\n",
            "          0.3813,  0.3251,  0.3813,  0.3251,  0.3251,  0.3251,  0.3813,  0.3251,\n",
            "          0.3813,  0.3813,  0.3251,  0.3251,  0.3813,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.3727,  0.3251,  0.3813,  0.1095,  0.1095,  0.1095,  0.3813,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.3727,  0.3251,  0.3251,  0.3867,  0.3251,\n",
            "             nan,  0.3813,     nan, -0.5112, -0.5112, -0.5770,  0.1974,  0.3251,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.3251,  0.3251,\n",
            "          0.3094,  0.3094,  0.3094,  0.3094,  0.1251,  0.0120, -0.1093, -0.1410,\n",
            "         -0.1039,  0.2121,  0.3474,  0.3556,  0.3680,  0.3251,  0.3251,  0.2794,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.3251,  0.3813,     nan,  0.3251,\n",
            "          0.2806,  0.2806,  0.3680,  0.3251,  0.2794,  0.2806,  0.3088,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2805,  0.2805,  0.2888,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.3251,  0.3251,  0.3813,\n",
            "          0.3813,     nan,  0.3813,  0.3813,  0.3251,  0.3813,  0.3552,  0.3813,\n",
            "          0.3251,  0.3251,  0.3485,  0.2806,  0.2806,  0.2806,  0.2806,  0.3813,\n",
            "             nan,  0.3813,  0.3251,  0.3251,  0.3251,  0.3251,  0.3813,  0.3813,\n",
            "          0.3861,  0.3813,  0.3813,  0.3251,  0.3813,  0.3813,  0.3251,  0.3813,\n",
            "          0.3813,  0.3813,  0.3485,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.3251, -0.5125, -0.4389,  0.2818,  0.3251,  0.3229,     nan,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.3680,  0.3813,  0.3813,  0.3251,  0.3813,  0.3251,  0.3251,  0.3251,\n",
            "          0.3251,  0.3813,  0.3251,     nan,  0.3251,  0.3251,  0.2515,  0.2039,\n",
            "          0.2681,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2708,\n",
            "          0.2093,     nan,  0.0199,  0.0135,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,  0.1673,  0.1673,  0.0654,  0.0654,  0.2443,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.4169,  0.3396,  0.3396,\n",
            "          0.3396,  0.3396,  0.3396,  0.3396,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2842,  0.2842,  0.2842,  0.2805,  0.2806,  0.2806,  0.2806],\n",
            "        [-0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0379,\n",
            "         -0.0033, -0.0335, -0.0033, -0.0335, -0.0335, -0.0335, -0.0033, -0.0335,\n",
            "         -0.0033, -0.0033, -0.0335, -0.0335, -0.0033, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0521, -0.0335, -0.0033, -0.0124, -0.0124, -0.0124, -0.0033,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0521, -0.0335, -0.0335,  0.0169, -0.0335,\n",
            "             nan, -0.0033,     nan, -0.0752, -0.0752, -0.0920, -0.1049, -0.0335,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0335, -0.0335,\n",
            "         -0.0610, -0.0610, -0.0610, -0.0610, -0.1349, -0.1844, -0.3084, -0.3146,\n",
            "         -0.3022, -0.0895, -0.0770, -0.0598, -0.0535, -0.0335, -0.0335, -0.0496,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0335, -0.0033,     nan, -0.0335,\n",
            "         -0.0399, -0.0399, -0.0535, -0.0335, -0.0496, -0.0399, -0.0153, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0536, -0.0536, -0.0502, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0335, -0.0335, -0.0033,\n",
            "         -0.0033,     nan, -0.0033, -0.0033, -0.0335, -0.0033, -0.1122, -0.0033,\n",
            "         -0.0335, -0.0335, -0.0225, -0.0399, -0.0399, -0.0399, -0.0399, -0.0033,\n",
            "             nan, -0.0033, -0.0335, -0.0335, -0.0335, -0.0335, -0.0033, -0.0033,\n",
            "          0.0063, -0.0033, -0.0033, -0.0335, -0.0033, -0.0033, -0.0335, -0.0033,\n",
            "         -0.0033, -0.0033, -0.0225, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0335,  0.0836,  0.2283, -0.0557, -0.0335, -0.0299,     nan,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0535, -0.0033, -0.0033, -0.0335, -0.0033, -0.0335, -0.0335, -0.0335,\n",
            "         -0.0335, -0.0033, -0.0335,     nan, -0.0335, -0.0335, -0.1053, -0.0131,\n",
            "         -0.0169, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0431,\n",
            "         -0.0408,     nan,  0.1171,  0.1234,     nan,     nan,     nan,     nan,\n",
            "             nan,     nan,  0.2090,  0.2090,  0.0755,  0.0755, -0.0610, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0379, -0.1057, -0.1057,\n",
            "         -0.1057, -0.1057, -0.1057, -0.1057, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0597, -0.0597, -0.0597, -0.0536, -0.0399, -0.0399, -0.0399]])\n",
            "tensor([[ 0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.4169,\n",
            "          0.3813,  0.3251,  0.3813,  0.3251,  0.3251,  0.3251,  0.3813,  0.3251,\n",
            "          0.3813,  0.3813,  0.3251,  0.3251,  0.3813,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.3727,  0.3251,  0.3813,  0.1095,  0.1095,  0.1095,  0.3813,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.3727,  0.3251,  0.3251,  0.3867,  0.3251,\n",
            "          0.3251,  0.3813,  0.3813, -0.5112, -0.5112, -0.5770,  0.1974,  0.3251,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.3251,  0.3251,\n",
            "          0.3094,  0.3094,  0.3094,  0.3094,  0.1251,  0.0120, -0.1093, -0.1410,\n",
            "         -0.1039,  0.2121,  0.3474,  0.3556,  0.3680,  0.3251,  0.3251,  0.2794,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.3251,  0.3813,  0.3813,  0.3251,\n",
            "          0.2806,  0.2806,  0.3680,  0.3251,  0.2794,  0.2806,  0.3088,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2805,  0.2805,  0.2888,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.3251,  0.3251,  0.3813,\n",
            "          0.3813,  0.3813,  0.3813,  0.3813,  0.3251,  0.3813,  0.3552,  0.3813,\n",
            "          0.3251,  0.3251,  0.3485,  0.2806,  0.2806,  0.2806,  0.2806,  0.3813,\n",
            "          0.3813,  0.3813,  0.3251,  0.3251,  0.3251,  0.3251,  0.3813,  0.3813,\n",
            "          0.3861,  0.3813,  0.3813,  0.3251,  0.3813,  0.3813,  0.3251,  0.3813,\n",
            "          0.3813,  0.3813,  0.3485,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.3251, -0.5125, -0.4389,  0.2818,  0.3251,  0.3229,  0.3229,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.3680,  0.3813,  0.3813,  0.3251,  0.3813,  0.3251,  0.3251,  0.3251,\n",
            "          0.3251,  0.3813,  0.3251,  0.3251,  0.3251,  0.3251,  0.2515,  0.2039,\n",
            "          0.2681,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.2708,\n",
            "          0.2093,  0.2093,  0.0199,  0.0135,  0.0135,  0.0135,  0.0135,  0.0135,\n",
            "          0.0135,  0.0135,  0.1673,  0.1673,  0.0654,  0.0654,  0.2443,  0.2806,\n",
            "          0.2806,  0.2806,  0.2806,  0.2806,  0.2806,  0.4169,  0.3396,  0.3396,\n",
            "          0.3396,  0.3396,  0.3396,  0.3396,  0.2806,  0.2806,  0.2806,  0.2806,\n",
            "          0.2806,  0.2842,  0.2842,  0.2842,  0.2805,  0.2806,  0.2806,  0.2806],\n",
            "        [-0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0379,\n",
            "         -0.0033, -0.0335, -0.0033, -0.0335, -0.0335, -0.0335, -0.0033, -0.0335,\n",
            "         -0.0033, -0.0033, -0.0335, -0.0335, -0.0033, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0521, -0.0335, -0.0033, -0.0124, -0.0124, -0.0124, -0.0033,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0521, -0.0335, -0.0335,  0.0169, -0.0335,\n",
            "         -0.0335, -0.0033, -0.0033, -0.0752, -0.0752, -0.0920, -0.1049, -0.0335,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0335, -0.0335,\n",
            "         -0.0610, -0.0610, -0.0610, -0.0610, -0.1349, -0.1844, -0.3084, -0.3146,\n",
            "         -0.3022, -0.0895, -0.0770, -0.0598, -0.0535, -0.0335, -0.0335, -0.0496,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0335, -0.0033, -0.0033, -0.0335,\n",
            "         -0.0399, -0.0399, -0.0535, -0.0335, -0.0496, -0.0399, -0.0153, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0536, -0.0536, -0.0502, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0335, -0.0335, -0.0033,\n",
            "         -0.0033, -0.0033, -0.0033, -0.0033, -0.0335, -0.0033, -0.1122, -0.0033,\n",
            "         -0.0335, -0.0335, -0.0225, -0.0399, -0.0399, -0.0399, -0.0399, -0.0033,\n",
            "         -0.0033, -0.0033, -0.0335, -0.0335, -0.0335, -0.0335, -0.0033, -0.0033,\n",
            "          0.0063, -0.0033, -0.0033, -0.0335, -0.0033, -0.0033, -0.0335, -0.0033,\n",
            "         -0.0033, -0.0033, -0.0225, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0335,  0.0836,  0.2283, -0.0557, -0.0335, -0.0299, -0.0299,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0535, -0.0033, -0.0033, -0.0335, -0.0033, -0.0335, -0.0335, -0.0335,\n",
            "         -0.0335, -0.0033, -0.0335, -0.0335, -0.0335, -0.0335, -0.1053, -0.0131,\n",
            "         -0.0169, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0431,\n",
            "         -0.0408, -0.0408,  0.1171,  0.1234,  0.1234,  0.1234,  0.1234,  0.1234,\n",
            "          0.1234,  0.1234,  0.2090,  0.2090,  0.0755,  0.0755, -0.0610, -0.0399,\n",
            "         -0.0399, -0.0399, -0.0399, -0.0399, -0.0399, -0.0379, -0.1057, -0.1057,\n",
            "         -0.1057, -0.1057, -0.1057, -0.1057, -0.0399, -0.0399, -0.0399, -0.0399,\n",
            "         -0.0399, -0.0597, -0.0597, -0.0597, -0.0536, -0.0399, -0.0399, -0.0399]])\n",
            "tensor([[ -0.2133,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.2697,  -0.2697,\n",
            "          -0.2697,  -0.2697,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1870,\n",
            "           0.1074,   0.1074,   0.1153,  -0.0869,  -0.1870,  -0.1870,   0.0388,\n",
            "           0.1153,   0.1082,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1870,\n",
            "          -0.1870,  -0.1943,  -0.1943,  -0.1943,  -0.1870,  -0.1870, -10.0845,\n",
            "          -0.1870,      nan,   0.1082,   0.1082,   0.1082,   0.0935,   0.0935,\n",
            "          -0.1870,  -0.1943,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1422,\n",
            "          -0.0882,   0.1074,   0.0736,   0.0736,  -0.1870,  -0.1870,  -0.1870,\n",
            "          -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.2697,  -0.1870,\n",
            "          -0.1870,  -0.1169,  -0.1169,  -0.1870,  -0.1870,  -0.1870,  -0.1870,\n",
            "          -0.1870,  -0.1870,  -0.1870],\n",
            "        [  0.0777,   0.0591,   0.0591,   0.0591,   0.0591,   0.0529,   0.0529,\n",
            "           0.0529,   0.0529,   0.0591,   0.0591,   0.0591,   0.0591,   0.0591,\n",
            "           0.1099,   0.1099,   0.1063,   0.0724,   0.0591,   0.0591,   0.1233,\n",
            "           0.1063,   0.1053,   0.0591,   0.0591,   0.0591,   0.0591,   0.0591,\n",
            "           0.0591,   0.0569,   0.0569,   0.0569,   0.0591,   0.0591, -14.2186,\n",
            "           0.0591,      nan,   0.1053,   0.1053,   0.1053,   0.0956,   0.0956,\n",
            "           0.0591,   0.0569,   0.0591,   0.0591,   0.0591,   0.0591,   0.0608,\n",
            "           0.0652,   0.1099,   0.1087,   0.1087,   0.0591,   0.0591,   0.0591,\n",
            "           0.0591,   0.0591,   0.0591,   0.0591,   0.0591,   0.0529,   0.0591,\n",
            "           0.0591,   0.0609,   0.0609,   0.0591,   0.0591,   0.0591,   0.0591,\n",
            "           0.0591,   0.0591,   0.0591]])\n",
            "tensor([[ -0.2133,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.2697,  -0.2697,\n",
            "          -0.2697,  -0.2697,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1870,\n",
            "           0.1074,   0.1074,   0.1153,  -0.0869,  -0.1870,  -0.1870,   0.0388,\n",
            "           0.1153,   0.1082,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1870,\n",
            "          -0.1870,  -0.1943,  -0.1943,  -0.1943,  -0.1870,  -0.1870, -10.0845,\n",
            "          -0.1870,  -0.1870,   0.1082,   0.1082,   0.1082,   0.0935,   0.0935,\n",
            "          -0.1870,  -0.1943,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1422,\n",
            "          -0.0882,   0.1074,   0.0736,   0.0736,  -0.1870,  -0.1870,  -0.1870,\n",
            "          -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.1870,  -0.2697,  -0.1870,\n",
            "          -0.1870,  -0.1169,  -0.1169,  -0.1870,  -0.1870,  -0.1870,  -0.1870,\n",
            "          -0.1870,  -0.1870,  -0.1870],\n",
            "        [  0.0777,   0.0591,   0.0591,   0.0591,   0.0591,   0.0529,   0.0529,\n",
            "           0.0529,   0.0529,   0.0591,   0.0591,   0.0591,   0.0591,   0.0591,\n",
            "           0.1099,   0.1099,   0.1063,   0.0724,   0.0591,   0.0591,   0.1233,\n",
            "           0.1063,   0.1053,   0.0591,   0.0591,   0.0591,   0.0591,   0.0591,\n",
            "           0.0591,   0.0569,   0.0569,   0.0569,   0.0591,   0.0591, -14.2186,\n",
            "           0.0591,   0.0591,   0.1053,   0.1053,   0.1053,   0.0956,   0.0956,\n",
            "           0.0591,   0.0569,   0.0591,   0.0591,   0.0591,   0.0591,   0.0608,\n",
            "           0.0652,   0.1099,   0.1087,   0.1087,   0.0591,   0.0591,   0.0591,\n",
            "           0.0591,   0.0591,   0.0591,   0.0591,   0.0591,   0.0529,   0.0591,\n",
            "           0.0591,   0.0609,   0.0609,   0.0591,   0.0591,   0.0591,   0.0591,\n",
            "           0.0591,   0.0591,   0.0591]])\n",
            "tensor([[0.2140, 0.2140, 0.2140, 0.2297, 0.2140, 0.2054, 0.2297, 0.2140, 0.2297,\n",
            "         0.2297, 0.2140, 0.2140, 0.2297, 0.2054, 0.2140, 0.2054, 0.2297, 0.2297,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140,\n",
            "         0.2297, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2054, 0.2140, 0.2140,\n",
            "         0.2054, 0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140,\n",
            "         0.2140, 0.0300, 0.0383, 0.0383, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300,\n",
            "         0.2297, 0.2140, 0.2054, 0.2140, 0.2054, 0.2140, 0.2140, 0.2140, 0.2140,\n",
            "         0.2140, 0.2297, 0.2140, 0.0332, 0.0332, 0.0332, 0.0300, 0.0300, 0.0300,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0382, 0.0454,\n",
            "         0.0555, 0.0555, 0.0736, 0.2140, 0.2140, 0.2297, 0.2297, 0.2140, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.0300, 0.0300,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0332, 0.0332, 0.0300, 0.0382, 0.2140,\n",
            "         0.2140, 0.2140, 0.2054, 0.2054, 0.2297, 0.2297, 0.2297, 0.2297, 0.0390,\n",
            "         0.0278, 0.0300, 0.0332, 0.0332, 0.0383, 0.0388, 0.0388, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2054, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2140, 0.0026,    nan, 0.2140, 0.2297, 0.2140, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2140, 0.2140, 0.2297, 0.2140,\n",
            "         0.2140, 0.2297, 0.2140, 0.2297, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2140, 0.2054, 0.2054, 0.2054, 0.2054, 0.2054, 0.2037,\n",
            "         0.2037, 0.2037, 0.2037, 0.1950, 0.2054, 0.2140, 0.2140, 0.2054, 0.2140,\n",
            "         0.2140, 0.2140, 0.1950, 0.2297, 0.2140, 0.2054, 0.2388, 0.2140, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2140, 0.0555, 0.0555, 0.0332, 0.0332,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.2297, 0.2297, 0.2297,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0555, 0.0555, 0.1190, 0.1190, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.0533, 0.0300, 0.0300,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140, 0.2054, 0.0300,\n",
            "         0.0390, 0.0300, 0.0300, 0.0300, 0.0332, 0.2140, 0.2297, 0.2140, 0.2054,\n",
            "         0.2297, 0.2297, 0.2297, 0.2060, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300,\n",
            "         0.2140, 0.2140, 0.2140, 0.2054, 0.2140, 0.2140, 0.2140, 0.2054, 0.2140,\n",
            "         0.2297, 0.2140, 0.2054, 0.2140, 0.2037, 0.2037, 0.2037, 0.1795, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2054, 0.2140,\n",
            "         0.0565, 0.0565, 0.0565, 0.0565, 0.2060, 0.2054, 0.2140, 0.2140, 0.2054,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2054, 0.2140, 0.2140],\n",
            "        [0.0934, 0.0934, 0.0934, 0.0692, 0.0934, 0.0885, 0.0692, 0.0934, 0.0692,\n",
            "         0.0692, 0.0934, 0.0934, 0.0692, 0.0885, 0.0934, 0.0885, 0.0692, 0.0692,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934,\n",
            "         0.0692, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0885, 0.0934, 0.0934,\n",
            "         0.0885, 0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934,\n",
            "         0.0934, 0.0904, 0.0890, 0.0890, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904,\n",
            "         0.0692, 0.0934, 0.0885, 0.0934, 0.0885, 0.0934, 0.0934, 0.0934, 0.0934,\n",
            "         0.0934, 0.0692, 0.0934, 0.0894, 0.0894, 0.0894, 0.0904, 0.0904, 0.0904,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0888, 0.1000,\n",
            "         0.1205, 0.1205, 0.1087, 0.0934, 0.0934, 0.0692, 0.0692, 0.0934, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0904, 0.0904,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0894, 0.0894, 0.0904, 0.0888, 0.0934,\n",
            "         0.0934, 0.0934, 0.0885, 0.0885, 0.0692, 0.0692, 0.0692, 0.0692, 0.0861,\n",
            "         0.0876, 0.0904, 0.0894, 0.0894, 0.0890, 0.1233, 0.1233, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0885, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0934, 0.0016,    nan, 0.0934, 0.0692, 0.0934, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0934, 0.0934, 0.0692, 0.0934,\n",
            "         0.0934, 0.0692, 0.0934, 0.0692, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0934, 0.0885, 0.0885, 0.0885, 0.0885, 0.0885, 0.0954,\n",
            "         0.0954, 0.0954, 0.0954, 0.0909, 0.0885, 0.0934, 0.0934, 0.0885, 0.0934,\n",
            "         0.0934, 0.0934, 0.0909, 0.0692, 0.0934, 0.0885, 0.1178, 0.0934, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0934, 0.1205, 0.1205, 0.0894, 0.0894,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0692, 0.0692, 0.0692,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.1205, 0.1205, 0.1113, 0.1113, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.1082, 0.0904, 0.0904,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934, 0.0885, 0.0904,\n",
            "         0.0861, 0.0904, 0.0904, 0.0904, 0.0894, 0.0934, 0.0692, 0.0934, 0.0885,\n",
            "         0.0692, 0.0692, 0.0692, 0.1005, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904,\n",
            "         0.0934, 0.0934, 0.0934, 0.0885, 0.0934, 0.0934, 0.0934, 0.0885, 0.0934,\n",
            "         0.0692, 0.0934, 0.0885, 0.0934, 0.0954, 0.0954, 0.0954, 0.1011, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0885, 0.0934,\n",
            "         0.0779, 0.0779, 0.0779, 0.0779, 0.1005, 0.0885, 0.0934, 0.0934, 0.0885,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0885, 0.0934, 0.0934]])\n",
            "tensor([[0.2140, 0.2140, 0.2140, 0.2297, 0.2140, 0.2054, 0.2297, 0.2140, 0.2297,\n",
            "         0.2297, 0.2140, 0.2140, 0.2297, 0.2054, 0.2140, 0.2054, 0.2297, 0.2297,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140,\n",
            "         0.2297, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2054, 0.2140, 0.2140,\n",
            "         0.2054, 0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140,\n",
            "         0.2140, 0.0300, 0.0383, 0.0383, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300,\n",
            "         0.2297, 0.2140, 0.2054, 0.2140, 0.2054, 0.2140, 0.2140, 0.2140, 0.2140,\n",
            "         0.2140, 0.2297, 0.2140, 0.0332, 0.0332, 0.0332, 0.0300, 0.0300, 0.0300,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0382, 0.0454,\n",
            "         0.0555, 0.0555, 0.0736, 0.2140, 0.2140, 0.2297, 0.2297, 0.2140, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.0300, 0.0300,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0332, 0.0332, 0.0300, 0.0382, 0.2140,\n",
            "         0.2140, 0.2140, 0.2054, 0.2054, 0.2297, 0.2297, 0.2297, 0.2297, 0.0390,\n",
            "         0.0278, 0.0300, 0.0332, 0.0332, 0.0383, 0.0388, 0.0388, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2054, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2140, 0.0026, 0.0026, 0.2140, 0.2297, 0.2140, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2140, 0.2140, 0.2297, 0.2140,\n",
            "         0.2140, 0.2297, 0.2140, 0.2297, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2140, 0.2054, 0.2054, 0.2054, 0.2054, 0.2054, 0.2037,\n",
            "         0.2037, 0.2037, 0.2037, 0.1950, 0.2054, 0.2140, 0.2140, 0.2054, 0.2140,\n",
            "         0.2140, 0.2140, 0.1950, 0.2297, 0.2140, 0.2054, 0.2388, 0.2140, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2140, 0.0555, 0.0555, 0.0332, 0.0332,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.2297, 0.2297, 0.2297,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0555, 0.0555, 0.1190, 0.1190, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.0533, 0.0300, 0.0300,\n",
            "         0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297,\n",
            "         0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2297, 0.2140, 0.2054, 0.0300,\n",
            "         0.0390, 0.0300, 0.0300, 0.0300, 0.0332, 0.2140, 0.2297, 0.2140, 0.2054,\n",
            "         0.2297, 0.2297, 0.2297, 0.2060, 0.0300, 0.0300, 0.0300, 0.0300, 0.0300,\n",
            "         0.2140, 0.2140, 0.2140, 0.2054, 0.2140, 0.2140, 0.2140, 0.2054, 0.2140,\n",
            "         0.2297, 0.2140, 0.2054, 0.2140, 0.2037, 0.2037, 0.2037, 0.1795, 0.2140,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2140, 0.2140, 0.2297, 0.2054, 0.2140,\n",
            "         0.0565, 0.0565, 0.0565, 0.0565, 0.2060, 0.2054, 0.2140, 0.2140, 0.2054,\n",
            "         0.2140, 0.2140, 0.2140, 0.2140, 0.2054, 0.2140, 0.2140],\n",
            "        [0.0934, 0.0934, 0.0934, 0.0692, 0.0934, 0.0885, 0.0692, 0.0934, 0.0692,\n",
            "         0.0692, 0.0934, 0.0934, 0.0692, 0.0885, 0.0934, 0.0885, 0.0692, 0.0692,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934,\n",
            "         0.0692, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0885, 0.0934, 0.0934,\n",
            "         0.0885, 0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934,\n",
            "         0.0934, 0.0904, 0.0890, 0.0890, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904,\n",
            "         0.0692, 0.0934, 0.0885, 0.0934, 0.0885, 0.0934, 0.0934, 0.0934, 0.0934,\n",
            "         0.0934, 0.0692, 0.0934, 0.0894, 0.0894, 0.0894, 0.0904, 0.0904, 0.0904,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0888, 0.1000,\n",
            "         0.1205, 0.1205, 0.1087, 0.0934, 0.0934, 0.0692, 0.0692, 0.0934, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0904, 0.0904,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0894, 0.0894, 0.0904, 0.0888, 0.0934,\n",
            "         0.0934, 0.0934, 0.0885, 0.0885, 0.0692, 0.0692, 0.0692, 0.0692, 0.0861,\n",
            "         0.0876, 0.0904, 0.0894, 0.0894, 0.0890, 0.1233, 0.1233, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0885, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0934, 0.0016, 0.0016, 0.0934, 0.0692, 0.0934, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0934, 0.0934, 0.0692, 0.0934,\n",
            "         0.0934, 0.0692, 0.0934, 0.0692, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0934, 0.0885, 0.0885, 0.0885, 0.0885, 0.0885, 0.0954,\n",
            "         0.0954, 0.0954, 0.0954, 0.0909, 0.0885, 0.0934, 0.0934, 0.0885, 0.0934,\n",
            "         0.0934, 0.0934, 0.0909, 0.0692, 0.0934, 0.0885, 0.1178, 0.0934, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0934, 0.1205, 0.1205, 0.0894, 0.0894,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0692, 0.0692, 0.0692,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.1205, 0.1205, 0.1113, 0.1113, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.1082, 0.0904, 0.0904,\n",
            "         0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692,\n",
            "         0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0692, 0.0934, 0.0885, 0.0904,\n",
            "         0.0861, 0.0904, 0.0904, 0.0904, 0.0894, 0.0934, 0.0692, 0.0934, 0.0885,\n",
            "         0.0692, 0.0692, 0.0692, 0.1005, 0.0904, 0.0904, 0.0904, 0.0904, 0.0904,\n",
            "         0.0934, 0.0934, 0.0934, 0.0885, 0.0934, 0.0934, 0.0934, 0.0885, 0.0934,\n",
            "         0.0692, 0.0934, 0.0885, 0.0934, 0.0954, 0.0954, 0.0954, 0.1011, 0.0934,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0934, 0.0934, 0.0692, 0.0885, 0.0934,\n",
            "         0.0779, 0.0779, 0.0779, 0.0779, 0.1005, 0.0885, 0.0934, 0.0934, 0.0885,\n",
            "         0.0934, 0.0934, 0.0934, 0.0934, 0.0885, 0.0934, 0.0934]])\n",
            "tensor([[-0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "          0.0093,  0.0093, -0.0030, -0.0095, -0.0095, -0.0041, -0.0095, -0.0041,\n",
            "         -0.0030, -0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0030, -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0030, -0.0030, -0.0030, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0041, -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0030, -0.0034,\n",
            "         -0.0034, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041,\n",
            "         -0.0030, -0.0030,  0.0341,  0.0341, -0.0030, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0030, -0.0030, -0.0041, -0.0030, -0.0041, -0.0041, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0030, -0.0041, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0095, -0.0095, -0.0095, -0.0095, -0.0095, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0095, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0041,  0.0114,  0.0114,     nan,     nan, -0.0030, -0.0041, -0.0030,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0030, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0030, -0.0041, -0.0030, -0.0095, -0.0095, -0.0095, -0.0030,\n",
            "         -0.0030, -0.0095, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0030, -0.0041,  0.0341,  0.0341, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0030, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0030,  0.0398,  0.0398,     nan,     nan, -0.0041, -0.0041,\n",
            "         -0.0030, -0.0030],\n",
            "        [ 0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0647,  0.0647,  0.0675,  0.0681,  0.0681,  0.0738,  0.0681,  0.0738,\n",
            "          0.0675,  0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,\n",
            "          0.0675,  0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0675,  0.0675,  0.0675,  0.0738,  0.0675,  0.0675,\n",
            "          0.0738,  0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0675,  0.0626,\n",
            "          0.0626,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,\n",
            "          0.0675,  0.0675,  0.0525,  0.0525,  0.0675,  0.0738,  0.0675,  0.0675,\n",
            "          0.0675,  0.0675,  0.0738,  0.0675,  0.0738,  0.0738,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0675,  0.0738,  0.0738,  0.0675,  0.0675,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0681,  0.0681,  0.0681,  0.0681,  0.0681,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0681,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0738,  0.0698,  0.0698,     nan,     nan,  0.0675,  0.0738,  0.0675,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0675,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,  0.0738,  0.0738,\n",
            "          0.0738,  0.0675,  0.0738,  0.0675,  0.0681,  0.0681,  0.0681,  0.0675,\n",
            "          0.0675,  0.0681,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0675,  0.0738,  0.0525,  0.0525,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0675,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0675,  0.1148,  0.1148,     nan,     nan,  0.0738,  0.0738,\n",
            "          0.0675,  0.0675]])\n",
            "tensor([[-0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "          0.0093,  0.0093, -0.0030, -0.0095, -0.0095, -0.0041, -0.0095, -0.0041,\n",
            "         -0.0030, -0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0030, -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0030, -0.0030, -0.0030, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0041, -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0030, -0.0034,\n",
            "         -0.0034, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041,\n",
            "         -0.0030, -0.0030,  0.0341,  0.0341, -0.0030, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0030, -0.0030, -0.0041, -0.0030, -0.0041, -0.0041, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0030, -0.0041, -0.0041, -0.0030, -0.0030,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0095, -0.0095, -0.0095, -0.0095, -0.0095, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0095, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0041,  0.0114,  0.0114,  0.0114,  0.0114, -0.0030, -0.0041, -0.0030,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0030, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0030, -0.0041, -0.0030, -0.0095, -0.0095, -0.0095, -0.0030,\n",
            "         -0.0030, -0.0095, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030, -0.0030, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0030, -0.0041,  0.0341,  0.0341, -0.0041, -0.0041, -0.0041, -0.0041,\n",
            "         -0.0041, -0.0041, -0.0030, -0.0030, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0030, -0.0030, -0.0030, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0041, -0.0030,\n",
            "         -0.0030, -0.0030,  0.0398,  0.0398,  0.0398,  0.0398, -0.0041, -0.0041,\n",
            "         -0.0030, -0.0030],\n",
            "        [ 0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0647,  0.0647,  0.0675,  0.0681,  0.0681,  0.0738,  0.0681,  0.0738,\n",
            "          0.0675,  0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,\n",
            "          0.0675,  0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0675,  0.0675,  0.0675,  0.0738,  0.0675,  0.0675,\n",
            "          0.0738,  0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0675,  0.0626,\n",
            "          0.0626,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,\n",
            "          0.0675,  0.0675,  0.0525,  0.0525,  0.0675,  0.0738,  0.0675,  0.0675,\n",
            "          0.0675,  0.0675,  0.0738,  0.0675,  0.0738,  0.0738,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0675,  0.0738,  0.0738,  0.0675,  0.0675,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0681,  0.0681,  0.0681,  0.0681,  0.0681,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0681,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0738,  0.0698,  0.0698,  0.0698,  0.0698,  0.0675,  0.0738,  0.0675,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0675,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,  0.0738,  0.0738,\n",
            "          0.0738,  0.0675,  0.0738,  0.0675,  0.0681,  0.0681,  0.0681,  0.0675,\n",
            "          0.0675,  0.0681,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,  0.0675,  0.0738,\n",
            "          0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0675,  0.0738,  0.0525,  0.0525,  0.0738,  0.0738,  0.0738,  0.0738,\n",
            "          0.0738,  0.0738,  0.0675,  0.0675,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0675,  0.0675,  0.0675,  0.0738,  0.0675,\n",
            "          0.0675,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0738,  0.0675,\n",
            "          0.0675,  0.0675,  0.1148,  0.1148,  0.1148,  0.1148,  0.0738,  0.0738,\n",
            "          0.0675,  0.0675]])\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=None,\n",
        "                                    directory_raw_data='/content/smallDataset',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=True,\n",
        "                                    spliting_long_sequences=False,\n",
        "                                    with_repeated_connections=True,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_size=2,\n",
        "                                    drop_nan=False,\n",
        "                                    save=False,\n",
        "                                    path_to_save_dataset=None,\n",
        "                                    process_raw_data=False,\n",
        "                                    download_raw_data=False,\n",
        "                                    processed_dataset_path=None,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QkVEIkF-jNT"
      },
      "outputs": [],
      "source": [
        "pos_id_concat=torch.cat([user['pos_id'] for user in list_users])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdT60UqmDqmh",
        "outputId": "c8a20e34-4e21-4cc8-e6ff-f548bd501385"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([595800])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "d_0yQhU5DhHN",
        "outputId": "057ae78d-0024-405e-93e0-12535e3151f5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "data = pos_id_concat.tolist()\n",
        "\n",
        "# Count the frequency of each integer\n",
        "counter = Counter(data)\n",
        "\n",
        "# Extracting data for plotting\n",
        "integers = list(counter.keys())\n",
        "frequencies = list(counter.values())\n",
        "\n",
        "# Creating the bar chart\n",
        "plt.bar(integers, frequencies, color='skyblue')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel('Integers')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Integers')\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAUMJOElHKF8"
      },
      "source": [
        "## prepocessing 2 : dropping missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI5DOUHPIWKV"
      },
      "source": [
        "### **no split**: For LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpVZqqyPHJY5",
        "outputId": "c0cbc3b2-4b29-44bc-9fa1-c2081fc007d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "getting raw data at: /content/drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\n",
            "processing dataframe: /content/dataset-telecom-6month/data_11.1~11.15\n",
            "reading dataframe: /content/dataset-telecom-6month/data_11.1~11.15.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_7.16~7.31\n",
            "reading dataframe: /content/dataset-telecom-6month/data_7.16~7.31.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_6.1~6.15\n",
            "reading dataframe: /content/dataset-telecom-6month/data_6.1~6.15.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_9.1~9.15\n",
            "reading dataframe: /content/dataset-telecom-6month/data_9.1~9.15.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_6.16~6.30\n",
            "reading dataframe: /content/dataset-telecom-6month/data_6.16~6.30.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_10.1~10.15\n",
            "reading dataframe: /content/dataset-telecom-6month/data_10.1~10.15.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_11.16~11.30\n",
            "reading dataframe: /content/dataset-telecom-6month/data_11.16~11.30.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_8.1~8.15\n",
            "reading dataframe: /content/dataset-telecom-6month/data_8.1~8.15.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_9.16~9.30\n",
            "reading dataframe: /content/dataset-telecom-6month/data_9.16~9.30.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_7.1~7.15\n",
            "reading dataframe: /content/dataset-telecom-6month/data_7.1~7.15.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_8.16~8.31\n",
            "reading dataframe: /content/dataset-telecom-6month/data_8.16~8.31.xlsx\n",
            "processing dataframe: /content/dataset-telecom-6month/data_10.16~10.31\n",
            "reading dataframe: /content/dataset-telecom-6month/data_10.16~10.31.xlsx\n",
            "creating directory: /content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_no_split\n",
            "saving processed data at: \n",
            "/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_no_split/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_no_split/vocab\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=\"/content/drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=True,\n",
        "                                    spliting_long_sequences=False,\n",
        "                                    with_repeated_connections=True,\n",
        "                                    max_sequence_length=None,\n",
        "                                    min_sequence_size=2,\n",
        "                                    drop_nan=True,\n",
        "                                    save=True,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_no_split\",\n",
        "                                    process_raw_data=True,\n",
        "                                    download_raw_data=False,\n",
        "                                    processed_dataset_path=\"/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_no_split\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke4FJLYQLdvR"
      },
      "source": [
        "### **split**: For Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9CacmrdLmxo",
        "outputId": "83ef75b9-05c5-401b-ea83-bf5274f9e890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processing dataframe: /content/dataset-telecom-6month/data_11.1~11.15\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_7.16~7.31\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_6.1~6.15\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_9.1~9.15\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_6.16~6.30\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_10.1~10.15\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_11.16~11.30\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_8.1~8.15\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_9.16~9.30\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_7.1~7.15\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_8.16~8.31\n",
            "using already read daframe\n",
            "processing dataframe: /content/dataset-telecom-6month/data_10.16~10.31\n",
            "using already read daframe\n",
            "spliting sequences longuer than : 100 steps\n",
            "creating directory: /content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_split_100\n",
            "saving processed data at: \n",
            "/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_split_100/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_split_100/vocab\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=\"/content/drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=True,\n",
        "                                    spliting_long_sequences=True,\n",
        "                                    with_repeated_connections=True,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_size=2,\n",
        "                                    drop_nan=True,\n",
        "                                    save=True,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_split_100\",\n",
        "                                    process_raw_data=True,\n",
        "                                    download_raw_data=False,\n",
        "                                    processed_dataset_path=\"/content/drive/MyDrive/telecomDataset6month_without_nan_with_repeat_split_100\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr5oE5gphnxy"
      },
      "outputs": [],
      "source": [
        "for user in list_users:\n",
        "  for key in user:\n",
        "    if user[key].shape[0]>100:\n",
        "      print(key,len(user[key]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF1KxHjIL8rk"
      },
      "source": [
        "## Preprocessing 3: merge repetitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRA1WX8UcsV",
        "outputId": "661a1316-e664-4804-bfd6-d6121d4801f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=\"/content\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=True,\n",
        "                                    spliting_long_sequences=False,\n",
        "                                    with_repeated_connections=False,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_size=2,\n",
        "                                    drop_nan=False,\n",
        "                                    save=False,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",\n",
        "                                    process_raw_data=False,\n",
        "                                    download_raw_data=False,\n",
        "                                    processed_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of sequence Repartition by length"
      ],
      "metadata": {
        "id": "MaQXlsPnc-Fm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3oCMGZR2yP72",
        "outputId": "e4287402-1800-4cdd-a8c4-6b8d0127610b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21.458622016936104\n",
            "21.458622016936104 25.618203688507787 100 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([2448., 1909., 1118., 1149.,  800.,  753.,  626.,  599.,  516.,\n",
              "         471.,  427.,  444.,  381.,  369.,  317.,  337.,  267.,  254.,\n",
              "         239.,  239.,  237.,  234.,  222.,  205.,  206.,  171.,  189.,\n",
              "         178.,  178.,  173.,  147.,  165.,  130.,  128.,  132.,  133.,\n",
              "         134.,  118.,  122.,  109.,  130.,  109.,  118.,  122.,  103.,\n",
              "         116.,   94.,   79.,   77.,   70.,   80.,   74.,   79.,   70.,\n",
              "          82.,   74.,   69.,   61.,   77.,   64.,   69.,   54.,   65.,\n",
              "          72.,   59.,   47.,   41.,   43.,   49.,   33.,   46.,   40.,\n",
              "          49.,   44.,   45.,   34.,   33.,   30.,   31.,   30.,   24.,\n",
              "          32.,   25.,   25.,   21.,   29.,   37.,   25.,   22.,   31.,\n",
              "          19.,   26.,   21.,   21.,   21.,   23.,   17.,   12.,   21.,\n",
              "         696.]),\n",
              " array([  1.  ,   1.99,   2.98,   3.97,   4.96,   5.95,   6.94,   7.93,\n",
              "          8.92,   9.91,  10.9 ,  11.89,  12.88,  13.87,  14.86,  15.85,\n",
              "         16.84,  17.83,  18.82,  19.81,  20.8 ,  21.79,  22.78,  23.77,\n",
              "         24.76,  25.75,  26.74,  27.73,  28.72,  29.71,  30.7 ,  31.69,\n",
              "         32.68,  33.67,  34.66,  35.65,  36.64,  37.63,  38.62,  39.61,\n",
              "         40.6 ,  41.59,  42.58,  43.57,  44.56,  45.55,  46.54,  47.53,\n",
              "         48.52,  49.51,  50.5 ,  51.49,  52.48,  53.47,  54.46,  55.45,\n",
              "         56.44,  57.43,  58.42,  59.41,  60.4 ,  61.39,  62.38,  63.37,\n",
              "         64.36,  65.35,  66.34,  67.33,  68.32,  69.31,  70.3 ,  71.29,\n",
              "         72.28,  73.27,  74.26,  75.25,  76.24,  77.23,  78.22,  79.21,\n",
              "         80.2 ,  81.19,  82.18,  83.17,  84.16,  85.15,  86.14,  87.13,\n",
              "         88.12,  89.11,  90.1 ,  91.09,  92.08,  93.07,  94.06,  95.05,\n",
              "         96.04,  97.03,  98.02,  99.01, 100.  ]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sum_len=0\n",
        "len_list=[]\n",
        "for user in list_users:\n",
        "  len_list.append(len(user['pos_id']))\n",
        "  sum_len+=len(user['pos_id'])\n",
        "print(sum_len/len(list_users))\n",
        "len_array=np.array(len_list)\n",
        "print(len_array.mean(),len_array.std(),len_array.max(),len_array.min())\n",
        "plt.hist(len_array,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mkzulva2d2ae",
        "outputId": "090e5dc1-7557-481d-91fb-3ddc54df599a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21.458622016936104\n",
            "21.458622016936104 25.618203688507787 100 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([2448., 1909., 1118., 1149.,  800.,  753.,  626.,  599.,  516.,\n",
              "         471.,  427.,  444.,  381.,  369.,  317.,  337.,  267.,  254.,\n",
              "         239.,  239.,  237.,  234.,  222.,  205.,  206.,  171.,  189.,\n",
              "         178.,  178.,  173.,  147.,  165.,  130.,  128.,  132.,  133.,\n",
              "         134.,  118.,  122.,  109.,  130.,  109.,  118.,  122.,  103.,\n",
              "         116.,   94.,   79.,   77.,   70.,   80.,   74.,   79.,   70.,\n",
              "          82.,   74.,   69.,   61.,   77.,   64.,   69.,   54.,   65.,\n",
              "          72.,   59.,   47.,   41.,   43.,   49.,   33.,   46.,   40.,\n",
              "          49.,   44.,   45.,   34.,   33.,   30.,   31.,   30.,   24.,\n",
              "          32.,   25.,   25.,   21.,   29.,   37.,   25.,   22.,   31.,\n",
              "          19.,   26.,   21.,   21.,   21.,   23.,   17.,   12.,   21.,\n",
              "         696.]),\n",
              " array([  1.  ,   1.99,   2.98,   3.97,   4.96,   5.95,   6.94,   7.93,\n",
              "          8.92,   9.91,  10.9 ,  11.89,  12.88,  13.87,  14.86,  15.85,\n",
              "         16.84,  17.83,  18.82,  19.81,  20.8 ,  21.79,  22.78,  23.77,\n",
              "         24.76,  25.75,  26.74,  27.73,  28.72,  29.71,  30.7 ,  31.69,\n",
              "         32.68,  33.67,  34.66,  35.65,  36.64,  37.63,  38.62,  39.61,\n",
              "         40.6 ,  41.59,  42.58,  43.57,  44.56,  45.55,  46.54,  47.53,\n",
              "         48.52,  49.51,  50.5 ,  51.49,  52.48,  53.47,  54.46,  55.45,\n",
              "         56.44,  57.43,  58.42,  59.41,  60.4 ,  61.39,  62.38,  63.37,\n",
              "         64.36,  65.35,  66.34,  67.33,  68.32,  69.31,  70.3 ,  71.29,\n",
              "         72.28,  73.27,  74.26,  75.25,  76.24,  77.23,  78.22,  79.21,\n",
              "         80.2 ,  81.19,  82.18,  83.17,  84.16,  85.15,  86.14,  87.13,\n",
              "         88.12,  89.11,  90.1 ,  91.09,  92.08,  93.07,  94.06,  95.05,\n",
              "         96.04,  97.03,  98.02,  99.01, 100.  ]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sum_len=0\n",
        "len_list=[]\n",
        "for user in list_users:\n",
        "  len_list.append(len(user['pos_id']))\n",
        "  sum_len+=len(user['pos_id'])\n",
        "print(sum_len/len(list_users))\n",
        "len_array=np.array(len_list)\n",
        "print(len_array.mean(),len_array.std(),len_array.max(),len_array.min())\n",
        "plt.hist(len_array,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K35diHP9eSr2",
        "outputId": "ce3081a2-a451-4fb1-fb05-e977796dd9f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21.458622016936104\n",
            "21.458622016936104 25.618203688507787 100 1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(array([2448., 1909., 1118., 1149.,  800.,  753.,  626.,  599.,  516.,\n",
              "         471.,  427.,  444.,  381.,  369.,  317.,  337.,  267.,  254.,\n",
              "         239.,  239.,  237.,  234.,  222.,  205.,  206.,  171.,  189.,\n",
              "         178.,  178.,  173.,  147.,  165.,  130.,  128.,  132.,  133.,\n",
              "         134.,  118.,  122.,  109.,  130.,  109.,  118.,  122.,  103.,\n",
              "         116.,   94.,   79.,   77.,   70.,   80.,   74.,   79.,   70.,\n",
              "          82.,   74.,   69.,   61.,   77.,   64.,   69.,   54.,   65.,\n",
              "          72.,   59.,   47.,   41.,   43.,   49.,   33.,   46.,   40.,\n",
              "          49.,   44.,   45.,   34.,   33.,   30.,   31.,   30.,   24.,\n",
              "          32.,   25.,   25.,   21.,   29.,   37.,   25.,   22.,   31.,\n",
              "          19.,   26.,   21.,   21.,   21.,   23.,   17.,   12.,   21.,\n",
              "         696.]),\n",
              " array([  1.  ,   1.99,   2.98,   3.97,   4.96,   5.95,   6.94,   7.93,\n",
              "          8.92,   9.91,  10.9 ,  11.89,  12.88,  13.87,  14.86,  15.85,\n",
              "         16.84,  17.83,  18.82,  19.81,  20.8 ,  21.79,  22.78,  23.77,\n",
              "         24.76,  25.75,  26.74,  27.73,  28.72,  29.71,  30.7 ,  31.69,\n",
              "         32.68,  33.67,  34.66,  35.65,  36.64,  37.63,  38.62,  39.61,\n",
              "         40.6 ,  41.59,  42.58,  43.57,  44.56,  45.55,  46.54,  47.53,\n",
              "         48.52,  49.51,  50.5 ,  51.49,  52.48,  53.47,  54.46,  55.45,\n",
              "         56.44,  57.43,  58.42,  59.41,  60.4 ,  61.39,  62.38,  63.37,\n",
              "         64.36,  65.35,  66.34,  67.33,  68.32,  69.31,  70.3 ,  71.29,\n",
              "         72.28,  73.27,  74.26,  75.25,  76.24,  77.23,  78.22,  79.21,\n",
              "         80.2 ,  81.19,  82.18,  83.17,  84.16,  85.15,  86.14,  87.13,\n",
              "         88.12,  89.11,  90.1 ,  91.09,  92.08,  93.07,  94.06,  95.05,\n",
              "         96.04,  97.03,  98.02,  99.01, 100.  ]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sum_len=0\n",
        "len_list=[]\n",
        "for user in list_users:\n",
        "  len_list.append(len(user['pos_id']))\n",
        "  sum_len+=len(user['pos_id'])\n",
        "print(sum_len/len(list_users))\n",
        "len_array=np.array(len_list)\n",
        "print(len_array.mean(),len_array.std(),len_array.max(),len_array.min())\n",
        "plt.hist(len_array,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKc-6WsExyck",
        "outputId": "c9706884-3ccf-48dd-800e-8b36637e6cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "nb_repeated_end_of_sequence=0\n",
        "len_1=0\n",
        "for user in list_users:\n",
        "  if len(user['pos_id'])==1:\n",
        "    len_1+=1\n",
        "    if user['pos_id'][-1]==user['pos_id_target'][-1]:\n",
        "      nb_repeated_end_of_sequence+=1\n",
        "print(nb_repeated_end_of_sequence/len_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dLZ5FSAdylO",
        "outputId": "a335dd40-6bea-4e5b-c33d-12f35e6c1fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n"
          ]
        }
      ],
      "source": [
        "nb_repeated_end_of_sequence=0\n",
        "len_1=0\n",
        "for user in list_users:\n",
        "  if len(user['pos_id'])==1:\n",
        "    len_1+=1\n",
        "    if user['pos_id'][-1]==user['pos_id_target'][-1]:\n",
        "      nb_repeated_end_of_sequence+=1\n",
        "print(nb_repeated_end_of_sequence/len_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5D9IoBtGX6R"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvAwpD4GrJu"
      },
      "source": [
        "## Reproducibility seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8p5d4mp9GDah"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import string\n",
        "import random\n",
        "def get_reproducible_seeds(name=\"ProjectLong\",nb_seeds=100):\n",
        "    # Calculate SHA-256 hash\n",
        "    sha256_hash = hashlib.sha256(name.encode()).hexdigest()\n",
        "    # Define character sets\n",
        "    digits = string.digits\n",
        "    # Use the hash to seed the random number generator\n",
        "    hash_as_int = int(sha256_hash, 16)\n",
        "    random.seed(hash_as_int)\n",
        "    # Generate a random list of seed of desired length\n",
        "    reproducibility_seeds = [random.randint(0,10000) for _ in range(nb_seeds)]\n",
        "\n",
        "    return reproducibility_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Mn8U0p9TGJXK"
      },
      "outputs": [],
      "source": [
        "reproducibility_seed=get_reproducible_seeds()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mzoE-MHqLa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8nhxCSLNHsd9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class VariableLengthDatasetWithPosID(Dataset):\n",
        "    def __init__(self, time_series,curriculum_learning, transform=None):\n",
        "        self.times_series=time_series\n",
        "        self.curriculum_learning=curriculum_learning\n",
        "    def __len__(self):\n",
        "        return len(self.times_series)\n",
        "    def __getitem__(self, idx):\n",
        "        user_dict=self.times_series[idx]\n",
        "        if self.curriculum_learning:\n",
        "          user_dict[\"idx\"]=idx\n",
        "        return  user_dict\n",
        "\n",
        "def create_dataset(list_users,curriculum_learning,split=[0.8,0.1,0.1]):\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users,curriculum_learning)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "\n",
        "  return dataset_list,len(dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGgl2XnIhDQ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nke01dG-KJxO"
      },
      "outputs": [],
      "source": [
        "def collate_fn_padd(batch_dict):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "\n",
        "\n",
        "    dict_batch={key: [d[key] for d in batch_dict] for key in batch_dict[0]}\n",
        "    if \"idx\" in dict_batch:\n",
        "      dict_batch[\"idx\"]=torch.tensor(dict_batch[\"idx\"])\n",
        "    dict_batch[\"lengths\"] = torch.tensor([ user[\"input\"].shape[0] for user in batch_dict ])\n",
        "    if \"input\" in dict_batch:\n",
        "      dict_batch[\"input\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"input\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"month\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"month\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"day\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"day\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"hour\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"hour\"],batch_first=True,padding_value=24)\n",
        "    dict_batch[\"minute\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"minute\"],batch_first=True,padding_value=60)\n",
        "    dict_batch[\"second\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"second\"],batch_first=True,padding_value=60)\n",
        "\n",
        "    dict_batch[\"time_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"time_target\"],batch_first=True,padding_value=-1)\n",
        "    dict_batch[\"pos_id\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id\"],batch_first=True,padding_value=len(vocab))\n",
        "    dict_batch[\"pos_id_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id_target\"],batch_first=True,padding_value=len(vocab))\n",
        "    #print(dict_batch[\"input\"])\n",
        "    return dict_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Yl1E6gY8_P"
      },
      "source": [
        "## Instanciate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GHpriSipY7Kz"
      },
      "outputs": [],
      "source": [
        "dataset_list,len_dataset=create_dataset(list_users,curriculum_learning=True)\n",
        "train_dataset=dataset_list[0]\n",
        "valid_dataset=dataset_list[1]\n",
        "test_dataset=dataset_list[2]\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=128,collate_fn=collate_fn_padd,shuffle=True)\n",
        "valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HodJbvKeMe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptycyS7FWE4b"
      },
      "source": [
        "## Transformer Encoder followed by LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oeWr0HDhJfo"
      },
      "source": [
        "### transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "l3_bACPajeQx"
      },
      "outputs": [],
      "source": [
        "def get_mask(bath_size,sequence_length,lengths,device):\n",
        "  mask=torch.zeros(bath_size,sequence_length).to(device)\n",
        "  for i, length in enumerate(lengths):\n",
        "    mask[i,length:]=float('-inf')\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsaggvjghDsq"
      },
      "source": [
        "#### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2yFXZxqeHMwi"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class VanillaPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = (x.transpose(0,1) + self.pe[:x.transpose(0,1).size(0)]).transpose(0,1)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dX-kk1ScG-_n"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.positional_embedding=nn.Embedding(num_embeddings=max_len,embedding_dim= d_model)\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(self.parameters()).device\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size,seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x += self.positional_embedding(torch.arange(0,x.shape[1]).to(self.device))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JzKYvfaWG6cH"
      },
      "outputs": [],
      "source": [
        "def get_PositionalEncoding(d_model: int, dropout: float = 0.1, max_len: int = 2000, learnable=False):\n",
        "  if learnable:\n",
        "    return LearnablePositionalEncoding(d_model, dropout, max_len)\n",
        "  else:\n",
        "    return VanillaPositionalEncoding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-pr0W6xhOkZ"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8SZQnLdN4UxY"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,  dropout=dropout, batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only=False,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,batch_first=batch_first)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers,nhead,dropout,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCkeqmkhRnT"
      },
      "source": [
        "### feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hmqQtP0UhZqg"
      },
      "outputs": [],
      "source": [
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=embedding_dim)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=embedding_dim)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=embedding_dim)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "\n",
        "  def forward(self,dict_batch):\n",
        "    embedding= self.month_embedding(dict_batch['month'])\n",
        "    embedding=+ self.day_embedding(dict_batch['day'])\n",
        "    embedding=+ self.hour_embedding(dict_batch['hour'])\n",
        "    embedding=+ self.minute_embedding(dict_batch['minute'])\n",
        "    embedding=+ self.second_embedding(dict_batch['second'])\n",
        "    return self.dropout(embedding)\n",
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    return self.dropout(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mCp5Pz6uy-FG"
      },
      "outputs": [],
      "source": [
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bbp1dVXWQs3"
      },
      "source": [
        "#### graph_deepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW174iJrhFC",
        "outputId": "073f38a8-01e6-4502-a5b3-f69d0553174a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpysal\n",
            "  Downloading libpysal-4.10-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.12.3)\n",
            "Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from libpysal) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.25.2)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (24.0)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.11.4)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.10->libpysal) (2.5)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (1.9.6)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->libpysal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->libpysal) (3.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.16.0)\n",
            "Installing collected packages: libpysal\n",
            "Successfully installed libpysal-4.10\n"
          ]
        }
      ],
      "source": [
        "!pip install libpysal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQvAPDZyWNtg",
        "outputId": "172e446f-0022-4c92-9ef0-1c5b64bcb222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.2.1+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt22cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.1\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->ogb)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=beaa12f411e33d1292888274cf66b994185124066120cd5f5650e5134708844b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, outdated, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ogb\n",
            "Successfully installed littleutils-0.2.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sdSTBOc3sKsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, hidden_dim1, hidden_dim2, output_dim,vocab,dropout,device):\n",
        "    super(GCN, self).__init__()\n",
        "    net=get_net(vocab)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "    self.conv1 = GCNConv(2, hidden_dim1)\n",
        "    self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
        "    self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.conv1(self.coordinates, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "    x = self.conv2(x, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv3(x, self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PQfII-W5lETE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozXR4sW0W0Y"
      },
      "source": [
        " #### Combine feature embeddng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I6mU1qWOjRP3"
      },
      "outputs": [],
      "source": [
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+use_gcn\n",
        "    self.concatenate_features=concatenate_features\n",
        "    self.embedding_dim=d_model\n",
        "    self.keep_input_positions=keep_input_positions\n",
        "    if keep_input_positions:\n",
        "      self.embedding_dim=self.embedding_dim-2\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,nb_of_pos_ids,dropout),TimeStampEmbedding(self.embedding_dim,dropout)]\n",
        "    if use_gcn:\n",
        "      list_feature_embedding.append(GCN(hidden_dim1, hidden_dim2, self.embedding_dim, vocab, dropout,device))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "    if self.keep_input_positions:\n",
        "      embedding=torch.cat((dic_batch[\"input\"],embedding),dim=2)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svMRI0xeji-7"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GSt_zuJRKgBh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(F.relu(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.layer_normalisation:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,d_model,nb_of_pos_ids,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,num_layers_transformer,encoder_only,nhead,learnable_pos_encoding,new_station_binary_classification,use_gcn,vocab,hidden_dim1, hidden_dim2,max_len,dropout,batch_first,concatenate_features,keep_input_positions,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.feature_embedding=Feature_embedding(d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device)\n",
        "\n",
        "    self.num_layers_transformer=num_layers_transformer\n",
        "    if num_layers_transformer>0:\n",
        "      self.pos_encoder = get_PositionalEncoding(d_model, dropout, max_len,learnable_pos_encoding)\n",
        "      self.transformer_model=get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,nhead,dropout,batch_first)\n",
        "\n",
        "    self.num_layers_lstm=num_layers_lstm\n",
        "    if num_layers_lstm>0:\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(d_model,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first) for layer in range(num_layers_lstm)])\n",
        "    self.linear_reg=nn.Linear(d_model,output_regression_size)\n",
        "    self.classifier=nn.Linear(d_model,output_classfication_size)\n",
        "\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.binary_classifier=nn.Linear(d_model,1)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.num_layers_transformer>0:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.num_layers_lstm>0:\n",
        "      if self.num_layers_transformer>0:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.num_layers_lstm>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=F.relu(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(torch.exp(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.new_station_binary_classification:\n",
        "      out[\"new_station\"]=  torch.nn.utils.rnn.PackedSequence( self.binary_classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZpbR8rG8kBn"
      },
      "source": [
        "## Baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jn0xR-ME8tRX"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class  Baseline_model(nn.Module):\n",
        "  def __init__(self,nb_of_pos_ids):\n",
        "    super().__init__()\n",
        "    self.nb_of_pos_ids=nb_of_pos_ids\n",
        "  def forward(self,dic_batch,reg):\n",
        "    out={}\n",
        "    out[\"next_station\"]=  torch.nn.utils.rnn.pack_padded_sequence(F.one_hot(dic_batch[\"pos_id\"],self.nb_of_pos_ids).float(), lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYH5OMnmELSa",
        "outputId": "96076644-6ee4-4c2b-dc14-79f5cbc2e334"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'classification': 0.0004939125298306914,\n",
              " 'total': 0.0004939125298306914,\n",
              " 'acc': 0.7219973673527132}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Baseline_model(len(vocab)+1)\n",
        "criterion=Total_loss(False)\n",
        "evaluate(model,valid_dataloader,None,criterion,device,reg=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88IEu1ZN50b1",
        "outputId": "54eb99be-f031-426c-d6ee-a0edf7b2540d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.3958e-02, 1.3890e-02, 6.6990e-03, 4.5001e-03, 4.6558e-03, 6.3707e-03,\n",
              "        1.2496e-02, 1.3228e-02, 6.7749e-02, 7.6975e-03, 1.0034e-02, 1.7613e-01,\n",
              "        3.2064e-03, 8.9171e-03, 6.3115e-04, 5.3042e-02, 2.8050e-03, 1.0247e-02,\n",
              "        5.5167e-03, 7.2668e-03, 7.5455e-01, 3.9288e-02, 1.9941e-02, 2.0137e-02,\n",
              "        6.0778e-02, 2.1938e-02, 2.0251e-03, 9.7655e-03, 8.7778e-03, 2.1396e-03,\n",
              "        8.0644e-03, 1.1148e-02, 1.2412e-02, 2.9908e-02, 1.5560e-03, 2.6892e-03,\n",
              "        2.3730e-02, 1.8345e-03, 1.3769e-02, 9.2302e-03, 7.2989e-03, 8.3533e-03,\n",
              "        5.9968e-02, 8.6533e-03, 7.7752e-04, 6.6639e-03, 6.0347e-02, 2.4936e-02,\n",
              "        9.3127e-03, 9.3299e-03, 1.3498e-01, 2.3055e-02, 4.2083e-03, 1.4018e-02,\n",
              "        6.6818e-03, 3.3214e-03, 4.3341e-02, 9.3677e-03, 2.3635e-03, 1.6182e-01,\n",
              "        8.1041e-03, 5.0747e-03, 1.4674e-02, 2.3883e-02, 7.0223e-03, 1.4062e-01,\n",
              "        1.0073e-02, 5.3781e-03, 5.9303e-03, 8.4234e-03, 1.1114e-02, 9.1733e-03,\n",
              "        3.3980e-03, 9.0783e-03, 9.0299e-03, 1.9671e-03, 2.9705e-03, 5.9280e-03,\n",
              "        2.6235e-03, 9.5585e-02, 2.1106e-02, 4.7057e-03, 3.8990e-02, 1.9372e-03,\n",
              "        9.6871e-03, 1.2500e-02, 8.8354e-03, 1.0728e-02, 7.3957e-03, 9.7934e-02,\n",
              "        6.2224e-03, 4.4604e-03, 1.6295e-02, 1.0871e-02, 6.1534e-03, 9.9106e-03,\n",
              "        1.9903e-02, 4.0948e-03, 2.1567e-01, 1.3518e-02], device='cuda:0')"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.rand(100).cuda()/torch.randint(high=100,size=(100,)).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28s2GCFETdYS"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_ujoc4c2mQh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "    self.criterion_gather_loss=torch.nn.CrossEntropyLoss(ignore_index=len(vocab),reduction='none')\n",
        "  def forward(self, out, target_pos_ids, index_training_element,gather_loss):\n",
        "    if not gather_loss:\n",
        "      loss_classification=self.criterion(out.data[index_training_element],target_pos_ids.data[index_training_element])\n",
        "      return loss_classification,None\n",
        "    else:\n",
        "      losses=self.criterion_gather_loss(out.data,target_pos_ids.data)\n",
        "      loss_classification=losses.mean()\n",
        "      return loss_classification,losses.detach().cpu()\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "\n",
        "  def forward(self,out,dict_batch):\n",
        "\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Loss_new_station_binary_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion =  nn.BCEWithLogitsLoss()\n",
        "  def forward(self,out,target):\n",
        "    loss_classification=self.criterion(out.data.squeeze(),target.float())\n",
        "    return loss_classification\n",
        "\n",
        "def get_repetition_labels(target_pos_ids,pos_ids):\n",
        "\n",
        "  return (target_pos_ids.data==pos_ids.data).type(torch.LongTensor)\n",
        "\n",
        "def upsampling_strategy(target, epoch, epochs_new_station_only,pourcentage_of_repeat_training_elment):\n",
        "\n",
        "    index_non_repeat =(target==0).nonzero()\n",
        "    coeff=pourcentage_of_repeat_training_elment/(1-pourcentage_of_repeat_training_elment)\n",
        "    index_for_training= index_non_repeat\n",
        "    if epoch>= epochs_new_station_only:\n",
        "      index_repeat = target.nonzero().squeeze()\n",
        "      nb_non_repeat= index_non_repeat.shape[0]\n",
        "      slice_repeat=index_repeat[torch.randperm(index_repeat.shape[0])[:int(coeff*nb_non_repeat)]].squeeze()\n",
        "      index_for_training = torch.cat((index_non_repeat.squeeze(),slice_repeat))\n",
        "    return index_for_training.squeeze()\n",
        "\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self,new_station_binary_classification) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.loss_new_station_binary_classification=Loss_new_station_binary_classification()\n",
        "\n",
        "  def forward(self, out, dict_batch, upsampling,upsampling_strategy,gather_loss,reg=False ):\n",
        "    loss={}\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    if self.new_station_binary_classification or upsampling:\n",
        "      pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "      target=get_repetition_labels(target_pos_ids,pos_ids)\n",
        "    else:\n",
        "      pos_ids=None\n",
        "      target=None\n",
        "\n",
        "    if upsampling:\n",
        "      index_training_element=upsampling_strategy(target)\n",
        "    else:\n",
        "      index_training_element=torch.arange(0,target_pos_ids.data.shape[0])\n",
        "\n",
        "    loss[\"classification\"],losses=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids,index_training_element,gather_loss)\n",
        "    loss[\"total\"]=loss[\"classification\"]\n",
        "    if gather_loss:\n",
        "      batch_sizes=target_pos_ids.batch_sizes\n",
        "      sorted_indices=target_pos_ids.sorted_indices\n",
        "      unsorted_indices=target_pos_ids.unsorted_indices\n",
        "      losses=torch.nn.utils.rnn.PackedSequence(losses, batch_sizes.cpu(), sorted_indices.cpu(), unsorted_indices.cpu())\n",
        "      losses,_=torch.nn.utils.rnn.pad_packed_sequence(losses, batch_first=True, padding_value=0.0)\n",
        "      losses=losses.sum(dim=1)/dict_batch[\"lengths\"]\n",
        "    if self.new_station_binary_classification:\n",
        "      loss[\"new_station\"]=self.loss_new_station_binary_classification(out[\"new_station\"],target)\n",
        "      loss[\"total\"]+=loss[\"new_station\"]\n",
        "\n",
        "    if reg:\n",
        "      loss[\"time_regression\"]=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss[\"total\"]+=loss[\"time_regression\"]\n",
        "\n",
        "    return loss,losses\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "IHCyYC32ToKU"
      },
      "outputs": [],
      "source": [
        "# @title evaluation\n",
        "from torch import autocast\n",
        "def evaluate(model,dataloader,upsampling,criterion,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        valid_result,_=criterion(out,dict_batch,upsampling,None,reg=reg,gather_loss=False)\n",
        "        valid_results=get_sum_valid_results(valid_results,valid_result)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    valid_results=get_mean_valid_results(valid_results,nb_points)\n",
        "    valid_results[\"acc\"]=acc/nb_points\n",
        "\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yLu25E-eTcbT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title training\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "def train(train_dataloader,\n",
        "          epochs_classifcation_only,\n",
        "          epochs_complete_problem,\n",
        "          input_size,\n",
        "          num_heads,\n",
        "          d_model,\n",
        "          nb_of_pos_ids,\n",
        "          num_layers_lstm,\n",
        "          lstm_layer_with_perceptron,\n",
        "          lstm_layer_with_layer_norm,\n",
        "          num_layers_transformer,\n",
        "          encoder_only,\n",
        "          output_regression_size,\n",
        "          output_classfication_size,\n",
        "          nb_batchs,\n",
        "          dropout,\n",
        "          max_len,\n",
        "          weight_decay,\n",
        "          lr,\n",
        "          learnable_pos_encoding,\n",
        "          new_station_binary_classification,\n",
        "          use_gcn,\n",
        "          vocab,hidden_dim1, hidden_dim2,\n",
        "          batch_first,\n",
        "          concatenate_features,\n",
        "          keep_input_positions,\n",
        "          upsampling,\n",
        "          upsampling_strategy,\n",
        "          curriculum_learning,\n",
        "          percentage_to_keep_learning_on,\n",
        "          epoch_before_curriculum,\n",
        "          epochs_new_station_only,\n",
        "          pourcentage_of_repeat_training_elment,\n",
        "          percentage_to_difficult_to_learn,\n",
        "          save_best_model,\n",
        "          path_best_model,\n",
        "          batch_size,\n",
        "          device):\n",
        "\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n",
        "                                         nb_of_pos_ids=nb_of_pos_ids,\n",
        "                                         output_regression_size=output_regression_size,\n",
        "                                         output_classfication_size=output_classfication_size,\n",
        "                                         num_layers_lstm=num_layers_lstm,\n",
        "                                         lstm_layer_with_perceptron=lstm_layer_with_perceptron,\n",
        "                                         lstm_layer_with_layer_norm=lstm_layer_with_perceptron,\n",
        "                                         num_layers_transformer=num_layers_transformer,\n",
        "                                         encoder_only=encoder_only,\n",
        "                                         nhead=num_heads,\n",
        "                                         learnable_pos_encoding=learnable_pos_encoding,\n",
        "                                         new_station_binary_classification=new_station_binary_classification,\n",
        "                                         use_gcn=use_gcn,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=hidden_dim1,\n",
        "                                         hidden_dim2=hidden_dim2,\n",
        "                                         max_len=max_len,\n",
        "                                         dropout=dropout,\n",
        "                                         batch_first = batch_first,\n",
        "                                         concatenate_features = concatenate_features,\n",
        "                                         keep_input_positions = keep_input_positions,device=device\n",
        "                                         ).to(device)\n",
        "  if save_best_model:\n",
        "    os.makedirs(path_best_model,exist_ok =True)\n",
        "  optimizer_encoder = optim.AdamW( model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  criterion = Total_loss( new_station_binary_classification = new_station_binary_classification)\n",
        "  train_losses, valid_results = {},{}\n",
        "  best_results={}\n",
        "  if curriculum_learning:\n",
        "    losses_all=torch.zeros(len_dataset)+float('-inf')\n",
        "    losses_all[train_dataloader.dataset.indices]=float('+inf')\n",
        "    reverse_dict={train_dataloader.dataset.indices[i]: i for i in range(len(train_dataloader.dataset.indices))}\n",
        "    print(len(train_dataloader.dataset.indices),len_dataset)\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    reg=epoch >= epochs_classifcation_only\n",
        "    if curriculum_learning:\n",
        "      gather_loss=epoch_before_curriculum<=epoch\n",
        "      losses_list=[]\n",
        "      idx_list=[]\n",
        "    else:\n",
        "      gather_loss=False\n",
        "\n",
        "    epoch_losses={}\n",
        "    model.train()\n",
        "    i=0\n",
        "    for dict_batch in train_dataloader:\n",
        "      optimizer_encoder.zero_grad()\n",
        "      i+=1\n",
        "      if i>=nb_batchs and not gather_loss:\n",
        "        break\n",
        "      dict_batch=set_dic_to(dict_batch,device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch, reg)\n",
        "        loss,losses=criterion(out, dict_batch,upsampling,lambda target: upsampling_strategy(target,epoch,epochs_new_station_only,pourcentage_of_repeat_training_elment) ,gather_loss, reg)\n",
        "        loss[\"total\"].backward()\n",
        "        optimizer_encoder.step()\n",
        "      epoch_losses=update_epoch_losses(epoch_losses,loss)\n",
        "      if gather_loss:\n",
        "        losses_list.append(losses)\n",
        "        idx_list.append(dict_batch[\"idx\"])\n",
        "      dict_batch.clear()\n",
        "      loss.clear()\n",
        "      out.clear()\n",
        "      del out, loss,dict_batch\n",
        "\n",
        "    if gather_loss:\n",
        "      losses_tensor=torch.cat(losses_list)\n",
        "      idx_tensor=torch.cat(idx_list)\n",
        "      losses_all[idx_tensor]=losses_tensor\n",
        "      indices_loss = losses_all.argsort(descending=True)\n",
        "      sampled_indices = indices_loss[int(percentage_to_difficult_to_learn*len(train_dataloader.dataset)):len(train_dataloader.dataset)]\n",
        "      sampled_indices_dictionnary=torch.tensor([reverse_dict[sampled_indice.item()] for sampled_indice in sampled_indices])  # Sample percentage of the dataset that according to difficulty to learn\n",
        "      train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn_padd, sampler=torch.utils.data.SubsetRandomSampler(sampled_indices_dictionnary))\n",
        "\n",
        "    epoch_loss=get_epoch_loss(epoch_losses,batch_size)\n",
        "    train_losses=update_train_losses(train_losses,epoch_loss,epoch)\n",
        "    valid_result = evaluate(model,valid_dataloader,upsampling,criterion,device)\n",
        "    best_results = update_best(model,valid_result,best_results,save_best_model,path_best_model)\n",
        "    valid_results = update_valid_results(valid_results,valid_result)\n",
        "    print_results(epoch_loss,valid_result,epoch)\n",
        "\n",
        "  return best_results,model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "8OL2WGr7cGZW"
      },
      "outputs": [],
      "source": [
        "# @title utils\n",
        "\n",
        "def set_dic_to(dict_batch,device):\n",
        "  for key in dict_batch:\n",
        "    if key!=\"lengths\":\n",
        "      dict_batch[key]=dict_batch[key].to(device)\n",
        "  return dict_batch\n",
        "\n",
        "def is_better(valid_result,best_result,key):\n",
        "  match key:\n",
        "    case \"acc\":\n",
        "      return valid_result>best_result\n",
        "    case _:\n",
        "      return valid_result<best_result\n",
        "\n",
        "def update_best(model,valid_result,best_results,save_best_model,path_best_model):\n",
        "  if best_results:\n",
        "    for key in valid_result:\n",
        "      if is_better(valid_result[key],best_results[key],key):\n",
        "        best_results[key]=valid_result[key]\n",
        "        if save_best_model:\n",
        "          save_model(model,path_best_model,key)\n",
        "  else:\n",
        "    for key in valid_result:\n",
        "      best_results[key]=valid_result[key]\n",
        "      if save_best_model:\n",
        "        save_model(model,path_best_model,key)\n",
        "  return best_results\n",
        "\n",
        "def save_model(model,path_best_model,key):\n",
        "  path=os.path.join(path_best_model,key)\n",
        "  torch.save(model.state_dict(), path+\".pth\")\n",
        "\n",
        "def get_sum_valid_results(valid_result,valid_result_batch):\n",
        "  if valid_result:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]+=valid_result_batch[key].item()\n",
        "  else:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]=valid_result_batch[key].item()\n",
        "  return valid_result\n",
        "\n",
        "def get_mean_valid_results(sum_valid_result,nb_element):\n",
        "  for key in sum_valid_result:\n",
        "    sum_valid_result[key]/=nb_element\n",
        "\n",
        "  return sum_valid_result\n",
        "\n",
        "def update_epoch_losses(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key].item())\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key].item()]\n",
        "  return dict_of_list\n",
        "\n",
        "def update_valid_results(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key])\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key]]\n",
        "  return dict_of_list\n",
        "\n",
        "def get_epoch_loss(epoch_losses,batch_size):\n",
        "\n",
        "  epoch_loss={}\n",
        "  for key in epoch_losses:\n",
        "    epoch_loss[key]=np.array(epoch_losses[key]).mean()/batch_size\n",
        "  return epoch_loss\n",
        "\n",
        "def print_results(epoch_loss,valid_result,epoch):\n",
        "\n",
        "  print(\"\\nepoch: \",epoch)\n",
        "  print(\"train :\", end=\"\\t\")\n",
        "  for key in epoch_loss:\n",
        "    print(key,epoch_loss[key], end=\"\\t\")\n",
        "  print(\"\\nvalid :\", end=\"\\t\")\n",
        "  for key in valid_result:\n",
        "    print(key,valid_result[key], end=\"\\t\")\n",
        "\n",
        "def update_train_losses(train_losses,epoch_loss,epoch):\n",
        "\n",
        "  if train_losses:\n",
        "    for key in epoch_loss:\n",
        "      if key in train_losses:\n",
        "        train_losses[key].append(epoch_loss[key])\n",
        "      else:\n",
        "        train_losses[key]=[float('nan')]*(epoch+1)+[epoch_loss[key]]\n",
        "  else:\n",
        "    for key in epoch_loss:\n",
        "      train_losses[key]=[epoch_loss[key]]\n",
        "  return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "HL0AZ-YJChyE",
        "outputId": "f1238f5a-449e-4273-aa5b-fd799b073254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.35 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:  0\n",
            "train :\tclassification 0.11273022821316352\ttotal 0.11273022821316352\t\n",
            "valid :\tclassification 0.0020530678877048857\ttotal 0.0020530678877048857\ttime_regression 0.0007774434514398475\tacc 0.11513297456624165\t\n",
            "epoch:  1\n",
            "train :\tclassification 0.08570626188929265\ttotal 0.08570626188929265\t\n",
            "valid :\tclassification 0.0023812097939102696\ttotal 0.0023812097939102696\ttime_regression 0.0014741503531927028\tacc 0.25735212022419224\t"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d38a14ff5d7f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Titre par défaut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m best_results,model =train(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs_complete_problem\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-56981036b316>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, epochs_classifcation_only, epochs_complete_problem, input_size, num_heads, d_model, nb_of_pos_ids, num_layers_lstm, lstm_layer_with_perceptron, lstm_layer_with_layer_norm, num_layers_transformer, encoder_only, output_regression_size, output_classfication_size, nb_batchs, dropout, max_len, weight_decay, lr, learnable_pos_encoding, new_station_binary_classification, use_gcn, vocab, hidden_dim1, hidden_dim2, batch_first, concatenate_features, keep_input_positions, upsampling, upsampling_strategy, curriculum_learning, percentage_to_keep_learning_on, epoch_before_curriculum, epochs_new_station_only, pourcentage_of_repeat_training_elment, percentage_to_difficult_to_learn, save_best_model, path_best_model, batch_size, device)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0moptimizer_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0mepoch_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_epoch_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgather_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mlosses_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-c1dc2a6ba728>\u001b[0m in \u001b[0;36mupdate_epoch_losses\u001b[0;34m(dict_of_list, dic)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdict_of_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0mdict_of_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Titre par défaut\n",
        "best_results,model =train(\n",
        "          train_dataloader,\n",
        "          epochs_classifcation_only=50,\n",
        "          epochs_complete_problem =25,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=300,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=1,\n",
        "          lstm_layer_with_perceptron=False,\n",
        "          lstm_layer_with_layer_norm=False,\n",
        "          num_layers_transformer=1,\n",
        "          encoder_only=True,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=133,\n",
        "          dropout=0.35,\n",
        "          max_len=100,\n",
        "          weight_decay=5e-4,\n",
        "          lr=7e-4,\n",
        "          learnable_pos_encoding=False,\n",
        "          new_station_binary_classification=False,\n",
        "          use_gcn=False,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          batch_first= True,\n",
        "          concatenate_features = False,\n",
        "          keep_input_positions = False,\n",
        "          upsampling=False,\n",
        "          upsampling_strategy=False,\n",
        "          curriculum_learning=False,\n",
        "          percentage_to_keep_learning_on=0,\n",
        "          percentage_to_difficult_to_learn=0.2,\n",
        "          epoch_before_curriculum=10,\n",
        "          epochs_new_station_only=0,\n",
        "          pourcentage_of_repeat_training_elment=0.1,\n",
        "          save_best_model=True,\n",
        "          path_best_model=\"test_0.5\",\n",
        "          device=device,\n",
        "          batch_size=64\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4FEU_h1YtX"
      },
      "source": [
        "## Instance of training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader.dataset.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJYae6hGVVBR",
        "outputId": "59e60894-c380-4946-9788-b857bd6c16c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.VariableLengthDatasetWithPosID at 0x7b83b28305b0>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "b-I9GHbVFG92",
        "outputId": "2a3f477a-6dfb-4186-94f1-7c0b0b5b753c"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Transformer_encoder_LSTM_decoder.__init__() got an unexpected keyword argument 'curriculum_learning'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4101e0294e1e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Titre par défaut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m best_results,model =train(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs_complete_problem\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b1041e14ca90>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs_classifcation_only, epochs_complete_problem, input_size, num_heads, d_model, nb_of_pos_ids, num_layers_lstm, lstm_layer_with_perceptron, lstm_layer_with_layer_norm, num_layers_transformer, encoder_only, output_regression_size, output_classfication_size, nb_batchs, dropout, max_len, weight_decay, lr, learnable_pos_encoding, new_station_binary_classification, use_gcn, vocab, hidden_dim1, hidden_dim2, batch_first, concatenate_features, keep_input_positions, upsampling, upsampling_strategy, curriculum_learning, percentage_to_keep_learning_on, epoch_before_curriculum, epochs_new_station_only, pourcentage_of_repeat_training_elment, save_best_model, path_best_model, batch_size, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs_complete_problem\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m   model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n\u001b[0m\u001b[1;32m     48\u001b[0m                                          \u001b[0mnb_of_pos_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_of_pos_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                          \u001b[0moutput_regression_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_regression_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Transformer_encoder_LSTM_decoder.__init__() got an unexpected keyword argument 'curriculum_learning'"
          ]
        }
      ],
      "source": [
        "# @title Titre par défaut\n",
        "best_results,model =train(\n",
        "          epochs_classifcation_only=40,\n",
        "          epochs_complete_problem =25,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=768,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=0,\n",
        "          lstm_layer_with_perceptron=True,\n",
        "          lstm_layer_with_layer_norm=True,\n",
        "          num_layers_transformer=1,\n",
        "          encoder_only=True,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=122,\n",
        "          dropout=0.35,\n",
        "          max_len=100,\n",
        "          weight_decay=1e-5,\n",
        "          lr=5e-4,\n",
        "          learnable_pos_encoding=False,\n",
        "          new_station_binary_classification=False,\n",
        "          use_gcn=False,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          batch_first= True,\n",
        "          concatenate_features = False,\n",
        "          keep_input_positions = False,\n",
        "          upsampling=False,\n",
        "          upsampling_strategy=None,\n",
        "          curriculum_learning=True,\n",
        "          percentage_to_keep_learning_on=0.3,\n",
        "          epoch_before_curriculum=20,\n",
        "          epochs_new_station_only=0,\n",
        "          pourcentage_of_repeat_training_elment=0.1,\n",
        "          save_best_model=True,\n",
        "          path_best_model=\"test_0.5\",\n",
        "          device=device,\n",
        "          batch_size=64\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fL5dCWywLoZ"
      },
      "source": [
        "# hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYy2A_VUOvPY"
      },
      "source": [
        "##model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHxNOtpIczjP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn.models import GCN,GAT,GraphSAGE\n",
        "import numpy as np\n",
        "\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "def get_layer(layer_type):\n",
        "  print(layer_type)\n",
        "  match layer_type:\n",
        "    case \"GraphSAGE\":\n",
        "      return GraphSAGE\n",
        "    case \"GCNConv\":\n",
        "      return GCN\n",
        "    case \"GAT\":\n",
        "      return GAT\n",
        "\n",
        "class GCN_embedding(nn.Module):\n",
        "  def __init__(self,output_dim,layer_type,num_layers_gcn,hidden_channels,activation_gcn,norm,net,device,normalize_features_independantly,dropout):\n",
        "    super(GCN_embedding, self).__init__()\n",
        "    self.normalize_features_independantly=normalize_features_independantly\n",
        "    if self.normalize_features_independantly:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(output_dim)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "    self.model=get_layer(layer_type)(in_channels=2, out_channels=output_dim, act=activation_gcn, norm=norm, num_layers=num_layers_gcn, hidden_channels=hidden_channels,dropout=dropout)\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.model(self.coordinates,self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    if self.normalize_features_independantly:\n",
        "      embedding=self.layer_normalisation(embedding)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHalywx1i82o"
      },
      "outputs": [],
      "source": [
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,normalize_features_independantly):\n",
        "    super().__init__()\n",
        "    #self.dropout = nn.Dropout(p=dropout)\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=embedding_dim)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=embedding_dim)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=embedding_dim)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.normalize_features_independantly=normalize_features_independantly\n",
        "    if self.normalize_features_independantly:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding= self.month_embedding(dict_batch['month'])\n",
        "    embedding=+ self.day_embedding(dict_batch['day'])\n",
        "    embedding=+ self.hour_embedding(dict_batch['hour'])\n",
        "    embedding=+ self.minute_embedding(dict_batch['minute'])\n",
        "    embedding=+ self.second_embedding(dict_batch['second'])\n",
        "    if self.normalize_features_independantly:\n",
        "      embedding = self.layer_normalisation(embedding)\n",
        "    return embedding\n",
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,normalize_features_independantly):\n",
        "    super().__init__()\n",
        "    self.normalize_features_independantly=normalize_features_independantly\n",
        "    if self.normalize_features_independantly:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(embedding_dim)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    if self.normalize_features_independantly:\n",
        "      embedding = self.layer_normalisation(embedding)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht0do-5IZMvA"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,config,net,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+config[\"use_gcn\"]\n",
        "    self.concatenate_features=config[\"concatenate_features\"]\n",
        "    self.embedding_dim=config[\"d_model\"]\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,config[\"nb_of_pos_ids\"],config[\"normalize_features_independantly\"]),TimeStampEmbedding(self.embedding_dim,config[\"normalize_features_independantly\"])]\n",
        "    if config[\"use_gcn\"]:\n",
        "      list_feature_embedding.append(GCN_embedding( self.embedding_dim,config[\"layer_type\"],config[\"num_layers_gcn\"],config[\"hidden_channels\"],config[\"activation_gcn\"],config[\"norm\"],net,device,config[\"normalize_features_independantly\"],config['dropout']))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "\n",
        "    return embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhN3oIqdSY5H"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers_transformer,num_heads,dropout_transformers,activation_transformers,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=num_heads, num_encoder_layers=num_layers_transformer, num_decoder_layers=num_layers_transformer, dropout=dropout_transformers,activation=get_activation(activation_transformers), batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,num_heads,dropout_transformers,activation_transformers,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads,batch_first=batch_first,activation=get_activation(activation_transformers),dropout=dropout_transformers)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers_transformer)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers_transformer,num_heads,dropout_transformers,activation_transformers,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy3XI4E5iCDi"
      },
      "outputs": [],
      "source": [
        "# @title Model\n",
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model,activation_lstm):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "    self.activation=get_activation(activation_lstm)\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(self.activation(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,activation_lstm,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model,activation_lstm)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.lstm_layer_with_layer_norm:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "class Abs(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.abs(x)\n",
        "\n",
        "class Exp(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.exp(x)\n",
        "\n",
        "class Sig(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def get_positive_function(config):\n",
        "  match config[\"positive_function\"]:\n",
        "    case \"relu\":\n",
        "      return nn.ReLU()\n",
        "    case \"abs\":\n",
        "      return Abs()\n",
        "    case \"exp\":\n",
        "      return Exp()\n",
        "    case \"sig\":\n",
        "      return Sig()\n",
        "\n",
        "\n",
        "def get_activation(activation):\n",
        "  match activation:\n",
        "    case \"ReLU\":\n",
        "      return nn.ReLU()\n",
        "    case \"Tanh\":\n",
        "      return nn.Tanh()\n",
        "    case \"LeakyReLU\":\n",
        "      return nn.LeakyReLU()\n",
        "    case \"SiLU\":\n",
        "      return nn.SiLU()\n",
        "    case \"GELU\":\n",
        "      return nn.GELU()\n",
        "    case \"ELU\":\n",
        "      return nn.ELU()\n",
        "    case \"Mish\":\n",
        "      return nn.Mish()\n",
        "    case \"ReLU6\":\n",
        "      return nn.ReLU6()\n",
        "    case \"PReLU\":\n",
        "      return nn.PReLU()\n",
        "    case \"SELU\":\n",
        "      return nn.SELU()\n",
        "    case \"CELU\":\n",
        "      return nn.CELU()\n",
        "    case \"Hardsigmoid\":\n",
        "      return nn.Hardsigmoid()\n",
        "    case \"Softplus\":\n",
        "      return nn.Softplus()\n",
        "    case \"Hardshrink\":\n",
        "      return nn.Hardshrink()\n",
        "    case \"Sigmoid\":\n",
        "      return nn.Sigmoid()\n",
        "    case \"Hardtanh\":\n",
        "      return nn.Hardtanh()\n",
        "    case \"Tanhshrink\":\n",
        "      return nn.Tanhshrink()\n",
        "    case \"RReLU\":\n",
        "      return nn.RReLU()\n",
        "    case \"Softshrink\":\n",
        "      return nn.Softshrink()\n",
        "    case \"Softsign\":\n",
        "      return nn.Softsign()\n",
        "    case \"LogSigmoid\":\n",
        "      return nn.LogSigmoid()\n",
        "    case \"Softmin\":\n",
        "      return nn.Softmin()\n",
        "    case \"Hardswish\":\n",
        "      return nn.Hardswish()\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,config,net,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=config[\"dropout\"])\n",
        "\n",
        "    self.normalize_features_globally=config[\"normalize_features_globally\"]\n",
        "    if self.normalize_features_globally:\n",
        "      self.global_layer_normalisation=torch.nn.LayerNorm(config[\"d_model\"])\n",
        "    self.feature_embedding=Feature_embedding(config,net,device)\n",
        "    self.activation=get_activation(config[\"activation\"])\n",
        "    if config[\"reg\"]:\n",
        "      self.positive_function=get_positive_function(config)\n",
        "    self.transformers_model=config[\"transformers_model\"]\n",
        "    if self.transformers_model>0:\n",
        "      self.num_layers_transformer=config[\"num_layers_transformer\"]\n",
        "      self.pos_encoder = get_PositionalEncoding(config[\"d_model\"], config[\"dropout_transformers\"], 100,config[\"learnable_pos_encoding\"])\n",
        "      self.transformer_model=get_Transformer_architecture(config[\"d_model\"],config[\"encoder_only\"],config[\"num_layers_transformer\"],config[\"num_heads\"],config[\"dropout_transformers\"],config[\"activation_transformers\"],True,)\n",
        "\n",
        "    self.lstm_model=config[\"lstm_model\"]\n",
        "    if self.lstm_model>0:\n",
        "      self.num_layers_lstm=config[\"num_layers_lstm\"]\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(config[\"d_model\"],2,config[\"nb_of_pos_ids\"],config[\"num_layers_lstm\"],config[\"lstm_layer_with_perceptron\"],config[\"lstm_layer_with_layer_norm\"],config[\"dropout_lstm\"],config[\"activation_lstm\"],True) for layer in range(config[\"num_layers_lstm\"])])\n",
        "    self.linear_reg=nn.Linear(config[\"d_model\"],2)\n",
        "    self.classifier=nn.Linear(config[\"d_model\"],config[\"nb_of_pos_ids\"])\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.transformers_model:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      if self.normalize_features_globally:\n",
        "        x= self.global_layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.lstm_model:\n",
        "      if self.transformers_model:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "      if self.normalize_features_globally:\n",
        "        x= self.global_layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.lstm_model>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=self.activation(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(self.positive_function(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNtPp4XO1fz"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bG3BJIKMGsM-"
      },
      "outputs": [],
      "source": [
        "from torch import autocast\n",
        "def evaluate(model,dataloader,device,reg=False):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    acc=acc/nb_points\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoDnxzqRWr-5"
      },
      "outputs": [],
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, out, target_pos_ids):\n",
        "    loss_classification=self.criterion(out.data,target_pos_ids.data)\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,out,dict_batch):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "\n",
        "  def forward(self, out, dict_batch, reg=False):\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    loss_classification=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids)\n",
        "    loss_total=loss_classification\n",
        "    if reg:\n",
        "      loss_time_regression=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss_total+=loss_time_regression\n",
        "    return loss_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ibzvugSLizvL"
      },
      "outputs": [],
      "source": [
        "# @title utils\n",
        "\n",
        "def f_unpack_dict(dct):\n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "\n",
        "    return res\n",
        "\n",
        "def get_file_name(name,path=\".\"):\n",
        "  exist=True\n",
        "  idx=0\n",
        "  while exist:\n",
        "    file_path=os.path.join(path,name+\"_\"+str(idx))\n",
        "    exist=os.path.exists(file_path)\n",
        "    idx+=1\n",
        "  return file_path\n",
        "\n",
        "\n",
        "def get_last_file_name(name,path=\".\"):\n",
        "  exist=True\n",
        "  idx=0\n",
        "  file_path=None\n",
        "  while exist:\n",
        "    last_file=file_path\n",
        "    file_path=os.path.join(path,name+\"_\"+str(idx))\n",
        "    exist=os.path.exists(file_path)\n",
        "    idx+=1\n",
        "  return last_file\n",
        "\n",
        "def get_file_name_2(name,path=\".\"):\n",
        "  exist=True\n",
        "  i=1\n",
        "  for file_or_folder in os.listdir(path):\n",
        "    if os.path.isfile(os.path.join(path,file_or_folder)) and file_or_folder.startswith(name):\n",
        "        idx=file_or_folder.split(\"_\")[-2]\n",
        "        if idx.isdigit():\n",
        "          i=max(i,int(idx)+1)\n",
        "  return os.path.join(path,name+\"_\"+str(i))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7DWtAusO6ue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4Bfn1b1O7cH"
      },
      "source": [
        "## hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkCi3L-UKKtv",
        "outputId": "13c9917a-2df7-4dfb-e2d3-6532de659915"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.9.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (23.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.5.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"ray[tune]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNO-AvQjH03u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.optim as optim\n",
        "from ray import train, tune\n",
        "from ray.tune.schedulers import ASHAScheduler,AsyncHyperBandScheduler\n",
        "from ray.util.accelerators import NVIDIA_TESLA_V100\n",
        "from hyperopt import hp,Trials\n",
        "import ray\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "\n",
        "def get_model(config,net,device):\n",
        "  return Transformer_encoder_LSTM_decoder(config,net=net,device=device\n",
        "                                          ).to(device)\n",
        "\n",
        "def update_best_acc(model,valid_acc,best_acc,nb_epochs_without_improvement):\n",
        "    if valid_acc > best_acc :\n",
        "      nb_epochs_without_improvement=0\n",
        "      best_acc=valid_acc\n",
        "    else:\n",
        "      nb_epochs_without_improvement+=1\n",
        "    return best_acc,nb_epochs_without_improvement\n",
        "def get_LRScheduler(optimizer,config,epochs):\n",
        "  match config[\"scheduler\"]:\n",
        "    case None:\n",
        "      return None\n",
        "    case \"StepLR\":\n",
        "      return optim.lr_scheduler.StepLR(optimizer,step_size=config[\"step_size\"],gamma=config[\"gamma\"])\n",
        "    case \"ReduceLROnPlateau\":\n",
        "      return optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=config[\"factor\"],patience=config[\"patience\"],threshold=config[\"threshold\"],cooldown=config[\"cooldown\"])\n",
        "    case \"ExponentialLR\":\n",
        "      return optim.lr_scheduler.ExponentialLR(optimizer,gamma=config[\"gamma\"])\n",
        "    case \"CosineAnnealingLR\":\n",
        "      return optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=config[\"T_max\"],eta_min=config[\"eta_min\"])\n",
        "    case \"CyclicLR\":\n",
        "      return optim.lr_scheduler.CyclicLR(optimizer,base_lr=config[\"base_lr\"],max_lr=config[\"max_lr\"],step_size_up=config[\"step_size_up\"],mode=config[\"mode\"],cycle_momentum=False)\n",
        "\n",
        "def get_otimizer(parameters,config):\n",
        "  match config[\"optimizer\"]:\n",
        "    case \"Adam\":\n",
        "      return optim.Adam(parameters,lr=config[\"lr\"],betas=(config[\"beta_1\"],config[\"beta_2\"]),eps=config[\"eps\"],weight_decay=config[\"weight_decay\"],amsgrad=config[\"amsgrad\"])\n",
        "    case \"AdamW\":\n",
        "      return optim.AdamW(parameters,lr=config[\"lr\"],betas=(config[\"beta_1\"],config[\"beta_2\"]),eps=config[\"eps\"],weight_decay=config[\"weight_decay\"],amsgrad=config[\"amsgrad\"])\n",
        "    case \"SGD\":\n",
        "      return optim.SGD(parameters,lr=config[\"lr\"],momentum=config[\"momentum\"],weight_decay=config[\"weight_decay\"],nesterov=config[\"nesterov\"])\n",
        "    case \"RMSprop\":\n",
        "      return optim.RMSprop(parameters,lr=config[\"lr\"],alpha=config[\"alpha\"],eps=config[\"eps\"],weight_decay=config[\"weight_decay\"],momentum=config[\"momentum\"],centered=config[\"centered\"])\n",
        "def apply_lr_scheduler(lr_scheduler,acc,config):\n",
        "  match config[\"scheduler\"]:\n",
        "    case \"ReduceLROnPlateau\":\n",
        "      lr_scheduler.step(acc)\n",
        "    case None:\n",
        "      pass\n",
        "    case _:\n",
        "      lr_scheduler.step()\n",
        "\n",
        "def train_(config,model,dataloaders):\n",
        "    print(config)\n",
        "    device=get_device()\n",
        "    epochs= config[\"epochs_classifcation_only\"]\n",
        "    if config[\"reg\"]:\n",
        "      epochs+=config[\"epochs_complete_problem\"]\n",
        "    optimizer_encoder = get_otimizer(model.parameters(),config)\n",
        "    lr_scheduler=get_LRScheduler(optimizer_encoder,config,epochs)\n",
        "    criterion = Total_loss()\n",
        "    best_acc=-1\n",
        "    nb_epochs_without_improvement=0\n",
        "    for epoch in range(epochs):\n",
        "      reg=epoch >= config[\"epochs_classifcation_only\"]\n",
        "      epoch_losses=[]\n",
        "      model.train()\n",
        "      i=0\n",
        "      for dict_batch in dataloaders[\"train\"]:\n",
        "        optimizer_encoder.zero_grad()\n",
        "        i+=1\n",
        "        if i>=config[\"nb_batchs\"]:\n",
        "          break\n",
        "        dict_batch=set_dic_to(dict_batch,device)\n",
        "        with autocast(device_type=device.type):\n",
        "          out=model(dict_batch, reg)\n",
        "          loss=criterion(out,dict_batch, reg)\n",
        "          if loss.isnan():\n",
        "            print(\"loss is undifined\")\n",
        "            return -1\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer_encoder.step()\n",
        "\n",
        "        epoch_losses.append(loss.item())\n",
        "\n",
        "        dict_batch.clear()\n",
        "        out.clear()\n",
        "        del out, loss, dict_batch\n",
        "      epoch_loss=np.array(epoch_losses).mean()\n",
        "      valid_acc = evaluate(model,dataloaders[\"valid\"],device)\n",
        "      apply_lr_scheduler(lr_scheduler,valid_acc,config)\n",
        "      best_acc, nb_epochs_without_improvement = update_best_acc(model,valid_acc,best_acc,nb_epochs_without_improvement)\n",
        "      if config[\"early_stopping\"]< nb_epochs_without_improvement:\n",
        "        return best_acc\n",
        "\n",
        "      print(\"epoch: \", epoch, \"loss : \", epoch_loss, \"acc: \", valid_acc)\n",
        "    return best_acc\n",
        "\n",
        "\n",
        "def get_datasets():\n",
        "    list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                      directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                      fixed_time_encoding=False,\n",
        "                                      input_position=True,\n",
        "                                      full_dataset=True,\n",
        "                                      spliting_long_sequences=False,\n",
        "                                      with_repeated_connections=False,\n",
        "                                      max_sequence_length=100,\n",
        "                                      min_sequence_size=2,\n",
        "                                      save=False,\n",
        "                                      path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",\n",
        "                                      download=False,\n",
        "                                      load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",)\n",
        "    net=get_net(vocab)\n",
        "    reproducibility_seed=get_reproducible_seeds()[0]\n",
        "    dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "    generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "    dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "    return dataset_list,net,len(vocab)+1\n",
        "\n",
        "def get_dataloaders(datasets,batch_size):\n",
        "    train_dataset=datasets[0]\n",
        "    valid_dataset=datasets[1]\n",
        "    train_dataloader=DataLoader(train_dataset,batch_size=batch_size,collate_fn=collate_fn_padd,shuffle=True)\n",
        "    valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "    return {\"train\":train_dataloader,\"valid\":valid_dataloader}\n",
        "\n",
        "def eval_config(config,data=None,net=None):\n",
        "    device=get_device()\n",
        "    config=f_unpack_dict(config)\n",
        "    dataloaders=get_dataloaders(data,config[\"batch_size\"])\n",
        "    if config[\"use_gcn\"]:\n",
        "      model=get_model(config,net,device)\n",
        "    else:\n",
        "      model=get_model(config,None,device)\n",
        "    best_acc = train_(config,model,dataloaders)\n",
        "    return {\"valid_accuracy\": best_acc}\n",
        "\n",
        "def run_xp(xp_name,storage_path,algo,num_samples=10, max_num_epochs=10, gpus_per_trial=1, test=True):\n",
        "  os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
        "  print(os.environ[\"OMP_NUM_THREADS\"])\n",
        "  datasets,net,nb_of_pos_ids=get_datasets()\n",
        "  ray.shutdown()\n",
        "  config_dict= {\n",
        "        \"max_len\":100,\n",
        "        \"nb_of_pos_ids\":nb_of_pos_ids,\n",
        "        \"batch_size\":2**hp.uniformint(\"batch_size\",4,7),\n",
        "        \"nb_batchs\":12*hp.uniformint(\"nb_batchs\",1,16),\n",
        "        \"early_stopping\":hp.uniformint(\"early_stopping\",1,10),\n",
        "        \"epochs_classifcation_only\":hp.uniformint(\"epochs_classifcation_only\",1,80),\n",
        "        \"reg_choice\":hp.choice(\"reg_choice\",\n",
        "                        [\n",
        "                            {\"reg\":True,\"epochs_complete_problem\":hp.uniformint(\"epochs_complete_problem\",0,50)},\n",
        "                            {\"reg\":False},\n",
        "                        ]),\n",
        "        \"learning_rate_scheduler_choice\": hp.choice(\n",
        "          \"learning_rate_scheduler_choice\",\n",
        "          [\n",
        "              {\n",
        "                  \"scheduler\": \"StepLR\",\n",
        "                  \"step_size\": hp.uniformint(\"step_size\", 1, 30),\n",
        "                  \"gamma\": hp.uniform(\"gamma_slr\", 0, 0.99),\n",
        "              },\n",
        "              {\n",
        "                  \"scheduler\": \"ReduceLROnPlateau\",\n",
        "                  \"factor\": hp.uniform(\"factor\", 0, 0.9),\n",
        "                  \"patience\": hp.uniformint(\"patience\", 1, 10),\n",
        "                  \"threshold\": hp.loguniform(\"threshold\",-12,-1),\n",
        "                  \"cooldown\":hp.uniformint(\"cooldown\",0,10)\n",
        "              },\n",
        "              {\n",
        "                  \"scheduler\":\"ExponentialLR\",\n",
        "                  \"gamma\":hp.uniform(\"gamma_elr\", 0, 0.9),\n",
        "              },\n",
        "            {\"scheduler\": None}  # No scheduler\n",
        "        ]\n",
        "    ),\n",
        "      \"optimizer\": hp.choice(\"optimizer\",[\"Adam\",\"AdamW\"]), \"lr\": hp.loguniform(\"lr\", -17, -2),\"beta_1\":hp.uniform(\"beta_1\", 0.8, 1), \"beta_2\" : hp.uniform(\"beta_2\", 0.95, 1),\"eps\": hp.loguniform(\"eps_adam\", -20, -12),\"weight_decay\":hp.loguniform(\"weight_decay_adam\",-20,-1),\"amsgrad\":hp.choice(\"amsgrad\",[True,False]),\n",
        "      \"input_size\":2,\n",
        "      \"d_model\":24*hp.uniformint(\"d_model\",1,60),\n",
        "      \"dropout\":hp.uniform(\"dropout\",0,1),\n",
        "      \"normalize_features_independantly\":hp.choice(\"normalize_features_independantly\",[True,False]),\n",
        "      \"normalize_features_globally\":hp.choice(\"normalize_features_globally\",[True,False]),\n",
        "      \"concatenate_features\":hp.choice(\"concatenate_features\",[True,False]),\n",
        "      \"use_gcn_choice\":hp.choice(\"use_gcn_choice\",\n",
        "                        [\n",
        "                            {\"use_gcn\":True,\n",
        "                             \"layer_type\":hp.choice(\"layer_type\",[\"GCNConv\",\"GraphSAGE\",\"GAT\"]),\n",
        "                             \"num_layers_gcn\":hp.uniformint(\"num_layers_gcn\",1,10),\n",
        "                             \"activation_gcn\": hp.choice(\"activation_gcn\",\n",
        "                              ['swish', 'ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']\n",
        "                             ),\n",
        "                             \"norm\": hp.choice(\"norm\",\n",
        "                                               ['BatchNorm', 'GraphNorm', 'LayerNorm', 'PairNorm', 'InstanceNorm']\n",
        "                             ),\n",
        "                             \"dropout_gcn\":hp.uniform(\"dropout_gcn\",0,1),\n",
        "                             \"hidden_channels\":2**hp.uniformint(\"hidden_channels\",6,11)\n",
        "                             },\n",
        "                            {\"use_gcn\":False}\n",
        "                        ]),\n",
        "      \"activation\": hp.choice(\n",
        "                \"activation\",\n",
        "                 ['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "      \"positive_function\":hp.choice(\"positive_function\",[\"relu\",\"exp\",\"abs\",\"sig\"]),\n",
        "      \"transformers_model\":True,\n",
        "      \"num_layers_transformer\":hp.uniformint(\"num_layers_transformer\",1,6),\n",
        "      \"encoder_only\":hp.choice(\"encoder_only\",[True,False]),\n",
        "      \"num_heads\":3*2**hp.uniformint('num_heads', 0, 3),\n",
        "      \"learnable_pos_encoding\": hp.choice(\"learnable_pos_encoding\",[True,False]),\n",
        "      \"activation_transformers\": hp.choice(\"activation_transformers\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "      \"dropout_transformers\":hp.uniform(\"dropout_transformers\",0,1),\n",
        "\n",
        "      \"lstm_model_choice\": hp.choice(\"lstm_model_choice\",\n",
        "                                     [{\"lstm_model\":True,\n",
        "                                       \"num_layers_lstm\":hp.uniformint(\"num_layers_lstm\",1,6),\n",
        "                                       \"lstm_layer_with_perceptron\":\n",
        "                                        hp.choice(\"lstm_layer_with_perceptron\",\n",
        "                                         [{\"lstm_layer_with_perceptron\":True,\n",
        "                                           \"activation_lstm\":hp.choice(\"activation_lstm\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),},\n",
        "                                          {\"lstm_layer_with_perceptron\":False,\n",
        "                                           \"activation_lstm\":None}]),\n",
        "                                       \"lstm_layer_with_layer_norm\":hp.choice(\"lstm_layer_with_layer_norm\",[True,False]),\n",
        "                                       \"dropout_lstm\":hp.uniform(\"dropout_lstm\",0,1),\n",
        "                                       },\n",
        "                                      {\"lstm_model\":False}])}\n",
        "  if algo==None:\n",
        "    algo = HyperOptSearch(space=config_dict, metric=\"valid_accuracy\", mode=\"max\", random_state_seed=get_reproducible_seeds()[0])\n",
        "  trainable_with_gpu = tune.with_resources(eval_config, {\"cpu\": 2, \"gpu\": 1})\n",
        "  tuner = tune.Tuner(\n",
        "        tune.with_parameters(trainable_with_gpu,data=datasets,net=net),\n",
        "        tune_config=tune.TuneConfig(\n",
        "                                search_alg=algo,\n",
        "                                max_concurrent_trials=1,\n",
        "                                num_samples=1 if test else num_samples,\n",
        "                                    ),\n",
        "        run_config=train.RunConfig(\n",
        "            name=xp_name,\n",
        "            storage_path=storage_path,\n",
        "            verbose=0)\n",
        "    )\n",
        "  # To enable GPUs, use this instead:\n",
        "  results = tuner.fit()\n",
        "  return results, algo\n",
        "\n",
        "\n",
        "def save_config_xps_to_drive(xps_name,drive_path,xp_size,xps_number,accuracy_target,max_num_epochs):\n",
        "  dic_config={\n",
        "      \"xps_name\":xps_name,\n",
        "      \"xp_size\":xp_size,\n",
        "      \"xps_number\":xps_number,\n",
        "      \"current_xp\": -1,\n",
        "      \"best_xp\": {\"idx\":-1, \"valid_accuracy\": -1}\n",
        "  }\n",
        "  xps_path=os.path.join(drive_path,xps_name)\n",
        "  xps_configs= os.path.join(xps_path,\"xps_configs\")\n",
        "  os.makedirs(xps_path,exist_ok=True)\n",
        "  if not os.path.exists(xps_configs):\n",
        "    torch.save(dic_config,xps_configs)\n",
        "  return xps_path,xps_configs\n",
        "\n",
        "\n",
        "def update_config_dictionnary(xps_configs,best_results,num_xp):\n",
        "\n",
        "  config_dic=torch.load(xps_configs)\n",
        "  config_dic[\"current_xp\"]=num_xp\n",
        "  if config_dic[\"best_xp\"][\"valid_accuracy\"]<best_results:\n",
        "    config_dic[\"best_xp\"][\"valid_accuracy\"]=best_results\n",
        "    config_dic[\"best_xp\"][\"idx\"]=num_xp\n",
        "  torch.save(config_dic,xps_configs)\n",
        "\n",
        "\n",
        "\n",
        "def update_and_save(xp_name,xps_path,xps_configs,storage_path,results,algo,num_xp,accuracy_target):\n",
        "  best_results=results.get_best_result(metric='valid_accuracy',mode='max').metrics['valid_accuracy']\n",
        "  accarucy_target_not_reached= best_results< accuracy_target\n",
        "  update_config_dictionnary(xps_configs,best_results,num_xp)\n",
        "  shutil.copytree(os.path.join(storage_path,xp_name),os.path.join(xps_path,xp_name),dirs_exist_ok=True)\n",
        "  if num_xp>=1:\n",
        "    shutil.rmtree(os.path.join(xps_path,\"xp_num_\"+str(num_xp-1)))\n",
        "  shutil.rmtree(os.path.join(storage_path,xp_name))\n",
        "  return accarucy_target_not_reached\n",
        "\n",
        "\n",
        "def run_all_xp(xps_name=\"hyperparameter_tuning_projet_long\", num_xp=0,algo=None, xp_size=10, xps_number=10, accuracy_target=0.98, max_num_epochs=30, storage_path='/content/',drive_path=\"/content/drive/MyDrive\"):\n",
        "    accarucy_target_not_reached=True\n",
        "    num_xp=num_xp\n",
        "    xps_path,xps_configs=save_config_xps_to_drive(xps_name,drive_path,xp_size,xps_number,accuracy_target,max_num_epochs)\n",
        "    while num_xp<xps_number and accarucy_target_not_reached:\n",
        "      xp_name= \"xp_num_\"+str(num_xp)\n",
        "      results,algo=run_xp(xp_name,storage_path,algo,num_samples=xp_size, max_num_epochs=max_num_epochs, gpus_per_trial=1, test=False)\n",
        "      accarucy_target_not_reached=update_and_save(xp_name,xps_path,xps_configs,storage_path,results,algo,num_xp,accuracy_target)\n",
        "      num_xp+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPhHgHrMlJUr",
        "outputId": "26076737-6c10-4dbc-ed14-34e3e0332aee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "xps_path=os.path.join(\"/content/drive/MyDrive\",\"hyperparameter_tuning_projet_long\")\n",
        "os.path.exists(xps_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpTBoYuDN21J"
      },
      "source": [
        "# test hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "s6BMsccMiKY1"
      },
      "outputs": [],
      "source": [
        "# @title test tuning\n",
        "from hyperopt import hp, pyll\n",
        "datasets,net,nb_of_pos_ids=get_datasets()\n",
        "space={\n",
        "        \"max_len\":100,\n",
        "        \"nb_of_pos_ids\":nb_of_pos_ids,\n",
        "        \"batch_size\":2**hp.uniformint(\"batch_size\",4,7),\n",
        "        \"nb_batchs\":12*hp.uniformint(\"nb_batchs\",1,16),\n",
        "        \"early_stopping\":hp.uniformint(\"early_stopping\",1,10),\n",
        "        \"epochs_classifcation_only\":hp.uniformint(\"epochs_classifcation_only\",1,80),\n",
        "        \"reg_choice\":hp.choice(\"reg_choice\",\n",
        "                        [\n",
        "                            {\"reg\":True,\"epochs_complete_problem\":hp.uniformint(\"epochs_complete_problem\",0,50)},\n",
        "                            {\"reg\":False},\n",
        "                        ]),\n",
        "        \"learning_rate_scheduler_choice\": hp.choice(\n",
        "          \"learning_rate_scheduler_choice\",\n",
        "          [\n",
        "              {\n",
        "                  \"scheduler\": \"StepLR\",\n",
        "                  \"step_size\": hp.uniformint(\"step_size\", 1, 30),\n",
        "                  \"gamma\": hp.uniform(\"gamma_slr\", 0, 0.99),\n",
        "              },\n",
        "              {\n",
        "                  \"scheduler\": \"ReduceLROnPlateau\",\n",
        "                  \"factor\": hp.uniform(\"factor\", 0, 0.9),\n",
        "                  \"patience\": hp.uniformint(\"patience\", 1, 10),\n",
        "                  \"threshold\": hp.loguniform(\"threshold\",-12,-1),\n",
        "                  \"cooldown\":hp.uniformint(\"cooldown\",0,10)\n",
        "              },\n",
        "              {\n",
        "                  \"scheduler\":\"ExponentialLR\",\n",
        "                  \"gamma\":hp.uniform(\"gamma_elr\", 0, 0.9),\n",
        "              },\n",
        "            {\"scheduler\": None}  # No scheduler\n",
        "        ]\n",
        "    ),\n",
        "      \"optimizer\": hp.choice(\"optimizer\",[\"Adam\",\"AdamW\"]), \"lr\": hp.loguniform(\"lr\", -17, -2),\"beta_1\":hp.uniform(\"beta_1\", 0.8, 1), \"beta_2\" : hp.uniform(\"beta_2\", 0.95, 1),\"eps\": hp.loguniform(\"eps_adam\", -20, -12),\"weight_decay\":hp.loguniform(\"weight_decay_adam\",-20,-1),\"amsgrad\":hp.choice(\"amsgrad\",[True,False]),\n",
        "      \"input_size\":2,\n",
        "      \"d_model\":24*hp.uniformint(\"d_model\",1,60),\n",
        "      \"dropout\":hp.uniform(\"dropout\",0,1),\n",
        "      \"normalize_features_independantly\":hp.choice(\"normalize_features_independantly\",[True,False]),\n",
        "      \"normalize_features_globally\":hp.choice(\"normalize_features_globally\",[True,False]),\n",
        "      \"concatenate_features\":hp.choice(\"concatenate_features\",[True,False]),\n",
        "      \"use_gcn_choice\":hp.choice(\"use_gcn_choice\",\n",
        "                        [\n",
        "                            {\"use_gcn\":True,\n",
        "                             \"layer_type\":hp.choice(\"layer_type\",[\"GCNConv\",\"GraphSAGE\",\"GAT\"]),\n",
        "                             \"num_layers_gcn\":hp.uniformint(\"num_layers_gcn\",1,10),\n",
        "                             \"activation_gcn\": hp.choice(\"activation_gcn\",\n",
        "                              ['swish', 'ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']\n",
        "                             ),\n",
        "                             \"norm\": hp.choice(\"norm\",\n",
        "                                               ['BatchNorm', 'GraphNorm', 'LayerNorm', 'PairNorm', 'InstanceNorm']\n",
        "                             ),\n",
        "                             \"dropout_gcn\":hp.uniform(\"dropout_gcn\",0,1),\n",
        "                             \"hidden_channels\":2**hp.uniformint(\"hidden_channels\",6,11)\n",
        "                             },\n",
        "\n",
        "                        ]),\n",
        "      \"activation\": hp.choice(\n",
        "                \"activation\",\n",
        "                 ['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "      \"positive_function\":hp.choice(\"positive_function\",[\"relu\",\"exp\",\"abs\",\"sig\"]),\n",
        "      \"transformers_model\":True,\n",
        "      \"num_layers_transformer\":hp.uniformint(\"num_layers_transformer\",1,6),\n",
        "      \"encoder_only\":hp.choice(\"encoder_only\",[True,False]),\n",
        "      \"num_heads\":3*2**hp.uniformint('num_heads', 0, 3),\n",
        "      \"learnable_pos_encoding\": hp.choice(\"learnable_pos_encoding\",[True,False]),\n",
        "      \"activation_transformers\": hp.choice(\"activation_transformers\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "      \"dropout_transformers\":hp.uniform(\"dropout_transformers\",0,1),\n",
        "\n",
        "      \"lstm_model_choice\": hp.choice(\"lstm_model_choice\",\n",
        "                                     [{\"lstm_model\":True,\n",
        "                                       \"num_layers_lstm\":hp.uniformint(\"num_layers_lstm\",1,6),\n",
        "                                       \"lstm_layer_with_perceptron\":\n",
        "                                        hp.choice(\"lstm_layer_with_perceptron\",\n",
        "                                         [{\"lstm_layer_with_perceptron\":True,\n",
        "                                           \"activation_lstm\":hp.choice(\"activation_lstm\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),},\n",
        "                                          {\"lstm_layer_with_perceptron\":False,\n",
        "                                           \"activation_lstm\":None}]),\n",
        "                                       \"lstm_layer_with_layer_norm\":hp.choice(\"lstm_layer_with_layer_norm\",[True,False]),\n",
        "                                       \"dropout_lstm\":hp.uniform(\"dropout_lstm\",0,1),\n",
        "                                       },\n",
        "                                      {\"lstm_model\":False}])}\n",
        "config=pyll.stochastic.sample(space)\n",
        "#print(config)\n",
        "eval_config(config,data=datasets,net=net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEF-Ae5AHtQB"
      },
      "outputs": [],
      "source": [
        "run_xp('test',\"/content/test\",None,num_samples=1, max_num_epochs=1, gpus_per_trial=1, test=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi533mpake8q",
        "outputId": "42c46abd-8038-451d-aa64-9fee518d5f14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/convert.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  data_dict[key] = torch.as_tensor(value)\n",
            "2024-03-09 13:57:14,115\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-09 13:57:28,135\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-09 13:57:28,138\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_5        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_5\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_5`\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.46247543101723976 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'SELU', 'amsgrad': False, 'batch_size': 32, 'beta_1': 0.8728750427634284, 'beta_2': 0.965288122361141, 'concatenate_features': False, 'd_model': 1368, 'dropout': 0.40879157761923046, 'dropout_transformers': 0.37161678946049426, 'early_stopping': 4, 'encoder_only': False, 'epochs_classifcation_only': 37, 'eps': 1.3094611765781139e-08, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.5461151587053534, 'scheduler': 'ExponentialLR', 'lr': 0.0020473633850835505, 'dropout_lstm': 0.46247543101723976, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 4, 'optimizer': 'Adam', 'positive_function': 'exp', 'epochs_complete_problem': 50, 'reg': True, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 0.0013614008723158906}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  7.415018125013872 acc:  0.04164526717727709\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  7.096808163436143 acc:  0.07489449121318358\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  2 loss :  6.66127067179113 acc:  0.10892526181810062\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  3 loss :  5.992337863762062 acc:  0.146573476542438\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  4 loss :  5.6109960229246765 acc:  0.16300828439363152\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  5 loss :  5.3939768351041355 acc:  0.17665185449835874\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  6 loss :  5.331494591452858 acc:  0.18017997900989213\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  7 loss :  5.176534277575833 acc:  0.18761583636647836\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  8 loss :  5.12933246239082 acc:  0.18877699126900832\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  9 loss :  5.143617049797432 acc:  0.18960319764196235\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  10 loss :  5.159988379978634 acc:  0.19025076479914252\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  11 loss :  5.142213941454054 acc:  0.19025076479914252\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  12 loss :  5.116828326578741 acc:  0.19063037313266196\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  13 loss :  5.093815686819437 acc:  0.19089833195632272\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  14 loss :  5.132269755943672 acc:  0.1909653216622379\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  15 loss :  5.112632204602648 acc:  0.19100998146618137\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  16 loss :  5.124445608445814 acc:  0.1909653216622379\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  17 loss :  5.1188762087922 acc:  0.19116629077998348\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  18 loss :  5.086978805648697 acc:  0.19116629077998348\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  19 loss :  5.106347427501545 acc:  0.19116629077998348\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  20 loss :  5.123482384048142 acc:  0.19114396087801175\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  21 loss :  5.10939780815498 acc:  0.19114396087801175\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5198357326882407 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'RReLU', 'amsgrad': False, 'batch_size': 64, 'beta_1': 0.9057781597879149, 'beta_2': 0.9555844326490242, 'concatenate_features': True, 'd_model': 864, 'dropout': 0.051593971615826295, 'dropout_transformers': 0.6156827537016912, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 62, 'eps': 2.6980125209685442e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 4.72038420755169e-08, 'dropout_lstm': 0.5198357326882407, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'abs', 'epochs_complete_problem': 31, 'reg': True, 'transformers_model': True, 'activation_gcn': 'swish', 'dropout_gcn': 0.15577692145400945, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 3, 'use_gcn': True, 'weight_decay': 1.534726701614865e-06}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  8.192181621689395 acc:  0.0002009691177455731\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  8.193418307476733 acc:  0.0002009691177455731\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  2 loss :  8.191039613930576 acc:  0.0002009691177455731\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  3 loss :  8.193282552512295 acc:  0.0002009691177455731\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'LogSigmoid', 'amsgrad': False, 'batch_size': 128, 'beta_1': 0.9790034645252312, 'beta_2': 0.9967038455132945, 'concatenate_features': True, 'd_model': 1128, 'dropout': 0.5332953237325695, 'dropout_transformers': 0.05476487720053535, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 29, 'eps': 8.051484865142137e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 1.813716104539348e-07, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'sig', 'epochs_complete_problem': 23, 'reg': True, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 6.259890361875171e-09}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  8.091820369256991 acc:  0.0006698970591519103\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  8.056303594714013 acc:  0.0005805774512649889\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  2 loss :  8.023629184081175 acc:  0.0005805774512649889\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  3 loss :  7.99557915803428 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  4 loss :  7.97390127627649 acc:  0.0014067838242190116\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  5 loss :  7.952369360166175 acc:  0.00194270147154054\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  6 loss :  7.931608342678747 acc:  0.0021883303932295735\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  7 loss :  7.916435580387294 acc:  0.002210660295201304\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  8 loss :  7.8993192877724905 acc:  0.0023669696090034163\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  9 loss :  7.8879469978475125 acc:  0.002523278922805529\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  10 loss :  7.8707680613081035 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  11 loss :  7.859398520995524 acc:  0.0027019181385793717\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  12 loss :  7.853065290183665 acc:  0.0028135676484380232\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  13 loss :  7.840884716711312 acc:  0.0026349284326641804\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  14 loss :  7.832075377491033 acc:  0.002523278922805529\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  15 loss :  7.818028614899823 acc:  0.0028358975504097538\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  16 loss :  7.80788787948751 acc:  0.003304825491816091\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  17 loss :  7.802924352271535 acc:  0.0029028872563249446\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  18 loss :  7.791774264005857 acc:  0.003371815197731282\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  19 loss :  7.7858241919045135 acc:  0.003818413237165889\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  20 loss :  7.781038052567812 acc:  0.004711609316035103\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  21 loss :  7.770717415854196 acc:  0.005604805394904317\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  22 loss :  7.761407424356336 acc:  0.006029073532367193\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  23 loss :  7.7570874401342085 acc:  0.006297032356027957\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  24 loss :  7.749886089396254 acc:  0.00652033137574526\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  25 loss :  7.749167727532788 acc:  0.006832950003349486\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  26 loss :  7.740386664310348 acc:  0.007212558336868901\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  27 loss :  7.731190775042382 acc:  0.007636826474331778\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  28 loss :  7.725042089123592 acc:  0.007860125494049082\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  29 loss :  8.871489694185346 acc:  0.004778599021950294\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  30 loss :  8.767898835868479 acc:  0.0045106401982895295\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  31 loss :  8.75207917044096 acc:  0.004979568139695867\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  32 loss :  8.754366259708583 acc:  0.005761114708706429\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  33 loss :  8.735021760530561 acc:  0.007190228434897171\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  34 loss :  8.72508555260774 acc:  0.007391197552642744\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'PReLU', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.803753134709503, 'beta_2': 0.9700349672732844, 'concatenate_features': False, 'd_model': 600, 'dropout': 0.5740035658667672, 'dropout_transformers': 0.10936065465322137, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 24, 'eps': 7.761411597981853e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 9, 'factor': 0.038940420740192494, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.00016564449579081013, 'lr': 0.005879314357136417, 'dropout_lstm': 0.045891598680307955, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 18, 'reg': True, 'transformers_model': True, 'activation_gcn': 'CELU', 'dropout_gcn': 0.0024982286739979043, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 1, 'use_gcn': True, 'weight_decay': 9.420749585781027e-06}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.045891598680307955 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  6.9797627705485885 acc:  0.19127794028984213\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  4.697954185870516 acc:  0.26246566777571845\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  2 loss :  3.7207771329318775 acc:  0.3034187079918719\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  3 loss :  3.3367452561354436 acc:  0.3356630864390505\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  4 loss :  3.1482975603151724 acc:  0.3513163477212335\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  5 loss :  2.981539359613627 acc:  0.3691802692986178\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  6 loss :  2.8933913627592456 acc:  0.3745171158698613\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  7 loss :  2.828906093324934 acc:  0.3785588281267445\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  8 loss :  2.7281493519534585 acc:  0.3875801085233236\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  9 loss :  2.6791216205148136 acc:  0.39351986244780385\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  10 loss :  2.619414808369484 acc:  0.404104235982404\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  11 loss :  2.553488645233026 acc:  0.40955273206350623\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  12 loss :  2.541455451179953 acc:  0.4119643614764531\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  13 loss :  2.5430109641131233 acc:  0.41377308353616327\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  14 loss :  2.5170514283060026 acc:  0.4154478261840431\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  15 loss :  2.5022484595034302 acc:  0.4166313109885448\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  16 loss :  2.502628662005192 acc:  0.41672063059643166\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  17 loss :  2.5161908864974976 acc:  0.41692159971417725\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  18 loss :  2.4718618092416715 acc:  0.4172118884398098\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  19 loss :  2.4856171347513922 acc:  0.4187749815778309\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  20 loss :  2.4854220081778133 acc:  0.4182167340285376\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  21 loss :  2.473018503990494 acc:  0.4191099301074068\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  22 loss :  2.4740039921608292 acc:  0.4190876002054351\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  23 loss :  2.4638812141258177 acc:  0.41964584775472835\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  24 loss :  3.90062672350587 acc:  0.4179264453029051\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  25 loss :  3.6022257704694733 acc:  0.41828372373445283\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  26 loss :  3.5415269326762995 acc:  0.41977982716655876\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  27 loss :  3.4919857037167588 acc:  0.42038273451979546\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  28 loss :  3.5224321689926277 acc:  0.42038273451979546\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  29 loss :  3.537894629630722 acc:  0.42040506442176717\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  30 loss :  3.540876106053841 acc:  0.4204943840296541\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  31 loss :  3.5200757639748708 acc:  0.4205613737355693\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  32 loss :  3.520726017591332 acc:  0.42040506442176717\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  33 loss :  3.5094720676165667 acc:  0.4204943840296541\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  34 loss :  3.5520230882308064 acc:  0.4205390438335976\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  35 loss :  3.5484432593113233 acc:  0.4206953531473997\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  36 loss :  3.543736299546827 acc:  0.42042739432373893\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  37 loss :  3.5308167153045913 acc:  0.4204720541276824\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  38 loss :  3.517421542095537 acc:  0.4205390438335976\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  39 loss :  3.533632983680533 acc:  0.42060603353951276\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  40 loss :  3.537772479177523 acc:  0.4207846727552866\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  41 loss :  3.5491327538209805 acc:  0.4208516624612018\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7206376162936046 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'PReLU', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8053139308288522, 'beta_2': 0.969610729773332, 'concatenate_features': False, 'd_model': 600, 'dropout': 0.9238093199449369, 'dropout_transformers': 0.5398274661428972, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 23, 'eps': 1.5461652706781504e-07, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 10, 'factor': 0.02088992191667773, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 8.060800312865181e-05, 'lr': 0.006806753867751586, 'dropout_lstm': 0.7206376162936046, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 2, 'optimizer': 'AdamW', 'positive_function': 'relu', 'epochs_complete_problem': 19, 'reg': True, 'transformers_model': True, 'activation_gcn': 'CELU', 'dropout_gcn': 0.007210621180824495, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 2, 'use_gcn': True, 'weight_decay': 2.5518317051814997e-05}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.048091886466427536 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'Hardswish', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8179324852579937, 'beta_2': 0.9832369401255537, 'concatenate_features': False, 'd_model': 288, 'dropout': 0.5813302649523124, 'dropout_transformers': 0.3990816788267606, 'early_stopping': 9, 'encoder_only': True, 'epochs_classifcation_only': 1, 'eps': 7.155010227539332e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 7, 'factor': 0.23906201742934824, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0049002611125559675, 'lr': 0.08249338289470098, 'dropout_lstm': 0.048091886466427536, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'GELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'Adam', 'positive_function': 'exp', 'epochs_complete_problem': 15, 'reg': True, 'transformers_model': True, 'activation_gcn': 'CELU', 'dropout_gcn': 0.4046800519034898, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 1, 'use_gcn': True, 'weight_decay': 0.0003405221376282976}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  7.960422287468149 acc:  0.0019873612754840006\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  8.533977700882598 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'PReLU', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8446473810527482, 'beta_2': 0.9741753618727778, 'concatenate_features': False, 'd_model': 480, 'dropout': 0.7947710255633229, 'dropout_transformers': 0.7216772463400571, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 18, 'eps': 4.778492536366715e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 8, 'factor': 0.37605151472800774, 'patience': 4, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.15729156089863433, 'lr': 0.0038779971059416728, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 2, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 7, 'reg': True, 'transformers_model': True, 'activation_gcn': 'SELU', 'dropout_gcn': 0.6446800777025647, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 4, 'use_gcn': True, 'weight_decay': 0.007993528814661316}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  7.482215670899972 acc:  0.0019873612754840006\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  7.33647855833256 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  2 loss :  7.30603225803908 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  3 loss :  7.287974653297296 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  4 loss :  7.27440433129252 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'PReLU', 'amsgrad': True, 'batch_size': 32, 'beta_1': 0.8375169190298005, 'beta_2': 0.9763659754438553, 'concatenate_features': False, 'd_model': 408, 'dropout': 0.6814240957492249, 'dropout_transformers': 0.29414540019012037, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 6, 'eps': 9.797598981335941e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 5, 'factor': 0.0951340505630236, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.00015496625003538127, 'lr': 0.012951836440407576, 'dropout_lstm': 0.38057489776597064, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 21, 'reg': True, 'transformers_model': True, 'activation_gcn': 'CELU', 'dropout_gcn': 0.2608984512800344, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 1, 'use_gcn': True, 'weight_decay': 8.97585103426501e-06}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.38057489776597064 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  7.667448244596782 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  7.380321231641267 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  2 loss :  7.315445699189839 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  3 loss :  7.282264137268067 acc:  0.005091217649554518\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  4 loss :  7.294727360574822 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  5 loss :  7.292171719199732 acc:  0.004867918629837215\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  6 loss :  8.31815058055677 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6353762589153101 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'Mish', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8846159889860212, 'beta_2': 0.9805355859538156, 'concatenate_features': False, 'd_model': 528, 'dropout': 0.7300959532336099, 'dropout_transformers': 0.47432514278281057, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 24, 'eps': 2.2780551592803755e-07, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 9, 'factor': 0.6902649215359907, 'patience': 5, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0022186397223413353, 'lr': 0.021356626708423938, 'dropout_lstm': 0.6353762589153101, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'abs', 'epochs_complete_problem': 15, 'reg': True, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.7281239733729508, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 5, 'use_gcn': True, 'weight_decay': 9.91468248756413e-05}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30885763417285333 and num_layers=1\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=2519)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'Sigmoid', 'amsgrad': True, 'batch_size': 16, 'beta_1': 0.8652976741711972, 'beta_2': 0.9886945602606051, 'concatenate_features': False, 'd_model': 816, 'dropout': 0.6287264835726325, 'dropout_transformers': 0.24715402584619361, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 14, 'eps': 3.577958636737483e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 6, 'factor': 0.3917293678882966, 'patience': 6, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.035226569379585966, 'lr': 0.000767325732562608, 'dropout_lstm': 0.30885763417285333, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 1, 'optimizer': 'AdamW', 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'LeakyReLU', 'dropout_gcn': 0.460205022525414, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'InstanceNorm', 'num_layers_gcn': 2, 'use_gcn': True, 'weight_decay': 3.7729656727512806e-06}\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  0 loss :  8.008416732665031 acc:  0.0014514436281624723\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  1 loss :  7.6213506514026275 acc:  0.01563093138021124\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  2 loss :  7.365594922342608 acc:  0.04655784561105777\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  3 loss :  7.0249152583460654 acc:  0.06355090101154456\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  4 loss :  6.599608495158534 acc:  0.09298171181028515\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  5 loss :  6.254704072398524 acc:  0.11736596476341468\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  6 loss :  5.867498453201786 acc:  0.12527075006140723\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  7 loss :  5.568299353507257 acc:  0.15331710693790054\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  8 loss :  5.142094178353586 acc:  0.19313132215349574\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  9 loss :  4.926036663978331 acc:  0.21304959471227922\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  10 loss :  4.798759452758297 acc:  0.21505928588973494\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  11 loss :  4.782657677127469 acc:  0.22832324766094278\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  12 loss :  4.597158525836083 acc:  0.23417368197753613\n",
            "\u001b[36m(eval_config pid=2519)\u001b[0m epoch:  13 loss :  4.503592637277419 acc:  0.23881830158765602\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-09 15:46:18,015\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-09 15:46:32,697\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-09 15:46:32,699\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_6        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_6\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_6`\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.17017608834550169 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'Hardshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.893403547407745, 'beta_2': 0.9715997195900961, 'concatenate_features': False, 'd_model': 696, 'dropout': 0.46769522308351585, 'dropout_transformers': 0.12331769655235181, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 29, 'eps': 5.1985446158011364e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 2, 'factor': 0.1431795988450012, 'patience': 2, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.00019876318312865901, 'lr': 0.000185746055057753, 'dropout_lstm': 0.17017608834550169, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 36, 'reg': True, 'transformers_model': True, 'activation_gcn': 'SELU', 'dropout_gcn': 0.5571597744888269, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 3, 'use_gcn': True, 'weight_decay': 3.158118901337592e-07}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.552622846810214 acc:  0.014067838242190116\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  7.1422723115208635 acc:  0.021883303932295737\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  6.951839533196875 acc:  0.04329767992318514\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  6.739498534834529 acc:  0.06238974610901458\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  6.530434407383563 acc:  0.08217403925596767\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  6.399307865694345 acc:  0.09342830984971975\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  6.338349951318948 acc:  0.1046379206395284\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  6.279213870864317 acc:  0.1101534064265458\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  8 loss :  6.233106567198972 acc:  0.12013487260790925\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  9 loss :  6.160352798829596 acc:  0.12091641917691981\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  10 loss :  6.133659161717059 acc:  0.12290378045240381\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  11 loss :  6.1580337731235 acc:  0.12457852310028358\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  12 loss :  6.191898845764528 acc:  0.12562802849295493\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  13 loss :  6.135443687438965 acc:  0.12696782261125875\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  14 loss :  6.127496380403818 acc:  0.12703481231717392\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  15 loss :  6.14796343194433 acc:  0.12743675055266507\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  16 loss :  6.126106583928487 acc:  0.127526070160552\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  17 loss :  6.136153600302087 acc:  0.12710180202308913\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  18 loss :  6.088438137468085 acc:  0.12734743094477816\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  19 loss :  6.117803194436682 acc:  0.1272357814349195\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  20 loss :  6.118855700435408 acc:  0.12725811133689124\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8120449929172127, 'beta_2': 0.9655642392631544, 'concatenate_features': False, 'd_model': 288, 'dropout': 0.5566061310872727, 'dropout_transformers': 0.10485488664086651, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 19, 'eps': 1.9197422298869585e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.005359048951003346, 'dropout_lstm': 0.04436989663239278, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 32, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.7055269764830446, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 4, 'use_gcn': True, 'weight_decay': 3.52366666891346e-05}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.04436989663239278 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.200717629504805 acc:  0.03050264609338365\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  6.263003746000659 acc:  0.160730634392515\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  5.178085447359486 acc:  0.25561038787039725\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  4.18637469636292 acc:  0.3032623986780698\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  3.6847652827992157 acc:  0.33847665408748856\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  3.3820964468627417 acc:  0.3565638746845901\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  3.204468771189201 acc:  0.37165888841747985\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  3.0619219791989365 acc:  0.37420449724225713\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  8 loss :  2.9904850791482365 acc:  0.38822767568050376\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  9 loss :  2.8919438975197926 acc:  0.39432373891878614\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  10 loss :  2.8085032270736052 acc:  0.39680235803764824\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  11 loss :  2.775271155253178 acc:  0.40057611147087063\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  12 loss :  2.7325045441379068 acc:  0.4052877207869057\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  13 loss :  2.6905224243132007 acc:  0.4090614742201282\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  14 loss :  2.6771284532146296 acc:  0.4088381752004109\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  15 loss :  2.660291898150404 acc:  0.41475559922291944\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  16 loss :  2.6047897849764143 acc:  0.41341580510461556\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  17 loss :  2.554217348579599 acc:  0.4164303418707992\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  18 loss :  2.569446333316194 acc:  0.41317017618292656\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  19 loss :  3.6169586682519994 acc:  0.4158274345175625\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  20 loss :  3.5259788517190627 acc:  0.41884197128374606\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  21 loss :  3.519286606492115 acc:  0.4220351472657035\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  22 loss :  3.5198757047412776 acc:  0.42221378648147734\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  23 loss :  3.4370621052108894 acc:  0.4242011477569613\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  24 loss :  3.439676451081989 acc:  0.42199048746176004\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  25 loss :  3.393745590658749 acc:  0.4213652502065516\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  26 loss :  3.402018136337024 acc:  0.4235759105017529\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  27 loss :  3.4123235109473478 acc:  0.42058370363754105\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  28 loss :  3.412761093187733 acc:  0.42670209677779514\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  29 loss :  3.371572077775202 acc:  0.4257865707969542\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  30 loss :  3.361916437870314 acc:  0.4238885291293571\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  31 loss :  3.3625495533983245 acc:  0.4261885090324454\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  32 loss :  3.3570349056179785 acc:  0.42781859187638166\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  33 loss :  3.32825618230996 acc:  0.4275506330527209\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  34 loss :  3.3384103514567145 acc:  0.4279972310921555\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  35 loss :  3.2901293710500252 acc:  0.4269030658955407\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  36 loss :  3.2885067603167366 acc:  0.42964964383806353\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  37 loss :  3.276138249565573 acc:  0.4274836433468057\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  38 loss :  3.3118240773176946 acc:  0.4256302614831521\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  39 loss :  3.29152969753041 acc:  0.43031954089721547\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  40 loss :  3.2703782109653248 acc:  0.428868097269053\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  41 loss :  3.2879616633182813 acc:  0.42971663354397877\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  42 loss :  3.2638047042013216 acc:  0.4271486948172298\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  43 loss :  3.2837789178896353 acc:  0.4288904271710247\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  44 loss :  3.258128645039406 acc:  0.427706942366523\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  45 loss :  3.2345914760557544 acc:  0.4303418707991872\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  46 loss :  3.2027951909714387 acc:  0.42822053011187283\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  47 loss :  3.2136346332165373 acc:  0.4279972310921555\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  48 loss :  3.213640555614183 acc:  0.4320389433490387\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  49 loss :  3.1957101621547666 acc:  0.43031954089721547\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  50 loss :  3.182140895298549 acc:  0.4298952727597526\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8093990698635409, 'beta_2': 0.9611936492649538, 'concatenate_features': False, 'd_model': 192, 'dropout': 0.5569250770268654, 'dropout_transformers': 0.03252090728120399, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 10, 'eps': 1.3618552964167525e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 9, 'factor': 0.6153840636275709, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 2.9583141123026654e-05, 'lr': 0.0062265139598756125, 'dropout_lstm': 0.06762510776537367, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 26, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.7149966422201484, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 5, 'use_gcn': True, 'weight_decay': 3.318406050458823e-05}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.06762510776537367 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.184302923562643 acc:  0.030368666681553268\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  6.526324478896348 acc:  0.07444789317374896\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  5.885284543871046 acc:  0.1596141392939285\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  4.943870160963152 acc:  0.2633811937565594\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  4.153068925950911 acc:  0.3000915525980841\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  3.704863998439762 acc:  0.3359087153607396\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  3.4287189386941335 acc:  0.35703280262599646\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  3.2631633181671997 acc:  0.3716142286135364\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  8 loss :  3.1435341234807366 acc:  0.3735122702811335\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  9 loss :  3.039418727367908 acc:  0.38467722126699866\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  10 loss :  4.058082835657613 acc:  0.3933858830359735\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  11 loss :  3.887537220974902 acc:  0.3977178840184892\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  12 loss :  3.894006614084844 acc:  0.3997945649018601\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  13 loss :  3.8309430592543596 acc:  0.4039479266686019\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  14 loss :  3.7726644669379388 acc:  0.4064488756894357\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  15 loss :  3.743395013409061 acc:  0.40928477323984547\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  16 loss :  3.7391891412801677 acc:  0.40968671147533664\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  17 loss :  3.7078574470706753 acc:  0.4088158452984391\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  18 loss :  3.675549715548962 acc:  0.4131031864770114\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  19 loss :  3.6683594463588474 acc:  0.41140611392715987\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  20 loss :  3.661140833701287 acc:  0.4171002389299511\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  21 loss :  3.615144171081223 acc:  0.4167429604984034\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  22 loss :  3.6031472066065646 acc:  0.4205613737355693\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  23 loss :  3.5905628504453007 acc:  0.4187973114798026\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  24 loss :  3.591235996126295 acc:  0.419757497264587\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  25 loss :  3.5515656788032373 acc:  0.4195788580488132\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  26 loss :  3.567835255936309 acc:  0.4205167139316258\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  27 loss :  3.5293910870185266 acc:  0.42178951834401446\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  28 loss :  3.496154791825301 acc:  0.4259205502087846\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  29 loss :  3.504670011413681 acc:  0.4265681173659648\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  30 loss :  3.46544104856211 acc:  0.4267690864837103\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  31 loss :  3.4421970710887777 acc:  0.4281312105039859\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  32 loss :  3.452301240467525 acc:  0.4287117879552509\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  33 loss :  3.42981997069779 acc:  0.42701471540539937\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  34 loss :  3.430302604928717 acc:  0.4275729629546926\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  35 loss :  3.419000192122026 acc:  0.427126364915258\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8114722556494044, 'beta_2': 0.9530059908215918, 'concatenate_features': False, 'd_model': 168, 'dropout': 0.5020539630556047, 'dropout_transformers': 0.15856159114327115, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 9, 'eps': 3.5077472172945515e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.7605947134362816, 'scheduler': 'StepLR', 'step_size': 20, 'lr': 0.001426211000769552, 'dropout_lstm': 0.2628991747539208, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 26, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.696092443275807, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 5, 'use_gcn': True, 'weight_decay': 0.10766110310737345}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2628991747539208 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.422557572372087 acc:  0.008864971082776946\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  7.038162569963295 acc:  0.012549404908112453\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  6.893608104181654 acc:  0.020253221088359422\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  6.742737861080024 acc:  0.02916285197507983\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  6.614171661493432 acc:  0.03575017305674028\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  6.460891767312552 acc:  0.04224817453051381\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  6.346326169166856 acc:  0.05158207355469709\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  6.223124169211351 acc:  0.06888774758278811\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  8 loss :  6.0782208770286035 acc:  0.07877989415626466\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  9 loss :  7.0643430520559996 acc:  0.08686331867003104\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  10 loss :  6.803029056723791 acc:  0.11057767456400866\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  11 loss :  6.673976515995637 acc:  0.1325279682022196\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  12 loss :  6.422157698915205 acc:  0.1520443025255119\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  13 loss :  6.2768859572082984 acc:  0.16546457361052186\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  14 loss :  6.147226424617622 acc:  0.18111783489270483\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  15 loss :  5.964809071926671 acc:  0.19882544715628697\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  16 loss :  5.836829189125818 acc:  0.20829332559230065\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  17 loss :  5.629235253079247 acc:  0.22425920550208783\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  18 loss :  5.542598982803694 acc:  0.23919790992117546\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  19 loss :  5.420931801541161 acc:  0.24384252953129537\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  20 loss :  5.215975022497978 acc:  0.26556952414978896\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  21 loss :  5.2022187728008245 acc:  0.27077239130920217\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  22 loss :  5.097285288890809 acc:  0.2782975682736753\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  23 loss :  5.077117035407146 acc:  0.28057521827479176\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  24 loss :  4.970431517098696 acc:  0.28990911729897506\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  25 loss :  4.9426427761107 acc:  0.29756827367527855\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  26 loss :  4.896536635988541 acc:  0.30277114083469175\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  27 loss :  4.835515402655565 acc:  0.29989058348033854\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  28 loss :  4.812456407619797 acc:  0.30868856485720025\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  29 loss :  4.737722617069274 acc:  0.30969341044592813\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  30 loss :  4.714463374086919 acc:  0.3157001540763236\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  31 loss :  4.656739749981247 acc:  0.31666033986110803\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  32 loss :  4.665584769867759 acc:  0.3241631869236094\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  33 loss :  4.625413072018223 acc:  0.32139427907911483\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  34 loss :  4.552461542245996 acc:  0.3268651050621888\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3543835904083893 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8331802847674417, 'beta_2': 0.9593731930811658, 'concatenate_features': False, 'd_model': 24, 'dropout': 0.5473929688370185, 'dropout_transformers': 0.19801823847430436, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 5, 'eps': 1.3047399262970825e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.028309372001014695, 'dropout_lstm': 0.3543835904083893, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 34, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.6316944658095947, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 4, 'use_gcn': True, 'weight_decay': 3.5788797151176953e-05}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.4022348077147155 acc:  0.005604805394904317\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  7.175433028828014 acc:  0.008463032847285801\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  7.082718622434389 acc:  0.011968827456847464\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  7.0199196722124 acc:  0.013933858830359734\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  6.976464031459568 acc:  0.013286291673179554\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  8.034816471846787 acc:  0.00911060000446598\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  7.917228105184916 acc:  0.00975816716164616\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  7.883980104139635 acc:  0.01560860147823951\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  8 loss :  7.8453012979947605 acc:  0.015563941674296049\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  9 loss :  7.7824735474753215 acc:  0.018288189714847154\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  10 loss :  7.805755011685244 acc:  0.021950293638210928\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  11 loss :  7.733213861505468 acc:  0.017060045106401984\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  12 loss :  7.696498874184135 acc:  0.026259964718754886\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  13 loss :  7.6686510472864535 acc:  0.03103856374070518\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  14 loss :  7.612472574193995 acc:  0.028024026974521582\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  15 loss :  7.579689122580148 acc:  0.030413326485496727\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  16 loss :  7.56178213666369 acc:  0.02351338677623205\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  17 loss :  7.463307987559926 acc:  0.03371815197731282\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  18 loss :  7.441792184656316 acc:  0.0345890181542103\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  19 loss :  7.597425037330681 acc:  0.03108322354464864\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  20 loss :  7.480159249339071 acc:  0.039992854431369046\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  21 loss :  7.3347281275929275 acc:  0.04385592747247839\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  22 loss :  7.272470157463234 acc:  0.04262778286403323\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  23 loss :  7.241487793155484 acc:  0.05265390884934015\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  24 loss :  7.136744839328152 acc:  0.057477167675233906\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  25 loss :  7.0897460550695035 acc:  0.061675189245919214\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  26 loss :  7.000803247198358 acc:  0.06238974610901458\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  27 loss :  6.981039047241211 acc:  0.06308197307013823\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  28 loss :  6.9428226430932956 acc:  0.07069646964249827\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  29 loss :  6.85585759569715 acc:  0.06567224169885894\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  30 loss :  6.8022654406674254 acc:  0.07125471719179152\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  31 loss :  6.739993408843354 acc:  0.08458566866891454\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  32 loss :  6.678983534966315 acc:  0.09070406180916865\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  33 loss :  6.671391243701215 acc:  0.08487595739454704\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  34 loss :  6.600740929583569 acc:  0.09521470200745819\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  35 loss :  6.5587921576066455 acc:  0.09688944465533797\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  36 loss :  6.481170460894392 acc:  0.10742915838599469\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  37 loss :  6.473002237039847 acc:  0.1028738583837617\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  38 loss :  6.47598166565795 acc:  0.10466025054150012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6369939979977044 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'ELU', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8504582474141027, 'beta_2': 0.9658551517181233, 'concatenate_features': False, 'd_model': 288, 'dropout': 0.34090196705138304, 'dropout_transformers': 0.03368542809139699, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 19, 'eps': 1.800481163662076e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.14584874740150017, 'scheduler': 'ExponentialLR', 'lr': 0.10831182197802239, 'dropout_lstm': 0.6369939979977044, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 31, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.7252779817803956, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 0.00014407719991872187}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.915060483255694 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  7.337747589234383 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  7.256541852028139 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  7.241296146761987 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  7.255868807146626 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  7.232042798688335 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  7.2327729194395 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  7.244648985708913 acc:  0.003438804903646473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.09228190214194004 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8257395196294761, 'beta_2': 0.9574942586502674, 'concatenate_features': False, 'd_model': 72, 'dropout': 0.40203427293609584, 'dropout_transformers': 0.8357377312447077, 'early_stopping': 9, 'encoder_only': True, 'epochs_classifcation_only': 11, 'eps': 1.6086054045291028e-06, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.000622599636700109, 'dropout_lstm': 0.09228190214194004, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 36, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanhshrink', 'dropout_gcn': 0.8179529531714029, 'hidden_channels': 256, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 5, 'use_gcn': True, 'weight_decay': 0.0004001106400817938}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.721367136447015 acc:  0.004443650492374339\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  7.285229797134856 acc:  0.006743630395462564\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  7.223013560928984 acc:  0.006855279905321215\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  7.16117183605354 acc:  0.017149364714288903\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  7.057290531204132 acc:  0.02208427305004131\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  6.937989206371193 acc:  0.029721099524373087\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  6.783761192938525 acc:  0.0388540294308108\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  6.635493978054938 acc:  0.049617042181184824\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  8 loss :  6.4717061276921255 acc:  0.05571310541946721\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  9 loss :  6.3156539494405965 acc:  0.07074112944644173\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  10 loss :  6.160770496208511 acc:  0.08621575151285085\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  11 loss :  7.204116935501555 acc:  0.09557198043900587\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  12 loss :  6.960275687143474 acc:  0.10407967309023514\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  13 loss :  6.806290777857432 acc:  0.11627179956679991\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  14 loss :  6.673731578324369 acc:  0.12607462653238952\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  15 loss :  6.537559289418295 acc:  0.14230846526583749\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  16 loss :  6.4885862744496965 acc:  0.14293370252104592\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  17 loss :  6.318106811203642 acc:  0.15863162360717237\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  18 loss :  6.233369641675207 acc:  0.16707232655248644\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  19 loss :  6.164826067621837 acc:  0.17700913292990644\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  20 loss :  6.061972104146809 acc:  0.18486925842395552\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  21 loss :  5.976390912861167 acc:  0.19049639372083157\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  22 loss :  5.909094228002125 acc:  0.20159435500078154\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  23 loss :  5.788161371996303 acc:  0.20355938637429383\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  24 loss :  5.729559590002734 acc:  0.21271464618270325\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  25 loss :  5.649929315030218 acc:  0.21916798785253333\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  26 loss :  5.614962109548603 acc:  0.2234106692271621\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  27 loss :  5.55078865668017 acc:  0.2298193510930487\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  28 loss :  5.4844874279227795 acc:  0.23455329031105554\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  29 loss :  5.439162388533175 acc:  0.23868432217582564\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  30 loss :  5.3743724880104295 acc:  0.24737065404282876\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  31 loss :  5.361954783251186 acc:  0.24757162316057432\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  32 loss :  5.316050752194342 acc:  0.2529754594377331\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  33 loss :  5.245820088300876 acc:  0.2597637496371391\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  34 loss :  5.234726914388691 acc:  0.26494428689458055\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  35 loss :  5.191242629182553 acc:  0.26635107071879954\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  36 loss :  5.187462110005453 acc:  0.273518969251725\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  37 loss :  5.106059417039335 acc:  0.2761762275863609\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  38 loss :  5.057270218512255 acc:  0.28173637317732175\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  39 loss :  5.0134132079735485 acc:  0.2843936315119577\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  40 loss :  5.0124887106661316 acc:  0.28836835406292566\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  41 loss :  4.994088763962249 acc:  0.28879262220038854\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  42 loss :  4.928867497130068 acc:  0.2945314070071232\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  43 loss :  4.893677645814633 acc:  0.2960498403412009\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  44 loss :  4.8812936480173805 acc:  0.30085076926512294\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  45 loss :  4.83745100969326 acc:  0.3012080476966706\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  46 loss :  4.856015011222063 acc:  0.30455753299243016\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8635551557819617, 'beta_2': 0.962310825370523, 'concatenate_features': False, 'd_model': 240, 'dropout': 0.8346116241358754, 'dropout_transformers': 0.01626758773258878, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 4, 'eps': 5.887120154364984e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 4, 'factor': 0.6504374050798645, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 3.348010901634151e-05, 'lr': 0.06204435875961056, 'dropout_lstm': 0.49458340224705044, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 29, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.5434564124036269, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'LayerNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 0.006665088873013913}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.49458340224705044 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  0 loss :  7.792528247833252 acc:  0.0056717951008195076\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  1 loss :  7.31061341212346 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  2 loss :  7.282583673183734 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  3 loss :  7.279660415649414 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  4 loss :  8.377780888630793 acc:  0.004220351472657035\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  5 loss :  8.297090556071355 acc:  0.004220351472657035\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  6 loss :  8.288591036429771 acc:  0.004220351472657035\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  7 loss :  8.286593198776245 acc:  0.004376660786459147\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  8 loss :  8.278572042171772 acc:  0.00455530000223299\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m epoch:  9 loss :  8.25870996255141 acc:  0.005738784806734698\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.09783773885038005 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8380576539671105, 'beta_2': 0.9552172011103627, 'concatenate_features': False, 'd_model': 360, 'dropout': 0.6892255406997925, 'dropout_transformers': 0.3425357666899207, 'early_stopping': 10, 'encoder_only': True, 'epochs_classifcation_only': 16, 'eps': 3.61595761545857e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.001828880336562395, 'dropout_lstm': 0.09783773885038005, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 44, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Softplus', 'dropout_gcn': 0.6688193945714589, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 4, 'use_gcn': True, 'weight_decay': 0.001444537878838157}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m loss is undifined\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.0006672822739215017 and num_layers=1\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Tanhshrink', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8154204433779334, 'beta_2': 0.9641096453801371, 'concatenate_features': False, 'd_model': 96, 'dropout': 0.6408253603588256, 'dropout_transformers': 0.2114062526656546, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 1, 'eps': 5.991296228994807e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.008424354482286935, 'dropout_lstm': 0.0006672822739215017, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 21, 'reg': True, 'transformers_model': True, 'activation_gcn': 'PReLU', 'dropout_gcn': 0.8717590175745938, 'hidden_channels': 256, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 1.677578434590601e-05}\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=29853)\u001b[0m loss is undifined\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-09 17:32:15,509\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-09 17:32:29,584\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-09 17:32:29,585\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_7        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_7\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_7`\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.432933107897778 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'PReLU', 'activation_transformers': 'Softplus', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8070366759880219, 'beta_2': 0.9614006732577878, 'concatenate_features': False, 'd_model': 168, 'dropout': 0.24268319231403462, 'dropout_transformers': 0.2685537891335524, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 7, 'eps': 9.733195628921873e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.2935348638634072, 'scheduler': 'StepLR', 'step_size': 7, 'lr': 0.0037557148861220167, 'dropout_lstm': 0.432933107897778, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'relu', 'epochs_complete_problem': 32, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Softsign', 'dropout_gcn': 0.7611313712612684, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'InstanceNorm', 'num_layers_gcn': 5, 'use_gcn': True, 'weight_decay': 5.92895131004427e-05}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  7.153942104819771 acc:  0.09539334122323204\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  5.671058748151872 acc:  0.20974476922046312\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  4.453367591737868 acc:  0.2679364937587924\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  3.8901019563208092 acc:  0.3154098653506911\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  3.5610765477160475 acc:  0.32760199182725586\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  3.359884820617996 acc:  0.34709599624857645\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  6 loss :  3.183100261888304 acc:  0.3577250295871201\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  7 loss :  3.9612402399103126 acc:  0.38999173793627046\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  8 loss :  3.7992783593131114 acc:  0.39861108009735835\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  9 loss :  3.790062435857066 acc:  0.40327802960945003\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  10 loss :  3.7161953632648173 acc:  0.4045061742178952\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  11 loss :  3.698114868644234 acc:  0.40448384431592344\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  12 loss :  3.6561114287876584 acc:  0.4079003193175982\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  13 loss :  3.6250940436249848 acc:  0.40678382421901166\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  14 loss :  3.6072963067701647 acc:  0.4139070629479937\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  15 loss :  3.5763763797866717 acc:  0.415782774713619\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  16 loss :  3.590553237008048 acc:  0.41558180559587343\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  17 loss :  3.5723258748754754 acc:  0.4148449188308063\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  18 loss :  3.57736125692621 acc:  0.41553714579192996\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  19 loss :  3.5672567207496484 acc:  0.41692159971417725\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  20 loss :  3.529044848221999 acc:  0.416608981086573\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  21 loss :  3.525178007312588 acc:  0.4174575173614988\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  22 loss :  3.574111171535679 acc:  0.4179711051068486\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  23 loss :  3.5430471547000058 acc:  0.41844003304825494\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  24 loss :  3.5017097146361023 acc:  0.4186410021660005\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  25 loss :  3.4751306647187348 acc:  0.41734586785164013\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  26 loss :  3.524840278225345 acc:  0.41832838353839624\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  27 loss :  3.5078183754340753 acc:  0.4187303217738874\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  28 loss :  3.509757992270943 acc:  0.418596342362057\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  29 loss :  3.4945425553755327 acc:  0.418350713440368\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  30 loss :  3.5110001113864926 acc:  0.4187526516758591\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  31 loss :  3.5339634618559086 acc:  0.4185070227541701\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  32 loss :  3.520335279144607 acc:  0.4185516825581136\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  33 loss :  3.529988819068962 acc:  0.4187973114798026\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  34 loss :  3.5270469572160628 acc:  0.4189982805975482\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  35 loss :  3.505152900735815 acc:  0.418596342362057\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  36 loss :  3.5271096446297387 acc:  0.41852935265614183\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  37 loss :  3.4977835691892185 acc:  0.41832838353839624\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  38 loss :  3.5077894500919156 acc:  0.41861867226402877\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6093619183257329 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'LeakyReLU', 'activation_transformers': 'Hardsigmoid', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8115719959973409, 'beta_2': 0.9585176603019991, 'concatenate_features': False, 'd_model': 456, 'dropout': 0.5155345118077617, 'dropout_transformers': 0.6141338333071231, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 35, 'eps': 3.1530376054872664e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 6, 'factor': 0.502647433459397, 'patience': 5, 'scheduler': 'ReduceLROnPlateau', 'threshold': 7.154045116605587e-06, 'lr': 0.014547683278319182, 'dropout_lstm': 0.6093619183257329, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'Softplus', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softmin', 'dropout_gcn': 0.470259510206167, 'hidden_channels': 512, 'layer_type': 'GraphSAGE', 'norm': 'BatchNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 9.702800441114743e-07}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  7.623787206761977 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  7.312296995595724 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'ReLU', 'amsgrad': True, 'batch_size': 32, 'beta_1': 0.8820614025559373, 'beta_2': 0.9793996555183352, 'concatenate_features': False, 'd_model': 312, 'dropout': 0.603398849133091, 'dropout_transformers': 0.06688546681692138, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 8, 'eps': 1.5709293719275172e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.036170509367338045, 'dropout_lstm': 0.7433887724877659, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 12, 'num_layers_transformer': 6, 'optimizer': 'Adam', 'positive_function': 'exp', 'epochs_complete_problem': 24, 'reg': True, 'transformers_model': True, 'activation_gcn': 'SELU', 'dropout_gcn': 0.7976206838988898, 'hidden_channels': 256, 'layer_type': 'GCNConv', 'norm': 'LayerNorm', 'num_layers_gcn': 4, 'use_gcn': True, 'weight_decay': 0.0005101645918947477}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7433887724877659 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  7.801028936681613 acc:  0.0010941651966147868\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  7.606881034206337 acc:  0.0009378558828126744\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  7.597711549678319 acc:  0.001496103432105933\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  7.599692257357315 acc:  0.0021660004912578434\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  7.577405123643472 acc:  0.0011834848045017081\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  7.554106060887726 acc:  0.000781546569010562\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  6 loss :  7.54123541334985 acc:  0.0016970725498515061\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  7 loss :  7.522090616360517 acc:  0.003416475001674743\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  8 loss :  8.607285331672346 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  9 loss :  8.601494936875895 acc:  0.0013397941183038206\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  10 loss :  8.553461115125199 acc:  0.001496103432105933\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  11 loss :  8.572433995528959 acc:  0.0033941450997030122\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  12 loss :  8.518537554942386 acc:  0.0015407632360493937\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'Softshrink', 'activation_transformers': 'SiLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8003591501314388, 'beta_2': 0.9545180339357354, 'concatenate_features': False, 'd_model': 240, 'dropout': 0.559115209439146, 'dropout_transformers': 0.31540883803773045, 'early_stopping': 9, 'encoder_only': True, 'epochs_classifcation_only': 12, 'eps': 2.3193339129065176e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.8432325650243659, 'scheduler': 'ExponentialLR', 'lr': 0.0002943570575586829, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'sig', 'epochs_complete_problem': 26, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.586374026279161, 'hidden_channels': 128, 'layer_type': 'GAT', 'norm': 'GraphNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 3.945282581819798e-06}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  7.5231826635507435 acc:  0.02069981912779403\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  7.120719007345346 acc:  0.04957238237724136\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  6.878956468288715 acc:  0.06522564365942433\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  6.6631518290593075 acc:  0.07672554317486546\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  6.487295689949622 acc:  0.08958756671058214\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  6.337744129621066 acc:  0.09646517651787509\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  6 loss :  6.2106681566972 acc:  0.10461559073755666\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  7 loss :  6.113924257571881 acc:  0.10749614809190988\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  8 loss :  6.030402807088999 acc:  0.105665096130228\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  9 loss :  5.960286767666156 acc:  0.10963981868119599\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  10 loss :  5.903191085962149 acc:  0.11582520152736529\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  11 loss :  5.851872125038734 acc:  0.12000089319607887\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  12 loss :  6.971615065061129 acc:  0.11801353192059487\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  13 loss :  6.890916728973389 acc:  0.12120670790255231\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  14 loss :  6.851071882247925 acc:  0.12154165643212826\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  15 loss :  6.8149337144998405 acc:  0.12167563584395864\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  16 loss :  6.795300322312575 acc:  0.12243485251099748\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  17 loss :  6.777085388623751 acc:  0.12395328584507515\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  18 loss :  6.760993528366089 acc:  0.12397561574704687\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  19 loss :  6.7366105776566725 acc:  0.12410959515887725\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  20 loss :  6.724773487677941 acc:  0.12413192506084898\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  21 loss :  6.726161380914541 acc:  0.12542705937520934\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  22 loss :  6.714308393918551 acc:  0.12571734810084184\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  23 loss :  6.713782361837533 acc:  0.1262086059442199\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  24 loss :  6.7060818708859955 acc:  0.1265658843757676\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  25 loss :  6.701124547078059 acc:  0.1267891833954849\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  26 loss :  6.696166838132418 acc:  0.12685617310140007\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  27 loss :  6.689410121624286 acc:  0.12705714221914566\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  28 loss :  6.690309895001925 acc:  0.1271687917290043\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  29 loss :  6.693739659969623 acc:  0.12725811133689124\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  30 loss :  6.693234872817993 acc:  0.12721345153294777\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  31 loss :  6.691816322620099 acc:  0.1271687917290043\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  32 loss :  6.688730008785541 acc:  0.12725811133689124\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  33 loss :  6.681490773421068 acc:  0.12728044123886295\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  34 loss :  6.684986114501953 acc:  0.12721345153294777\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  35 loss :  6.685600794278658 acc:  0.12732510104280642\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  36 loss :  6.681247780873226 acc:  0.12741442065069333\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  37 loss :  6.679920548659105 acc:  0.12754840006252371\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Hardshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8233164765330816, 'beta_2': 0.9676483467100429, 'concatenate_features': False, 'd_model': 192, 'dropout': 0.4631983835941546, 'dropout_transformers': 0.15132408760384097, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 42, 'eps': 2.0828148388247606e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.00013979595540235802, 'dropout_lstm': 0.520859458787047, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 12, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'ReLU', 'dropout_gcn': 0.7022151706892515, 'hidden_channels': 512, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 3, 'use_gcn': True, 'weight_decay': 0.02776810806606303}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.520859458787047 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  7.724498954896004 acc:  0.005604805394904317\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  7.316152474188035 acc:  0.007190228434897171\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  7.218288778489636 acc:  0.02029788089230288\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  7.055392643713182 acc:  0.03441037893843646\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  6.833796602679837 acc:  0.06585088091463279\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  6.592020302434121 acc:  0.09907777504856753\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  6 loss :  6.326561137168638 acc:  0.12283679074648862\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  7 loss :  6.141936508301765 acc:  0.13661434026304625\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  8 loss :  5.939644478213403 acc:  0.15222294174128576\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  9 loss :  5.760766189329086 acc:  0.17247616282964517\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  10 loss :  5.594489746709024 acc:  0.18797311479802603\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  11 loss :  5.42707780714958 acc:  0.19659245695911395\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  12 loss :  5.281539024845246 acc:  0.20755643882723354\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  13 loss :  5.150155319706086 acc:  0.21615345108634973\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  14 loss :  5.044822268332204 acc:  0.22448250452180515\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  15 loss :  4.9458596506426415 acc:  0.23341446531049728\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  16 loss :  4.843956181310838 acc:  0.23899694080342987\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  17 loss :  4.721753628023209 acc:  0.24540562266931648\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  18 loss :  4.647819202176986 acc:  0.251657995221401\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  19 loss :  4.6029895336397235 acc:  0.2606569457160083\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  20 loss :  4.530359454308787 acc:  0.26556952414978896\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  21 loss :  4.4578960787865425 acc:  0.2705714221914566\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  22 loss :  4.410429709957492 acc:  0.2768461246455128\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  23 loss :  4.3551291558050345 acc:  0.2801509501373289\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  24 loss :  4.281945797704881 acc:  0.2863586628854699\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  25 loss :  4.264958567773142 acc:  0.2880557354353214\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  26 loss :  4.189889746327554 acc:  0.29343724181050845\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  27 loss :  4.132498901121078 acc:  0.29741196436147643\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  28 loss :  4.096052245170839 acc:  0.3003148516178014\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  29 loss :  4.055170551423104 acc:  0.30551771877721456\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  30 loss :  3.9963914932743196 acc:  0.30571868789496015\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  31 loss :  3.9855203151702883 acc:  0.3099167094656454\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  32 loss :  3.947795360319076 acc:  0.31197106044704465\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  33 loss :  3.922483222715316 acc:  0.31625840162561686\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  34 loss :  3.8719511047486335 acc:  0.3202777839805283\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  35 loss :  3.8700059013981973 acc:  0.32367192908023135\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  36 loss :  3.8004791367438533 acc:  0.32367192908023135\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  37 loss :  3.7810477164483838 acc:  0.32751267221936897\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  38 loss :  3.803321749164212 acc:  0.33226894133934753\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  39 loss :  3.7338878923846828 acc:  0.3312194359466762\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  40 loss :  3.688779490993869 acc:  0.33421164281088805\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  41 loss :  3.694246632053006 acc:  0.33369805506553823\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'Softmin', 'activation_transformers': 'Softshrink', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8503739748619117, 'beta_2': 0.9847299603435692, 'concatenate_features': False, 'd_model': 96, 'dropout': 0.3025716793298758, 'dropout_transformers': 0.4482851716154957, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 3, 'eps': 8.29356739889865e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 3, 'factor': 0.6252187554633478, 'patience': 7, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0010941111980781616, 'lr': 0.0011253704778029067, 'dropout_lstm': 0.6821866828161312, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 28, 'reg': True, 'transformers_model': True, 'activation_gcn': 'ReLU6', 'dropout_gcn': 0.8996229433090326, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 0.00025831978665771127}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6821866828161312 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  7.971646402483788 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  7.86250075224404 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  7.768969428873508 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  9.392920672336471 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  9.09491808169356 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  8.8927237519594 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.18769979649085572 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'Sigmoid', 'activation_transformers': 'GELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8686839574998357, 'beta_2': 0.9983785608270106, 'concatenate_features': False, 'd_model': 528, 'dropout': 0.3893872019593233, 'dropout_transformers': 0.09847950847062709, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 20, 'eps': 1.1101423376694327e-08, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'lr': 0.0201196090117802, 'dropout_lstm': 0.18769979649085572, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'ReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 3, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 41, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Hardsigmoid', 'dropout_gcn': 0.5252883862441602, 'hidden_channels': 256, 'layer_type': 'GAT', 'norm': 'InstanceNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 0.00012625045533676372}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  8.979416085892364 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  7.644626946008506 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  7.619484460654379 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  7.641077939201804 acc:  0.001362124020275551\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  7.641680156483369 acc:  0.004666949512091642\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  7.666901023448014 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  6 loss :  7.679539880832704 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  7 loss :  7.705284819883459 acc:  0.0013174642163320902\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  8 loss :  7.705571330895944 acc:  0.0015854230399928544\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  9 loss :  7.727607378438742 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  10 loss :  7.73266599158279 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  11 loss :  7.750116336245497 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  12 loss :  7.745369522511458 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  13 loss :  7.746109673956863 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  14 loss :  7.754209470348198 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  15 loss :  7.748294209231849 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2585913143013201 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'Softsign', 'amsgrad': True, 'batch_size': 32, 'beta_1': 0.8303663896802684, 'beta_2': 0.9507760256387041, 'concatenate_features': False, 'd_model': 384, 'dropout': 0.1434904628328234, 'dropout_transformers': 0.5166656633223636, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 26, 'eps': 6.908289117703492e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.01068455848233385, 'scheduler': 'StepLR', 'step_size': 26, 'lr': 0.0031583272617062568, 'dropout_lstm': 0.2585913143013201, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'Adam', 'positive_function': 'relu', 'epochs_complete_problem': 35, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Hardshrink', 'dropout_gcn': 0.5936786044364435, 'hidden_channels': 2048, 'layer_type': 'GraphSAGE', 'norm': 'PairNorm', 'num_layers_gcn': 4, 'use_gcn': True, 'weight_decay': 2.408792349203531e-05}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'ReLU6', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8138754763322318, 'beta_2': 0.9601956167660672, 'concatenate_features': False, 'd_model': 744, 'dropout': 0.6750826861581273, 'dropout_transformers': 0.27860803261499284, 'early_stopping': 9, 'encoder_only': True, 'epochs_classifcation_only': 15, 'eps': 4.108890916335962e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.005354931093760975, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 12, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 38, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Hardswish', 'dropout_gcn': 0.6567966715967563, 'hidden_channels': 128, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 5, 'use_gcn': True, 'weight_decay': 1.8380821225844745e-06}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  7.536389167050281 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  7.314093739153391 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  7.2932238636246645 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  7.283894814640643 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  7.283626309360367 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  7.275849037859813 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  6 loss :  7.255966134818204 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  7 loss :  7.263561300484531 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  8 loss :  7.27279456839504 acc:  0.004666949512091642\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  9 loss :  7.257618938583925 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  10 loss :  7.293817479926419 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  11 loss :  7.2740145533917895 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  12 loss :  7.253900349858296 acc:  0.002657258334635911\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  13 loss :  7.265279160924704 acc:  0.002657258334635911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.12879150831801814 and num_layers=1\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'LeakyReLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8586803563832891, 'beta_2': 0.9748707378354176, 'concatenate_features': False, 'd_model': 24, 'dropout': 0.7155898147690412, 'dropout_transformers': 0.23945158970774255, 'early_stopping': 10, 'encoder_only': True, 'epochs_classifcation_only': 30, 'eps': 2.617086975940454e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.5035956479647519, 'scheduler': 'ExponentialLR', 'lr': 0.000639269136150067, 'dropout_lstm': 0.12879150831801814, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'RReLU', 'dropout_gcn': 0.3646075233097861, 'hidden_channels': 256, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 3, 'use_gcn': True, 'weight_decay': 0.0009282499865591453}\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  0 loss :  8.988347178239088 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  1 loss :  8.484334204747126 acc:  0.0033494852957595515\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  2 loss :  8.301857515481801 acc:  0.00390773284505281\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  3 loss :  8.212193092933068 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  4 loss :  8.166965440603404 acc:  0.00390773284505281\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  5 loss :  8.13941648556636 acc:  0.003818413237165889\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  6 loss :  8.130189976325402 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  7 loss :  8.120386178676899 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  8 loss :  8.120735212472768 acc:  0.0038630730411093497\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  9 loss :  8.116788145212027 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  10 loss :  8.11590945170476 acc:  0.00390773284505281\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  11 loss :  8.115694581545316 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=56167)\u001b[0m epoch:  12 loss :  8.115771836500901 acc:  0.00390773284505281\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-09 18:51:50,105\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-09 18:52:04,102\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-09 18:52:04,104\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_8        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_8\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_8`\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3405060443255087 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Softmin', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8971434076107372, 'beta_2': 0.9729876362601937, 'concatenate_features': False, 'd_model': 120, 'dropout': 0.8614919697062138, 'dropout_transformers': 0.397472687714232, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 39, 'eps': 1.9909235694267976e-07, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 9, 'factor': 0.3427446604846367, 'patience': 3, 'scheduler': 'ReduceLROnPlateau', 'threshold': 2.2417785945204364e-05, 'lr': 0.0015089221521900175, 'dropout_lstm': 0.3405060443255087, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 12, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 13, 'reg': True, 'transformers_model': True, 'activation_gcn': 'LogSigmoid', 'dropout_gcn': 0.9395757260649558, 'hidden_channels': 64, 'layer_type': 'GAT', 'norm': 'LayerNorm', 'num_layers_gcn': 2, 'use_gcn': True, 'weight_decay': 5.564826047478168e-07}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  7.435057759909106 acc:  0.004108701962798384\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  7.268698951960858 acc:  0.006096063238282384\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  7.263914562644759 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  7.2490719076226515 acc:  0.005582475492932586\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  7.2283332959519635 acc:  0.007927115199964273\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  7.195490282867591 acc:  0.009266909318268093\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  7.117500090474233 acc:  0.007435857356586205\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  7.03803320080822 acc:  0.016278498537391422\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  6.993417153183702 acc:  0.01951633432329232\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  6.965603763520406 acc:  0.019092066185829443\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  6.927278783308898 acc:  0.022307572069758613\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  6.897908560268542 acc:  0.026684232856217762\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  6.8605874695703 acc:  0.032110399035348236\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  6.817805829472567 acc:  0.0361297813902597\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  6.790114712340669 acc:  0.03894334903869772\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  6.744191671541224 acc:  0.04488310296317799\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  6.6969037655136345 acc:  0.0490811245338633\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  6.652006636115269 acc:  0.051850032378357856\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  6.6217420388266675 acc:  0.05499854855637184\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  6.590572834014893 acc:  0.053368465712435524\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  6.55243969213276 acc:  0.05707522943974276\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  6.542244913690377 acc:  0.06134024071634325\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  6.524529574429177 acc:  0.06321595248196861\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  6.499244013381878 acc:  0.06618582944420874\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  6.498334143174256 acc:  0.06540428287519817\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  6.480026332495724 acc:  0.068731438268986\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  6.4715727786119075 acc:  0.06460040640421588\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  6.460460001261446 acc:  0.06701203581716277\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  6.441884063301286 acc:  0.07054016032869616\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  6.426758107090496 acc:  0.06902172699461849\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  6.418823089899193 acc:  0.07279548042784093\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  6.417997018204933 acc:  0.0740236250362861\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  6.3859274899148195 acc:  0.0747605118013532\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  6.375539392700994 acc:  0.07431391376191858\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  6.374680476663 acc:  0.07531875935064646\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  35 loss :  6.381875577397372 acc:  0.07583234709599625\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  36 loss :  6.3625249812740305 acc:  0.0757206975861376\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  37 loss :  6.370975379544403 acc:  0.07431391376191858\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  38 loss :  6.349780502119613 acc:  0.07708282160641315\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  39 loss :  7.917218457965951 acc:  0.07612263582162875\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  40 loss :  7.676511142890491 acc:  0.07652457405711989\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  41 loss :  7.6057986214522915 acc:  0.07493915101712703\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  42 loss :  7.547002480292195 acc:  0.07429158385994686\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  43 loss :  7.517437295763904 acc:  0.07433624366389031\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  44 loss :  7.503038403875541 acc:  0.0740236250362861\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  45 loss :  7.450318705973201 acc:  0.0737333363106536\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  46 loss :  7.497144821426631 acc:  0.07453721278163589\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7878905606828505 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'LogSigmoid', 'amsgrad': True, 'batch_size': 32, 'beta_1': 0.8340252059929878, 'beta_2': 0.9525795508121829, 'concatenate_features': True, 'd_model': 240, 'dropout': 0.7533427190650045, 'dropout_transformers': 0.04276217843477209, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 21, 'eps': 1.2248091063521204e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 0.07709410632471966, 'dropout_lstm': 0.7878905606828505, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'LogSigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'Adam', 'positive_function': 'abs', 'epochs_complete_problem': 33, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Sigmoid', 'dropout_gcn': 0.7522134632363342, 'hidden_channels': 1024, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 5, 'use_gcn': True, 'weight_decay': 0.0021292042475750056}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'CELU', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.8258437130768813, 'beta_2': 0.9911336030424938, 'concatenate_features': False, 'd_model': 432, 'dropout': 0.41975934769784384, 'dropout_transformers': 0.21880536283906654, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 27, 'eps': 4.2124419545028716e-07, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lr': 6.179180984419846e-05, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 1, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 30, 'reg': True, 'transformers_model': True, 'activation_gcn': 'ELU', 'dropout_gcn': 0.863694311462361, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 4, 'use_gcn': True, 'weight_decay': 6.185181372303372e-06}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  7.879309130386567 acc:  0.005694125002791238\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  7.544779965575312 acc:  0.006542661277716991\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  7.374529771401849 acc:  0.010204765201080768\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  7.282915001184168 acc:  0.01371055981064243\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  7.16458766561159 acc:  0.01871245785231003\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  7.130289970988959 acc:  0.023870665207779737\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  7.049320368699624 acc:  0.028247325994238886\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  6.984461294093602 acc:  0.035951142174485855\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  6.9550979976922696 acc:  0.04256079315811804\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  6.850707275766722 acc:  0.05135877453497979\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  6.798297103022186 acc:  0.05607038385101489\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  6.686880816876049 acc:  0.06426545787463993\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  6.681590288457736 acc:  0.0729294598396713\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  6.5779625462814115 acc:  0.08145948239287229\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  6.484633358431534 acc:  0.08929727798494964\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  6.445390425937276 acc:  0.09760400151843333\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  6.400387260275827 acc:  0.10602237456177567\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  6.315799639258586 acc:  0.1082553647589487\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  6.280341914002324 acc:  0.11254270593752093\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  6.19863357006664 acc:  0.1182591608422839\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  6.091384995151573 acc:  0.12270281133465824\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  6.097125469798773 acc:  0.12748141035660854\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  5.995977280845104 acc:  0.13511823683094032\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  5.96175999036977 acc:  0.13607842261572473\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  5.894278935983148 acc:  0.13978518634303194\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  5.865049751711563 acc:  0.13987450595091888\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  5.742327381187762 acc:  0.14646182703257932\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  6.898075231364076 acc:  0.1462161981108903\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  6.740127744809003 acc:  0.14916374517115868\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  6.713768005371094 acc:  0.15789473684210525\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  6.613910439988257 acc:  0.16106558292209097\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  6.549195759732958 acc:  0.16445972802179398\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  6.524709963462722 acc:  0.16850144027867717\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  6.512127365864498 acc:  0.17202956479021056\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  6.390655947403169 acc:  0.17638389567469798\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  35 loss :  6.361089578816588 acc:  0.17819261773440814\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  36 loss :  6.323771859558535 acc:  0.17928678293102293\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  37 loss :  6.287094250531264 acc:  0.18151977312819598\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  38 loss :  6.184798818239024 acc:  0.18254694861889556\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  39 loss :  6.171075250061465 acc:  0.18799544469999777\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  40 loss :  6.03972774827984 acc:  0.18942455842618852\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  41 loss :  6.182736477381747 acc:  0.19138958979970078\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  42 loss :  6.024478166875705 acc:  0.19540897215461225\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  43 loss :  5.957552621062373 acc:  0.1973740035281245\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  44 loss :  5.952642736300616 acc:  0.19864680794051315\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  45 loss :  5.885312953465421 acc:  0.20400598441372841\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  46 loss :  5.889039093339947 acc:  0.20572538686555167\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  47 loss :  5.823445561905982 acc:  0.2092981711810285\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  48 loss :  5.828865232601972 acc:  0.2099457383382087\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  49 loss :  5.765485078516141 acc:  0.2128932853984771\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  50 loss :  5.745373416954363 acc:  0.21519326530156532\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  51 loss :  5.718954878793636 acc:  0.2179621731460599\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  52 loss :  5.671678986347897 acc:  0.22055244177478062\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  53 loss :  5.647997527055337 acc:  0.2227631020699819\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  54 loss :  5.571560322398871 acc:  0.22638054618940223\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  55 loss :  5.527004315819539 acc:  0.22738539177813008\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  56 loss :  5.513911791250739 acc:  0.2293727530536141\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'Tanhshrink', 'activation_transformers': 'RReLU', 'amsgrad': True, 'batch_size': 64, 'beta_1': 0.91367317155809, 'beta_2': 0.9951488098200931, 'concatenate_features': True, 'd_model': 552, 'dropout': 0.596376613754576, 'dropout_transformers': 0.18475367107502377, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 11, 'eps': 1.2885736900290641e-06, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.9800753067373487, 'scheduler': 'StepLR', 'step_size': 8, 'lr': 0.002673440566691919, 'dropout_lstm': 0.4528712563366303, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': False, 'num_heads': 12, 'num_layers_transformer': 2, 'optimizer': 'AdamW', 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Hardtanh', 'dropout_gcn': 0.6149178889400385, 'hidden_channels': 256, 'layer_type': 'GraphSAGE', 'norm': 'PairNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 6.0376656212293216e-05}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4528712563366303 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  7.355110655297766 acc:  0.08983319563227117\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  6.249996035249083 acc:  0.21867673000915527\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  5.331319939006459 acc:  0.24562892168903377\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  4.554067146528017 acc:  0.28542080700265726\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  3.970044744598282 acc:  0.3290980952593618\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  3.6205144462051924 acc:  0.3500658732108166\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  3.4246354053070496 acc:  0.3637317732175156\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  3.2719964781007564 acc:  0.38608400509121765\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  3.124712200431557 acc:  0.3912645423486591\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  3.083916140603019 acc:  0.3963557599982136\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  3.031419789040839 acc:  0.4020275550990331\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8459534370402754 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8416099262443333, 'beta_2': 0.9871335253156874, 'concatenate_features': False, 'd_model': 648, 'dropout': 0.48672962113723284, 'dropout_transformers': 0.3544405119729336, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 18, 'eps': 4.589853043900701e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 6, 'factor': 0.7769695190233077, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.05838697180538053, 'lr': 0.0002725190122822327, 'dropout_lstm': 0.8459534370402754, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 44, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Mish', 'dropout_gcn': 0.44858787447079485, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 1.4692519072085264e-05}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  7.741533174353131 acc:  0.052296630417792464\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  6.609739917819783 acc:  0.1551928187035259\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  5.601182040521654 acc:  0.25109974767210774\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  4.860232959359379 acc:  0.2967197374003528\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  4.31880238500692 acc:  0.32889712614161626\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  3.959575196443978 acc:  0.35075810017194026\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  3.7855836213645286 acc:  0.3650715673358194\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  3.5876417604543396 acc:  0.3778666011656209\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  3.4499763917114774 acc:  0.3844539222472813\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  3.3634101778773937 acc:  0.3935645222517473\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  3.23348643011966 acc:  0.39798584284214994\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  3.098881862931332 acc:  0.4048857825514146\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  3.0994193473104703 acc:  0.4055556796105665\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  3.0213019160901085 acc:  0.4091731237299868\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  2.9810090994430802 acc:  0.4102449590246299\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  2.929951716277559 acc:  0.41243328941785945\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  2.896968582929191 acc:  0.41480025902686285\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  2.8529148020986783 acc:  0.41685461000826207\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  4.257529582007456 acc:  0.413549784516446\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  3.858942048024323 acc:  0.41654199138065784\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  3.855586601515948 acc:  0.4181720742245942\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  3.807899147777234 acc:  0.42116428108880605\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  3.7487816002409335 acc:  0.422325435991336\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  3.7296188887903248 acc:  0.4230623227564031\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  3.7584953793024614 acc:  0.42377687961949845\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  3.7286387055607166 acc:  0.42779626197440995\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  3.7214503934827903 acc:  0.4263224884442757\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  3.64819015890865 acc:  0.42665743697385167\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  3.676557896500927 acc:  0.4279302413862403\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  3.6188586485587946 acc:  0.42759529285666437\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  3.638156907033112 acc:  0.4297389634459505\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  3.6286025006892317 acc:  0.4307214791327066\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  3.6155758873891024 acc:  0.4312127369760847\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  3.5777108628871077 acc:  0.43052051001496106\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  3.6088965787725935 acc:  0.4283545095237032\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  35 loss :  3.5463675927307645 acc:  0.429448674720318\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  36 loss :  3.576714907662343 acc:  0.429694303642007\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  37 loss :  3.563003220800626 acc:  0.4316370051135476\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  38 loss :  3.535525431067257 acc:  0.4331554384476252\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  39 loss :  3.5598710108611544 acc:  0.43076613893665006\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  40 loss :  3.525844796229217 acc:  0.43353504678114463\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  41 loss :  3.5235727116213007 acc:  0.43331174776142733\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  42 loss :  3.505136582811 acc:  0.4322399124667843\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  43 loss :  3.526765665765536 acc:  0.4338030056048054\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  44 loss :  3.4682141603049583 acc:  0.43449523256592903\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  45 loss :  3.4970375481298412 acc:  0.43465154187973115\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  46 loss :  3.4941519034110895 acc:  0.4357680369783177\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  47 loss :  3.523441811739388 acc:  0.4341602840363531\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  48 loss :  3.489431615603172 acc:  0.4337806757028337\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  49 loss :  3.4679729211128363 acc:  0.4349418306053636\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  50 loss :  3.4987817335936984 acc:  0.4357903668802894\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  51 loss :  3.4638263209391447 acc:  0.43525444923296785\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  52 loss :  3.4512011358293435 acc:  0.4358796864881763\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  53 loss :  3.459405640424308 acc:  0.43619230511578055\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  54 loss :  3.482444169157642 acc:  0.4366612330571869\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  55 loss :  3.4652669672238625 acc:  0.4343165933501552\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  56 loss :  3.393333362320722 acc:  0.4349864904093071\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  57 loss :  3.3984050710322493 acc:  0.4360136659000067\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  58 loss :  3.4567356675358143 acc:  0.4363262845276109\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  59 loss :  3.3662057044142384 acc:  0.4368845320769042\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  60 loss :  3.397355798947609 acc:  0.43599133599803497\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  61 loss :  3.4235008975206793 acc:  0.4358573565862046\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9963718735690347 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.847605346016229, 'beta_2': 0.9686478105235155, 'concatenate_features': False, 'd_model': 648, 'dropout': 0.37652605694152574, 'dropout_transformers': 0.13639967313839685, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 18, 'eps': 4.662497393121671e-09, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 6, 'factor': 0.7887735542778849, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.354130431203357, 'lr': 3.242144363353754e-05, 'dropout_lstm': 0.9963718735690347, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 36, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 6, 'num_layers_transformer': 6, 'optimizer': 'AdamW', 'positive_function': 'exp', 'epochs_complete_problem': 48, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Mish', 'dropout_gcn': 0.45290244616368797, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 10, 'use_gcn': True, 'weight_decay': 1.8047841204611146e-07}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  8.00416315623692 acc:  0.007592166670388317\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  7.745011152539934 acc:  0.01502802402697452\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  7.486821583339146 acc:  0.02474153138467722\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  7.24284576688494 acc:  0.04005984413728424\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  7.064788341522217 acc:  0.05095683629948865\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  6.939766202654157 acc:  0.06703436571913449\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  6.771503366742816 acc:  0.08458566866891454\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  6.602357809884207 acc:  0.09546033092914723\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  6.523588112422398 acc:  0.11419511868342898\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  6.389755712236677 acc:  0.12544938927718108\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  6.2994408062526155 acc:  0.136056092713753\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  6.117646271841867 acc:  0.14773463144496796\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  6.046737561907087 acc:  0.15566174664493224\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  5.963867664337158 acc:  0.15930152066632428\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  5.862830911363874 acc:  0.16566554272826742\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  5.766368743351528 acc:  0.17091306969162406\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  5.732038443429129 acc:  0.17810329812652123\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  5.662296336037772 acc:  0.18435567067860573\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  6.94126443862915 acc:  0.18225665989326306\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  6.705735410962786 acc:  0.19034008440702946\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  6.655429077148438 acc:  0.1925060848982873\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  6.552768066951207 acc:  0.19665944666502913\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  6.457540144239153 acc:  0.20105843735346002\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  6.429709829602923 acc:  0.20342540696246345\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  6.31718339920044 acc:  0.21050398588750197\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  6.266439969199045 acc:  0.2131165844181944\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  6.296252754756383 acc:  0.21653305941986914\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  6.259506130218506 acc:  0.21950293638210927\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  6.182107775551932 acc:  0.22111068932407388\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  6.115133013044085 acc:  0.22463881383560727\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  6.172615732465472 acc:  0.22763102069981914\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  6.138798986162458 acc:  0.22990867070093562\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  6.096721594674246 acc:  0.23211933099613694\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  6.058591134207589 acc:  0.2352008574682357\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  6.1067538125174385 acc:  0.23439698099725342\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  35 loss :  6.02020308630807 acc:  0.2354018265859813\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  36 loss :  5.970368317195347 acc:  0.23964450796061004\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  37 loss :  6.0377999033246725 acc:  0.24227943639327423\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  38 loss :  5.964781665802002 acc:  0.24591921041466627\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  39 loss :  5.891071196964809 acc:  0.24877743786704776\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  40 loss :  5.974956103733608 acc:  0.2491793761025389\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  41 loss :  5.881354454585484 acc:  0.24571824129692071\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  42 loss :  5.924123927525112 acc:  0.25016189178929504\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  43 loss :  5.820233140672956 acc:  0.25364535649688497\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  44 loss :  5.831915719168527 acc:  0.25516378983096266\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  45 loss :  5.787460599626813 acc:  0.2577763883616551\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  46 loss :  5.886744144984654 acc:  0.2567492128709555\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  47 loss :  5.782234682355608 acc:  0.25853560502869394\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  48 loss :  5.822399234771728 acc:  0.25965210012728046\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  49 loss :  5.786646107264928 acc:  0.26083558493178216\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  50 loss :  5.701981503622872 acc:  0.2632918741486725\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  51 loss :  5.669941398075649 acc:  0.2635375030703615\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  52 loss :  5.696429525102888 acc:  0.2633811937565594\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  53 loss :  5.690534033094134 acc:  0.2652569055221848\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  54 loss :  5.6846553938729425 acc:  0.264653998168948\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  55 loss :  5.616711984361921 acc:  0.26905298885737894\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  56 loss :  5.683086654118129 acc:  0.26987919523033294\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  57 loss :  5.6634392738342285 acc:  0.27077239130920217\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  58 loss :  5.654263074057443 acc:  0.2705714221914566\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  59 loss :  5.571439347948346 acc:  0.2718442266038452\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  60 loss :  5.629021794455392 acc:  0.27282674229060133\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  61 loss :  5.520454965318952 acc:  0.2746131344483398\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  62 loss :  5.56353099005563 acc:  0.2738985775852444\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  63 loss :  5.501598603384835 acc:  0.2756403099390394\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  64 loss :  5.626083333151681 acc:  0.27744903199874954\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  65 loss :  5.559292357308524 acc:  0.2789228055288837\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8574185726940275 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'SiLU', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8010506505722862, 'beta_2': 0.9707042708604, 'concatenate_features': False, 'd_model': 336, 'dropout': 0.4858646407671578, 'dropout_transformers': 0.3145182979418979, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 32, 'eps': 2.606136963220284e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 8, 'factor': 0.7140266394665409, 'patience': 7, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0072386589738796785, 'lr': 0.00028354270039856035, 'dropout_lstm': 0.8574185726940275, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 12, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'Adam', 'positive_function': 'exp', 'epochs_complete_problem': 45, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Mish', 'dropout_gcn': 0.21816398602050235, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 1.2162750123028772e-05}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  8.405651699412953 acc:  0.0023223098050599556\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  7.9959634867581455 acc:  0.0028805573543532145\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  7.722445271231911 acc:  0.007525176964473126\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  7.545576485720548 acc:  0.014380456869794342\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  7.514541929418391 acc:  0.016993055400486793\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  7.36633149060336 acc:  0.019962932362726928\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  7.287314848466353 acc:  0.03152982158408325\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  7.189999580383301 acc:  0.034008440702945314\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  7.13616401498968 acc:  0.039211307862358484\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  7.039946946230802 acc:  0.050979166201460376\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  6.9675493673844775 acc:  0.06899939709264677\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  6.856874942779541 acc:  0.07134403679967845\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  6.779555580832741 acc:  0.07875756425429292\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  6.761735525998202 acc:  0.08085657503963557\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  6.444632443514737 acc:  0.09195453631958556\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  6.676746368408203 acc:  0.10374472456065918\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  6.4745472994717685 acc:  0.10745148828796641\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  6.4043169455094775 acc:  0.1100194270147154\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  6.340568195689809 acc:  0.11997856329410714\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  6.295172821391713 acc:  0.1267221936895697\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  6.215886159376665 acc:  0.13049594712279214\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  6.212707736275413 acc:  0.14023178438246656\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  6.045043425126509 acc:  0.1419511868342898\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  6.097384279424494 acc:  0.1501239309559431\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  5.980964530598033 acc:  0.15838599468548334\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  5.800060965798118 acc:  0.15948015988209813\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  5.807996706529097 acc:  0.16354420204095305\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  5.708052461797541 acc:  0.16908201772994216\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  5.8053881471807305 acc:  0.17616059665498068\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  5.732913840900768 acc:  0.1775673804791997\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  5.640945607965643 acc:  0.18178773195185674\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  5.516957976601341 acc:  0.18062657704932675\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  8.609980756586248 acc:  0.1258289976107005\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  7.126319495114413 acc:  0.15811803586182258\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  6.811167890375311 acc:  0.16821115155304467\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  35 loss :  6.853271874514493 acc:  0.171828595672465\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  36 loss :  6.713221810080788 acc:  0.17734408145948238\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  37 loss :  6.66258092360063 acc:  0.18234597950115\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  38 loss :  6.729454213922674 acc:  0.1877944755822522\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  39 loss :  6.6012680313803935 acc:  0.18833039322957373\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  40 loss :  6.56763913414695 acc:  0.19351093048701518\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  41 loss :  6.51739688353105 acc:  0.1958332402920751\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  42 loss :  6.728539596904408 acc:  0.19826719960699374\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  43 loss :  6.572483366185969 acc:  0.19837884911685238\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  44 loss :  6.627929210662842 acc:  0.1990487461760043\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  45 loss :  6.413209958509966 acc:  0.2022865819619052\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  46 loss :  6.511211828751997 acc:  0.20282249960922671\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  47 loss :  6.613303357904607 acc:  0.20440792264921956\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  48 loss :  6.466567516326904 acc:  0.2072214902976576\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  49 loss :  6.5583484823053535 acc:  0.20990107853426523\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  50 loss :  6.384803208437833 acc:  0.21007971775003909\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  51 loss :  6.413146539167925 acc:  0.2109729138289083\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  52 loss :  6.557978760112416 acc:  0.21275930598664672\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  53 loss :  6.4707159562544385 acc:  0.21505928588973494\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  54 loss :  6.424341331828725 acc:  0.2158408324587455\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  55 loss :  6.300704002380371 acc:  0.21577384275283032\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  56 loss :  6.401718356392601 acc:  0.21731460598887972\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  57 loss :  6.409524744207209 acc:  0.21756023491056875\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  58 loss :  6.39460394599221 acc:  0.21874371971507045\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  59 loss :  6.283664573322643 acc:  0.22006118393140253\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  60 loss :  6.208362145857378 acc:  0.22023982314717638\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  61 loss :  6.325622862035578 acc:  0.22075341089252618\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  62 loss :  6.301331996917725 acc:  0.22207087510885828\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  63 loss :  6.3601179989901455 acc:  0.22314271040350134\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  64 loss :  6.191694909876043 acc:  0.22506308197307015\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  65 loss :  6.353202299638228 acc:  0.22492910256123977\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  66 loss :  6.23502068086104 acc:  0.2252640510908157\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  67 loss :  6.249656287106601 acc:  0.22504075207109842\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  68 loss :  6.210357319224965 acc:  0.22588928834602415\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  69 loss :  6.314986012198708 acc:  0.2276086907978474\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  70 loss :  6.2885401899164375 acc:  0.22890382511220775\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  71 loss :  6.221421415155584 acc:  0.22843489717080143\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  72 loss :  6.112176851792769 acc:  0.22872518589643392\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  73 loss :  6.277725783261386 acc:  0.22932809324967063\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  74 loss :  6.230577078732577 acc:  0.23006498001473774\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  75 loss :  6.370950352061879 acc:  0.23022128932853986\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  76 loss :  6.237507169896906 acc:  0.2310028358975504\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'Hardswish', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8430793904283986, 'beta_2': 0.9573536133831201, 'concatenate_features': False, 'd_model': 648, 'dropout': 0.3295272562540932, 'dropout_transformers': 0.0227768697330879, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 47, 'eps': 3.8447436694696256e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 7, 'factor': 0.5868201295961334, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.06872406117987466, 'lr': 1.670664140900181e-05, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 24, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'sig', 'epochs_complete_problem': 17, 'reg': True, 'transformers_model': True, 'activation_gcn': 'swish', 'dropout_gcn': 0.33296851114107134, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 3.3105763244167675e-06}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  8.007548891979715 acc:  0.0012504745104168992\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  7.859354889911154 acc:  0.004733939218006833\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  7.753791560297427 acc:  0.011276600495723824\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  7.6644187180892285 acc:  0.01944934461737713\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  7.588648485100788 acc:  0.0278900475626912\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  7.50062026148257 acc:  0.036755018645468145\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  7.443385414455248 acc:  0.04515106178683875\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  7.383512123771336 acc:  0.05770046669495121\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  7.285540684409764 acc:  0.06870910836701427\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  7.231933904730755 acc:  0.07688185248866758\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  7.1431245389192 acc:  0.08355849317821495\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  7.132656242536462 acc:  0.08634973092468123\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  7.072468239328136 acc:  0.09269142308465265\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  7.027000966279403 acc:  0.0958176093606949\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  7.002666597780974 acc:  0.10079717750039077\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  6.944144995316215 acc:  0.10298550789362035\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  6.898125441178031 acc:  0.1061116941696626\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  6.909748160320779 acc:  0.11754460397918853\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  6.87655148298844 acc:  0.117008686331867\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  6.829082198764967 acc:  0.11982225398030502\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  6.8078264568163 acc:  0.12643190496393722\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  6.77679615435393 acc:  0.13045128731884867\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  6.774243893830673 acc:  0.13493959761516647\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  6.705320316812267 acc:  0.13773083536163278\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  6.694429003674051 acc:  0.14168322801062902\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  6.70967321810515 acc:  0.14382689859991515\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  6.648941703464674 acc:  0.14679677556215528\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  6.602787536123524 acc:  0.14672978585624008\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  6.58174585259479 acc:  0.15184333340776635\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  6.544911819955577 acc:  0.14920840497510215\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  6.536474103512972 acc:  0.153361766741844\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  6.474228796751603 acc:  0.15610834468436682\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  6.503153116806693 acc:  0.15825201527365296\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  6.501825498498005 acc:  0.15970345890181542\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  6.4443679270537 acc:  0.16115490252997788\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  35 loss :  6.468074321746826 acc:  0.16260634615814037\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  36 loss :  6.392911060996678 acc:  0.16329857311926402\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  37 loss :  6.419911218726116 acc:  0.16577719223812606\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  38 loss :  6.374933491582456 acc:  0.16588884174798474\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  39 loss :  6.397899171580439 acc:  0.1673402853761472\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  40 loss :  6.400550137395444 acc:  0.16769756380769488\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  41 loss :  6.389576828998068 acc:  0.16912667753388563\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  42 loss :  6.405657913373864 acc:  0.17149364714288903\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  43 loss :  6.337754954462466 acc:  0.17111403880936962\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  44 loss :  6.342105824014415 acc:  0.17149364714288903\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  45 loss :  6.326524651568869 acc:  0.17207422459415403\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  46 loss :  6.329659150994343 acc:  0.17238684322175826\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  47 loss :  7.420556835506273 acc:  0.17178393586852153\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  48 loss :  7.422953647115956 acc:  0.17037715204430254\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  49 loss :  7.333433109781017 acc:  0.17162762655471941\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  50 loss :  7.367274947788404 acc:  0.17283344126119288\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  51 loss :  7.349133740300718 acc:  0.17316838979076882\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  52 loss :  7.412902811299199 acc:  0.17332469910457093\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  53 loss :  7.335871489151664 acc:  0.17316838979076882\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  54 loss :  7.23284681983616 acc:  0.1733916888104861\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  55 loss :  7.34620475769043 acc:  0.17377129714400555\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  56 loss :  7.355639872343644 acc:  0.17298975057499497\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  57 loss :  7.379478454589844 acc:  0.17394993635977937\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  58 loss :  7.2052464692488964 acc:  0.17488779224259204\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  59 loss :  7.256720522175664 acc:  0.17678583391018912\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  60 loss :  7.192981367525847 acc:  0.17649554518455665\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  61 loss :  7.317437151203984 acc:  0.17665185449835874\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  62 loss :  7.218410595603611 acc:  0.17765670008708662\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  63 loss :  7.173682357953942 acc:  0.17729942165553894\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9113162901812932 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'Mish', 'amsgrad': True, 'batch_size': 16, 'beta_1': 0.8195006403459264, 'beta_2': 0.9651395145925041, 'concatenate_features': False, 'd_model': 504, 'dropout': 0.6294007100595713, 'dropout_transformers': 0.5612077415522012, 'early_stopping': 10, 'encoder_only': True, 'epochs_classifcation_only': 37, 'eps': 1.0448961286717434e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 5, 'factor': 0.818588307692669, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0032457289362999514, 'lr': 0.00016054178923072675, 'dropout_lstm': 0.9113162901812932, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': False, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'abs', 'epochs_complete_problem': 43, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Mish', 'dropout_gcn': 0.43446308447827514, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 2.9470645130844586e-05}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  8.70336141424664 acc:  0.005381506375187013\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  8.234067262229273 acc:  0.008105754415738116\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  7.91709880505578 acc:  0.008864971082776946\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  7.641839916423216 acc:  0.01429113726190742\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  7.62467738329354 acc:  0.019494004421320592\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  7.567134663210076 acc:  0.02087845834356787\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  7.480050361762612 acc:  0.021079427461313444\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  7.391505710149215 acc:  0.028403635308041\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  7.29078081906852 acc:  0.033494852957595515\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  7.096355890823623 acc:  0.039412276980104057\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  7.129374366695598 acc:  0.04421320590402608\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  6.981876939030017 acc:  0.06754795346448429\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  7.034171395382638 acc:  0.07545273876247684\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  6.8470185247518245 acc:  0.0884487417100239\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  6.719115184525312 acc:  0.09648750641984681\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  6.6637717748092395 acc:  0.10704955005247527\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  6.5040009142988815 acc:  0.11256503583949266\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  6.498157396154888 acc:  0.11834848045017082\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  6.50254620535899 acc:  0.11935332603889869\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  6.390473398111634 acc:  0.1322376794765871\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  6.50258703555091 acc:  0.1369492887926222\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  6.144875469854322 acc:  0.14362592948216957\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  6.184439675282624 acc:  0.15021325056383003\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  6.0292234501596225 acc:  0.14559096085568185\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  6.276622400445453 acc:  0.16032869615702386\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  6.079586045216706 acc:  0.16222673782462094\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  5.978790250875182 acc:  0.1740392559676663\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  6.012644610162509 acc:  0.18274791773664115\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  5.971098616971808 acc:  0.18223432999129133\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  5.889684079057079 acc:  0.17589263783131992\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  5.795195151183565 acc:  0.19393519862447803\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  5.763634883751304 acc:  0.19598954960587722\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  5.82553491754047 acc:  0.1974409932340397\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  5.935154785544185 acc:  0.20451957215907823\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  5.874698578301123 acc:  0.20400598441372841\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  35 loss :  5.628771571789757 acc:  0.20230891186387692\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  36 loss :  5.663948544001175 acc:  0.21521559520353706\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  37 loss :  7.430521172992254 acc:  0.2131165844181944\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  38 loss :  7.3518323009297 acc:  0.2156845231449434\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  39 loss :  7.018003584974903 acc:  0.2161981108902932\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  40 loss :  7.170746973005392 acc:  0.22262912265815152\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  41 loss :  6.8497321726912155 acc:  0.22463881383560727\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  42 loss :  7.121042138439114 acc:  0.22562132952236338\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  43 loss :  6.861876819093348 acc:  0.22809994864122546\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  44 loss :  6.473363092390158 acc:  0.23046691825022889\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  45 loss :  6.704283989081948 acc:  0.23089118638769177\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  46 loss :  6.416625378495556 acc:  0.23252126923162808\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  47 loss :  6.786651296130682 acc:  0.2317397226626175\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  48 loss :  6.6077193648128185 acc:  0.23488823884063148\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  49 loss :  6.450473437875004 acc:  0.23403970256570575\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  50 loss :  6.784586801367291 acc:  0.23926489962709063\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  51 loss :  6.35562602544235 acc:  0.23616104325302012\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  52 loss :  6.527948201712915 acc:  0.2388629613915995\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  53 loss :  6.515408160322804 acc:  0.23948819864680795\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  54 loss :  6.7327374442149015 acc:  0.2428376839425675\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  55 loss :  6.299765958624371 acc:  0.2440211687470692\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  56 loss :  6.395296274605444 acc:  0.24723667463099838\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  57 loss :  6.464131072416144 acc:  0.24783958198423509\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  58 loss :  6.420580217393778 acc:  0.24864345845521738\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  59 loss :  6.3350909847324175 acc:  0.24810754080789585\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  60 loss :  6.300328198125807 acc:  0.25210459326083556\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  61 loss :  6.343212434801004 acc:  0.2504745104168993\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  62 loss :  6.0806303266751565 acc:  0.2529977893397048\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  63 loss :  6.328797405048952 acc:  0.2522609025746377\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  64 loss :  6.376872644586078 acc:  0.2544938927718107\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  65 loss :  6.391936314308037 acc:  0.2560793158118036\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  66 loss :  6.2415614047292935 acc:  0.254471562869839\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  67 loss :  6.233772326323946 acc:  0.2577987182636268\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  68 loss :  6.283977847988322 acc:  0.25755308934193777\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  69 loss :  5.86078068361444 acc:  0.262755956501351\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  70 loss :  6.088488481812558 acc:  0.26128218297121675\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  71 loss :  6.118044077339819 acc:  0.26358216287430497\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  72 loss :  6.300803123894385 acc:  0.26342585356050285\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  73 loss :  6.261713375479488 acc:  0.2646316682669763\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  74 loss :  6.4115247403161 acc:  0.2661947614049974\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  75 loss :  6.078339140293962 acc:  0.26628408101288437\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  76 loss :  6.16107745897972 acc:  0.26570350356161937\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  77 loss :  6.126307972406937 acc:  0.26731125650358395\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  78 loss :  6.022708593788794 acc:  0.26670834915034725\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  79 loss :  5.859674453735352 acc:  0.26773552464104683\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.026650651535130444 and num_layers=1\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=76060)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8701615988023331, 'beta_2': 0.963314507901611, 'concatenate_features': False, 'd_model': 384, 'dropout': 0.8022628204614237, 'dropout_transformers': 0.3468875667283515, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 35, 'eps': 2.9202715362438606e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 10, 'factor': 0.45715982066054545, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.014548296637890557, 'lr': 0.0009993664513646416, 'dropout_lstm': 0.026650651535130444, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'PReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.5035551890626084, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 0.00018723701569436625}\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  0 loss :  7.3202621973477875 acc:  0.05832570395015966\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  1 loss :  5.924424618941087 acc:  0.20137105598106425\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  2 loss :  4.728970336914062 acc:  0.27488109327200055\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  3 loss :  4.119013720292312 acc:  0.32755733202331244\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  4 loss :  3.781606670526358 acc:  0.34412611928633635\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  5 loss :  3.5522217438771175 acc:  0.3642007011589219\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  6 loss :  3.3837344132936917 acc:  0.3779559207735078\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  7 loss :  3.2562116091067974 acc:  0.3834714065605252\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  8 loss :  3.1567734443224396 acc:  0.3923810374472456\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  9 loss :  3.0812399240640493 acc:  0.39876738941116047\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  10 loss :  2.944181310213529 acc:  0.41359444432038944\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  11 loss :  2.8871453046798705 acc:  0.41794877520487683\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  12 loss :  2.8564785718917847 acc:  0.417189558537838\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  13 loss :  2.827211689949036 acc:  0.42020409530402164\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  14 loss :  2.798796772956848 acc:  0.41942254873501106\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  15 loss :  2.7733198697750385 acc:  0.42221378648147734\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  16 loss :  2.751002834393428 acc:  0.42134292030457987\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  17 loss :  2.726528347455538 acc:  0.42473706540428285\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  18 loss :  2.705274928533114 acc:  0.4230846526583748\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  19 loss :  2.6820894938248854 acc:  0.4248263850121698\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  20 loss :  2.6675840872984664 acc:  0.42469240560033944\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  21 loss :  2.648803683427664 acc:  0.4269477256994842\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  22 loss :  2.6306934448388906 acc:  0.42826518991581625\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  23 loss :  2.618711809011606 acc:  0.4298729428577809\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  24 loss :  2.599801186414865 acc:  0.42458075609048074\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  25 loss :  2.5858084770349357 acc:  0.4263224884442757\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  26 loss :  2.5654943227767943 acc:  0.42822053011187283\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  27 loss :  2.555638753450834 acc:  0.428533148739477\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  28 loss :  2.5402659049400915 acc:  0.4277516021704665\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  29 loss :  2.494804728948153 acc:  0.4343165933501552\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  30 loss :  2.484565622989948 acc:  0.4358573565862046\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  31 loss :  2.4715306978959304 acc:  0.43384766540874886\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  32 loss :  2.4672104156934296 acc:  0.43599133599803497\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  33 loss :  2.460823079255911 acc:  0.43563405756648726\n",
            "\u001b[36m(eval_config pid=76060)\u001b[0m epoch:  34 loss :  2.453049379128676 acc:  0.4355000781546569\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-09 22:18:37,651\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-09 22:18:51,871\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-09 22:18:51,873\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_9        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_9\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_9`\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9475094142187104 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8824047496624504, 'beta_2': 0.9927983151159079, 'concatenate_features': False, 'd_model': 576, 'dropout': 0.8970708071925857, 'dropout_transformers': 0.6855450086209975, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 35, 'eps': 5.40901894743974e-09, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 0, 'factor': 0.3137747817615456, 'patience': 7, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.01386260635607955, 'lr': 0.000371297300037523, 'dropout_lstm': 0.9475094142187104, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'PReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 36, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.2580532694730291, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 0.0038425421299021633}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  7.640206050872803 acc:  0.004242681374628765\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  7.407889080047608 acc:  0.005604805394904317\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  7.405951118469238 acc:  0.005470825983073934\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.3464274406433105 acc:  0.007636826474331778\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.316802733285087 acc:  0.013196972065292634\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.2955992289951865 acc:  0.015742580890069892\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.29158217566354 acc:  0.022463881383560726\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  7.241721071515765 acc:  0.03901033874461291\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  7.194464710780553 acc:  0.051850032378357856\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  9 loss :  7.139919349125454 acc:  0.05316749659468995\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  10 loss :  7.14099428994315 acc:  0.05962083826452002\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  11 loss :  7.103768130711146 acc:  0.06656543777772815\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  12 loss :  7.113984216962542 acc:  0.07040618091686578\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  13 loss :  7.074144894736154 acc:  0.0781546569010562\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  14 loss :  7.0821216038295205 acc:  0.08047696670611616\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  15 loss :  7.069460487365722 acc:  0.08525556572806646\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  16 loss :  7.021039649418422 acc:  0.08842641180805216\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  17 loss :  7.048650891440255 acc:  0.08978853582832771\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  18 loss :  7.015937287466866 acc:  0.09121764955451846\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  19 loss :  7.003067915780203 acc:  0.09240113435902017\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  20 loss :  7.0091263226100375 acc:  0.09273608288859612\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  21 loss :  7.0391454560416085 acc:  0.0948574235759105\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  22 loss :  6.984711224692209 acc:  0.09635352700801643\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  23 loss :  6.974900899614607 acc:  0.09597391867449702\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  24 loss :  6.9746860367911205 acc:  0.09751468191054641\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  25 loss :  6.997973414829799 acc:  0.09760400151843333\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  26 loss :  6.981960187639509 acc:  0.09805059955786793\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  27 loss :  6.968867778778076 acc:  0.0988991358327937\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  28 loss :  6.95713597706386 acc:  0.099122434852511\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  29 loss :  6.996730899810791 acc:  0.10028358975504098\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  30 loss :  6.9628414562770296 acc:  0.10095348681419289\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  31 loss :  6.981391307285854 acc:  0.10124377553982539\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  32 loss :  6.979193605695452 acc:  0.10157872406940134\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  33 loss :  6.948317813873291 acc:  0.10202532210883594\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  34 loss :  6.914359515053885 acc:  0.10193600250094902\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.021010891190155454 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8598068368248964, 'beta_2': 0.9785275021562171, 'concatenate_features': False, 'd_model': 912, 'dropout': 0.9544745612272543, 'dropout_transformers': 0.37782149523897424, 'early_stopping': 10, 'encoder_only': False, 'epochs_classifcation_only': 40, 'eps': 8.220663910505923e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 10, 'factor': 0.8676273118421395, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.3329360399021074, 'lr': 0.00010536127088711697, 'dropout_lstm': 0.021010891190155454, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'PReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 48, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 3, 'num_layers_transformer': 4, 'optimizer': 'Adam', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.3851592371582882, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 0.04260886729719352}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  9.104238510131836 acc:  0.0017640622557666972\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  8.434385380846388 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  8.14942741394043 acc:  0.005716454904762968\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.926721156911647 acc:  0.003572784315476855\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.7817882578423685 acc:  0.004979568139695867\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.655242595266788 acc:  0.0012058147064734385\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.594347527686586 acc:  0.005068887747582788\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  7.559822407174618 acc:  0.005560145590960856\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  7.5197251908322595 acc:  0.005537815688989125\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  9 loss :  7.475329642600202 acc:  0.003684433825335507\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  10 loss :  7.46260641990824 acc:  0.0072795480427840925\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  11 loss :  7.417023394970184 acc:  0.006721300493490834\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  12 loss :  7.405463005634064 acc:  0.0075028470625013955\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  13 loss :  7.369841666931801 acc:  0.009892146573476543\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  14 loss :  7.386815811725373 acc:  0.009244579416296363\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  15 loss :  7.347054653979362 acc:  0.009177589710381172\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  16 loss :  7.349358771709686 acc:  0.005068887747582788\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  17 loss :  7.338774772400551 acc:  0.006587321081660451\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  18 loss :  7.310314543703769 acc:  0.016613447066967376\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  19 loss :  7.301030311178653 acc:  0.018020230891186387\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  20 loss :  7.2914184306530245 acc:  0.017752272067525623\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  21 loss :  7.26607531689583 acc:  0.015072683830917982\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  22 loss :  7.273458075016103 acc:  0.020923118147511334\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  23 loss :  7.273020013849786 acc:  0.012817363731773217\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  24 loss :  7.277585820948824 acc:  0.007078578925038519\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  25 loss :  7.253484360715176 acc:  0.016926065694571602\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  26 loss :  7.250088671420483 acc:  0.014514436281624724\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  27 loss :  7.2310738259173455 acc:  0.015429962262465667\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  28 loss :  7.227205154743601 acc:  0.013844539222472814\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  29 loss :  7.233718608288055 acc:  0.010517383828684992\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  30 loss :  7.230246746793706 acc:  0.010294084808967688\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  31 loss :  7.207019602998774 acc:  0.012750374025858026\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  32 loss :  7.21869958715236 acc:  0.009043610298550789\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8766752426285469, 'beta_2': 0.9819900209191422, 'concatenate_features': False, 'd_model': 408, 'dropout': 0.7903064943261415, 'dropout_transformers': 0.4893873678280658, 'early_stopping': 5, 'encoder_only': False, 'epochs_classifcation_only': 43, 'eps': 3.1049572260057114e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 3, 'factor': 0.4452933750330022, 'patience': 6, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0975045360752773, 'lr': 0.0010350640222187867, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.5040368125418767, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 0.010992342112632645}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  7.495393405526372 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  7.309416641623287 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  7.317930617574918 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.290154473256257 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.3006257768404685 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.295822224374545 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.2898638369673385 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8516303704396007 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.9036977653740202, 'beta_2': 0.976214786702643, 'concatenate_features': False, 'd_model': 696, 'dropout': 0.996217722760673, 'dropout_transformers': 0.6194453420714159, 'early_stopping': 8, 'encoder_only': False, 'epochs_classifcation_only': 28, 'eps': 2.141866523072952e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 8, 'factor': 0.46715339023594155, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.033549643065378915, 'lr': 0.00010570454105908996, 'dropout_lstm': 0.8516303704396007, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'SELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 24, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 6, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Tanhshrink', 'dropout_gcn': 0.28792435019112644, 'hidden_channels': 1024, 'layer_type': 'GAT', 'norm': 'BatchNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 0.00017876466383747475}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  25.927382925282355 acc:  0.0003126186276042248\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  23.238969222359035 acc:  0.0001563093138021124\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8108919921231444 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Sigmoid', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8687923316096355, 'beta_2': 0.9997969725489848, 'concatenate_features': False, 'd_model': 792, 'dropout': 0.9482870098579141, 'dropout_transformers': 0.343153657240167, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 47, 'eps': 6.54715730432111e-09, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.88752681241732, 'scheduler': 'ExponentialLR', 'lr': 4.468485354438369e-05, 'dropout_lstm': 0.8108919921231444, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Mish', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'LeakyReLU', 'dropout_gcn': 0.16967603725491354, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 0.0002702687837242428}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  8.397099872735831 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  7.417721917079045 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  7.337662175985483 acc:  0.002925217158296675\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.312476712006789 acc:  0.0024786191188620682\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.293382666661189 acc:  0.002389299510975147\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.284371280670166 acc:  0.002411629412946877\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.27587509888869 acc:  0.002545608824777259\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  7.267981848349938 acc:  0.002277650001116495\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  7.262456413415762 acc:  0.002411629412946877\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.12320742851133296 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Tanh', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8559984491168217, 'beta_2': 0.9893861649669662, 'concatenate_features': False, 'd_model': 360, 'dropout': 0.28084917874655, 'dropout_transformers': 0.51205112555445, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 33, 'eps': 1.493875453092755e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 7, 'factor': 0.7347055689503474, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.15094100764765525, 'lr': 0.0005584121962397815, 'dropout_lstm': 0.12320742851133296, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'GELU', 'dropout_gcn': 0.42696571162849395, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 10, 'use_gcn': True, 'weight_decay': 0.25588025061346487}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  7.191231654240535 acc:  0.06433244758055512\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  6.254561802057119 acc:  0.14911908536721524\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  5.3648667262150695 acc:  0.20085746823571443\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  4.70507041490995 acc:  0.24643279816001606\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  4.245410174589891 acc:  0.2744568251345377\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  3.916651151730464 acc:  0.3035526874037023\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  3.671072244644165 acc:  0.32112632025545407\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  3.484586893595182 acc:  0.33360873545765135\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  3.342625845395602 acc:  0.3500658732108166\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  9 loss :  3.2298490120814396 acc:  0.3573677511555724\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  10 loss :  3.131482511300307 acc:  0.3702297746912891\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  11 loss :  3.0496859238697933 acc:  0.3815510349909564\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  12 loss :  2.968665975790757 acc:  0.38778107764106917\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  13 loss :  2.9164851702176606 acc:  0.39182278989795233\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  14 loss :  2.878609008055467 acc:  0.3952392648996271\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  15 loss :  2.8391971551454986 acc:  0.4033450193153652\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  16 loss :  2.8040355920791624 acc:  0.4043052051001496\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  17 loss :  2.7735572851621186 acc:  0.4073867315722484\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  18 loss :  2.744957373692439 acc:  0.40903914431815647\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  19 loss :  2.7156865413372335 acc:  0.41223232030011386\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  20 loss :  2.69045666547922 acc:  0.4147332693209477\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  21 loss :  2.6724619278540978 acc:  0.41480025902686285\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  22 loss :  2.647806035555326 acc:  0.4191545899113503\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  23 loss :  2.6254628584935116 acc:  0.42024875510796506\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  24 loss :  2.6065768737059374 acc:  0.4228836835406293\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  25 loss :  2.5920595150727492 acc:  0.4244467766786504\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  26 loss :  2.571566915512085 acc:  0.42569725118906726\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  27 loss :  2.5571220067831186 acc:  0.42667976687582343\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  28 loss :  2.539720604969905 acc:  0.42842149922961836\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  29 loss :  2.522743892669678 acc:  0.42877877766116607\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  30 loss :  2.4976134795408984 acc:  0.42980595315186565\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  31 loss :  2.4858786729665905 acc:  0.4331777683495969\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  32 loss :  2.4742050390977126 acc:  0.4325748609963602\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.13411077797821924 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Tanh', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8553228928451825, 'beta_2': 0.9980924791211099, 'concatenate_features': False, 'd_model': 744, 'dropout': 0.16112595841872743, 'dropout_transformers': 0.6350314779542958, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 52, 'eps': 2.750227238903908e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 7, 'factor': 0.8876562474410392, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.17922197520430164, 'lr': 6.122475491836882e-05, 'dropout_lstm': 0.13411077797821924, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'Adam', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'GELU', 'dropout_gcn': 0.1221470572207044, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 10, 'use_gcn': True, 'weight_decay': 0.038404342926012004}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  7.8221942021296575 acc:  0.008195074023625036\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  7.478827968010536 acc:  0.008708661768974835\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  7.3367087510915905 acc:  0.00652033137574526\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.292219873575064 acc:  0.006163052944197575\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.274766478171715 acc:  0.006609650983632182\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.266593071130606 acc:  0.006163052944197575\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.256818081782415 acc:  0.006609650983632182\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  7.251961062504695 acc:  0.00975816716164616\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  7.252321199270395 acc:  0.007994104905879464\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  9 loss :  7.25319621012761 acc:  0.010584373534600183\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  10 loss :  7.253575570766742 acc:  0.006163052944197575\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  11 loss :  7.255162033667931 acc:  0.008775651474890026\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  12 loss :  7.256732518856342 acc:  0.006855279905321215\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  13 loss :  7.257895099199735 acc:  0.006743630395462564\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  14 loss :  7.253828345812284 acc:  0.005962083826452002\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  15 loss :  7.253502434950608 acc:  0.006765960297434294\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  16 loss :  7.2550741635836085 acc:  0.004086372060826653\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'PReLU', 'activation_transformers': 'Tanh', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8862537412436345, 'beta_2': 0.9868944916690819, 'concatenate_features': False, 'd_model': 624, 'dropout': 0.28786927850241056, 'dropout_transformers': 0.5615329842408041, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 33, 'eps': 1.6098703943417517e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 6, 'factor': 0.7377567772060345, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.05887923881705194, 'lr': 2.6085032100555506e-06, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'GELU', 'dropout_gcn': 0.4208398615359763, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 10, 'use_gcn': True, 'weight_decay': 0.23550948189817375}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  8.082033333411584 acc:  0.0009155259808409441\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  8.038635518000676 acc:  0.0015407632360493937\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  7.998344641465407 acc:  0.002009691177455731\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.95897206893334 acc:  0.0022553200991447648\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.922237238517174 acc:  0.0026349284326641804\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.885054349899292 acc:  0.002679588236607641\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.85129930789654 acc:  0.0025679387267489896\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  7.818648925194373 acc:  0.0029475470602684053\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  7.786801528930664 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  9 loss :  7.759291249055129 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  10 loss :  7.731188660401564 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  11 loss :  7.709351400228647 acc:  0.0037514235312506978\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  12 loss :  7.690739037440373 acc:  0.003684433825335507\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  13 loss :  7.672919574150672 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  14 loss :  7.656802452527559 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  15 loss :  7.640757120572603 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  16 loss :  7.624912349994366 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  17 loss :  7.610625593478863 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  18 loss :  7.595863386300894 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  19 loss :  7.5825871834388145 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30171661471178096 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Tanh', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.892830429581783, 'beta_2': 0.9892761592826943, 'concatenate_features': False, 'd_model': 456, 'dropout': 0.1128698991348488, 'dropout_transformers': 0.5125275377178645, 'early_stopping': 8, 'encoder_only': False, 'epochs_classifcation_only': 55, 'eps': 1.2100431512882628e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 5, 'factor': 0.6904854796571124, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.01381412308060531, 'lr': 4.293922169908989e-06, 'dropout_lstm': 0.30171661471178096, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardtanh', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softplus', 'dropout_gcn': 0.3630518131969, 'hidden_channels': 2048, 'layer_type': 'GAT', 'norm': 'BatchNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 0.13620178849849104}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  8.02567659157973 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  7.933180720989521 acc:  0.002232990197173034\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  7.851393222808838 acc:  0.002992206864211866\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.776935228934655 acc:  0.0031708460799857088\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.706376031728891 acc:  0.003126186276042248\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.64034025485699 acc:  0.003371815197731282\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.578568407205435 acc:  0.0033494852957595515\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  7.526739043455858 acc:  0.0033941450997030122\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  7.481501296850351 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  9 loss :  7.444561921633207 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  10 loss :  7.413605807377742 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  11 loss :  7.387705392103928 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  12 loss :  7.369206193777231 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  13 loss :  7.355286785272452 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  14 loss :  7.34515238908621 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  15 loss :  7.334870287088248 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  16 loss :  7.325610050788293 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.23249375508271128 and num_layers=1\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=126928)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Tanh', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.9000784247753183, 'beta_2': 0.994419773258422, 'concatenate_features': False, 'd_model': 360, 'dropout': 0.09573193273717229, 'dropout_transformers': 0.730393667969016, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 49, 'eps': 8.900838991520083e-09, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 7, 'factor': 0.8232881712995168, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.023501326391796726, 'lr': 7.206581341987176e-06, 'dropout_lstm': 0.23249375508271128, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 48, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'GELU', 'dropout_gcn': 0.29028960907199913, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 0.05903838719551293}\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  0 loss :  8.064864605031115 acc:  0.0004912578433780676\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  1 loss :  8.011340780461088 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  2 loss :  7.969317730436933 acc:  0.001362124020275551\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  3 loss :  7.931325110983341 acc:  0.0017194024518232365\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  4 loss :  7.892988113646812 acc:  0.0021883303932295735\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  5 loss :  7.855961373511781 acc:  0.0029028872563249446\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  6 loss :  7.823602615518773 acc:  0.002925217158296675\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  7 loss :  7.791512895137705 acc:  0.0033941450997030122\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  8 loss :  7.755153808187931 acc:  0.003818413237165889\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  9 loss :  7.723710344192829 acc:  0.004465980394346068\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  10 loss :  7.700246232621213 acc:  0.004733939218006833\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  11 loss :  7.671404808125597 acc:  0.004599959806176451\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  12 loss :  7.6484223528111235 acc:  0.004689279414063372\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  13 loss :  7.619550573064926 acc:  0.004332000982515687\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  14 loss :  7.595727879950341 acc:  0.004309671080543956\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  15 loss :  7.5725673310300134 acc:  0.004287341178572226\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  16 loss :  7.539489715657336 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=126928)\u001b[0m epoch:  17 loss :  7.524652156424015 acc:  0.0037960833351941585\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-10 01:48:16,070\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-10 01:48:30,077\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-10 01:48:30,079\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_10       |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_10\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_10`\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.07475668757368548 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'ELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.9242861878570409, 'beta_2': 0.9848202215943959, 'concatenate_features': False, 'd_model': 864, 'dropout': 0.254553022469161, 'dropout_transformers': 0.46119417767685733, 'early_stopping': 8, 'encoder_only': False, 'epochs_classifcation_only': 38, 'eps': 5.5651446218037955e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 8, 'factor': 0.6662698366544038, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.22651300929628237, 'lr': 1.9459430860057303e-05, 'dropout_lstm': 0.07475668757368548, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softmin', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'PReLU', 'dropout_gcn': 0.5234049692068036, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 10, 'use_gcn': True, 'weight_decay': 0.21256113025685341}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  7.759177417021531 acc:  0.008172744121653306\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  7.36376659319951 acc:  0.012147466672621307\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  7.20421016399677 acc:  0.013308621575151286\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  7.124053140786978 acc:  0.021124087265256906\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  7.059629902472863 acc:  0.03566085344885336\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  7.001175113824698 acc:  0.04740638188598352\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  6.946286058425903 acc:  0.05874997208762254\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  6.8950100458585295 acc:  0.06801688140589063\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  6.845257278589102 acc:  0.07531875935064646\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  6.796659634663508 acc:  0.08061094611794654\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  6.749487102948702 acc:  0.08920795837706272\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  6.702884905154889 acc:  0.0944108255364759\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  6.6600861622737 acc:  0.09740303240068776\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  6.627040529251099 acc:  0.10260589956010092\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  6.597581320542556 acc:  0.1043476319138959\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  6.565692160679744 acc:  0.10814371524909006\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  6.537089575254 acc:  0.11100194270147154\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  6.503621567212618 acc:  0.11403880936962686\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  6.478569635978112 acc:  0.11660674809637586\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  6.446247581335214 acc:  0.11966594466650292\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  20 loss :  6.419902852865365 acc:  0.12125136770649576\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  21 loss :  6.386393018869254 acc:  0.12395328584507515\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  22 loss :  6.357134477908795 acc:  0.12629792555210684\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  23 loss :  6.332310089698205 acc:  0.12792800839604315\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  24 loss :  6.298032210423396 acc:  0.13205904026081325\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  25 loss :  6.273455054943378 acc:  0.1343813500658732\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  26 loss :  6.2389234506166895 acc:  0.1373288971261416\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  27 loss :  6.20902109512916 acc:  0.13811044369515219\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  28 loss :  6.179335953639104 acc:  0.14233079516780922\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  29 loss :  6.15323125399076 acc:  0.14396087801174554\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  30 loss :  6.12278110063993 acc:  0.1468860951700422\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  31 loss :  6.096165242561927 acc:  0.1484268584060916\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  32 loss :  6.078208402486948 acc:  0.149967621642141\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  33 loss :  6.057268890967736 acc:  0.15126275595650135\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  34 loss :  6.039437789183396 acc:  0.15266953978072037\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  35 loss :  6.017193618187537 acc:  0.1544782618404305\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  36 loss :  5.999941759843093 acc:  0.15501417948775204\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  37 loss :  5.981837980563824 acc:  0.15655494272380144\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2002548490789817 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'ReLU6', 'activation_transformers': 'Softplus', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8720045590011754, 'beta_2': 0.9957808174762333, 'concatenate_features': False, 'd_model': 504, 'dropout': 0.22900578129549026, 'dropout_transformers': 0.7832947259328129, 'early_stopping': 10, 'encoder_only': False, 'epochs_classifcation_only': 44, 'eps': 4.15002752724105e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 4, 'factor': 0.893867409516335, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.09562508717621973, 'lr': 0.0002280841554360718, 'dropout_lstm': 0.2002548490789817, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'LeakyReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'Adam', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Mish', 'dropout_gcn': 0.21654204063615895, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 0.0037261049287238193}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  7.284444753940289 acc:  0.07326440836924726\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  5.7140519362229565 acc:  0.2364959917825961\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  4.32721440241887 acc:  0.3057856776008753\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  3.751308611723093 acc:  0.33648929281200457\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  3.475033063154954 acc:  0.3569211531161378\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  3.316038957008949 acc:  0.3645133197865261\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  3.200558035190289 acc:  0.3797423129312462\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  3.1146265561764057 acc:  0.39175580019203715\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  3.054539286173307 acc:  0.3934752026438604\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  3.002610969543457 acc:  0.3965343992139875\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  2.961814014728253 acc:  0.3988567090190474\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  2.903918981552124 acc:  0.408994484514213\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  2.87849767941695 acc:  0.40539937029676437\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  2.856221077992366 acc:  0.4081236183373155\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  2.847432943490835 acc:  0.4073197418663332\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  2.8279792638925407 acc:  0.40807895853337206\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  2.8154542244397676 acc:  0.41082553647589487\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  2.8015600534585805 acc:  0.40769935019985265\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  2.791838889855605 acc:  0.40696246343478554\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  2.7839223531576303 acc:  0.4114954335350468\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  20 loss :  2.7703512833668635 acc:  0.4068508139249269\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  21 loss :  2.7648873879359317 acc:  0.40955273206350623\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  22 loss :  2.7615828330700216 acc:  0.4094187526516759\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  23 loss :  2.7490224343079785 acc:  0.41576044481164726\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  24 loss :  2.7438824782004723 acc:  0.4095304021615345\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  25 loss :  2.7173029276040883 acc:  0.41495656834066497\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  26 loss :  2.708974106495197 acc:  0.4162070428510819\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  27 loss :  2.702517691025367 acc:  0.4110711653975839\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  28 loss :  2.700275569695693 acc:  0.4154924859879865\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  29 loss :  2.7003265142440798 acc:  0.41220999039814216\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  30 loss :  2.697280076833872 acc:  0.4137060938302481\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  31 loss :  2.6941228829897366 acc:  0.41395172275193715\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  32 loss :  2.69394775720743 acc:  0.41285755755532233\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  33 loss :  2.681740324313824 acc:  0.4154924859879865\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  34 loss :  2.6854944375845102 acc:  0.4142420114775696\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  35 loss :  2.683610719900865 acc:  0.4117857222606793\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  36 loss :  2.677700576415429 acc:  0.4144653104972869\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'Softshrink', 'activation_transformers': 'SELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8771275330096263, 'beta_2': 0.9806653207604392, 'concatenate_features': False, 'd_model': 408, 'dropout': 0.016259578112159778, 'dropout_transformers': 0.4313914444694629, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 25, 'eps': 3.032786419501319e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 6, 'factor': 0.5628425851195804, 'patience': 7, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.008847691408119049, 'lr': 0.0005155403321929072, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 3, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softsign', 'dropout_gcn': 0.3651928913192898, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'InstanceNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 0.01667494456692459}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  7.348583151743962 acc:  0.01042806422079807\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  6.94128370651832 acc:  0.017394993635977938\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  6.618478118456327 acc:  0.037983163253913314\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  6.265900149712196 acc:  0.06250139561887323\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  5.925738499714778 acc:  0.08835942210213697\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  5.591390859163725 acc:  0.11787955250876449\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  5.233544180943416 acc:  0.1461492084049751\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  4.861142763724694 acc:  0.174396534399214\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  4.524373173713684 acc:  0.20858361431793315\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  4.225626314603366 acc:  0.23102516579952215\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  4.066672156407283 acc:  0.24551727217917513\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  3.927698982678927 acc:  0.26679766875823413\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  3.816737393232492 acc:  0.27258111336891233\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  3.7130535969367395 acc:  0.2857334256302615\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  3.618891682991615 acc:  0.3003595114217449\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  3.531292062539321 acc:  0.3115021325056383\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  3.457052408731901 acc:  0.319228278587857\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  3.3893263688454263 acc:  0.32874081682781414\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  3.3260334124931923 acc:  0.3403300359511422\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  3.2697038393754227 acc:  0.34886005850434315\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  20 loss :  3.2122668064557587 acc:  0.3574794006654311\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  21 loss :  3.159520638906039 acc:  0.3659647634146886\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  22 loss :  3.1130518491451555 acc:  0.36891231047495704\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  23 loss :  3.060767254462609 acc:  0.3801665810687091\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  24 loss :  3.032102527985206 acc:  0.3821316124422214\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5557660126118771 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'Softmin', 'activation_transformers': 'ReLU', 'amsgrad': False, 'batch_size': 128, 'beta_1': 0.8620987032789871, 'beta_2': 0.9930734080595738, 'concatenate_features': False, 'd_model': 336, 'dropout': 0.2826817688854423, 'dropout_transformers': 0.3896252565879134, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 41, 'eps': 3.181880853634432e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 10, 'factor': 0.7703536844050567, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.022404252909315175, 'lr': 1.3854675898747718e-05, 'dropout_lstm': 0.5557660126118771, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Tanh', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 36, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softmin', 'dropout_gcn': 0.3307007928784683, 'hidden_channels': 1024, 'layer_type': 'GAT', 'norm': 'BatchNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 2.2123826322765952e-08}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  8.02131734575544 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  8.02000732421875 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  8.019162368774413 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  8.018189021519252 acc:  0.001027175490699596\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  8.016600717817035 acc:  0.000714556863095371\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  8.016343525477819 acc:  0.000781546569010562\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  8.015680939810617 acc:  0.0005582475492932586\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  8.015081460135324 acc:  0.0006475671571801799\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  8.014339229038784 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  8.013446235656739 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  8.012598855154854 acc:  0.0011834848045017081\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  8.011887632097517 acc:  0.0011834848045017081\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  8.011221613202776 acc:  0.0012058147064734385\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  8.011077117919921 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  8.010179001944406 acc:  0.001116495098586517\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  8.010088266645159 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  8.009453283037459 acc:  0.0010941651966147868\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  8.009712491716657 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  8.009173665727888 acc:  0.0011834848045017081\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  8.008152198791503 acc:  0.0011834848045017081\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'SiLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8510868787137222, 'beta_2': 0.9876584708394055, 'concatenate_features': False, 'd_model': 264, 'dropout': 0.18242205405374456, 'dropout_transformers': 0.8966039202450461, 'early_stopping': 8, 'encoder_only': False, 'epochs_classifcation_only': 31, 'eps': 2.3656952633521378e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 7, 'factor': 0.51072519572211, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.05072831275414088, 'lr': 2.9735420522528812e-05, 'dropout_lstm': 0.018915880663376856, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softsign', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 0.005651918986261523}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.018915880663376856 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  8.214815475571324 acc:  0.0012951343143603599\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  7.934879383570712 acc:  0.002277650001116495\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  7.746781389478227 acc:  0.0031485161780139786\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  7.627096572392423 acc:  0.0037737534332224283\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  7.533872275285318 acc:  0.00455530000223299\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  7.456700439184484 acc:  0.005024227943639327\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  7.414084750162044 acc:  0.005493155885045665\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  7.377550104974022 acc:  0.005895094120536811\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  7.3409549417630044 acc:  0.006810620101377755\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  7.318636323364688 acc:  0.007569836768416586\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  7.288101814162563 acc:  0.008306723533483688\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  7.26022563853734 acc:  0.009177589710381172\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  7.234957204738134 acc:  0.009534868141928858\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  7.234013946963028 acc:  0.009780497063617891\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  7.219283876284747 acc:  0.010539713730656722\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  7.221004136851136 acc:  0.010963981868119598\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  7.198782618616669 acc:  0.011499899515441126\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  7.188458080023107 acc:  0.012013487260790925\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  7.173263402052329 acc:  0.012504745104168992\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  7.171687005271374 acc:  0.013241631869236095\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  20 loss :  7.1567502156109875 acc:  0.014067838242190116\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  21 loss :  7.14306205427143 acc:  0.01502802402697452\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  22 loss :  7.1345061449937415 acc:  0.01560860147823951\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  23 loss :  7.121380120935575 acc:  0.01625616863541969\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  24 loss :  7.124521725614306 acc:  0.01688140589062814\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  25 loss :  7.107845890689903 acc:  0.017796931871469083\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  26 loss :  7.0991648351642445 acc:  0.018645468146394836\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  27 loss :  7.0807902107776055 acc:  0.01933769510751848\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  28 loss :  7.075617937974527 acc:  0.019672643637094433\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  29 loss :  7.062830528742831 acc:  0.0196503137351227\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  30 loss :  7.017885026797442 acc:  0.019627983833150973\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16785058766552297 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'LeakyReLU', 'activation_transformers': 'Hardsigmoid', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.9102518460508129, 'beta_2': 0.9901804235603807, 'concatenate_features': False, 'd_model': 984, 'dropout': 0.0701904807232056, 'dropout_transformers': 0.4168514606948802, 'early_stopping': 10, 'encoder_only': False, 'epochs_classifcation_only': 61, 'eps': 7.866841414417332e-09, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 5, 'factor': 0.29180304109591315, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0010203347728637905, 'lr': 0.0003647586069115523, 'dropout_lstm': 0.16785058766552297, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 6, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'ReLU', 'dropout_gcn': 0.4782668370542624, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'LayerNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 0.002115208253228192}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  6.393441728445199 acc:  0.25893754326418505\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  3.753106480378371 acc:  0.3446397070316861\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  2.9613944567166843 acc:  0.38101511734363486\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  2.6381507176619308 acc:  0.39586450215483554\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  2.428978400964003 acc:  0.40624790657169013\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  2.272691022432767 acc:  0.4134381350065873\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  2.145963868728051 acc:  0.41223232030011386\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  2.0342620207713202 acc:  0.4146886095170042\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  1.9295391504581159 acc:  0.4119643614764531\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  1.8401237827080947 acc:  0.4136614340263046\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  1.6303441487825834 acc:  0.42098564187303217\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  1.5486461391815773 acc:  0.418596342362057\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  1.5041834336060744 acc:  0.4187526516758591\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  1.4645665994057289 acc:  0.41808275461670724\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  1.4293096560698288 acc:  0.4142643413795413\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  1.3936628956061143 acc:  0.41551481588995826\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  1.3594266469662006 acc:  0.4129022173592658\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  1.3251378472034747 acc:  0.41167407275082063\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  1.293303110049321 acc:  0.4095304021615345\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  1.2598340685550984 acc:  0.40580130853225554\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  20 loss :  1.2287823942991403 acc:  0.4079673090235134\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.40974582714222696 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'LeakyReLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8393691652653288, 'beta_2': 0.9831150552356388, 'concatenate_features': False, 'd_model': 672, 'dropout': 0.352621821362294, 'dropout_transformers': 0.5799541827426433, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 22, 'eps': 4.698428901990161e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.43246423136802375, 'scheduler': 'StepLR', 'step_size': 17, 'lr': 0.0008442427918012912, 'dropout_lstm': 0.40974582714222696, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'Adam', 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Hardshrink', 'dropout_gcn': 0.054033086621460225, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 10, 'use_gcn': True, 'weight_decay': 1.3008148205287822e-06}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  6.1157073626151455 acc:  0.2486657883571891\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  3.865840954046983 acc:  0.3348368800660965\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  3.1869531869888306 acc:  0.3711229707701583\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  2.878623784505404 acc:  0.39644507960610054\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  2.6807523232239943 acc:  0.4060469374539446\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  2.539262287433331 acc:  0.4181497443226224\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  2.4351068680103007 acc:  0.4173235379496684\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  2.351297090603755 acc:  0.4242234776589331\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  2.275524089886592 acc:  0.4203157448138803\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  2.20773999782709 acc:  0.42884576736708124\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  2.1472582945456873 acc:  0.42370988991358327\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  2.0963636453335104 acc:  0.42082933255923005\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  2.044729404266064 acc:  0.42185650804992963\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  2.0033321710733265 acc:  0.42292834334457274\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  1.9560307218478277 acc:  0.4222807761873925\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  1.9154470049417935 acc:  0.42158854922626887\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  1.8700442534226638 acc:  0.41810508451867895\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  1.6951208325532767 acc:  0.4321729227608691\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  1.6025950651902419 acc:  0.43016323158341335\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  1.5620742201805116 acc:  0.42920304579862895\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  20 loss :  1.5316278466811546 acc:  0.4273273340330036\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  21 loss :  1.5025921684045058 acc:  0.4276176227586361\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2360779832531093 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Softshrink', 'amsgrad': False, 'batch_size': 128, 'beta_1': 0.8887954072140558, 'beta_2': 0.984162987381541, 'concatenate_features': False, 'd_model': 552, 'dropout': 0.4494431179259928, 'dropout_transformers': 0.6432044758309338, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 57, 'eps': 1.3943753774307348e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 8, 'factor': 0.8232239301613836, 'patience': 6, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.11531326005257864, 'lr': 0.009569367762938107, 'dropout_lstm': 0.2360779832531093, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'ELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 12, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'ReLU6', 'dropout_gcn': 0.48424474639290127, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'PairNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 0.0005638488851405756}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  214.63038002360952 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  32.360368902033024 acc:  0.0006475671571801799\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  26.243722568858754 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  36.4094272960316 acc:  0.0002009691177455731\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  45.05716809359464 acc:  0.0003126186276042248\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  68.95044742931019 acc:  0.0\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  66.11141725019975 acc:  0.0007368867650671013\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  57.20637061379173 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  55.01330080899325 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  81.85012089122425 acc:  0.002679588236607641\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  81.26749004017223 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  74.36232202703303 acc:  0.0004465980394346069\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  78.04731473055753 acc:  0.0017417323537949668\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  73.55629452792081 acc:  0.0\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  76.68489768288352 acc:  8.931960788692137e-05\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  inf acc:  0.00022329901971730344\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'Sigmoid', 'activation_transformers': 'GELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8680257912795983, 'beta_2': 0.9910178041100014, 'concatenate_features': False, 'd_model': 576, 'dropout': 0.3056599173605442, 'dropout_transformers': 0.47330069309444267, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 49, 'eps': 2.4066771893439816e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.13940142200028732, 'scheduler': 'ExponentialLR', 'lr': 0.0017806552150896851, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 0.013138013699035623}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  7.526426040209257 acc:  0.001630082843936315\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  7.307772647417509 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  7.250646741573627 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  7.243154742167547 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  7.241356068391067 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  7.242335565273578 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  7.241198077568641 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  7.241082737996028 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  7.240963473686805 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6991245178853365 and num_layers=1\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=178501)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Softsign', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8357817840280396, 'beta_2': 0.9861782154423473, 'concatenate_features': False, 'd_model': 792, 'dropout': 0.5318853909048669, 'dropout_transformers': 0.4993910891537811, 'early_stopping': 8, 'encoder_only': False, 'epochs_classifcation_only': 33, 'eps': 5.412392665268253e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 3, 'factor': 0.45076164483768083, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.003845529110133434, 'lr': 8.39298888474893e-05, 'dropout_lstm': 0.6991245178853365, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'PReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.572153000716435, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 5.3147643085488656e-08}\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  0 loss :  7.594498521190578 acc:  0.004198021570685304\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  1 loss :  7.2459620297965355 acc:  0.015943550007815464\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  2 loss :  7.12117989588592 acc:  0.03729093629278967\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  3 loss :  6.979851480257714 acc:  0.0490811245338633\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  4 loss :  6.80947511479006 acc:  0.0722595627805194\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  5 loss :  6.55461311340332 acc:  0.10675926132684277\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  6 loss :  6.2944822392221225 acc:  0.1501909206618583\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  7 loss :  5.987948167122017 acc:  0.17343634861442958\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  8 loss :  5.740743701740847 acc:  0.20623897461090146\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  9 loss :  5.530100668891001 acc:  0.21668936873367126\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  10 loss :  5.274912624035851 acc:  0.23314650648683652\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  11 loss :  5.110024314815715 acc:  0.24696871580733762\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  12 loss :  4.909898378081241 acc:  0.2596297702253087\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  13 loss :  4.836238780264127 acc:  0.26673067905231895\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  14 loss :  4.778184276516154 acc:  0.27195587611370386\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  15 loss :  4.709082134699417 acc:  0.27849853739142083\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  16 loss :  4.61812874422235 acc:  0.2830091775897104\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  17 loss :  4.561068320678452 acc:  0.28850233347475607\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  18 loss :  4.422083717281535 acc:  0.29265569524149787\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  19 loss :  4.419652474128593 acc:  0.29482169573275574\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  20 loss :  4.35916363990913 acc:  0.2991313668132997\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  21 loss :  4.348461563304319 acc:  0.30357501730567404\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  22 loss :  4.240672669168246 acc:  0.3092021526025501\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  23 loss :  4.1994034031690175 acc:  0.31000602907353236\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  24 loss :  4.137970281859576 acc:  0.31427104035013287\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  25 loss :  4.071565531067929 acc:  0.31744188643011856\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  26 loss :  4.073313591843944 acc:  0.3196078869213764\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  27 loss :  4.0147059610334495 acc:  0.3208360315298216\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  28 loss :  3.9933377847833147 acc:  0.3228010629033339\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  29 loss :  4.003679485644325 acc:  0.32411852711966593\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  30 loss :  3.967608589237019 acc:  0.3260835584931782\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  31 loss :  3.936788320541382 acc:  0.32818256927852085\n",
            "\u001b[36m(eval_config pid=178501)\u001b[0m epoch:  32 loss :  3.944201077445079 acc:  0.32773597123908627\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-10 06:59:15,359\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-10 06:59:29,589\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-10 06:59:29,591\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_11       |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_11\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_11`\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8873517232505047 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'Tanhshrink', 'activation_transformers': 'CELU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8469359934129312, 'beta_2': 0.9920213606565464, 'concatenate_features': False, 'd_model': 720, 'dropout': 0.22432495411078623, 'dropout_transformers': 0.6846028975672626, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 35, 'eps': 1.6989818412169822e-08, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 9, 'factor': 0.40273389899008455, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.3650532798638993, 'lr': 0.00024644299201953323, 'dropout_lstm': 0.8873517232505047, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softplus', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 48, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'LogSigmoid', 'dropout_gcn': 0.43936285031566386, 'hidden_channels': 2048, 'layer_type': 'GAT', 'norm': 'InstanceNorm', 'num_layers_gcn': 10, 'use_gcn': True, 'weight_decay': 0.0009987664678401485}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  7.792983268169647 acc:  0.04421320590402608\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  6.4581188242486185 acc:  0.16524127459080454\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  5.2103945955317075 acc:  0.25561038787039725\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  4.4083699571325425 acc:  0.2958712011254271\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  3.968840147586579 acc:  0.31918361878391355\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  3.665353039477734 acc:  0.33305048790835806\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  3.4903612999205893 acc:  0.3450193153652055\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  3.258141832148775 acc:  0.3636871134135721\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  3.1829375358338052 acc:  0.3686890114552397\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  3.067139889331574 acc:  0.37453944577183307\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  10 loss :  2.994733384315004 acc:  0.3787597972444901\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  11 loss :  2.8059080052883068 acc:  0.39858875019538664\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  12 loss :  2.702452482061183 acc:  0.40537704039479266\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  13 loss :  2.7313424982923142 acc:  0.4048857825514146\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  14 loss :  2.6492554887812187 acc:  0.4091954536319586\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  15 loss :  2.6269858948727873 acc:  0.41082553647589487\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  16 loss :  2.5687905321729945 acc:  0.4160060737333363\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  17 loss :  2.5658668355738863 acc:  0.4154924859879865\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  18 loss :  2.5476125859199685 acc:  0.4169885894200924\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  19 loss :  2.4938523515741875 acc:  0.41531384677221267\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  20 loss :  2.4910907998998115 acc:  0.4191099301074068\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  21 loss :  2.4451179960940745 acc:  0.41654199138065784\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  22 loss :  2.4557951967766942 acc:  0.41886430118571777\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  23 loss :  2.449329528402775 acc:  0.41844003304825494\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  24 loss :  2.4291742304538158 acc:  0.42105263157894735\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  25 loss :  2.375414660636415 acc:  0.4238215394234419\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  26 loss :  2.3811802559710564 acc:  0.42221378648147734\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  27 loss :  2.3416567761847316 acc:  0.42315164236429004\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  28 loss :  2.3513718412277544 acc:  0.42140991001049505\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  29 loss :  2.3535215144461774 acc:  0.4238885291293571\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  30 loss :  2.2768540204839502 acc:  0.42786325168032513\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  31 loss :  2.2632882011697646 acc:  0.42884576736708124\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  32 loss :  2.27191473068075 acc:  0.42842149922961836\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  33 loss :  2.228028236551488 acc:  0.42942634481834624\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  34 loss :  2.2501721889414688 acc:  0.4293146953084876\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7591576323004351 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Softmin', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8787832546349036, 'beta_2': 0.9794614150400472, 'concatenate_features': False, 'd_model': 456, 'dropout': 0.3182887783878324, 'dropout_transformers': 0.3524351301897265, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 24, 'eps': 9.432886038376799e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.1663442169306319, 'scheduler': 'StepLR', 'step_size': 30, 'lr': 2.537589825697458e-05, 'dropout_lstm': 0.7591576323004351, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Tanhshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'RReLU', 'dropout_gcn': 0.4076992373020526, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 4.904049073845325e-07}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  7.963541030883789 acc:  0.004890248531808946\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  7.6710827838943665 acc:  0.006408681865886608\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  7.424017423606781 acc:  0.007994104905879464\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  7.3106861057051695 acc:  0.007703816180246969\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  7.270150288041815 acc:  0.007346537748699283\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  7.23057182151151 acc:  0.008530022553200992\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  7.213634605867317 acc:  0.00848536274925753\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  7.201524389795511 acc:  0.009266909318268093\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  7.179738665201578 acc:  0.010830002456289217\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  7.164168639355395 acc:  0.015831900497956814\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  10 loss :  7.133750570825784 acc:  0.021548355402719783\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  11 loss :  7.117216110229492 acc:  0.02661724315030257\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  12 loss :  7.0793520973389406 acc:  0.03159681128999844\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  13 loss :  7.062193135181105 acc:  0.03818413237165889\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  14 loss :  7.027818691299622 acc:  0.04077440100037961\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  15 loss :  7.003836516874382 acc:  0.04807627894513543\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  16 loss :  6.958812127630394 acc:  0.049036464729919835\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  17 loss :  6.923965258770679 acc:  0.05385972355581359\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  18 loss :  6.898796937551843 acc:  0.05428399169327647\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  19 loss :  6.863910192466644 acc:  0.06046937453944577\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  20 loss :  6.832404498594353 acc:  0.06301498336422304\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  21 loss :  6.794786642832928 acc:  0.06676640689547372\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  22 loss :  6.742045839148831 acc:  0.0686644485630708\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  23 loss :  6.724851820842329 acc:  0.07071879954447\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6021940578314289 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'ReLU6', 'amsgrad': False, 'batch_size': 16, 'beta_1': 0.8560079387040943, 'beta_2': 0.9770185600071885, 'concatenate_features': False, 'd_model': 624, 'dropout': 0.8789534153907244, 'dropout_transformers': 0.8210609957869288, 'early_stopping': 10, 'encoder_only': False, 'epochs_classifcation_only': 46, 'eps': 3.3762601692610384e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 4, 'factor': 0.20439745881834723, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.008082723498476304, 'lr': 0.00016949864707399537, 'dropout_lstm': 0.6021940578314289, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Sigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'Adam', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 0.08125287956492143}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  8.68287492567493 acc:  0.004242681374628765\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  7.794878135188934 acc:  0.007882455396020812\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  7.535741535309822 acc:  0.005560145590960856\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  7.420692939143027 acc:  0.010673693142487105\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  7.374276634954637 acc:  0.017752272067525623\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  7.336564670070525 acc:  0.029073532367192907\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  7.322052841801797 acc:  0.028158006386351964\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  7.301515748423915 acc:  0.027331800013397942\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  7.3254613814815395 acc:  0.025143469620168366\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  7.310609783664827 acc:  0.017707612263582164\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  10 loss :  7.293577329574092 acc:  0.025679387267489896\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  11 loss :  7.288673973083496 acc:  0.025143469620168366\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  12 loss :  7.262940425257529 acc:  0.021302726481030747\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  13 loss :  7.254658203740274 acc:  0.022977469128910525\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  14 loss :  7.268056844895886 acc:  0.016479467655136994\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  15 loss :  7.243632036639798 acc:  0.021124087265256906\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'SiLU', 'activation_transformers': 'LogSigmoid', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8279445474575827, 'beta_2': 0.9895892615757698, 'concatenate_features': False, 'd_model': 384, 'dropout': 0.8265664011044547, 'dropout_transformers': 0.5924826493356612, 'early_stopping': 8, 'encoder_only': False, 'epochs_classifcation_only': 28, 'eps': 5.326421484389159e-08, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.3099830168331202, 'scheduler': 'ExponentialLR', 'lr': 0.004142973817139036, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softshrink', 'dropout_gcn': 0.538035251252991, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'LayerNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 2.2939868509403e-07}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  7.395350966086754 acc:  0.0027465779425228324\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  7.288800400954027 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  7.267351656693679 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  7.261327226345355 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  7.258533525466919 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  7.256380187548124 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  7.258469988749577 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  7.256000555478609 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  7.257545243776762 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  7.256963487771841 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.28992179175451804 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'RReLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8418445287825518, 'beta_2': 0.9974007141424321, 'concatenate_features': False, 'd_model': 504, 'dropout': 0.26383959234516385, 'dropout_transformers': 0.29826441429435613, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 68, 'eps': 6.7320314414354435e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 10, 'factor': 0.72059884132601, 'patience': 5, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.018814575051917044, 'lr': 0.0006258210910229073, 'dropout_lstm': 0.28992179175451804, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardsigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'GELU', 'dropout_gcn': 0.19573725634377448, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 2.5770276227583585e-09}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  6.93971022092379 acc:  0.16028403635308042\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  4.505204153060913 acc:  0.32447580555121364\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  3.356739172568688 acc:  0.37029676439720427\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  2.9358350405326257 acc:  0.3921577384275283\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  2.711556306252113 acc:  0.41049058794631893\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  2.5504453640717726 acc:  0.418931290891633\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  2.4322324422689583 acc:  0.41810508451867895\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  2.292739904843844 acc:  0.430274881093272\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  2.2150786821658794 acc:  0.4319942835450952\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  2.157071018218994 acc:  0.4347855212915615\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  10 loss :  2.1023402507488544 acc:  0.4331777683495969\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  11 loss :  2.0537810591550976 acc:  0.4338253355067771\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  12 loss :  2.010008919239044 acc:  0.43134671638791505\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  13 loss :  1.9671624302864075 acc:  0.4343612531540987\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  14 loss :  1.9232610097298255 acc:  0.4340263046245227\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  15 loss :  1.8838711674396809 acc:  0.42730500413103184\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3675742600969122 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'RReLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8404370957075653, 'beta_2': 0.9970318502804578, 'concatenate_features': False, 'd_model': 816, 'dropout': 0.03322573152454367, 'dropout_transformers': 0.3000904816803683, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 78, 'eps': 2.063719200777474e-09, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 10, 'factor': 0.5501590613338911, 'patience': 4, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.002053357063439174, 'lr': 1.2266140954238106e-06, 'dropout_lstm': 0.3675742600969122, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardsigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 3, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Hardswish', 'dropout_gcn': 0.029374091695053478, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'PairNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 1.130562422867092e-08}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  8.33915806550246 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  8.090825942846445 acc:  0.002277650001116495\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  7.938482361573439 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  7.823289295343252 acc:  0.006073733336310653\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  7.728983079470121 acc:  0.00781546569010562\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  7.643501545832708 acc:  0.01040573431882634\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  7.579154509764451 acc:  0.011991157358819195\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  7.539597602990957 acc:  0.013308621575151286\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  7.501566377052894 acc:  0.015072683830917982\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  7.462265278742864 acc:  0.016546457361052185\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  10 loss :  7.42759558237516 acc:  0.018466828930620995\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  11 loss :  7.392019033432007 acc:  0.020364870598218072\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  12 loss :  7.3546327297504135 acc:  0.02262019069736284\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  13 loss :  7.320656611369206 acc:  0.024406582855101267\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  14 loss :  7.290101821605976 acc:  0.027130830895652366\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  15 loss :  7.256649431815514 acc:  0.02992206864211866\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  16 loss :  7.2243706446427565 acc:  0.03302592501618918\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  17 loss :  7.191552308889536 acc:  0.03700064756715718\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  18 loss :  7.159971692011907 acc:  0.04037246276488846\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  19 loss :  7.1298906399653506 acc:  0.043833597570506665\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  20 loss :  7.098965905262873 acc:  0.0470267735524641\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  21 loss :  7.071513392375066 acc:  0.04892481522006118\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  22 loss :  7.057435138408954 acc:  0.05093450639751691\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  23 loss :  7.037055213634784 acc:  0.05296652747694438\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  24 loss :  7.021762998287494 acc:  0.054797579438626266\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  25 loss :  7.006089661671565 acc:  0.05694125002791238\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  26 loss :  6.9876517332517185 acc:  0.05895094120536811\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  27 loss :  6.97539094044612 acc:  0.06096063238282384\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  28 loss :  6.95727878350478 acc:  0.06317129267802514\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  29 loss :  6.942319906674898 acc:  0.06522564365942433\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  30 loss :  6.923732790580162 acc:  0.06734698434673872\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  31 loss :  6.90592161325308 acc:  0.06926735591630753\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  32 loss :  6.892485658939068 acc:  0.07132170689770671\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  33 loss :  6.873578596115112 acc:  0.0729964495455865\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  34 loss :  6.859549716802744 acc:  0.07525176964473126\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  35 loss :  6.84567681826078 acc:  0.07688185248866758\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  36 loss :  6.83310462144705 acc:  0.0779313578813389\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  37 loss :  6.820939507851234 acc:  0.07898086327401023\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  38 loss :  6.811525557591365 acc:  0.08007502847062502\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  39 loss :  6.803571513982919 acc:  0.08087890494160731\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  40 loss :  6.794707437661978 acc:  0.08183909072639171\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  41 loss :  6.786756335772001 acc:  0.08273228680526093\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  42 loss :  6.779973209821261 acc:  0.08369247259004534\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  43 loss :  6.768464612960815 acc:  0.08442935935511243\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  44 loss :  6.7588078132042515 acc:  0.08532255543398165\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  45 loss :  6.754026908140916 acc:  0.08621575151285085\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  46 loss :  6.745713472366333 acc:  0.0871312774936918\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  47 loss :  6.733402707026555 acc:  0.08820311278833486\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  48 loss :  6.729170098671546 acc:  0.08902931916128888\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  49 loss :  6.719715188099788 acc:  0.0898555255342429\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  50 loss :  6.710584669846755 acc:  0.09086037112297077\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  51 loss :  6.7010953353001526 acc:  0.09141861867226403\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  52 loss :  6.699761632772592 acc:  0.09177589710381172\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  53 loss :  6.694679340949425 acc:  0.09222249514324632\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  54 loss :  6.689404843403743 acc:  0.09269142308465265\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  55 loss :  6.683197711064265 acc:  0.09320501083000246\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  56 loss :  6.678036062534039 acc:  0.09362927896746533\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  57 loss :  6.676582171366765 acc:  0.09403121720295649\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  58 loss :  6.672060808768639 acc:  0.09445548534041935\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  59 loss :  6.666585603127113 acc:  0.09496907308576916\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  60 loss :  6.660712550236629 acc:  0.09559431034097761\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  61 loss :  6.658368495794443 acc:  0.09608556818435567\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  62 loss :  6.653775299512423 acc:  0.09659915592970547\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  63 loss :  6.649556702833909 acc:  0.097135073577027\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  64 loss :  6.643640202742357 acc:  0.0978049706361789\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  65 loss :  6.638576977069562 acc:  0.09840787798941562\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  66 loss :  6.633006275617159 acc:  0.09865350691110467\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  67 loss :  6.633756989699144 acc:  0.09907777504856753\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  68 loss :  6.629298250491803 acc:  0.09930107406828484\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  69 loss :  6.6317922372084395 acc:  0.09950204318603041\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  70 loss :  6.6221961681659405 acc:  0.09977000200969117\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  71 loss :  6.622754115324754 acc:  0.10001563093138022\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  72 loss :  6.620067589099591 acc:  0.10028358975504098\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  73 loss :  6.617558193206787 acc:  0.10057387848067346\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  74 loss :  6.61832679601816 acc:  0.10081950740236251\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  75 loss :  6.612120466965895 acc:  0.10102047652010808\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  76 loss :  6.6092682324923 acc:  0.10119911573588192\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  77 loss :  6.611923720286443 acc:  0.10173503338320344\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2893358987857999 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'SELU', 'amsgrad': False, 'batch_size': 128, 'beta_1': 0.8308650951794245, 'beta_2': 0.9991758808323082, 'concatenate_features': False, 'd_model': 576, 'dropout': 0.9757466525014642, 'dropout_transformers': 0.26597416051080663, 'early_stopping': 4, 'encoder_only': False, 'epochs_classifcation_only': 73, 'eps': 2.490019651684676e-06, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.7646562586484056, 'scheduler': 'StepLR', 'step_size': 3, 'lr': 6.759685953817142e-06, 'dropout_lstm': 0.2893358987857999, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardsigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 24, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 1.0840171415310708e-07}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  14.991700545601223 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  14.892747671707816 acc:  0.0005359176473215282\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  14.763190020685611 acc:  0.0006922269611236406\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  14.635822088822074 acc:  0.0008708661768974834\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  14.55785954516867 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  14.523759427277938 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  14.419086870939836 acc:  0.0012951343143603599\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  14.356052481609842 acc:  0.0015407632360493937\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  14.302108723184336 acc:  0.0016077529419645847\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  14.21327603381613 acc:  0.0016747426478797758\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  10 loss :  14.182356253914211 acc:  0.00194270147154054\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  11 loss :  14.211825287860373 acc:  0.002054350981399192\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  12 loss :  14.115205101344896 acc:  0.0021660004912578434\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  13 loss :  14.130170490430748 acc:  0.0021213406873143827\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  14 loss :  14.080146540766178 acc:  0.0022553200991447648\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  15 loss :  13.993964153787363 acc:  0.002232990197173034\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  16 loss :  14.000525847725246 acc:  0.0022999799030882255\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  17 loss :  13.966056740802268 acc:  0.0023669696090034163\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  18 loss :  13.974511312401813 acc:  0.0023669696090034163\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  19 loss :  13.908856806547746 acc:  0.0024339593149186075\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  20 loss :  13.909508870995563 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  21 loss :  13.875529869743016 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  22 loss :  13.86174371968145 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  23 loss :  13.856121063232422 acc:  0.0025679387267489896\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  24 loss :  13.84249384506889 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  25 loss :  13.816138350445291 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  26 loss :  13.823323457137398 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3212255518444806 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'RReLU', 'amsgrad': True, 'batch_size': 16, 'beta_1': 0.8521790752160912, 'beta_2': 0.9940823244968789, 'concatenate_features': False, 'd_model': 504, 'dropout': 0.13554747155572527, 'dropout_transformers': 0.25141866282605724, 'early_stopping': 5, 'encoder_only': False, 'epochs_classifcation_only': 67, 'eps': 2.8317531210804887e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 1, 'factor': 0.6060961586493641, 'patience': 3, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.01864843329399275, 'lr': 0.0011546464948380022, 'dropout_lstm': 0.3212255518444806, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardswish', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 3, 'optimizer': 'Adam', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'ELU', 'dropout_gcn': 0.13401340396552325, 'hidden_channels': 1024, 'layer_type': 'GAT', 'norm': 'BatchNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 2.3868610379047352e-09}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  7.740864740353879 acc:  0.006542661277716991\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  7.341694417400895 acc:  0.02670656275818949\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  7.068275460573 acc:  0.04052877207869057\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  6.662640754307542 acc:  0.09836321818547217\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  6.148095879599313 acc:  0.12705714221914566\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  5.453322972092673 acc:  0.16736261527811894\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  4.8107961962156205 acc:  0.19616818882165107\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  4.44620672118998 acc:  0.21809615255789028\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  4.172616533029859 acc:  0.23350378491838422\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  9 loss :  3.81181419007132 acc:  0.23942120894089275\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  10 loss :  3.5846556257978777 acc:  0.262421007971775\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  11 loss :  3.3806267519977604 acc:  0.27642185650804996\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  12 loss :  3.2810516903333573 acc:  0.2864033226894134\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  13 loss :  3.3110902465392495 acc:  0.29866243887189337\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  14 loss :  3.1738032358829105 acc:  0.30551771877721456\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  15 loss :  3.0206753182634016 acc:  0.3255699707478284\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  16 loss :  3.015310782138432 acc:  0.3286514972199272\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  17 loss :  3.0136049239434928 acc:  0.33595337516468304\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  18 loss :  2.835205238556193 acc:  0.3467387178170288\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  19 loss :  2.7995593547821045 acc:  0.3488823884063149\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  20 loss :  2.845366692988672 acc:  0.3551794207623429\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  21 loss :  2.7036224146869694 acc:  0.36230265949132484\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  22 loss :  2.727034387187423 acc:  0.36393274233526113\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  23 loss :  2.7228141844829667 acc:  0.37085501194649756\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  24 loss :  2.6868328947887243 acc:  0.3695822075341089\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  25 loss :  2.5533461570739746 acc:  0.37630350802759976\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  26 loss :  2.5323958207513684 acc:  0.378022910479423\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  27 loss :  2.7360118137341796 acc:  0.3804568697943416\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  28 loss :  2.5296224121735476 acc:  0.38061317910814374\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  29 loss :  2.6020870097329682 acc:  0.3846325614630552\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  30 loss :  2.6495244380469636 acc:  0.38429761293347925\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  31 loss :  2.6224144786317773 acc:  0.3855480874438961\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  32 loss :  2.5392192568734426 acc:  0.3868432217582565\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  33 loss :  2.563277201117756 acc:  0.38845097470022105\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  34 loss :  2.510538566892392 acc:  0.3896791193086662\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  35 loss :  2.5048061620409245 acc:  0.3902373668579595\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  36 loss :  2.581459225895249 acc:  0.39054998548556374\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  37 loss :  2.4891934740209134 acc:  0.39059464528950716\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  38 loss :  2.502022034653993 acc:  0.3917111403880937\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  39 loss :  2.454305071697057 acc:  0.39153250117231986\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  40 loss :  2.4612655895892703 acc:  0.39235870754527385\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  41 loss :  2.4858469573136803 acc:  0.39291695509456714\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  42 loss :  2.5534326283731192 acc:  0.3932295737221714\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  43 loss :  2.517595329017283 acc:  0.3935645222517473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  44 loss :  2.4382798593735027 acc:  0.3932295737221714\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  45 loss :  2.517496554650993 acc:  0.3932519036241431\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  46 loss :  2.5010271462324623 acc:  0.3938101511734364\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  47 loss :  2.3789951422504174 acc:  0.3943460688207579\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  48 loss :  2.5334003060777612 acc:  0.39425674921287096\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  49 loss :  2.396856510750601 acc:  0.3945916977424469\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  50 loss :  2.465673148074997 acc:  0.39506062568385325\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  51 loss :  2.5427166312654443 acc:  0.3947703369582208\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  52 loss :  2.465784212139165 acc:  0.39461402764441866\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  53 loss :  2.4486403621245767 acc:  0.3953062546055423\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  54 loss :  2.450326901729976 acc:  0.39521693499765537\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  55 loss :  2.4050595025035824 acc:  0.39557421342920307\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  56 loss :  2.3098228612792826 acc:  0.3951499452917402\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  57 loss :  2.440167236550946 acc:  0.3959761516646942\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  58 loss :  2.4138652286796924 acc:  0.3961101310765246\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  59 loss :  2.4092439691597054 acc:  0.3961101310765246\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  60 loss :  2.4565664485236196 acc:  0.39599848156666595\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  61 loss :  2.406169006757647 acc:  0.39608780117455283\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  62 loss :  2.458228555795188 acc:  0.39608780117455283\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  63 loss :  2.464027568558666 acc:  0.3961771207824398\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  64 loss :  2.409386608088128 acc:  0.39640041980215707\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  65 loss :  2.5458218416320944 acc:  0.3965567291159592\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  66 loss :  2.4051729641228077 acc:  0.3962664403903267\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'RReLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8442080127723031, 'beta_2': 0.972688673560823, 'concatenate_features': False, 'd_model': 672, 'dropout': 0.20530782425783373, 'dropout_transformers': 0.3261044689796081, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 59, 'eps': 7.234991247883837e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 9, 'factor': 0.8544302002676851, 'patience': 4, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0003479768278902414, 'lr': 0.0024564381618987336, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 6, 'num_layers_transformer': 5, 'optimizer': 'AdamW', 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Sigmoid', 'dropout_gcn': 0.0871009735124541, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'InstanceNorm', 'num_layers_gcn': 7, 'use_gcn': True, 'weight_decay': 3.558640931340543e-09}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  0 loss :  7.431560721764198 acc:  0.002389299510975147\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  1 loss :  7.316066455841065 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  2 loss :  7.299625044602614 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  3 loss :  7.291979048802302 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  4 loss :  7.289379229912391 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  5 loss :  7.285864624610314 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  6 loss :  7.2782065868377686 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  7 loss :  7.279465488287118 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m epoch:  8 loss :  7.277540056522076 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4666009986549591 and num_layers=1\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=254755)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'RReLU', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.823508562534844, 'beta_2': 0.9977526307862321, 'concatenate_features': False, 'd_model': 912, 'dropout': 0.7369970183188918, 'dropout_transformers': 0.9924918672234586, 'early_stopping': 5, 'encoder_only': False, 'epochs_classifcation_only': 70, 'eps': 4.237512555140232e-09, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.5806735797129333, 'scheduler': 'ExponentialLR', 'lr': 0.03565095037709473, 'dropout_lstm': 0.4666009986549591, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'CELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Hardsigmoid', 'dropout_gcn': 0.2359313856412988, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 6, 'use_gcn': True, 'weight_decay': 6.971974618479223e-09}\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=254755)\u001b[0m loss is undifined\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-31-a45cd2199ea0>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-10 12:16:08,466\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-10 12:16:22,593\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-10 12:16:22,595\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_12       |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 10              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_12\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_12`\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=332436)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4066042448583769 and num_layers=1\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=332436)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'Hardswish', 'amsgrad': True, 'batch_size': 128, 'beta_1': 0.8735625643886906, 'beta_2': 0.9879766112864684, 'concatenate_features': False, 'd_model': 312, 'dropout': 0.5849798186302936, 'dropout_transformers': 0.23357325687370373, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 63, 'eps': 2.1908781255491455e-08, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 10, 'factor': 0.647914861880982, 'patience': 5, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.005688924113687664, 'lr': 0.00012087203666512984, 'dropout_lstm': 0.4066042448583769, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'ReLU6', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 24, 'num_layers_transformer': 3, 'optimizer': 'AdamW', 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Hardtanh', 'dropout_gcn': 0.20428809431810083, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'LayerNorm', 'num_layers_gcn': 8, 'use_gcn': True, 'weight_decay': 5.822102371932885e-08}\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  0 loss :  7.535287336202768 acc:  0.005582475492932586\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  1 loss :  7.2448849164522615 acc:  0.012147466672621307\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  2 loss :  7.179317551392775 acc:  0.015965879909787196\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  3 loss :  7.098426540081317 acc:  0.02279882991313668\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  4 loss :  7.003913721671471 acc:  0.028492954915927918\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  5 loss :  6.892886616633489 acc:  0.04110934952995556\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  6 loss :  6.768558322466337 acc:  0.05191702208427305\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  7 loss :  6.62948595560514 acc:  0.0668110666994172\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  8 loss :  6.5212388332073505 acc:  0.07735078043007391\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  9 loss :  6.406451848837046 acc:  0.08978853582832771\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  10 loss :  6.28394974561838 acc:  0.10378938436460264\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  11 loss :  6.155827129804171 acc:  0.11392715985976822\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  12 loss :  6.022066582166231 acc:  0.12386396623718822\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  13 loss :  5.8942834267249475 acc:  0.13286291673179554\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  14 loss :  5.765909169270442 acc:  0.1443181564432932\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  15 loss :  5.633470652653621 acc:  0.15273652948663555\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  16 loss :  5.512296056747436 acc:  0.16399080008038766\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  17 loss :  5.395590694134052 acc:  0.17441886430118572\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  18 loss :  5.279374867219191 acc:  0.1886876716611214\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  19 loss :  5.171150009448712 acc:  0.1986244780385414\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  20 loss :  5.074545720907358 acc:  0.2126029966728446\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  21 loss :  4.96688808294443 acc:  0.22291941138378402\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  22 loss :  4.87578154710623 acc:  0.22847955697474487\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  23 loss :  4.779915512525118 acc:  0.23604939374316147\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  24 loss :  4.735317376943735 acc:  0.23743384766540876\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  25 loss :  4.668458755199726 acc:  0.24415514815889958\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  26 loss :  4.622630034960233 acc:  0.24877743786704776\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  27 loss :  4.56198016680204 acc:  0.25364535649688497\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  28 loss :  4.519098608310406 acc:  0.25639193443940783\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  29 loss :  4.46995210647583 acc:  0.26092490453966904\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  30 loss :  4.4172860787465025 acc:  0.26652970993457337\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  31 loss :  4.3773147546328035 acc:  0.26909764866132235\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  32 loss :  4.3320429453483 acc:  0.2723354844472233\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  33 loss :  4.284365734687218 acc:  0.27436750552665073\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  34 loss :  4.245715267841632 acc:  0.2787218364111382\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  35 loss :  4.200468277931213 acc:  0.28298684768773863\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  36 loss :  4.16717158464285 acc:  0.2871402094544805\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  37 loss :  4.129462817999033 acc:  0.2911149320054485\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  38 loss :  4.089887586006752 acc:  0.2948886854386709\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  39 loss :  4.0535220788075375 acc:  0.29716633543978743\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  40 loss :  4.031700864204994 acc:  0.2980595315186566\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  41 loss :  4.010459584456224 acc:  0.2995109751468191\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  42 loss :  3.9870967131394606 acc:  0.3029497800504656\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  43 loss :  3.9609383528049174 acc:  0.30390996583525\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  44 loss :  3.9416826009750365 acc:  0.3056740280910167\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  45 loss :  3.92405453645266 acc:  0.3082642967197374\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  46 loss :  3.89951690527109 acc:  0.31022932809324966\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  47 loss :  3.881825454418476 acc:  0.3140030815264721\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  48 loss :  3.8607889615572417 acc:  0.3131098854476029\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  49 loss :  3.843976398614737 acc:  0.3170176182926557\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  50 loss :  3.826503639954787 acc:  0.31741955652814685\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  51 loss :  3.805469764195956 acc:  0.31837974231293126\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  52 loss :  3.789239500119136 acc:  0.31963021682334813\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  53 loss :  3.766048704660856 acc:  0.32170689770671906\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  54 loss :  3.7512934079537024 acc:  0.32402920751177905\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  55 loss :  3.7317869553199183 acc:  0.3239398879038921\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  56 loss :  3.7236196059447066 acc:  0.3248107540807896\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  57 loss :  3.709765944114098 acc:  0.3258602594734609\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  58 loss :  3.7006277579527636 acc:  0.32601656878726304\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  59 loss :  3.6922962518838736 acc:  0.32757966192528415\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  60 loss :  3.6807683137746956 acc:  0.3282495589844361\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  61 loss :  3.6711844279215886 acc:  0.3276243217292276\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  62 loss :  3.657388690801767 acc:  0.32992430163231584\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=332436)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6516294573892489 and num_layers=1\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=332436)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'Mish', 'amsgrad': False, 'batch_size': 128, 'beta_1': 0.9321025130700245, 'beta_2': 0.9750782557227647, 'concatenate_features': False, 'd_model': 768, 'dropout': 0.4204636263741281, 'dropout_transformers': 0.192207877184198, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 74, 'eps': 4.645652484433411e-06, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 1, 'factor': 0.1707885425208746, 'patience': 5, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.04564899584070276, 'lr': 0.0008228443116515809, 'dropout_lstm': 0.6516294573892489, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'SiLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features_globally': False, 'normalize_features_independantly': True, 'num_heads': 12, 'num_layers_transformer': 4, 'optimizer': 'AdamW', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 0.00011425313279515088}\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  0 loss :  7.014909052513015 acc:  0.17951008195074022\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  1 loss :  4.805493223835045 acc:  0.2866936114150459\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  2 loss :  3.5441259800548286 acc:  0.3278699506509166\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  3 loss :  3.1149257068902676 acc:  0.3640443918451198\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  4 loss :  2.851753335603526 acc:  0.37554429136056094\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  5 loss :  2.6609626487946847 acc:  0.389120871759373\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  6 loss :  2.543935523906224 acc:  0.3986334099993301\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  7 loss :  2.336083452466508 acc:  0.42723801442511666\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  8 loss :  2.2385049605033767 acc:  0.43060982962284794\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  9 loss :  2.1890312349292596 acc:  0.4302972109952437\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  10 loss :  2.1355896516585013 acc:  0.43308844874171004\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  11 loss :  2.0822795982092197 acc:  0.43326708795748387\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  12 loss :  2.0578363428653126 acc:  0.43179331442734964\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  13 loss :  2.03878850668249 acc:  0.4347855212915615\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  14 loss :  1.9847618391816044 acc:  0.4369515217828194\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  15 loss :  1.9734998471300367 acc:  0.43715249090056496\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  16 loss :  1.968665836562573 acc:  0.4369068619788759\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  17 loss :  1.9537390621615127 acc:  0.43708550119464973\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  18 loss :  1.9454665771672424 acc:  0.4372864703123953\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  19 loss :  1.9395236851463855 acc:  0.4376214188419713\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  20 loss :  1.9545853910311846 acc:  0.4369515217828194\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  21 loss :  1.9313247270987068 acc:  0.4376214188419713\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  22 loss :  1.9361368316999623 acc:  0.43735346001831055\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  23 loss :  1.931682699163195 acc:  0.43721948060648014\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  24 loss :  1.9324869202895902 acc:  0.43719715070450843\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  25 loss :  1.9452729107628406 acc:  0.43721948060648014\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  26 loss :  1.9287404778977515 acc:  0.4372641404104236\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=332436)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6562032603966567 and num_layers=1\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=332436)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'Mish', 'amsgrad': False, 'batch_size': 128, 'beta_1': 0.9199283002730441, 'beta_2': 0.9749592077410718, 'concatenate_features': False, 'd_model': 888, 'dropout': 0.704971074975648, 'dropout_transformers': 0.18056165133307772, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 73, 'eps': 4.00626147670881e-06, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.6534130050984984, 'scheduler': 'StepLR', 'step_size': 25, 'lr': 3.789263839445154e-05, 'dropout_lstm': 0.6562032603966567, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'SiLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features_globally': True, 'normalize_features_independantly': True, 'num_heads': 6, 'num_layers_transformer': 5, 'optimizer': 'Adam', 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False, 'weight_decay': 7.325885547453418e-05}\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  0 loss :  7.904285142119502 acc:  0.01230377598642342\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  1 loss :  7.388147481730287 acc:  0.05397137306567224\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  2 loss :  7.120438804089184 acc:  0.09992631132349329\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  3 loss :  6.79023424336608 acc:  0.13250563830024786\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  4 loss :  6.39129582257338 acc:  0.1682334814550164\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  5 loss :  6.022723251665142 acc:  0.19851282852868277\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  6 loss :  5.641535342579156 acc:  0.21854275059732486\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  7 loss :  5.355733408054835 acc:  0.23939887903892101\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  8 loss :  5.116722516610589 acc:  0.2607239354219235\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  9 loss :  4.842096187699009 acc:  0.27651117611593684\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  10 loss :  4.6907840782487895 acc:  0.28702855994462184\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  11 loss :  4.527344542489925 acc:  0.29834982024428913\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  12 loss :  4.421480937742851 acc:  0.30571868789496015\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  13 loss :  4.31455882166473 acc:  0.3122836790746489\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  14 loss :  4.1983326623137565 acc:  0.32030011388250007\n",
            "\u001b[36m(eval_config pid=332436)\u001b[0m epoch:  15 loss :  4.086882161422515 acc:  0.32675345555233015\n"
          ]
        }
      ],
      "source": [
        "run_all_xp(xps_name=\"hyperparameter_tuning_projet_long_with_reduced_search_space_and_layer_normalization\", num_xp=0, algo=None, xp_size=20, xps_number=10, accuracy_target=0.98, max_num_epochs=None, storage_path='/content/tuning',drive_path=\"/content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQlM5RK2VTjU",
        "outputId": "6bbb653e-9b12-42a9-807a-abd51980e98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-06 18:25:57,062\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-06 18:26:10,677\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-06 18:26:10,679\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_0        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_0\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_0`\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7139129724354693 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'Softplus', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1200, 'dropout': 0.25926108944253146, 'dropout_StationIdEmbedding': 0.6617067241662258, 'dropout_timeStampEmbedding': 0.703771911609112, 'dropout_transformers': 0.2106148532882447, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 78, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.6455072704467397, 'scheduler': 'StepLR', 'step_size': 12, 'dropout_lstm': 0.7139129724354693, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'CELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 36, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 6, 'alpha': 0.9048161384865954, 'centered': False, 'eps': 2.830262974672483e-07, 'lr': 0.0001538268984097869, 'momentum': 0.015507939593735698, 'optimizer': 'RMSprop', 'weight_decay': 5.006192140894922e-06, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.141670772007533 acc:  0.0025679387267489896\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  7.710882895333427 acc:  0.0014067838242190116\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.517875943865095 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.443558979034424 acc:  0.0021660004912578434\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.374841662815639 acc:  0.003416475001674743\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.317255469730922 acc:  0.0019873612754840006\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.327423177446637 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  7.284202643803188 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  7.313279492514474 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  7.361218643188477 acc:  0.0024562892168903377\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  7.327044595990862 acc:  0.004666949512091642\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  7.368080030168806 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  7.271814400809152 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  7.275568212781634 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  7.3209308215550015 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  7.23683124269758 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'Hardswish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 504, 'dropout': 0.3291271414458241, 'dropout_StationIdEmbedding': 0.25297478241770976, 'dropout_timeStampEmbedding': 0.3974229173352428, 'dropout_transformers': 0.233546966213382, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 62, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'alpha': 0.9820220267903932, 'centered': True, 'eps': 1.3704179718814581e-08, 'lr': 0.0035072715527129047, 'momentum': 0.49900480812856207, 'optimizer': 'RMSprop', 'weight_decay': 0.032580408773536185, 'positive_function': 'relu', 'epochs_complete_problem': 36, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Softmin', 'dropout_gcn': 0.19159830365229769, 'hidden_channels': 512, 'layer_type': 'GraphSAGE', 'norm': 'InstanceNorm', 'num_layers_gcn': 8, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  7.7753709857746705 acc:  0.0011834848045017081\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  7.473089096909862 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.421832731214621 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.4286474373381015 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.425255209712659 acc:  0.0024562892168903377\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.404662447460627 acc:  0.002009691177455731\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.404230529979124 acc:  0.001429113726190742\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  7.382139117030774 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'CELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1368, 'dropout': 0.09552411043273457, 'dropout_StationIdEmbedding': 0.14634456035876497, 'dropout_timeStampEmbedding': 0.6274796931061111, 'dropout_transformers': 0.24045343711296796, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 57, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.605982408994092, 'scheduler': 'ExponentialLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 4, 'alpha': 0.9628295861427273, 'centered': False, 'eps': 8.494756680333419e-08, 'lr': 0.0007819182523463567, 'momentum': 0.10461780143512661, 'optimizer': 'RMSprop', 'weight_decay': 1.1562009209415351e-07, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  7.512898442053026 acc:  0.00017863921577384274\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  7.307995479337631 acc:  0.0017417323537949668\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.2748148056768605 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.263883055410077 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.2590862797152615 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.238992377250425 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.237813512740597 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  7.243990756619361 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'PReLU', 'batch_size': 32, 'concatenate_features': False, 'd_model': 1104, 'dropout': 0.6614457120385521, 'dropout_StationIdEmbedding': 0.3826982164322794, 'dropout_timeStampEmbedding': 0.8425306180018143, 'dropout_transformers': 0.3124666466183116, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 69, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.0649490436609362, 'scheduler': 'StepLR', 'step_size': 23, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 24, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.9795235166428282, 'beta_2': 0.9995885093712042, 'eps': 3.6340882196783776e-08, 'lr': 9.30356039741827e-05, 'optimizer': 'AdamW', 'weight_decay': 0.010810401620123476, 'positive_function': 'abs', 'epochs_complete_problem': 20, 'reg': True, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.44155211776964376, 'hidden_channels': 256, 'layer_type': 'GAT', 'norm': 'InstanceNorm', 'num_layers_gcn': 1, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  7.684342008241465 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  7.348259449005127 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.314682329204721 acc:  0.0023223098050599556\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.351960645595067 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.2798167752548 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.276536290074738 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.254162801823146 acc:  0.011410579907554206\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  7.261039290629642 acc:  0.010539713730656722\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  7.252629904679849 acc:  0.008619342161087912\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  7.18986867850935 acc:  0.025813366679320278\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  7.039705807054546 acc:  0.039367617176160594\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  6.9229924846702895 acc:  0.05647232208650604\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  6.72789388978985 acc:  0.06502467454167876\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  6.6006833331685675 acc:  0.07509546033092915\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  6.376358408323476 acc:  0.08641672063059644\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  6.231044191709707 acc:  0.09211084563338767\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  16 loss :  6.081253743507493 acc:  0.1073621686800795\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  17 loss :  5.984289975233481 acc:  0.11841547015608601\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  18 loss :  5.87349005820046 acc:  0.11779023290087756\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  19 loss :  5.670591656590851 acc:  0.13516289663488376\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  20 loss :  5.532946049327582 acc:  0.14447446575709533\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  21 loss :  5.477495771058848 acc:  0.15354040595761784\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  22 loss :  5.416565801056338 acc:  0.16311993390349017\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  23 loss :  5.206013216099269 acc:  0.1654199138065784\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  24 loss :  5.174766694995719 acc:  0.16926065694571601\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  25 loss :  5.121310798215195 acc:  0.17147131724091733\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  26 loss :  5.131433748863112 acc:  0.17196257508429538\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  27 loss :  5.0524912317034225 acc:  0.1732130495947123\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  28 loss :  5.168368930548009 acc:  0.1741955652814684\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  29 loss :  5.047147703842378 acc:  0.17486546234062034\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  30 loss :  5.136017517304756 acc:  0.17533439028202666\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  31 loss :  5.100281688529001 acc:  0.17428488488935534\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  32 loss :  5.12947416305542 acc:  0.17587030792934819\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  33 loss :  5.134997173094414 acc:  0.17834892704821026\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  34 loss :  5.0846205328551815 acc:  0.17879552508764487\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  35 loss :  5.0609567870556464 acc:  0.18062657704932675\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  36 loss :  5.010678230876654 acc:  0.1816090927360829\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  37 loss :  5.11301377793433 acc:  0.18281490744255632\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  38 loss :  4.993942603259019 acc:  0.1830158765603019\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  39 loss :  5.058995364417492 acc:  0.18422169126677534\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  40 loss :  5.154976515702798 acc:  0.18395373244311458\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  41 loss :  4.978944160568882 acc:  0.18453430989437955\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  42 loss :  5.1074022407263096 acc:  0.18540517607127704\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  43 loss :  5.085286227750107 acc:  0.18607507313042895\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  44 loss :  4.953038185415133 acc:  0.1869905991112699\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  45 loss :  4.950248933174241 acc:  0.18678962999352433\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  46 loss :  4.990644219895484 acc:  0.187146908425072\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  47 loss :  5.001370873249752 acc:  0.18737020744478933\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  48 loss :  4.903319308455561 acc:  0.1874371971507045\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  49 loss :  4.885884721514205 acc:  0.18766049617042183\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  50 loss :  4.9730932846875255 acc:  0.18761583636647836\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  51 loss :  4.98325733399727 acc:  0.18759350646450662\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  52 loss :  4.913334231981089 acc:  0.18799544469999777\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  53 loss :  4.942698646599139 acc:  0.18815175401379988\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  54 loss :  4.991234564445388 acc:  0.18799544469999777\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  55 loss :  4.915015119901845 acc:  0.18812942411182815\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  56 loss :  4.954727313887905 acc:  0.188308063327602\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  57 loss :  4.988996818032064 acc:  0.18846437264140412\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  58 loss :  4.9519564803217495 acc:  0.1885313623473193\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  59 loss :  4.971609270068961 acc:  0.18837505303351718\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  60 loss :  5.000043912672661 acc:  0.18828573342563026\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  61 loss :  4.865597953259106 acc:  0.1879507848960543\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  62 loss :  5.049313787003638 acc:  0.18815175401379988\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  63 loss :  4.863285068055274 acc:  0.18837505303351718\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  64 loss :  4.876593502474503 acc:  0.18835272313154544\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  65 loss :  4.952342332248956 acc:  0.18835272313154544\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  66 loss :  4.8959399243475685 acc:  0.1883973829354889\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  67 loss :  4.933929456791407 acc:  0.1883973829354889\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.42443929217145804 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'ReLU6', 'activation_transformers': 'SiLU', 'batch_size': 32, 'concatenate_features': True, 'd_model': 576, 'dropout': 0.9606034360693589, 'dropout_StationIdEmbedding': 0.7021465025350949, 'dropout_timeStampEmbedding': 0.9768470093851694, 'dropout_transformers': 0.1405650029640072, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 54, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 8, 'factor': 0.6666864900356849, 'patience': 5, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.00018455992332687843, 'dropout_lstm': 0.42443929217145804, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 5, 'lr': 0.0006312750469574822, 'momentum': 0.2638135939945019, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 1.63967054716315e-08, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.280596562435752 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.256532337791041 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  8.224960668463456 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  8.196979803788034 acc:  0.0004465980394346069\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  8.18907984683388 acc:  0.0005359176473215282\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  8.158261329249331 acc:  0.0007592166670388317\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  8.13242059506868 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  8.109882224233527 acc:  0.001116495098586517\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  8.099840891988654 acc:  0.0013844539222472813\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  8.091785054457816 acc:  0.001563093138021124\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  8.073656242772152 acc:  0.0017640622557666972\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  8.06886716139944 acc:  0.0018310519616818882\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  8.067283655467786 acc:  0.002143670589286113\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  8.039673604463276 acc:  0.002411629412946877\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  8.03457391136571 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  8.025623582538806 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  16 loss :  8.006140553323846 acc:  0.002657258334635911\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  17 loss :  7.978066665247867 acc:  0.002791237746466293\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  18 loss :  7.978230787578382 acc:  0.002724248040551102\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  19 loss :  7.967106076290733 acc:  0.002992206864211866\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  20 loss :  7.954159867136102 acc:  0.0030145367661835966\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  21 loss :  7.949796882428621 acc:  0.003103856374070518\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  22 loss :  7.931167075508519 acc:  0.00326016568787263\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  23 loss :  7.917120296076725 acc:  0.003304825491816091\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  24 loss :  7.925385746202971 acc:  0.0032155058839291695\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  25 loss :  7.911214065551758 acc:  0.0033271553937878214\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  26 loss :  7.922129375056216 acc:  0.003416475001674743\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  27 loss :  7.901907855586002 acc:  0.003416475001674743\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  28 loss :  7.89169518320184 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  29 loss :  7.892362203096089 acc:  0.0035281245115333943\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  30 loss :  7.887232665011758 acc:  0.0035281245115333943\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  31 loss :  7.884603696120413 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  32 loss :  7.880587939212197 acc:  0.003505794609561664\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  33 loss :  7.846661166140907 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  34 loss :  7.853250147167005 acc:  0.003550454413505125\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  35 loss :  7.861166537435431 acc:  0.003505794609561664\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  36 loss :  7.85407028700176 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  37 loss :  7.824381682747289 acc:  0.003505794609561664\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  38 loss :  7.847518293481124 acc:  0.0035281245115333943\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  39 loss :  7.811666669343647 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  40 loss :  7.8315651793228955 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  41 loss :  7.833738156368858 acc:  0.003572784315476855\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  42 loss :  7.828666436044793 acc:  0.003550454413505125\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  43 loss :  7.824642623098273 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  44 loss :  7.811587092750951 acc:  0.0036174441194203157\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  45 loss :  7.802470824592992 acc:  0.0036621039233637764\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  46 loss :  7.809590088693719 acc:  0.0037290936292789676\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  47 loss :  7.792563317951403 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  48 loss :  7.774012128930343 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  49 loss :  7.785027273077714 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  50 loss :  7.80104072470414 acc:  0.0037737534332224283\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  51 loss :  7.783702398601331 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  52 loss :  7.780288314819336 acc:  0.0037960833351941585\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  53 loss :  7.785500491292853 acc:  0.003818413237165889\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Tanhshrink', 'batch_size': 64, 'concatenate_features': False, 'd_model': 1368, 'dropout': 0.6238939123266131, 'dropout_StationIdEmbedding': 0.1292615821199019, 'dropout_timeStampEmbedding': 0.8781617151784016, 'dropout_transformers': 0.8811515134982568, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 35, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 14, 'eta_min': 0.0052881174150808735, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 4, 'lr': 2.9157911340962906e-06, 'momentum': 0.4682233132149382, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 1.0037748582491188e-08, 'positive_function': 'exp', 'epochs_complete_problem': 34, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Softplus', 'dropout_gcn': 0.9030576356970846, 'hidden_channels': 64, 'layer_type': 'GAT', 'norm': 'InstanceNorm', 'num_layers_gcn': 8, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'Softsign', 'batch_size': 32, 'concatenate_features': False, 'd_model': 480, 'dropout': 0.6733854350253079, 'dropout_StationIdEmbedding': 0.9748155140913536, 'dropout_timeStampEmbedding': 0.12653239977988118, 'dropout_transformers': 0.3572181452939681, 'early_stopping': 8, 'encoder_only': False, 'epochs_classifcation_only': 48, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 6, 'factor': 0.4045063769233146, 'patience': 7, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0017478509844923245, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 12, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.8625488143127279, 'beta_2': 0.9983153990220303, 'eps': 1.0235009212261073e-08, 'lr': 0.0009705224723279593, 'optimizer': 'Adam', 'weight_decay': 0.06676877242895549, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  7.723357775125159 acc:  0.0016970725498515061\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  7.640044327241829 acc:  0.004890248531808946\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.653185735265892 acc:  0.003371815197731282\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.64516965452447 acc:  0.0004912578433780676\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.667519506201686 acc:  0.0009155259808409441\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.690660545624882 acc:  0.0015854230399928544\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.653785935367447 acc:  0.0017640622557666972\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  \n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m 7.654637141400073 acc:  0.001027175490699596\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  7.679623379764787 acc:  0.0019873612754840006\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  7.620445423815624 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Tanh', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1368, 'dropout': 0.4280420135087235, 'dropout_StationIdEmbedding': 0.7887871836946115, 'dropout_timeStampEmbedding': 0.4772474533184021, 'dropout_transformers': 0.662459819786631, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 26, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 2.0653326999894176e-06, 'max_lr': 0.27236898166820467, 'mode': 'triangular', 'scheduler': 'CyclicLR', 'step_size_up': 14, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 4, 'lr': 5.827440793657726e-07, 'momentum': 0.21656257520048455, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 2.016587174016015e-07, 'positive_function': 'abs', 'epochs_complete_problem': 46, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Softsign', 'dropout_gcn': 0.7499520501904579, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'GraphNorm', 'num_layers_gcn': 4, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.1156139592178 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.042719076607973 acc:  0.0038630730411093497\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.773064766221374 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.495565414428711 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.424582849022086 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.392427910375231 acc:  0.002099010785342652\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.391183995108568 acc:  0.0013397941183038206\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  7.364311738778617 acc:  0.0012058147064734385\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2747819132178011 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Softplus', 'batch_size': 64, 'concatenate_features': True, 'd_model': 504, 'dropout': 0.3957791952489571, 'dropout_StationIdEmbedding': 0.702372492734025, 'dropout_timeStampEmbedding': 0.09474334414227226, 'dropout_transformers': 0.0463891876881809, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 58, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.016169922985659214, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.2747819132178011, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'SiLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 3, 'lr': 1.1078303311413616e-05, 'momentum': 0.26188080369782574, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.0004189190842914088, 'positive_function': 'relu', 'epochs_complete_problem': 47, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.145222040528026 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.14349975799049 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  8.142906924199792 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  8.14208585856347 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  8.141110201787683 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  8.139245571370898 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  8.143215696238938 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5503585263795484 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'ReLU6', 'batch_size': 32, 'concatenate_features': False, 'd_model': 696, 'dropout': 0.4321527117415681, 'dropout_StationIdEmbedding': 0.03868435467749587, 'dropout_timeStampEmbedding': 0.6928207271119554, 'dropout_transformers': 0.014403770793150472, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 39, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 12, 'eta_min': 0.07250020026056199, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.5503585263795484, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softsign', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 2, 'alpha': 0.9113688075049072, 'centered': True, 'eps': 2.808398740903699e-06, 'lr': 4.10486923010962e-07, 'momentum': 0.1742385654185275, 'optimizer': 'RMSprop', 'weight_decay': 1.187599422660745e-08, 'positive_function': 'abs', 'epochs_complete_problem': 19, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.634506872144796 acc:  0.0003126186276042248\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  10.352408595004324 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'LogSigmoid', 'batch_size': 16, 'concatenate_features': True, 'd_model': 768, 'dropout': 0.43404767392452515, 'dropout_StationIdEmbedding': 0.5115991835013279, 'dropout_timeStampEmbedding': 0.03669710922497316, 'dropout_transformers': 0.7793126564077055, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 64, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 28, 'eta_min': 0.02215536930446558, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 24, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 24, 'num_layers_transformer': 6, 'alpha': 0.9275788812049294, 'centered': False, 'eps': 5.783023301794646e-06, 'lr': 5.70888403847423e-07, 'momentum': 0.14234176774838492, 'optimizer': 'RMSprop', 'weight_decay': 0.00039246007806665494, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'ELU', 'dropout_gcn': 0.17891469171526153, 'hidden_channels': 2048, 'layer_type': 'GAT', 'norm': 'LayerNorm', 'num_layers_gcn': 6, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.12246152629023 acc:  0.0010718352946430564\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.034792423248291 acc:  0.003706763727307237\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.753981071969737 acc:  8.931960788692137e-05\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.604520383088485 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.603159697159477 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.612694615903108 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.497958805250085 acc:  0.000781546569010562\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  7.606812186863111 acc:  0.003416475001674743\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  7.631114026774531 acc:  0.002969876962240136\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  7.655013374660326 acc:  0.00042426813746287653\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  7.61395730143008 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  7.627248017684273 acc:  0.001429113726190742\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  7.6701069085494336 acc:  6.698970591519103e-05\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  7.777148889458698 acc:  0.0012281446084451688\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  7.710559140080991 acc:  6.698970591519103e-05\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  7.742739677429199 acc:  0.001630082843936315\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  16 loss :  7.72588586807251 acc:  0.0006922269611236406\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  17 loss :  7.723996473395306 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  18 loss :  7.705101013183594 acc:  0.0007368867650671013\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6828398700211156 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'Softmin', 'activation_transformers': 'SELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1080, 'dropout': 0.43075567017964045, 'dropout_StationIdEmbedding': 0.3223490839324896, 'dropout_timeStampEmbedding': 0.11959456607335395, 'dropout_transformers': 0.32149231283350965, 'early_stopping': 9, 'encoder_only': False, 'epochs_classifcation_only': 31, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 3, 'factor': 0.5410076163403952, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.00010877615201678248, 'dropout_lstm': 0.6828398700211156, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 12, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 4, 'alpha': 0.9667077810181143, 'centered': False, 'eps': 3.516832501133623e-08, 'lr': 8.615261318640419e-06, 'momentum': 0.30860464238821306, 'optimizer': 'RMSprop', 'weight_decay': 0.00027063099652533274, 'positive_function': 'abs', 'epochs_complete_problem': 18, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.019879081032492 acc:  0.0013397941183038206\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.020033923062412 acc:  0.001362124020275551\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  8.020997741005637 acc:  0.0015184333340776633\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  8.020327307961203 acc:  0.001630082843936315\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  8.020408977161754 acc:  0.0017640622557666972\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  8.020615230907094 acc:  0.0018087220597101578\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  8.019808075644754 acc:  0.0018310519616818882\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  8.020765044472434 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  8.019878994334828 acc:  0.0018087220597101578\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  8.020042766224254 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  8.019204053011807 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  8.020388516512783 acc:  0.00194270147154054\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  8.019359588623047 acc:  0.00194270147154054\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  8.01939955624667 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  8.018892461603338 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  8.01924688165838 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  16 loss :  8.019352045926182 acc:  0.0018087220597101578\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  17 loss :  8.019651586359197 acc:  0.002009691177455731\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  18 loss :  8.018320430408824 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  19 loss :  8.019055886702104 acc:  0.00194270147154054\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  20 loss :  8.019464492797852 acc:  0.0018310519616818882\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  21 loss :  8.019747127186168 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  22 loss :  8.019196683710272 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  23 loss :  8.019123857671564 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  24 loss :  8.019150733947754 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  25 loss :  8.01840790835294 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  26 loss :  8.019336873834783 acc:  0.0018533818636536185\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9428669887968822 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'RReLU', 'batch_size': 32, 'concatenate_features': False, 'd_model': 144, 'dropout': 0.30505538233349916, 'dropout_StationIdEmbedding': 0.5032306905819148, 'dropout_timeStampEmbedding': 0.8559672395122064, 'dropout_transformers': 0.6058989839613876, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 52, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.9428669887968822, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Tanh', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 5, 'alpha': 0.9132665251491426, 'centered': True, 'eps': 1.0681541341594436e-06, 'lr': 0.00022473415354181788, 'momentum': 0.09082775790003822, 'optimizer': 'RMSprop', 'weight_decay': 5.510074339651595e-09, 'positive_function': 'sig', 'epochs_complete_problem': 28, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  7.838113997059484 acc:  0.003371815197731282\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Tanh', 'batch_size': 128, 'concatenate_features': True, 'd_model': 480, 'dropout': 0.733797920561741, 'dropout_StationIdEmbedding': 0.22404208966807704, 'dropout_timeStampEmbedding': 0.6115560596108587, 'dropout_transformers': 0.7193928893078161, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 27, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 0.003563260646682764, 'max_lr': 0.05492730749376794, 'mode': 'exp_range', 'scheduler': 'CyclicLR', 'step_size_up': 18, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 36, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 12, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.9711031512281438, 'beta_2': 0.9692721421474142, 'eps': 8.869389620199954e-09, 'lr': 0.05800861176479262, 'optimizer': 'AdamW', 'weight_decay': 2.638729604280237e-08, 'positive_function': 'exp', 'epochs_complete_problem': 41, 'reg': True, 'transformers_model': True, 'activation_gcn': 'SELU', 'dropout_gcn': 0.2567966435972764, 'hidden_channels': 128, 'layer_type': 'GraphSAGE', 'norm': 'PairNorm', 'num_layers_gcn': 1, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  7.576977797916958 acc:  0.001116495098586517\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  7.433067430768694 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.500073977879116 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.498208086831229 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.443463856833322 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.390187740325928 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.369619887215751 acc:  0.0008931960788692137\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'Hardswish', 'batch_size': 64, 'concatenate_features': False, 'd_model': 408, 'dropout': 0.665146435790974, 'dropout_StationIdEmbedding': 0.42906771209417016, 'dropout_timeStampEmbedding': 0.4702705877777681, 'dropout_transformers': 0.4870487116399056, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 35, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 3.3306067818274087e-06, 'max_lr': 0.1467969648554335, 'mode': 'exp_range', 'scheduler': 'CyclicLR', 'step_size_up': 27, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 4, 'lr': 0.0008003162560382942, 'momentum': 0.2965617635371624, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.001704207234665908, 'positive_function': 'sig', 'epochs_complete_problem': 36, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.087551096866004 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.076332323174727 acc:  0.0002679588236607641\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  8.044042692686382 acc:  0.0007368867650671013\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  7.99167245061774 acc:  0.002545608824777259\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  7.905370180230392 acc:  0.00585043431659335\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  7.763498868440327 acc:  0.006564991179688721\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  7.5699186074106315 acc:  0.006073733336310653\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  7.446257962678608 acc:  0.0038630730411093497\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  7.378013269524825 acc:  0.0045106401982895295\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  7.354314633419639 acc:  0.0075028470625013955\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  7.321782092044228 acc:  0.007458187258557935\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  7.292111888684724 acc:  0.012058147064734386\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  7.272750357577675 acc:  0.020320210794274613\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  7.230117290898373 acc:  0.024272603443270886\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  7.17553441900956 acc:  0.031016233838733448\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  7.081731510162354 acc:  0.03872005001898041\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  16 loss :  6.968053165235017 acc:  0.050845186789629994\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  17 loss :  6.816717172923841 acc:  0.061608199540004016\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  18 loss :  6.666805613668341 acc:  0.07726146082218699\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  19 loss :  6.522519161826686 acc:  0.08945358729875176\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  20 loss :  6.346654199299059 acc:  0.09892146573476543\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  21 loss :  6.184644076698705 acc:  0.11046602505415001\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  22 loss :  6.035679280130487 acc:  0.12038050152959828\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  23 loss :  5.873156125921952 acc:  0.1345823191836188\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  24 loss :  5.812066956570274 acc:  0.13549784516445973\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  25 loss :  5.7340931189687625 acc:  0.14496572360047338\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  26 loss :  5.597234991977089 acc:  0.1495210236027064\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  27 loss :  5.492813235835025 acc:  0.16200343880490364\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  28 loss :  5.402165141858553 acc:  0.1648393363553134\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  29 loss :  5.326574636760511 acc:  0.17598195743920683\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  30 loss :  5.230512312838906 acc:  0.1898711564656231\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  31 loss :  5.149873874061986 acc:  0.19025076479914252\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  32 loss :  5.057874870300293 acc:  0.20175066431458366\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  33 loss :  5.030094006187038 acc:  0.20306812853091574\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  34 loss :  4.991101405495091 acc:  0.20483219078668244\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  35 loss :  5.8338344824941535 acc:  0.20474287117879553\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  36 loss :  5.802380205455579 acc:  0.21579617265480205\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  37 loss :  5.783175453386809 acc:  0.21849809079338142\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  38 loss :  5.794926317114579 acc:  0.21530491481142397\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  39 loss :  5.8369243119892325 acc:  0.21360784226157248\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  40 loss :  5.767747623042056 acc:  0.22801062903333855\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  41 loss :  5.697406367251747 acc:  0.23196302168233482\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  42 loss :  5.68411024495175 acc:  0.23450863050711207\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  43 loss :  5.704104644373843 acc:  0.23703190942991761\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  44 loss :  5.608707372765792 acc:  0.23185137217247617\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  45 loss :  5.570561574634753 acc:  0.24134158051046156\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  46 loss :  5.580515565370258 acc:  0.2434182613938325\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  47 loss :  5.572089712243331 acc:  0.24297166335439788\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  48 loss :  5.508589247653359 acc:  0.2514123662997119\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  49 loss :  5.462103336735775 acc:  0.24873277806310432\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  50 loss :  5.443658286646793 acc:  0.25791036777348547\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  51 loss :  5.441287632992393 acc:  0.25286380992787444\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  52 loss :  5.443859973706697 acc:  0.257039501596588\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  53 loss :  5.450860831612035 acc:  0.260433646696291\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  54 loss :  5.485501470063862 acc:  0.2605229663041779\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  55 loss :  5.482288902684262 acc:  0.2613938324810754\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  56 loss :  5.4881165906002645 acc:  0.25871424424446776\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  57 loss :  5.472454387263248 acc:  0.2582899761070049\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  58 loss :  5.48296927401894 acc:  0.257285130518277\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  59 loss :  5.489509984066612 acc:  0.2556773775763124\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  60 loss :  5.382304583097759 acc:  0.25069780943661657\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  61 loss :  5.4960581126965975 acc:  0.25384632561463055\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  62 loss :  5.436123888116134 acc:  0.25038519080901234\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  63 loss :  5.4390968021593595 acc:  0.2500725721814081\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.18390114890082632 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'Softshrink', 'batch_size': 16, 'concatenate_features': False, 'd_model': 1056, 'dropout': 0.2450016010491265, 'dropout_StationIdEmbedding': 0.8751907044052445, 'dropout_timeStampEmbedding': 0.4980229201128875, 'dropout_transformers': 0.5459436867073139, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 25, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.04660905496478961, 'scheduler': 'StepLR', 'step_size': 17, 'dropout_lstm': 0.18390114890082632, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 12, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8753430804674709, 'beta_2': 0.9604872687745583, 'eps': 2.2476785782047834e-06, 'lr': 0.0005002538302103282, 'optimizer': 'AdamW', 'weight_decay': 1.570523768502373e-08, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  7.7371301583840815 acc:  0.041890896098966124\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  7.334439875374378 acc:  0.051202465221177676\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.1236017992798715 acc:  0.06127325101042806\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  6.832228304634632 acc:  0.05709755934171449\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  6.618042401864495 acc:  0.06683339660138891\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  6.497593503602793 acc:  0.11305629368287073\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  5.992248924685196 acc:  0.11698635642989527\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  5.921653176697207 acc:  0.14507737311033206\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  5.5857977262685 acc:  0.15052586919143424\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  5.337933345579765 acc:  0.16611214076770203\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  5.335964797248303 acc:  0.17709845253779335\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  5.2822388125137545 acc:  0.19371189960476073\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  5.077045591784195 acc:  0.18944688832816023\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'Softplus', 'batch_size': 64, 'concatenate_features': False, 'd_model': 720, 'dropout': 0.6713799315302484, 'dropout_StationIdEmbedding': 0.07807473167543788, 'dropout_timeStampEmbedding': 0.7194000255447135, 'dropout_transformers': 0.22017744352414936, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 67, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 1, 'factor': 0.7095096372468199, 'patience': 8, 'scheduler': 'ReduceLROnPlateau', 'threshold': 5.406942736701601e-05, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.917245234448603, 'beta_2': 0.9979760828897231, 'eps': 1.7949982882503993e-08, 'lr': 0.07180319954970667, 'optimizer': 'AdamW', 'weight_decay': 2.672399778915436e-06, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'ELU', 'dropout_gcn': 0.009800963274890107, 'hidden_channels': 2048, 'layer_type': 'GCNConv', 'norm': 'InstanceNorm', 'num_layers_gcn': 9, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8077015242197328 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Softshrink', 'batch_size': 16, 'concatenate_features': True, 'd_model': 1440, 'dropout': 0.424930653895912, 'dropout_StationIdEmbedding': 0.8356749901292031, 'dropout_timeStampEmbedding': 0.7748568722037691, 'dropout_transformers': 0.8864292197781687, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 61, 'input_size': 2, 'learnable_pos_encoding': False, 'T_max': 5, 'eta_min': 0.000845888121512347, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.8077015242197328, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softmin', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.9854597264805187, 'beta_2': 0.9841297150729207, 'eps': 1.6678915826240303e-06, 'lr': 9.967385128087866e-06, 'optimizer': 'Adam', 'weight_decay': 0.0025454757870817793, 'positive_function': 'relu', 'epochs_complete_problem': 25, 'reg': True, 'transformers_model': True, 'activation_gcn': 'LeakyReLU', 'dropout_gcn': 0.32495698355976477, 'hidden_channels': 128, 'layer_type': 'GAT', 'norm': 'PairNorm', 'num_layers_gcn': 8, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  9.890950782136768 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.34833169108286 acc:  0.015340642654578747\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  7.349357290417736 acc:  0.09351762945760668\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  6.372865814189012 acc:  0.1696179353772637\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  5.303770889162393 acc:  0.18504789763972937\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  4.966848503232627 acc:  0.174396534399214\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  4.8192630001387675 acc:  0.20342540696246345\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  4.472871115070363 acc:  0.23964450796061004\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  4.257264189695189 acc:  0.2664627202286582\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  3.951787107277915 acc:  0.30741576044481167\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  3.8924723208262657 acc:  0.3164370408413907\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  3.8465081859009427 acc:  0.3265748163365563\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  3.9320883900707306 acc:  0.278319898175647\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  4.101634069262999 acc:  0.2181854721657772\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  4.4001382370893865 acc:  0.20690887167005337\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  4.4298922391462074 acc:  0.20748944912131836\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  16 loss :  4.353121144609301 acc:  0.20407297411964362\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  17 loss :  4.206842972970134 acc:  0.24480271531607975\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  18 loss :  4.018880276155722 acc:  0.2941294687716321\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'Softmin', 'activation_transformers': 'ReLU6', 'batch_size': 32, 'concatenate_features': False, 'd_model': 888, 'dropout': 0.5965985814957975, 'dropout_StationIdEmbedding': 0.31853065290482796, 'dropout_timeStampEmbedding': 0.505034365973917, 'dropout_transformers': 0.13719489145214714, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 26, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 12, 'eta_min': 4.9775741633749974e-05, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 3, 'lr': 0.0023012085231793656, 'momentum': 0.20611453843350197, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 0.0028375541616436576, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Hardsigmoid', 'dropout_gcn': 0.33183221332300483, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 10, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.020710471368606 acc:  0.0006475671571801799\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.020564602267358 acc:  0.0012504745104168992\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  8.020109539647256 acc:  0.0016524127459080454\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  8.019956428773941 acc:  0.0017863921577384275\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  8.019889296254805 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  8.019730057254915 acc:  0.0018310519616818882\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  8.01917774446549 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  8.019357133680774 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  8.01899232556743 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  8.019188277952132 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  8.019149903328188 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  8.019285964965821 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  8.018815446669056 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  13 loss :  8.018896662804389 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  14 loss :  8.018870144505655 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  15 loss :  8.01905328689083 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.40693957329744346 and num_layers=1\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=60669)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'ReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 552, 'dropout': 0.31418065391016503, 'dropout_StationIdEmbedding': 0.9170122040932379, 'dropout_timeStampEmbedding': 0.7519874014575396, 'dropout_transformers': 0.24473354482957022, 'early_stopping': 5, 'encoder_only': False, 'epochs_classifcation_only': 24, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.40693957329744346, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 12, 'num_layers_transformer': 1, 'alpha': 0.9877232284848576, 'centered': True, 'eps': 3.96938206445058e-07, 'lr': 0.0018253708553199364, 'momentum': 0.28568258748991526, 'optimizer': 'RMSprop', 'weight_decay': 0.1769820160131162, 'positive_function': 'abs', 'epochs_complete_problem': 31, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  0 loss :  8.009873847027759 acc:  0.0022999799030882255\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  1 loss :  8.016720731775244 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  2 loss :  8.017503758410474 acc:  0.0037737534332224283\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  3 loss :  8.017505218932678 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  4 loss :  8.017398093963836 acc:  0.0023223098050599556\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  5 loss :  8.017497709581068 acc:  0.002389299510975147\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  6 loss :  8.017508159984242 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  7 loss :  8.017457821986058 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  8 loss :  8.014653782744508 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  9 loss :  7.799489464793172 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  10 loss :  7.7032837734355795 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  11 loss :  7.6853931166908955 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=60669)\u001b[0m epoch:  12 loss :  7.6879384534342305 acc:  0.002925217158296675\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-06 20:07:13,292\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-06 20:07:28,015\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-06 20:07:28,016\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_1        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_1\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_1`\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Softshrink', 'batch_size': 128, 'concatenate_features': False, 'd_model': 120, 'dropout': 0.8301701469867244, 'dropout_StationIdEmbedding': 0.5655882640525879, 'dropout_timeStampEmbedding': 0.2939189411543632, 'dropout_transformers': 0.4648875006740148, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 11, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 9.368540868939229e-07, 'max_lr': 0.10821345862655626, 'mode': 'exp_range', 'scheduler': 'CyclicLR', 'step_size_up': 28, 'dropout_lstm': 0.9508510389294802, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardsigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 24, 'num_layers_transformer': 5, 'lr': 0.08447637166331295, 'momentum': 0.008015512419474596, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.2627864584937101, 'positive_function': 'relu', 'epochs_complete_problem': 3, 'reg': True, 'transformers_model': True, 'activation_gcn': 'LeakyReLU', 'dropout_gcn': 0.6613457094402753, 'hidden_channels': 64, 'layer_type': 'GAT', 'norm': 'PairNorm', 'num_layers_gcn': 4, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9508510389294802 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'LeakyReLU', 'batch_size': 128, 'concatenate_features': False, 'd_model': 288, 'dropout': 0.8209879365270651, 'dropout_StationIdEmbedding': 0.44365202679620575, 'dropout_timeStampEmbedding': 0.27366779202346486, 'dropout_transformers': 0.9729843269123895, 'early_stopping': 10, 'encoder_only': True, 'epochs_classifcation_only': 15, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 5.4378989153586717e-05, 'max_lr': 0.3093565932930617, 'mode': 'triangular2', 'scheduler': 'CyclicLR', 'step_size_up': 1, 'dropout_lstm': 0.8190940671037286, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softmin', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 24, 'num_layers_transformer': 5, 'amsgrad': True, 'beta_1': 0.8034842027718958, 'beta_2': 0.9824751586383071, 'eps': 5.956372855819424e-06, 'lr': 6.02971710119714e-07, 'optimizer': 'Adam', 'weight_decay': 0.00023837966392902175, 'positive_function': 'sig', 'epochs_complete_problem': 10, 'reg': True, 'transformers_model': True, 'activation_gcn': 'PReLU', 'dropout_gcn': 0.98470581307162, 'hidden_channels': 128, 'layer_type': 'GAT', 'norm': 'PairNorm', 'num_layers_gcn': 6, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8190940671037286 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'Hardswish', 'batch_size': 16, 'concatenate_features': False, 'd_model': 312, 'dropout': 0.5396895566751263, 'dropout_StationIdEmbedding': 0.8075607913239145, 'dropout_timeStampEmbedding': 0.35021425948049095, 'dropout_transformers': 0.45738321584959396, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 45, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 0.014809498970484393, 'max_lr': 0.1296010430061614, 'mode': 'exp_range', 'scheduler': 'CyclicLR', 'step_size_up': 29, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 24, 'num_layers_transformer': 2, 'lr': 0.05041525421689873, 'momentum': 0.4714499827178006, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.313315942268527, 'positive_function': 'sig', 'epochs_complete_problem': 23, 'reg': True, 'transformers_model': True, 'activation_gcn': 'LeakyReLU', 'dropout_gcn': 0.5838443304631716, 'hidden_channels': 256, 'layer_type': 'GAT', 'norm': 'PairNorm', 'num_layers_gcn': 3, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5892482624658961 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Sigmoid', 'activation_transformers': 'Hardshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 336, 'dropout': 0.02871094933921825, 'dropout_StationIdEmbedding': 0.5973034841166309, 'dropout_timeStampEmbedding': 0.9761405416663385, 'dropout_transformers': 0.9935022305177372, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 76, 'input_size': 2, 'learnable_pos_encoding': False, 'T_max': 2, 'eta_min': 1.0163479384012925e-05, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.5892482624658961, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'ReLU6', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 4, 'amsgrad': True, 'beta_1': 0.933261928544172, 'beta_2': 0.9825257647149878, 'eps': 5.267299521705942e-07, 'lr': 3.776445571903993e-07, 'optimizer': 'Adam', 'weight_decay': 1.5278438743409054e-05, 'positive_function': 'relu', 'epochs_complete_problem': 39, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'PReLU', 'activation_transformers': 'Hardtanh', 'batch_size': 16, 'concatenate_features': True, 'd_model': 912, 'dropout': 0.19181154680743065, 'dropout_StationIdEmbedding': 0.9911896237026416, 'dropout_timeStampEmbedding': 0.5879748932864318, 'dropout_transformers': 0.8535716725540936, 'early_stopping': 10, 'encoder_only': True, 'epochs_classifcation_only': 5, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 2, 'eta_min': 0.0004051141569890603, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 24, 'num_layers_transformer': 3, 'amsgrad': True, 'beta_1': 0.9990764989068798, 'beta_2': 0.9530714655773819, 'eps': 1.7888012567440073e-07, 'lr': 1.0748967559938573e-05, 'optimizer': 'Adam', 'weight_decay': 0.001278147777854455, 'positive_function': 'relu', 'epochs_complete_problem': 26, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  8.07371009698435 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  7.685710229793517 acc:  0.0017863921577384275\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  7.443371820850532 acc:  0.0022999799030882255\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  7.400675549226649 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  7.383055041818058 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  8.433155604771205 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  8.507695506600772 acc:  0.002523278922805529\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  8.489301949989896 acc:  0.0025679387267489896\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  8.47714658544845 acc:  0.002545608824777259\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  8.507470303222913 acc:  0.0023669696090034163\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  8.44711613855442 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  8.475712744127803 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  8.474041990873193 acc:  0.0026349284326641804\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  8.46684570873485 acc:  0.0028358975504097538\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  8.40850657374919 acc:  0.0029028872563249446\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  8.362995600500026 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  8.44516944083847 acc:  0.0025679387267489896\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  8.429168080081459 acc:  0.0023669696090034163\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  8.402783770521149 acc:  0.002143670589286113\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  8.329125596695587 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  8.370917095857507 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  8.420888696398054 acc:  0.002232990197173034\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  8.39024408324426 acc:  0.002545608824777259\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  8.360802209677816 acc:  0.003304825491816091\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  8.36158360553389 acc:  0.0033494852957595515\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  8.377133673980456 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  8.320778385931705 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  8.400482678613743 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  8.36517995545844 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  8.32918325392138 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  8.374940363298945 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Mish', 'batch_size': 128, 'concatenate_features': False, 'd_model': 48, 'dropout': 0.9599599377538068, 'dropout_StationIdEmbedding': 0.4207419624484802, 'dropout_timeStampEmbedding': 0.42032324427711865, 'dropout_transformers': 0.5391492972432135, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 43, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 6.692661919810062e-05, 'max_lr': 0.05016332037826411, 'mode': 'triangular2', 'scheduler': 'CyclicLR', 'step_size_up': 20, 'dropout_lstm': 0.005930944669753857, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 12, 'num_layers_transformer': 6, 'lr': 6.980446531435832e-05, 'momentum': 0.012510845552782368, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 4.867302302555901e-06, 'positive_function': 'exp', 'epochs_complete_problem': 14, 'reg': True, 'transformers_model': True, 'activation_gcn': 'LogSigmoid', 'dropout_gcn': 0.026928764193590138, 'hidden_channels': 128, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 7, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.005930944669753857 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  8.686156132346705 acc:  0.0004465980394346069\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  8.60980284841437 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  8.49911426744963 acc:  0.0003126186276042248\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  8.424844149539345 acc:  0.0002456289216890338\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  8.371795523794074 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  8.333946499071622 acc:  0.0006922269611236406\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  8.28223560734799 acc:  0.001027175490699596\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  8.218767999347888 acc:  0.002076680883370922\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  8.131423533590217 acc:  0.002925217158296675\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  8.020259084199605 acc:  0.0036174441194203157\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  7.901914867601897 acc:  0.0036397740213920463\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  7.786102495695415 acc:  0.0037960833351941585\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  7.6721999168396 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  7.593778073160272 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  7.51016933039615 acc:  0.004488310296317799\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  7.460464020779258 acc:  0.004376660786459147\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  7.415165002722489 acc:  0.004220351472657035\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  7.381454573179546 acc:  0.00457762990420472\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  7.344120843786943 acc:  0.004398990688430878\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  7.327186760149504 acc:  0.004711609316035103\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  7.296949517099481 acc:  0.004867918629837215\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  7.281625185514751 acc:  0.003818413237165889\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  7.267205333709716 acc:  0.0048455887278654845\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  7.2612669693796255 acc:  0.005225197061384901\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  7.25607425790084 acc:  0.005872764218565081\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  7.243738345095986 acc:  0.0056717951008195076\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  7.2488248925460015 acc:  0.00515820735546971\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  7.2441333017851175 acc:  0.007368867650671014\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  7.227741196281031 acc:  0.005225197061384901\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  7.229402632462351 acc:  0.005917424022508541\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  7.222329460947138 acc:  0.005962083826452002\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  7.2208047666047745 acc:  0.005493155885045665\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  7.217106081310072 acc:  0.006408681865886608\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  7.222782375938014 acc:  0.005917424022508541\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  7.210481437883879 acc:  0.006185382846169305\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Hardsigmoid', 'batch_size': 16, 'concatenate_features': False, 'd_model': 864, 'dropout': 0.530864328646683, 'dropout_StationIdEmbedding': 0.5734420846694784, 'dropout_timeStampEmbedding': 0.2289745911000377, 'dropout_transformers': 0.4241367210174926, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 16, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.0012499298747719045, 'scheduler': 'ExponentialLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 5, 'amsgrad': False, 'beta_1': 0.8092649590697468, 'beta_2': 0.9861468063704495, 'eps': 2.096439470849257e-09, 'lr': 6.872976873201176e-06, 'optimizer': 'Adam', 'weight_decay': 1.0126490439148928e-06, 'positive_function': 'sig', 'epochs_complete_problem': 42, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Softshrink', 'dropout_gcn': 0.45102094330981835, 'hidden_channels': 512, 'layer_type': 'GAT', 'norm': 'LayerNorm', 'num_layers_gcn': 10, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  7.983290035306043 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  7.883747945305045 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  7.8787633735714975 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  7.888237585548226 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  7.888084429820985 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  7.890391317032676 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  7.888014174599684 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Tanhshrink', 'activation_transformers': 'Sigmoid', 'batch_size': 128, 'concatenate_features': True, 'd_model': 216, 'dropout': 0.8111906187891494, 'dropout_StationIdEmbedding': 0.790091380648746, 'dropout_timeStampEmbedding': 0.7973492994383862, 'dropout_transformers': 0.6442532769556406, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 50, 'input_size': 2, 'learnable_pos_encoding': False, 'T_max': 25, 'eta_min': 0.32652290090419206, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.08071215445972812, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'PReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 24, 'num_layers_transformer': 4, 'lr': 0.007683460016217922, 'momentum': 0.37050981359324386, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.006906876154550657, 'positive_function': 'relu', 'epochs_complete_problem': 29, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.08071215445972812 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  8.66463127869826 acc:  0.001027175490699596\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  8.464838145329402 acc:  0.0046222897081481815\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  8.20645870062021 acc:  0.007994104905879464\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  7.853411436080933 acc:  0.009400888730098474\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  7.59308888728802 acc:  0.01103097157403479\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  7.4211236183459945 acc:  0.01299600294754706\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  7.30122703405527 acc:  0.01752897304780832\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  7.204567021590012 acc:  0.024920170600451062\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  7.129659498654879 acc:  0.02279882991313668\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  7.078083097017728 acc:  0.055824754929325864\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  7.028105882497934 acc:  0.02407163432552531\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  6.997994591639592 acc:  0.021235736775115557\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  6.9540163883796104 acc:  0.03706763727307237\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  6.935874836261456 acc:  0.020900788245539603\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  6.892827239403358 acc:  0.040997700020096915\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  6.950068983664879 acc:  0.01951633432329232\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  7.156177568435669 acc:  0.010874662260232678\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  7.268460673552293 acc:  0.010003796083335193\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9948722394784901 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'SiLU', 'activation_transformers': 'Softmin', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1200, 'dropout': 0.8978344152517316, 'dropout_StationIdEmbedding': 0.6351256648989213, 'dropout_timeStampEmbedding': 0.5332537847067437, 'dropout_transformers': 0.7912071529729393, 'early_stopping': 9, 'encoder_only': True, 'epochs_classifcation_only': 71, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.8894909941303533, 'scheduler': 'StepLR', 'step_size': 5, 'dropout_lstm': 0.9948722394784901, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'ReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 48, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.9527327445081933, 'beta_2': 0.9710864404063841, 'eps': 1.052996476375188e-06, 'lr': 0.005467900278907302, 'optimizer': 'Adam', 'weight_decay': 0.32635530412566527, 'positive_function': 'sig', 'epochs_complete_problem': 34, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'GELU', 'batch_size': 64, 'concatenate_features': False, 'd_model': 360, 'dropout': 0.7362641889989356, 'dropout_StationIdEmbedding': 0.7086529012877063, 'dropout_timeStampEmbedding': 0.9316416923123648, 'dropout_transformers': 0.9257699731437827, 'early_stopping': 10, 'encoder_only': True, 'epochs_classifcation_only': 38, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 3.225760868408335e-07, 'max_lr': 0.19963975408846357, 'mode': 'triangular', 'scheduler': 'CyclicLR', 'step_size_up': 6, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 12, 'num_layers_transformer': 6, 'lr': 0.0001313744948603213, 'momentum': 0.1136222767379999, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 1.1208283884249933e-05, 'positive_function': 'relu', 'epochs_complete_problem': 2, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Hardtanh', 'dropout_gcn': 0.7992180228498926, 'hidden_channels': 64, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 3, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  8.106738986739193 acc:  0.00022329901971730344\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  8.096878063247864 acc:  0.0003126186276042248\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  8.0654021803155 acc:  0.0006922269611236406\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  8.014328485511871 acc:  0.0017863921577384275\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  7.945225715637207 acc:  0.003974722550968001\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  7.84243594020246 acc:  0.0041310318647701134\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  7.703721948416836 acc:  0.003550454413505125\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  7.563149578600044 acc:  0.004108701962798384\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  7.4806469319814655 acc:  0.0036397740213920463\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  7.440552050808826 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  7.408374499125653 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  7.391842428460179 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  7.381083792950734 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  7.3808426971895145 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  7.370722977511854 acc:  0.003572784315476855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  7.352582041039525 acc:  0.005046557845611058\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  7.343863021896546 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  7.321549346648067 acc:  0.0027019181385793717\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  7.315216926207025 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  7.286428440048034 acc:  0.004309671080543956\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  7.293645404907594 acc:  0.00326016568787263\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  7.267058159931596 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  7.2859260662492495 acc:  0.0037960833351941585\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  7.2814811223960785 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  7.2534616252025925 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  7.27037342485175 acc:  0.0036397740213920463\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7881903375504518 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Softshrink', 'activation_transformers': 'ELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1224, 'dropout': 0.5671097847313655, 'dropout_StationIdEmbedding': 0.2431507577512859, 'dropout_timeStampEmbedding': 0.6683385154989354, 'dropout_transformers': 0.3828629598690466, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 80, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7881903375504518, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8477600838001569, 'beta_2': 0.990649735091279, 'eps': 1.4317116847659442e-07, 'lr': 1.0082885521871854e-05, 'optimizer': 'Adam', 'weight_decay': 2.891384205322503e-07, 'positive_function': 'relu', 'epochs_complete_problem': 11, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  8.393771321361601 acc:  0.0016524127459080454\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  8.158955826185137 acc:  0.008038764709822925\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  7.7365623469128035 acc:  0.017037715204430252\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  7.310345010607654 acc:  0.04090838041220999\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  6.94189758700226 acc:  0.08291092602103477\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  6.599139820218711 acc:  0.12292611035437555\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  6.336469393126003 acc:  0.15036955987763215\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  6.134011380959556 acc:  0.17714311234173682\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  5.914486735279024 acc:  0.1967041064689726\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  5.755203499219805 acc:  0.20865060402384833\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  5.568563391400882 acc:  0.22414755599222919\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  5.459024614064481 acc:  0.2346649398209142\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  5.3222728824116174 acc:  0.24424446776678652\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  5.212988401582728 acc:  0.2532210883594221\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  5.152501488231239 acc:  0.26128218297121675\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  4.991655893974904 acc:  0.26804814326865106\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  4.900922560567007 acc:  0.27664515552776725\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  4.828633091212567 acc:  0.2835897550409754\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  4.735040366337562 acc:  0.2904227050443249\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  4.658102432470671 acc:  0.29680905700823973\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  4.58191811601529 acc:  0.3023245427952571\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  4.468177138822865 acc:  0.3090905030926914\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  4.421309464889047 acc:  0.31433803005604805\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  4.378862665585823 acc:  0.3200991447647545\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  4.311449082109941 acc:  0.32447580555121364\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  4.249326461272714 acc:  0.3290757653573901\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  4.2136588471098095 acc:  0.3333631065359623\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  4.132749450144344 acc:  0.33677958153763704\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  4.085321212938319 acc:  0.3395931491860751\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  4.043153770306971 acc:  0.34374651095281694\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  4.020919062080184 acc:  0.3466717281111136\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  3.9477707041495758 acc:  0.35075810017194026\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  3.9166928610876592 acc:  0.3532367192908023\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  3.8945375774543325 acc:  0.35703280262599646\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  3.830124894985978 acc:  0.35877453497979145\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  3.800717498619519 acc:  0.36132014380456867\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  3.770339198137453 acc:  0.36399973204117636\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  3.7480004695073474 acc:  0.36480360851215865\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  3.7064090162047543 acc:  0.36755018645468146\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  39 loss :  3.6596119915627683 acc:  0.3688899805729853\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  40 loss :  3.654220479945238 acc:  0.3709666614563562\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  41 loss :  3.6135757843237273 acc:  0.37346761047719\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  42 loss :  3.5840767902853603 acc:  0.37556662126253265\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  43 loss :  3.5540845406617168 acc:  0.37688408547886476\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  44 loss :  3.527918197721711 acc:  0.380010271754907\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  45 loss :  3.503174197611384 acc:  0.38056851930420027\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  46 loss :  3.4774178996760186 acc:  0.3824665609717973\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  47 loss :  3.4711187322726422 acc:  0.3852131389143202\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  48 loss :  3.4440934757911723 acc:  0.3849451800906594\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  49 loss :  3.4162854062325043 acc:  0.38688788156219994\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  50 loss :  3.385612260608773 acc:  0.3889868923475426\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  51 loss :  3.37618759539739 acc:  0.3904606658776768\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  52 loss :  3.344518067324973 acc:  0.39159949087823503\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  53 loss :  3.320195862135962 acc:  0.39255967666301944\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  54 loss :  3.2954250780075633 acc:  0.3949713060759663\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  55 loss :  3.276284478721818 acc:  0.3946586874483621\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  56 loss :  3.2758282404295436 acc:  0.3961324609784963\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  57 loss :  3.251275636762849 acc:  0.397896523234263\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  58 loss :  3.2507132722445182 acc:  0.39820914186186723\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  59 loss :  3.220857016079089 acc:  0.3982984614697542\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  60 loss :  3.1977214338891793 acc:  0.40033048254918163\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  61 loss :  3.1949254093369888 acc:  0.40057611147087063\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  62 loss :  3.1843892117445383 acc:  0.4010227095103053\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  63 loss :  3.1676931318812347 acc:  0.4023848335305808\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  64 loss :  3.1438020748617763 acc:  0.4029877408838175\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  65 loss :  3.1236706653814665 acc:  0.4029877408838175\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  66 loss :  3.096973784931043 acc:  0.40455083402183867\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  67 loss :  3.0922128991930897 acc:  0.40662751490520954\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  68 loss :  3.090031631329921 acc:  0.4062702364736619\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  69 loss :  3.081799377321573 acc:  0.4068061541209834\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  70 loss :  3.0674431873241645 acc:  0.4085925462787218\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  71 loss :  3.0399494533139375 acc:  0.4070517830426724\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  72 loss :  3.048680650002045 acc:  0.40955273206350623\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  73 loss :  3.0284451240020274 acc:  0.41008864971082776\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  74 loss :  3.0224917434272967 acc:  0.41082553647589487\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  75 loss :  3.008364797262621 acc:  0.4120983408882835\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  76 loss :  2.99371709873539 acc:  0.4117857222606793\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  77 loss :  2.981871862061985 acc:  0.4132594957908135\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  78 loss :  2.975150576436707 acc:  0.41361677422236115\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  79 loss :  2.9733233651565634 acc:  0.4136391041243329\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  80 loss :  4.033896783259527 acc:  0.41413036196771097\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  81 loss :  3.9754851363716326 acc:  0.4143090011834848\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  82 loss :  3.958176800093726 acc:  0.4150682178505236\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  83 loss :  3.948692114565385 acc:  0.4148225889288346\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  84 loss :  3.962065900183473 acc:  0.4154478261840431\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  85 loss :  3.9462621424210633 acc:  0.4171672286358663\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  86 loss :  3.9173873319675785 acc:  0.4175468369693857\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  87 loss :  3.912364627678357 acc:  0.4191545899113503\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  88 loss :  3.907600236812811 acc:  0.41884197128374606\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  89 loss :  3.8966268706696194 acc:  0.4191099301074068\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  90 loss :  3.88383840890455 acc:  0.41919924971529376\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8247204716264702 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Softshrink', 'activation_transformers': 'Softshrink', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1272, 'dropout': 0.12854579791817278, 'dropout_StationIdEmbedding': 0.19238450021312126, 'dropout_timeStampEmbedding': 0.6538570582005737, 'dropout_transformers': 0.3992301696591553, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 80, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'dropout_lstm': 0.8247204716264702, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8386744976190655, 'beta_2': 0.9907557316370873, 'eps': 1.6619452567615624e-07, 'lr': 1.0323229721054282e-05, 'optimizer': 'Adam', 'weight_decay': 1.3631584396945346e-07, 'positive_function': 'relu', 'epochs_complete_problem': 7, 'reg': True, 'transformers_model': True, 'activation_gcn': 'Hardshrink', 'dropout_gcn': 0.07379743955334861, 'hidden_channels': 256, 'layer_type': 'GAT', 'norm': 'PairNorm', 'num_layers_gcn': 5, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  8.201357873458436 acc:  0.002679588236607641\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  8.019099858886037 acc:  0.001563093138021124\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  8.018220192893258 acc:  0.001563093138021124\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  8.017428078464956 acc:  0.001563093138021124\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  8.016528720962269 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8303086957303105 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Softshrink', 'activation_transformers': 'ELU', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1248, 'dropout': 0.49585900096956337, 'dropout_StationIdEmbedding': 0.2864036836777289, 'dropout_timeStampEmbedding': 0.7806548104755296, 'dropout_transformers': 0.13241337753429783, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 75, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.8303086957303105, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8774676219672991, 'beta_2': 0.9907819147418413, 'eps': 4.9234655156455195e-06, 'lr': 1.770748858481716e-06, 'optimizer': 'Adam', 'weight_decay': 2.247764259217346e-09, 'positive_function': 'relu', 'epochs_complete_problem': 13, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  8.642015472132497 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  8.616681213778351 acc:  0.00042426813746287653\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  8.58664959013774 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  8.55744610531792 acc:  0.0006475671571801799\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  8.534793384412197 acc:  0.0008485362749257531\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  8.508793546267205 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  8.479249494862183 acc:  0.001116495098586517\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  8.448816923570883 acc:  0.001362124020275551\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  8.416285604706609 acc:  0.0017863921577384275\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  8.393054862297019 acc:  0.002143670589286113\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  8.362920346684481 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  8.322897851155066 acc:  0.003304825491816091\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  8.286978242284965 acc:  0.00453297010026126\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  8.232911896331148 acc:  0.005895094120536811\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  8.182709798762936 acc:  0.0076144965723600475\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  8.127115399425566 acc:  0.009378558828126745\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  8.067036141899868 acc:  0.010696023044458835\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  7.993489599976864 acc:  0.011522229417412858\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  7.916103702565138 acc:  0.012460085300225531\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  7.856822625504738 acc:  0.012616394614027644\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  7.781373353528727 acc:  0.013152312261349173\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  7.733510381888345 acc:  0.013576580398812049\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  7.68392515432148 acc:  0.015072683830917982\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  7.638252470505799 acc:  0.017439653439921397\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  7.6036186642671755 acc:  0.019627983833150973\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  7.562254646061603 acc:  0.022285242167786882\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  7.502822062107905 acc:  0.024160953933412232\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  7.487838962315265 acc:  0.026505593640443918\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  7.46414274445379 acc:  0.028850233347475603\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  7.427580394045845 acc:  0.03313757452604783\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  7.3575204554652665 acc:  0.037313266194761406\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  7.367188920525356 acc:  0.04046178237277538\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  7.306950302024163 acc:  0.04515106178683875\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  7.265299020637392 acc:  0.049773351494986934\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  7.25853721758458 acc:  0.05636067257664739\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  7.211456803127109 acc:  0.05917424022508541\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  7.118647767611199 acc:  0.0637295402273184\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  7.107178383472703 acc:  0.0719692740548869\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  7.052242408872275 acc:  0.0778643681754237\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  39 loss :  6.993163553207957 acc:  0.08195074023625036\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  40 loss :  6.992847232918464 acc:  0.08809146327847621\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  41 loss :  6.925544207008722 acc:  0.0923564745550767\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  42 loss :  6.9040186168011575 acc:  0.09666614563562066\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  43 loss :  6.82516409839011 acc:  0.10195833240292075\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  44 loss :  6.786362912642394 acc:  0.10470491034544359\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  45 loss :  6.789507054533634 acc:  0.1085456534845812\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  46 loss :  6.758087335456728 acc:  0.11602617064511087\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  47 loss :  6.658436717787338 acc:  0.11834848045017082\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  48 loss :  6.65545282313961 acc:  0.1217202956479021\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  49 loss :  6.614573608518271 acc:  0.1265882142777393\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  50 loss :  6.572210094691571 acc:  0.12924547261237523\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  51 loss :  6.582849030719378 acc:  0.1340910613402407\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  52 loss :  6.560628404167934 acc:  0.13779782506754795\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  53 loss :  6.45596263296317 acc:  0.1398298461469754\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  54 loss :  6.500498759184833 acc:  0.14418417703146283\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  55 loss :  6.477260524689839 acc:  0.14619386820891855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  56 loss :  6.36313905266567 acc:  0.14936471428890427\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  57 loss :  6.363815117880936 acc:  0.15237925105508787\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  58 loss :  6.348732451493827 acc:  0.15559475693901703\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  59 loss :  6.353991216389921 acc:  0.15834133488153987\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  60 loss :  6.302003119004334 acc:  0.16057432507871292\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  61 loss :  6.240001576109083 acc:  0.1627849853739142\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  62 loss :  6.293212803246464 acc:  0.16651407900319318\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  63 loss :  6.1980265647328965 acc:  0.16812183194515776\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  64 loss :  6.180254791419544 acc:  0.17098005939753924\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  65 loss :  6.168719384058607 acc:  0.1735256682223165\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  66 loss :  6.112228698131301 acc:  0.17636156577272627\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  67 loss :  6.126704702826695 acc:  0.17680816381216086\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  68 loss :  6.0965251647989165 acc:  0.17901882410736217\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  69 loss :  6.065308982789205 acc:  0.18239063930509344\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  70 loss :  6.03707052150946 acc:  0.18292655695241497\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  71 loss :  6.064602477388232 acc:  0.18603041332648548\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  72 loss :  5.988251436443229 acc:  0.18815175401379988\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  73 loss :  6.039702035993805 acc:  0.18942455842618852\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  74 loss :  5.934584929680949 acc:  0.19121095058392693\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  75 loss :  7.1141421657582224 acc:  0.19056338342674675\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  76 loss :  6.958473532611787 acc:  0.19290802313377844\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  77 loss :  6.944847254228842 acc:  0.1956099412723578\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  78 loss :  6.927888770378073 acc:  0.19665944666502913\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  79 loss :  6.917497227953366 acc:  0.19735167362615277\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  80 loss :  6.896544161891438 acc:  0.20043320009825158\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  81 loss :  6.867457552105969 acc:  0.2020856128441596\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  82 loss :  6.874580617974566 acc:  0.20282249960922671\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  83 loss :  6.80569282002474 acc:  0.20322443784471786\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  84 loss :  6.8097245081557025 acc:  0.20387200500189803\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  85 loss :  6.757179405052624 acc:  0.20733313980751625\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  86 loss :  6.818542105989306 acc:  0.20748944912131836\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  87 loss :  6.743042608830317 acc:  0.20894089274948083\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6593645563236669 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'ELU', 'batch_size': 16, 'concatenate_features': True, 'd_model': 1008, 'dropout': 0.3605725842176154, 'dropout_StationIdEmbedding': 0.3609065767551209, 'dropout_timeStampEmbedding': 0.570607302955783, 'dropout_transformers': 0.5624993018383793, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 59, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'dropout_lstm': 0.6593645563236669, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8365637728609997, 'beta_2': 0.9770824657171174, 'eps': 5.985097869887318e-07, 'lr': 6.341197488531829e-05, 'optimizer': 'Adam', 'weight_decay': 0.00016958082404274464, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Hardswish', 'dropout_gcn': 0.36585932478972727, 'hidden_channels': 1024, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 9, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  7.714339630332535 acc:  0.013308621575151286\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  7.31909578860163 acc:  0.040082174039255965\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  6.959034114540694 acc:  0.06935667552419444\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  6.504909284100561 acc:  0.10629033338543643\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  6.186875821587568 acc:  0.1492307348770739\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  5.947606061033146 acc:  0.16885871871022487\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  5.774046111249638 acc:  0.18634303195408972\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  5.5414427711578185 acc:  0.20757876872920528\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  5.312444118682496 acc:  0.22358930844293592\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  5.112605036375765 acc:  0.23486590893865977\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  5.049270183026434 acc:  0.2456512515910055\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  4.979922875672758 acc:  0.25069780943661657\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  4.879403526911479 acc:  0.25695018198870107\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  4.754583066095135 acc:  0.2718888864077887\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  4.675475026319127 acc:  0.27356362905566844\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  4.548041298003968 acc:  0.2818256927852087\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  4.561986854690278 acc:  0.28834602416095395\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  4.402683879087071 acc:  0.29421878837951904\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  4.295075062506213 acc:  0.30196726436370946\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  4.1024178502088535 acc:  0.30763905946452896\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  4.1438078152205415 acc:  0.308710894759172\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  4.201592773734452 acc:  0.31308755554563117\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  4.008242084594544 acc:  0.3222204854520689\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  4.024054373095849 acc:  0.32054574280418907\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  3.9928016705427343 acc:  0.32742335261148203\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  3.9496671139837027 acc:  0.3294330437889378\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  3.877458306843649 acc:  0.3308398276131568\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  3.8759879066558653 acc:  0.3343902820266619\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  3.7553777666149024 acc:  0.3378514168322801\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  3.8564592692666424 acc:  0.3448630060514034\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  3.768440403624209 acc:  0.3479445325235022\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  3.7425680645925556 acc:  0.34982024428912756\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  3.698189266427548 acc:  0.3494629658575799\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  3.716522719331844 acc:  0.3487484089944845\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  3.6823877531611275 acc:  0.3552017506643146\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  3.6037627522816913 acc:  0.35627358595895764\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  3.5717223304474426 acc:  0.3565862045865619\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  3.583915499156107 acc:  0.35805997811669604\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  3.466619929153762 acc:  0.3602259786079539\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  39 loss :  3.5733525710191554 acc:  0.3667016501797557\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  40 loss :  3.493040508852747 acc:  0.36607641292454723\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  41 loss :  3.517112582029697 acc:  0.36909094969073086\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  42 loss :  3.4507786171165056 acc:  0.3695598776321372\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  43 loss :  3.5056251266045484 acc:  0.37201616684902755\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  44 loss :  3.389285313155123 acc:  0.37319965165352925\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  45 loss :  3.351668297173734 acc:  0.37297635263381196\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  46 loss :  3.351295409802191 acc:  0.3764374874394301\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  47 loss :  3.3146463489818 acc:  0.3778666011656209\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  48 loss :  3.3775926707033626 acc:  0.37554429136056094\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  49 loss :  3.355611386413346 acc:  0.3773976732242145\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  50 loss :  3.2707533886332714 acc:  0.3799209521470201\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  51 loss :  3.3357702001126226 acc:  0.37873746734251834\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  52 loss :  3.353751909233139 acc:  0.3768394256749213\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  53 loss :  3.261646851808011 acc:  0.38217627224616485\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  54 loss :  3.3315910836179814 acc:  0.379184065381953\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  55 loss :  3.296845140571366 acc:  0.3815510349909564\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  56 loss :  3.2486846789628445 acc:  0.38510148940446154\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  57 loss :  3.264022488793927 acc:  0.38320344773686443\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  58 loss :  3.2027750514938447 acc:  0.38702186097403035\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6820397738894634 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'ELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 984, 'dropout': 0.36137238237566216, 'dropout_StationIdEmbedding': 0.001895044032539983, 'dropout_timeStampEmbedding': 0.368372085802816, 'dropout_transformers': 0.5720927204261574, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 73, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.6820397738894634, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.8358288102519749, 'beta_2': 0.9748144032174882, 'eps': 6.165560646873057e-08, 'lr': 9.43401259328304e-05, 'optimizer': 'Adam', 'weight_decay': 5.366267094044646e-05, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  7.369270573119204 acc:  0.08353616327624322\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  6.16626712376486 acc:  0.1955206216644709\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  5.508271168805882 acc:  0.24364156041354978\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  5.01468244998041 acc:  0.27316169082017727\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  4.655061326341001 acc:  0.29564790210570974\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  4.3968677463645705 acc:  0.31645937074336244\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  4.149057578183934 acc:  0.33347475604582094\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  3.951834041915254 acc:  0.3432105933054954\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  3.8008824665389374 acc:  0.35410758546769977\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  3.7088656868049483 acc:  0.3633298349820244\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  3.5852513741590304 acc:  0.3684433825335507\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  3.474619397146259 acc:  0.3754996315566175\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  3.4287025314605164 acc:  0.3801665810687091\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  3.3255940839915934 acc:  0.38726748989571935\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  3.2692347715000905 acc:  0.3908179443092245\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  3.2087323223045487 acc:  0.39604314137060936\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  3.1564241811900797 acc:  0.3969586673514503\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  3.070671838200735 acc:  0.4000848536274926\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  3.068474004368582 acc:  0.4041265658843758\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  3.028635307700334 acc:  0.4050867516691602\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  2.9795239071646136 acc:  0.4082799276511176\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  2.93795249419298 acc:  0.41022262912265817\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  2.9462309554665387 acc:  0.41185271196659445\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  2.898481698807128 acc:  0.4135051247125025\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  2.874725049127362 acc:  0.41558180559587343\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  2.813196256489097 acc:  0.41803809481276377\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  2.788663448687799 acc:  0.41866333206797224\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  2.786900733045475 acc:  0.41946720853895453\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  2.743373646707592 acc:  0.41832838353839624\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  2.7433091666170224 acc:  0.421499229618382\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  2.7139087565644773 acc:  0.42221378648147734\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  2.690439873826718 acc:  0.420918652167117\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  2.688309710896658 acc:  0.42252640510908157\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  2.664599332980767 acc:  0.4240001786392158\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  2.635271516388762 acc:  0.42458075609048074\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  2.6376229503197584 acc:  0.4262331688363888\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  2.620314466739129 acc:  0.42627782864033226\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  2.6061099053856855 acc:  0.42804189089609895\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  2.60455999688474 acc:  0.4272603443270884\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  39 loss :  2.5723570692325066 acc:  0.4274836433468057\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  40 loss :  2.5500233344689103 acc:  0.42824286001384454\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  41 loss :  2.5440524580949795 acc:  0.43058749972087623\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  42 loss :  2.535510127415914 acc:  0.4301185717794699\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  43 loss :  2.529737835158845 acc:  0.42980595315186565\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.46012352861682093 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'ELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1176, 'dropout': 0.24085212538021228, 'dropout_StationIdEmbedding': 0.002762140268031453, 'dropout_timeStampEmbedding': 0.2054455380730971, 'dropout_transformers': 0.3616520701255228, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 71, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.46012352861682093, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.8303874274237837, 'beta_2': 0.9643748194109263, 'eps': 6.434880517763961e-08, 'lr': 5.506564773909709e-05, 'optimizer': 'Adam', 'weight_decay': 1.4197120191748532e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  7.422426777685474 acc:  0.04566464953218855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  6.6032871029334155 acc:  0.153361766741844\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  5.956253442935601 acc:  0.21086126431904964\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  5.4566089293200095 acc:  0.24471339570819284\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  5.122436195076583 acc:  0.26849474130808565\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  4.7683666823153015 acc:  0.2878101065136324\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  4.537703287101792 acc:  0.3050934506397517\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  4.355706286287593 acc:  0.32139427907911483\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  4.204001346748032 acc:  0.33066118839738295\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  4.047812950111435 acc:  0.3415135207556439\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  3.9102917502740184 acc:  0.3506911104660251\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  3.8006677784605656 acc:  0.35703280262599646\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  3.686207871237201 acc:  0.3659424335127169\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  3.6122037536369827 acc:  0.371480249201706\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  3.530211925506592 acc:  0.37726369381238417\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  3.4511989233736506 acc:  0.38121608646138044\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  3.3660451252303436 acc:  0.3864189536207936\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  3.316565038201338 acc:  0.3896567894066945\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  3.2726234033436117 acc:  0.3942120894089275\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  3.204552816059775 acc:  0.3971373065672242\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  3.1776026337446566 acc:  0.40030815264720987\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  3.1280816757750367 acc:  0.4045731639238104\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  3.0581911109878632 acc:  0.40638188598352054\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  3.041192478762415 acc:  0.40854788647477835\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  3.02957374464252 acc:  0.41145077373110334\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  2.993438306682838 acc:  0.4124779492218029\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  2.956670581223722 acc:  0.415782774713619\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  2.950464499924711 acc:  0.41712256883192284\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  2.910261358329636 acc:  0.4195341982448697\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  2.8731744731971602 acc:  0.42082933255923005\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  2.8350227178927665 acc:  0.4229953330504879\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  2.82024120856188 acc:  0.424067168345131\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  2.8202491291982685 acc:  0.4249603644240002\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  2.7797145872058984 acc:  0.4259875399146998\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  2.765624812977043 acc:  0.426545787463993\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  2.7486962287011973 acc:  0.428287519817788\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  2.7159761483083944 acc:  0.4300515820735547\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  2.7103197103488945 acc:  0.429694303642007\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  2.7040610913031116 acc:  0.42940401491637453\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  39 loss :  2.677229661427572 acc:  0.43090011834848047\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  40 loss :  2.642909034997403 acc:  0.4298729428577809\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  41 loss :  2.6440740959373064 acc:  0.43170399481946276\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  42 loss :  2.624294876338479 acc:  0.4324632114865016\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  43 loss :  2.6247150369746954 acc:  0.4318826340352366\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  44 loss :  2.585557960464569 acc:  0.4355447379586004\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  45 loss :  2.6049719113789633 acc:  0.4345622222718442\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  46 loss :  2.5942177629756356 acc:  0.43422727374226827\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.43434834532781436 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'ELU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1152, 'dropout': 0.011013474382048194, 'dropout_StationIdEmbedding': 0.00790098293560111, 'dropout_timeStampEmbedding': 0.2214004849006575, 'dropout_transformers': 0.2814219152974762, 'early_stopping': 2, 'encoder_only': False, 'epochs_classifcation_only': 72, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.43434834532781436, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8173809535101957, 'beta_2': 0.9629981549756063, 'eps': 5.535905596153284e-08, 'lr': 0.0019149273234474703, 'optimizer': 'Adam', 'weight_decay': 1.600197278886951e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  6.978514671325684 acc:  0.15755978831252931\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  5.578635058036217 acc:  0.19835651921488065\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  5.0047308628375715 acc:  0.21032534667172811\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  4.708114049984858 acc:  0.21282629569256192\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  4.514688315758338 acc:  0.2154612241252261\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  4.370883220892686 acc:  0.21860974030324007\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  4.243651584478525 acc:  0.2109059241229931\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  4.1533183611356295 acc:  0.20706518098385548\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30367792710605657 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'LeakyReLU', 'activation_transformers': 'PReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1320, 'dropout': 0.13199651356910314, 'dropout_StationIdEmbedding': 0.1349210556895789, 'dropout_timeStampEmbedding': 0.19303613251472868, 'dropout_transformers': 0.6966804329161463, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 66, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.30367792710605657, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.8963213758870979, 'beta_2': 0.9540348522499347, 'eps': 2.338940433303009e-09, 'lr': 0.00011960294116477681, 'optimizer': 'Adam', 'weight_decay': 0.00010384410442619273, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  7.34616985854569 acc:  0.07335372797713419\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  6.368587720644224 acc:  0.16941696625951813\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  5.776766323543095 acc:  0.22696112364066723\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  5.301588812074461 acc:  0.2568162025768707\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  4.956083596169532 acc:  0.2793470736663466\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  4.665060363449417 acc:  0.29696536632204185\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  4.4769617344116 acc:  0.3136904628988679\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  4.2115654945373535 acc:  0.32226514525601235\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  3.9957482781443563 acc:  0.3318223432999129\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  3.9033298008925432 acc:  0.33773976732242145\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  3.773499390462062 acc:  0.34591251144407476\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  3.6840956244435343 acc:  0.35200857468235713\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  3.581098108024864 acc:  0.35995801978429315\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  3.455324037925347 acc:  0.36462496929638477\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  3.40096997214364 acc:  0.36987249625974145\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  3.3185966781802945 acc:  0.3730880021436706\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  3.2852154094856103 acc:  0.37632583792957147\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  3.197357502850619 acc:  0.38157336489292815\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  3.184345698856807 acc:  0.3843646026393944\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  3.1070162349647577 acc:  0.3892101913672599\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  3.0779482201262787 acc:  0.39133153205457427\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  3.0380302342501553 acc:  0.39059464528950716\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  3.0290401482081912 acc:  0.3961771207824398\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  2.9850152672587575 acc:  0.39718196637116765\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  2.9434636556185207 acc:  0.39992854431369046\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  2.934349920366194 acc:  0.40131299823593775\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  2.889656372003622 acc:  0.4017372663734006\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  2.8623149128226966 acc:  0.4047294732376125\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  2.8338813915119303 acc:  0.4064712055914075\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  2.837163553371296 acc:  0.4076323604939374\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  2.7955350725800843 acc:  0.4078779894156265\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  2.756877037195059 acc:  0.41031194873054505\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  2.739638168495018 acc:  0.41089252618181005\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  2.734822620045055 acc:  0.41207601098631175\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  2.744842204180631 acc:  0.415782774713619\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  2.7032341473586077 acc:  0.41560413549784514\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  2.6718943102376445 acc:  0.4167876203023469\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  2.6626516138757026 acc:  0.41618471294911014\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  2.656953771631201 acc:  0.41768081638121607\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  39 loss :  2.6298554310431848 acc:  0.41924390951923723\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  40 loss :  2.632571980669782 acc:  0.42062836344148447\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  41 loss :  2.63199015930816 acc:  0.42058370363754105\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  42 loss :  2.576614886730701 acc:  0.4214768997164102\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  43 loss :  2.5764983050473087 acc:  0.42062836344148447\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  44 loss :  2.6027497411607863 acc:  0.4219011678538731\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  45 loss :  2.578943861114395 acc:  0.42333028158006386\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  46 loss :  2.5758629161994775 acc:  0.42252640510908157\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  47 loss :  2.551611885324225 acc:  0.4241788178549896\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  48 loss :  2.525154079590644 acc:  0.4243797869727352\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  49 loss :  2.5335470271277263 acc:  0.4242234776589331\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  50 loss :  2.502127869979485 acc:  0.4238438693254137\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  51 loss :  2.525129443282014 acc:  0.4263224884442757\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  52 loss :  2.505235700340538 acc:  0.42842149922961836\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  53 loss :  2.4955733539341214 acc:  0.4271040350132863\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  54 loss :  2.49740799823841 acc:  0.42837683942567495\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5246383891759951 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'CELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 648, 'dropout': 0.20522205289613513, 'dropout_StationIdEmbedding': 0.07569626942763033, 'dropout_timeStampEmbedding': 0.3486033441766751, 'dropout_transformers': 0.18105456410747045, 'early_stopping': 2, 'encoder_only': False, 'epochs_classifcation_only': 55, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.8085756537070063, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.5246383891759951, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.8208303674611096, 'beta_2': 0.9667559904178682, 'eps': 4.369601211192274e-08, 'lr': 0.010345308052864882, 'optimizer': 'Adam', 'weight_decay': 2.0125254769307256e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  7.066092830932069 acc:  0.1534287564477592\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  4.867574808840266 acc:  0.2136301721635442\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  4.216584839506777 acc:  0.21467967755621553\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  3.9125029412572254 acc:  0.2251747314829288\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  3.738116188677485 acc:  0.22950673246544448\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  3.6180911635210413 acc:  0.22705044324855414\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  3.532129360530191 acc:  0.23683094031217203\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  3.4657468410309202 acc:  0.23551347609583995\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  3.4183940501984007 acc:  0.24871044816113258\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  3.3584717390779963 acc:  0.2485764687493022\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  3.3219710866848153 acc:  0.24806288100395238\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  3.272610180392237 acc:  0.24906772659268026\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  3.2709639172354144 acc:  0.24839782953352835\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  3.2214627522908286 acc:  0.2522162427706942\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  3.1913144674129827 acc:  0.25348904718308285\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  3.2034283598026114 acc:  0.25243954179041156\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  3.160056435419414 acc:  0.25636960453743607\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  3.1317137572579754 acc:  0.25435991335998037\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  3.1362792960184063 acc:  0.25563271777236896\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  3.1255981693724673 acc:  0.2586025947346091\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  3.1278868164131026 acc:  0.2593841413036197\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  3.1398090159821654 acc:  0.2600093785588281\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  3.1057871587262182 acc:  0.259853069245026\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  3.0990372774843684 acc:  0.25987539914699775\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  3.101603512278574 acc:  0.26047830650023446\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  3.0853949706711457 acc:  0.2592278319898176\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  3.1078543834343644 acc:  0.26092490453966904\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  3.1180687721617923 acc:  0.2605229663041779\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  3.0992167795489647 acc:  0.261014224147556\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  3.0886938886014286 acc:  0.26108121385347116\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  3.0520576245770483 acc:  0.26108121385347116\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  3.0865353452945183 acc:  0.2613045128731885\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  3.0987127669557126 acc:  0.2617511109126231\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  3.08386659764958 acc:  0.26163946140276445\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  3.088144660709861 acc:  0.2617734408145948\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  3.0797910033585785 acc:  0.26166179130473616\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  3.0866797898343936 acc:  0.26179577071656657\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  3.075399284591218 acc:  0.2613268427751602\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  3.1009930250887385 acc:  0.2616171315007927\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3222794518038893 and num_layers=1\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=86188)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'Tanhshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 984, 'dropout': 0.08480769825751716, 'dropout_StationIdEmbedding': 0.17963035583085768, 'dropout_timeStampEmbedding': 0.4170156959961103, 'dropout_transformers': 0.5926570139752845, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 74, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.3222794518038893, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': True, 'beta_1': 0.8604000400634382, 'beta_2': 0.9754826879665086, 'eps': 3.304684136482976e-07, 'lr': 4.720358505500233e-05, 'optimizer': 'Adam', 'weight_decay': 3.0391831322775776e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  0 loss :  7.934535154929528 acc:  0.017908581381327737\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  1 loss :  6.954682610585139 acc:  0.09650983632181855\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  2 loss :  6.280689041431134 acc:  0.1582073554697095\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  3 loss :  5.785936439954318 acc:  0.21048165598553023\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  4 loss :  5.394444190538847 acc:  0.2434405912958042\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  5 loss :  5.071703030512883 acc:  0.2697005560145591\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  6 loss :  4.795263972649208 acc:  0.2880334055333497\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  7 loss :  4.567933409030621 acc:  0.30734877073889644\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  8 loss :  4.371794588749225 acc:  0.32215349574615365\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  9 loss :  4.205422546313359 acc:  0.33499318937989864\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  10 loss :  4.056466152117802 acc:  0.3446843668356296\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  11 loss :  3.92988490324754 acc:  0.3524551727217918\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  12 loss :  3.8166494791324324 acc:  0.3608065560592189\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  13 loss :  3.7148909440407385 acc:  0.3664783511600384\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  14 loss :  3.629356862948491 acc:  0.37328897126141614\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  15 loss :  3.5526315615727353 acc:  0.37887144675434875\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  16 loss :  3.476452662394597 acc:  0.3844539222472813\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  17 loss :  3.413733926186195 acc:  0.3884733046021928\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  18 loss :  3.35710829771482 acc:  0.392760645780765\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  19 loss :  3.305520228239206 acc:  0.3952392648996271\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  20 loss :  3.2570285356961763 acc:  0.3997945649018601\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  21 loss :  3.2115160887057965 acc:  0.40167027666748545\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  22 loss :  3.171302160849938 acc:  0.40484112274747114\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  23 loss :  3.1372859092859122 acc:  0.40890516490632606\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  24 loss :  3.0994342418817373 acc:  0.4113837840251881\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  25 loss :  3.066802228414095 acc:  0.41203135118236833\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  26 loss :  3.040417374097384 acc:  0.41464394971306073\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  27 loss :  3.00600639123183 acc:  0.4181497443226224\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  28 loss :  2.981980558542105 acc:  0.41765848647924436\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  29 loss :  2.9565307507148155 acc:  0.4195565281468414\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  30 loss :  2.934003754762503 acc:  0.4212312707947212\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  31 loss :  2.9091742680622983 acc:  0.42185650804992963\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  32 loss :  2.8891341631229106 acc:  0.4218788379519014\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  33 loss :  2.871492229975187 acc:  0.4246254158944242\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  34 loss :  2.8442835037524885 acc:  0.4249826943259719\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  35 loss :  2.8312653596584614 acc:  0.4248487149141415\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  36 loss :  2.812590846648583 acc:  0.4256079315811804\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  37 loss :  2.7949418288010817 acc:  0.4253399727575196\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  38 loss :  2.7764650326508744 acc:  0.42607685952258667\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  39 loss :  2.765993668482854 acc:  0.42819820020990107\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  40 loss :  2.747255875514104 acc:  0.42904673648482683\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  41 loss :  2.7339707649671112 acc:  0.42906906638679854\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  42 loss :  2.7181765207877526 acc:  0.42964964383806353\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  43 loss :  2.7067901061131403 acc:  0.4309224482504522\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  44 loss :  2.6941738367080688 acc:  0.4319942835450952\n",
            "\u001b[36m(eval_config pid=86188)\u001b[0m epoch:  45 loss :  2.682091701947726 acc:  0.4303418707991872\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-07 00:16:49,221\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-07 00:17:03,791\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-07 00:17:03,793\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_2        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_2\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_2`\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3185351864747675 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'ReLU6', 'activation_transformers': 'Tanhshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1440, 'dropout': 0.06481227396936776, 'dropout_StationIdEmbedding': 0.1708766743744856, 'dropout_timeStampEmbedding': 0.07422947228325949, 'dropout_transformers': 0.0794317889686042, 'early_stopping': 1, 'encoder_only': False, 'epochs_classifcation_only': 63, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.3770916843597625, 'scheduler': 'StepLR', 'step_size': 30, 'dropout_lstm': 0.3185351864747675, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'alpha': 0.938452613050594, 'centered': True, 'eps': 3.6555484290291812e-09, 'lr': 0.08737157533127207, 'momentum': 0.4793949095942875, 'optimizer': 'RMSprop', 'weight_decay': 2.535044971156741e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1763145348079828 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'Tanhshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 816, 'dropout': 0.26070687905349665, 'dropout_StationIdEmbedding': 0.08653789669073876, 'dropout_timeStampEmbedding': 0.16094489916002702, 'dropout_transformers': 0.3143248311414082, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 79, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.1763145348079828, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8964571198071845, 'beta_2': 0.9575675306205829, 'eps': 3.6159380704877956e-07, 'lr': 2.9024428536914103e-05, 'optimizer': 'AdamW', 'weight_decay': 7.080137709065855e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  7.992803896390475 acc:  0.007971775003907732\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  7.210398354897133 acc:  0.043610298550789364\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  6.659728593092698 acc:  0.11801353192059487\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  6.0411898282858045 acc:  0.17676350400821741\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  5.502544535123385 acc:  0.2237902775606815\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  5.041511205526499 acc:  0.2584239555188353\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  4.660384108470036 acc:  0.28638099278744167\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  4.350908769094027 acc:  0.30947011142621084\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  4.093732268993671 acc:  0.32860683741598373\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  3.8868792588894183 acc:  0.34492999575731864\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  3.7112732905607957 acc:  0.35739008105754416\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  11 loss :  3.5663635914142313 acc:  0.3676618359645401\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  12 loss :  3.4340036135453444 acc:  0.37768796194984705\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  13 loss :  3.3283984459363496 acc:  0.3852354688162919\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  14 loss :  3.2316810131072997 acc:  0.3920684188196414\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  15 loss :  3.1476197664554304 acc:  0.3991469977446799\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  16 loss :  3.0796898438380316 acc:  0.4028314315700154\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  17 loss :  3.0136470244481015 acc:  0.4068731438268986\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  18 loss :  2.957270288467407 acc:  0.4106245673581493\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  19 loss :  2.9041437515845665 acc:  0.4154254962820713\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  20 loss :  2.854418167701134 acc:  0.416608981086573\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  21 loss :  2.8135515433091385 acc:  0.4187749815778309\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  22 loss :  2.775945834013132 acc:  0.4205390438335976\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  23 loss :  2.7339409809846145 acc:  0.42373221981555503\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  24 loss :  2.697172190592839 acc:  0.4263894781501909\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  25 loss :  2.667392818744366 acc:  0.4263224884442757\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  26 loss :  2.6380578848031853 acc:  0.42779626197440995\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  27 loss :  2.6092634952985323 acc:  0.4297389634459505\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  28 loss :  2.5833501650736883 acc:  0.43036420070115894\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  29 loss :  2.557438111305237 acc:  0.43094477815242394\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  30 loss :  2.5339702331102814 acc:  0.43154768550566064\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  31 loss :  2.5079033081348125 acc:  0.43215059285889734\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  32 loss :  2.48730890750885 acc:  0.43362436638903157\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  33 loss :  2.465559346859272 acc:  0.43386999531072057\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  34 loss :  2.4450344470831062 acc:  0.434584552173816\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  35 loss :  2.4279422668310313 acc:  0.43465154187973115\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  36 loss :  2.405574607849121 acc:  0.4352991090369113\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  37 loss :  2.3889400738936204 acc:  0.43605832570395014\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  38 loss :  2.3716567873954775 acc:  0.4358796864881763\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  39 loss :  2.357432921116169 acc:  0.43757675903802784\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  40 loss :  2.3405820764028107 acc:  0.4369738516847911\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  41 loss :  2.3229784672076885 acc:  0.4372864703123953\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  42 loss :  2.306348563157595 acc:  0.4371078310966215\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  43 loss :  2.295723746373103 acc:  0.4392291717839359\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  44 loss :  2.278423621104314 acc:  0.4383136458030949\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  45 loss :  2.2670641037134023 acc:  0.4388495634504165\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  46 loss :  2.2490947402440584 acc:  0.4372864703123953\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.14415152640995627 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'SiLU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 792, 'dropout': 0.2680783994408866, 'dropout_StationIdEmbedding': 0.0739238728613087, 'dropout_timeStampEmbedding': 0.02209788354660225, 'dropout_transformers': 0.337638725755541, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 69, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 10, 'factor': 0.017659676725530205, 'patience': 2, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.2702315615474546, 'dropout_lstm': 0.14415152640995627, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'alpha': 0.999167930780396, 'centered': False, 'eps': 2.695980735175652e-09, 'lr': 0.036935167045001105, 'momentum': 0.3891524761786034, 'optimizer': 'RMSprop', 'weight_decay': 0.007137477686874513, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m loss is undifined\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.020742256607691828 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'Softsign', 'batch_size': 128, 'concatenate_features': True, 'd_model': 816, 'dropout': 0.18102681845824922, 'dropout_StationIdEmbedding': 0.10392545818255872, 'dropout_timeStampEmbedding': 0.15437432061990405, 'dropout_transformers': 0.29882918903011946, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 78, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.020742256607691828, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.9024997607854329, 'beta_2': 0.9575548291052323, 'eps': 3.30971932219932e-07, 'lr': 0.00042685753975095584, 'optimizer': 'AdamW', 'weight_decay': 4.769814423250419e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  6.377861888468766 acc:  0.2588035638523547\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  4.013160617411637 acc:  0.36326284527610925\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  3.1998174050275017 acc:  0.40470714333564073\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  2.859230410151121 acc:  0.42000312618627605\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  2.662421504990393 acc:  0.4281535404059576\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  2.526163521934958 acc:  0.43349038697720116\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  2.4270212770510122 acc:  0.43353504678114463\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  2.330848998382312 acc:  0.4338030056048054\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  2.2441759209672942 acc:  0.43657191344929996\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  2.177238975252424 acc:  0.4352991090369113\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  2.105619553758317 acc:  0.4361253154098654\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  11 loss :  2.0404709357173503 acc:  0.4352767791349396\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.008118261989633554 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'Softsign', 'batch_size': 128, 'concatenate_features': True, 'd_model': 648, 'dropout': 0.16644297114258483, 'dropout_StationIdEmbedding': 0.11797401955244144, 'dropout_timeStampEmbedding': 0.14776856522258514, 'dropout_transformers': 0.2745678392768168, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 79, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.3017648723625681, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.008118261989633554, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.9073516314833748, 'beta_2': 0.9583504918664006, 'eps': 3.632032183754977e-07, 'lr': 0.000292092262624945, 'optimizer': 'AdamW', 'weight_decay': 8.406124772483723e-08, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  6.713554097824738 acc:  0.2064176138266753\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  5.034677369253976 acc:  0.2594288011075631\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  4.608875771530536 acc:  0.273273340330036\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  4.4906218492684244 acc:  0.27767233101846683\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  4.457627238345747 acc:  0.2786995065091664\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  4.447366988959432 acc:  0.27903445503874236\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  4.44197169872893 acc:  0.2792577540584597\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  4.453089870324655 acc:  0.27923542415648794\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  4.453422686632941 acc:  0.2792577540584597\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  4.437963305401201 acc:  0.2793024138624031\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  4.43874351517493 acc:  0.2792577540584597\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  11 loss :  4.433477005036939 acc:  0.2792577540584597\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  12 loss :  4.441267538471382 acc:  0.2793024138624031\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.09006872349014237 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'Softsign', 'batch_size': 128, 'concatenate_features': True, 'd_model': 792, 'dropout': 0.48146394546781046, 'dropout_StationIdEmbedding': 0.37022445255740244, 'dropout_timeStampEmbedding': 0.001576203477589161, 'dropout_transformers': 0.06945179989196446, 'early_stopping': 5, 'encoder_only': False, 'epochs_classifcation_only': 55, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.09006872349014237, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 4, 'alpha': 0.9493078422155424, 'centered': True, 'eps': 1.2815905764013671e-08, 'lr': 1.1795758089441892e-05, 'momentum': 0.3973259665301577, 'optimizer': 'RMSprop', 'weight_decay': 1.9495491873470937e-07, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  7.648957934335013 acc:  0.05225197061384901\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  6.97069521485088 acc:  0.1118504789763973\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  6.530262216229305 acc:  0.15588504566464953\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  6.170477862670043 acc:  0.18185472165777192\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  5.879418065614789 acc:  0.20112542705937522\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  5.599837998363459 acc:  0.21284862559453363\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  5.369990299795275 acc:  0.22356697854096422\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  5.1521716920014855 acc:  0.2396891677645535\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  4.972863237434458 acc:  0.2515910055154858\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  4.776626631478283 acc:  0.2605899560100931\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  4.624982726908176 acc:  0.27197820601567557\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  11 loss :  4.480588409388177 acc:  0.2807315275885939\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  12 loss :  4.342909224679537 acc:  0.28975280798517294\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  13 loss :  4.199844010522432 acc:  0.29732264475358955\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  14 loss :  4.105064398774477 acc:  0.3063662550521403\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  15 loss :  3.9841565194531023 acc:  0.31453899917379363\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  16 loss :  3.878725194485388 acc:  0.32228747515798406\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  17 loss :  3.79279928118269 acc:  0.3296116830047116\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  18 loss :  3.716233188860884 acc:  0.33749413840073245\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  19 loss :  3.6401247220618704 acc:  0.34452805752182747\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  20 loss :  3.5750961014043505 acc:  0.3496862648772972\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  21 loss :  3.479658855456058 acc:  0.35544737958600364\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  22 loss :  3.4258491123948143 acc:  0.3608512158631624\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  23 loss :  3.3816713662905116 acc:  0.36437934037469577\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  24 loss :  3.3206560589442744 acc:  0.37105598106424315\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  25 loss :  3.281037831974921 acc:  0.37268606390817943\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  26 loss :  3.2233007243860547 acc:  0.37697340508675165\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  27 loss :  3.1821021788588193 acc:  0.3798762923430766\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  28 loss :  3.1315023364307724 acc:  0.3830917982270058\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  29 loss :  3.1038613274832754 acc:  0.38706652077797377\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  30 loss :  3.0561751940540063 acc:  0.39037134626978987\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  31 loss :  3.029250802280747 acc:  0.39314025411428444\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  32 loss :  2.995828058118018 acc:  0.3949936359779381\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  33 loss :  2.957351180994622 acc:  0.39718196637116765\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  34 loss :  2.923341000191519 acc:  0.398477100685528\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  35 loss :  2.8850224953945554 acc:  0.4017372663734006\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  36 loss :  2.869578082984853 acc:  0.40383627715874326\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  37 loss :  2.834444988553769 acc:  0.40772168010182436\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  38 loss :  2.8138774399445436 acc:  0.40682848402295513\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  39 loss :  2.7944136824563284 acc:  0.408659535984637\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  40 loss :  2.7747668194993635 acc:  0.41243328941785945\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  41 loss :  2.747288256048042 acc:  0.4130808565750396\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  42 loss :  2.715120010286848 acc:  0.41529151687024096\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  43 loss :  2.7076078441655524 acc:  0.4146886095170042\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  44 loss :  2.693268027261039 acc:  0.41437599088939997\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  45 loss :  2.6687920940256564 acc:  0.41922157961726547\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  46 loss :  2.6424001533294392 acc:  0.41890896098966124\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  47 loss :  2.628407547407061 acc:  0.42096331197106046\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  48 loss :  2.6075927475902523 acc:  0.4221244668735904\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  49 loss :  2.610659115782408 acc:  0.42196815755978834\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  50 loss :  2.5793243301248996 acc:  0.42279436393274233\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  51 loss :  2.5661728226135825 acc:  0.42529531295357614\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  52 loss :  2.542194571450492 acc:  0.4245360962865373\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  53 loss :  2.5330767988044527 acc:  0.42701471540539937\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  54 loss :  2.516419132179189 acc:  0.42522832324766097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1990058380600325 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'LogSigmoid', 'batch_size': 128, 'concatenate_features': True, 'd_model': 432, 'dropout': 0.28625512985266344, 'dropout_StationIdEmbedding': 0.2868159256091363, 'dropout_timeStampEmbedding': 0.058309692541478206, 'dropout_transformers': 0.10218858329978486, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 47, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.3635282361292788, 'scheduler': 'StepLR', 'step_size': 1, 'dropout_lstm': 0.1990058380600325, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.9297688430477632, 'beta_2': 0.951051691380568, 'eps': 1.0322456175370109e-06, 'lr': 0.003596473300952738, 'optimizer': 'AdamW', 'weight_decay': 4.616716870529124e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  7.081336521240602 acc:  0.16879172900430967\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  5.716614298073642 acc:  0.222673782462095\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  4.984891385917204 acc:  0.2504745104168993\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  4.636011066206967 acc:  0.26023267757854546\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  4.49005582533687 acc:  0.26443069914923073\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  4.445582050875009 acc:  0.2646763280709198\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  4.42257757933743 acc:  0.26570350356161937\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  4.425512175962149 acc:  0.26612777169908225\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  4.432402883667543 acc:  0.2660607819931671\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  4.438448029828359 acc:  0.2658821427773932\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  4.397069284714848 acc:  0.2658821427773932\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2394864782197647 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Tanhshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 648, 'dropout': 0.04781246254922128, 'dropout_StationIdEmbedding': 0.22059608669842368, 'dropout_timeStampEmbedding': 0.1624345622398177, 'dropout_transformers': 0.026182522295456034, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 77, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 1, 'factor': 0.025681708553783655, 'patience': 1, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.1549368317282937, 'dropout_lstm': 0.2394864782197647, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8849779365933899, 'beta_2': 0.957263514555617, 'eps': 2.53010705914709e-06, 'lr': 0.013297716341973785, 'optimizer': 'AdamW', 'weight_decay': 5.290132749821607e-07, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  6.680663499487451 acc:  0.2223165040305473\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  4.611268603658101 acc:  0.24902306678873679\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  3.7276569676686484 acc:  0.2963177991648617\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  3.3091634871011757 acc:  0.3174642163320903\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  3.1592554299228164 acc:  0.33191166290779983\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  3.113978072821376 acc:  0.3395484893821316\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  3.0837704819369027 acc:  0.3398387781077641\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  3.0749495144349983 acc:  0.33970479869593373\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  3.0712590906993453 acc:  0.34030770604917043\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  3.0737228048853127 acc:  0.34024071634325526\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  3.0936781009995795 acc:  0.3401960565393118\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  11 loss :  3.0936718963714966 acc:  0.3401960565393118\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  12 loss :  3.0932372707918465 acc:  0.3401960565393118\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  13 loss :  3.0956572164972145 acc:  0.3401960565393118\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  14 loss :  3.0891670950924057 acc:  0.34024071634325526\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'SELU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 576, 'dropout': 0.35517325406828515, 'dropout_StationIdEmbedding': 0.0338387158261461, 'dropout_timeStampEmbedding': 0.2994051668846006, 'dropout_transformers': 0.19996992503119904, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 60, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.061941791384783146, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 4, 'alpha': 0.92879834111521, 'centered': False, 'eps': 5.830483708495168e-06, 'lr': 3.146608365264491e-06, 'momentum': 0.02799926162679206, 'optimizer': 'RMSprop', 'weight_decay': 0.0006512454362373307, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.061941791384783146 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  8.122256311319642 acc:  0.00037960833351941584\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  8.05166454638465 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  7.986280457448151 acc:  0.0005582475492932586\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  7.933377637701519 acc:  0.0008038764709822924\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  7.873404850394039 acc:  0.001116495098586517\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  7.823244935375149 acc:  0.001496103432105933\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  7.77035544282299 acc:  0.0021213406873143827\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  7.716384063332768 acc:  0.0027465779425228324\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  7.676770218348099 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  7.6309798531613104 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  7.602573370529433 acc:  0.005024227943639327\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  11 loss :  7.573338573261843 acc:  0.0056271352968760464\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  12 loss :  7.537675655494302 acc:  0.006274702454056227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  13 loss :  7.510953854706328 acc:  0.006922269611236407\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  14 loss :  7.474712953729145 acc:  0.007547506866444857\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  15 loss :  7.453673209174204 acc:  0.008530022553200992\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  16 loss :  7.422250820418536 acc:  0.009132929906437711\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  17 loss :  7.4031117649401645 acc:  0.009981466181363464\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  18 loss :  7.371906385583393 acc:  0.011321260299667284\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  19 loss :  7.357571601867676 acc:  0.01230377598642342\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  20 loss :  7.339677778341002 acc:  0.013487260790925128\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  21 loss :  7.310192867860955 acc:  0.01496103432105933\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  22 loss :  7.2864487373222735 acc:  0.016591117164995645\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  23 loss :  7.276969384338896 acc:  0.017908581381327737\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  24 loss :  7.2401380700580145 acc:  0.01940468481343367\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  25 loss :  7.239985716544975 acc:  0.02134738628497421\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  26 loss :  7.215529255947824 acc:  0.022508541187504186\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  27 loss :  7.210527323060116 acc:  0.02400464461961012\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  28 loss :  7.196028871051336 acc:  0.02536676863988567\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  29 loss :  7.1730855683148915 acc:  0.026773552464104684\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  30 loss :  7.156334141553459 acc:  0.027666748542973896\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  31 loss :  7.147754515631724 acc:  0.02927450148493848\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  32 loss :  7.108819985793809 acc:  0.03032400687760981\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  33 loss :  7.112134796077922 acc:  0.03240068776098073\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  34 loss :  7.093516745809781 acc:  0.03351718285956724\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  35 loss :  7.089789503711765 acc:  0.03572784315476855\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  36 loss :  7.0703322362091585 acc:  0.0373802559006766\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  37 loss :  7.05926403756869 acc:  0.038563740705178304\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  38 loss :  7.034466711141295 acc:  0.040796730902351336\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  39 loss :  7.021077592494124 acc:  0.04148895786347498\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  40 loss :  7.028881800376762 acc:  0.043186030413326484\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  41 loss :  7.008636531183275 acc:  0.04465980394346069\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  42 loss :  6.99572665004407 acc:  0.045932608355849315\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  43 loss :  6.994490251702778 acc:  0.047585021101757365\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  44 loss :  6.956121533603992 acc:  0.0490811245338633\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  45 loss :  6.9635362786761785 acc:  0.05064421767188442\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  46 loss :  6.920995760772188 acc:  0.05138110443695152\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  47 loss :  6.93537927886187 acc:  0.0530558470848313\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  48 loss :  6.917526140051373 acc:  0.05399370296764397\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  49 loss :  6.923998469013279 acc:  0.05511019806623049\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  50 loss :  6.90303157547773 acc:  0.05662863140030815\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  51 loss :  6.9159806138378075 acc:  0.05803541522452717\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  52 loss :  6.88780374850257 acc:  0.05941986914677445\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  53 loss :  6.880399194814391 acc:  0.06024607551972847\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  54 loss :  6.880726693040233 acc:  0.061407230422258444\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  55 loss :  6.867880223161083 acc:  0.06254605542281669\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  56 loss :  6.856706449540995 acc:  0.06435477748252685\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  57 loss :  6.841677002987619 acc:  0.06591787062054798\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  58 loss :  6.837394972979012 acc:  0.0664537882678695\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  59 loss :  6.832174672918804 acc:  0.06774892258222986\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3620687895634662 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'Softsign', 'batch_size': 128, 'concatenate_features': True, 'd_model': 864, 'dropout': 0.15264521577592885, 'dropout_StationIdEmbedding': 0.09257820445673795, 'dropout_timeStampEmbedding': 0.25366459898702465, 'dropout_transformers': 0.5076449951348454, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 66, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.2999990737302809, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.3620687895634662, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.9511006928012369, 'beta_2': 0.954996327858048, 'eps': 2.651662343920685e-07, 'lr': 1.2732979423880814e-06, 'optimizer': 'AdamW', 'weight_decay': 0.0006292089957419837, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  8.26911686383761 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  8.253533928210919 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  8.249050367795505 acc:  0.00037960833351941584\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  8.246727642646203 acc:  0.00037960833351941584\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  8.246940994262696 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  8.249335560431847 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'RReLU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1104, 'dropout': 0.11007253643316839, 'dropout_StationIdEmbedding': 0.4510045081743991, 'dropout_timeStampEmbedding': 0.3172203591504018, 'dropout_transformers': 0.2918492291883989, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 33, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8955151012913805, 'beta_2': 0.9504008807508265, 'eps': 7.473758783181554e-07, 'lr': 2.528325314832884e-05, 'optimizer': 'AdamW', 'weight_decay': 1.5309113475345312e-08, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  7.675854065838982 acc:  0.0034834647075899336\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  7.275645264056551 acc:  0.014760065203313757\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  7.113557815551758 acc:  0.06317129267802514\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  6.879986001663849 acc:  0.10055154857870174\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  6.597066919342811 acc:  0.12029118192171136\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  6.305310613968793 acc:  0.13375611281066477\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  6.013883983387666 acc:  0.15412098340888283\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  5.7618738382804295 acc:  0.17151597704486077\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  8 loss :  5.531744203647645 acc:  0.1869236094053547\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  9 loss :  5.3273615877167515 acc:  0.20146037558895116\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  10 loss :  5.136042667036297 acc:  0.21461268785030033\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  11 loss :  4.961151086983561 acc:  0.22620190697362838\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  12 loss :  4.8184426011157635 acc:  0.23596007413527453\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  13 loss :  4.688630404592562 acc:  0.2440881584529844\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  14 loss :  4.548310167649213 acc:  0.2545385525757542\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  15 loss :  4.449693771971374 acc:  0.25951812071545005\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  16 loss :  4.314532231883843 acc:  0.2664627202286582\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  17 loss :  4.229668242590768 acc:  0.27099569032891946\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  18 loss :  4.135966831896486 acc:  0.2770024339593149\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  19 loss :  4.060372837451326 acc:  0.28370140455083404\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  20 loss :  3.9865773044714405 acc:  0.29129357122122235\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  21 loss :  3.9133261872940706 acc:  0.29660808789049414\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  22 loss :  3.856954438345773 acc:  0.3020119241676529\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  23 loss :  3.7779374563393473 acc:  0.3087555545631155\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  24 loss :  3.7302114362476253 acc:  0.3130652256436594\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  25 loss :  3.691640437150202 acc:  0.3205010830002456\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  26 loss :  3.6350350740576993 acc:  0.32498939329656346\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  27 loss :  3.5902576366392505 acc:  0.3286738271218989\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  28 loss :  3.547231602067707 acc:  0.3360873545765134\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  29 loss :  3.510524092602129 acc:  0.34068731438268984\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  30 loss :  3.4781403281107672 acc:  0.34280865507000424\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  31 loss :  3.438558630582665 acc:  0.3477212335037849\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  32 loss :  3.399576866326212 acc:  0.3528571109572829\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.13567017543400267 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'Tanh', 'batch_size': 128, 'concatenate_features': True, 'd_model': 936, 'dropout': 0.2144262302829353, 'dropout_StationIdEmbedding': 0.7438218963442336, 'dropout_timeStampEmbedding': 0.10865370973419583, 'dropout_transformers': 0.17973301367981903, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 53, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 5, 'factor': 0.27395867139810964, 'patience': 4, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.009191231847049753, 'dropout_lstm': 0.13567017543400267, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 1, 'alpha': 0.9957518132404564, 'centered': False, 'eps': 9.074941891790476e-07, 'lr': 0.015637712543671915, 'momentum': 0.22449799860618114, 'optimizer': 'RMSprop', 'weight_decay': 2.5044459760886222e-05, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m loss is undifined\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.0009188326006648317 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'ReLU', 'batch_size': 32, 'concatenate_features': True, 'd_model': 552, 'dropout': 0.3200467844444552, 'dropout_StationIdEmbedding': 0.33205066692000035, 'dropout_timeStampEmbedding': 0.1524203744540469, 'dropout_transformers': 0.0016516744885342605, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 41, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.0009188326006648317, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 12, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 4, 'amsgrad': False, 'beta_1': 0.9237934647699764, 'beta_2': 0.9603204025088408, 'eps': 9.798426465484697e-08, 'lr': 0.0003416372305808973, 'optimizer': 'AdamW', 'weight_decay': 0.008069193667814516, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  7.994993469931862 acc:  0.0027019181385793717\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  7.787747253071178 acc:  0.002679588236607641\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  7.595497304742986 acc:  0.0035281245115333943\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  7.578171469948509 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  7.5018346959894355 acc:  0.0017194024518232365\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Sigmoid', 'activation_transformers': 'Hardshrink', 'batch_size': 64, 'concatenate_features': True, 'd_model': 720, 'dropout': 0.47488466896183584, 'dropout_StationIdEmbedding': 0.2595047942208853, 'dropout_timeStampEmbedding': 0.0030906655757370616, 'dropout_transformers': 0.4298392081369931, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 21, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.8973377305984398, 'scheduler': 'StepLR', 'step_size': 29, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'lr': 5.394247126015418e-07, 'momentum': 0.11853627431223135, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 2.919470342488512e-07, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  8.070110477939728 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  8.069357256735525 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  8.070815578583748 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  8.070866184849892 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  8.071175243008522 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  8.068607404155117 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  8.069700604100381 acc:  0.0008262063729540227\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.20720656406138455 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'PReLU', 'activation_transformers': 'LeakyReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 816, 'dropout': 0.24487312738527295, 'dropout_StationIdEmbedding': 0.4782462609563159, 'dropout_timeStampEmbedding': 0.4465247279357142, 'dropout_transformers': 0.24187457152905742, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 50, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.20720656406138455, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.9450418529054735, 'beta_2': 0.965252582715933, 'eps': 2.7647566945256997e-08, 'lr': 2.678227992161398e-06, 'optimizer': 'AdamW', 'weight_decay': 5.811893997859784e-06, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  8.117569656015556 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  8.077181272417585 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  8.035919603900375 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  8.003533425732194 acc:  0.0016077529419645847\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  7.961379505763544 acc:  0.0030368666681553267\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  7.922243907072834 acc:  0.004086372060826653\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  6 loss :  7.876921359623704 acc:  0.005448496081102204\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  7 loss :  7.826553362552251 acc:  0.005269856865328361\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.04588539207539638 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'Mish', 'batch_size': 128, 'concatenate_features': False, 'd_model': 1056, 'dropout': 0.40500501796957167, 'dropout_StationIdEmbedding': 0.40069081027562425, 'dropout_timeStampEmbedding': 0.054204397348860206, 'dropout_transformers': 0.507200548196927, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 2, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.8667331256154139, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.04588539207539638, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 5, 'amsgrad': False, 'beta_1': 0.9069226788355222, 'beta_2': 0.9702066855498683, 'eps': 3.963272655257734e-06, 'lr': 0.0010873151281478677, 'optimizer': 'AdamW', 'weight_decay': 0.044452372252998903, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  5.867896494112517 acc:  0.26990152513230464\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  3.4517531495345266 acc:  0.35468816291896477\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Tanhshrink', 'activation_transformers': 'Softplus', 'batch_size': 128, 'concatenate_features': True, 'd_model': 744, 'dropout': 0.16828977497640757, 'dropout_StationIdEmbedding': 0.5275672792888184, 'dropout_timeStampEmbedding': 0.255465140910663, 'dropout_transformers': 0.3186637733374214, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 57, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 10, 'factor': 0.8623210500755677, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 8.393184938349315e-06, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'alpha': 0.9708199552772591, 'centered': True, 'eps': 1.1075566123276954e-07, 'lr': 4.958819789945677e-05, 'momentum': 0.3808687155186945, 'optimizer': 'RMSprop', 'weight_decay': 2.1762477794129237e-09, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  7.485147050710824 acc:  0.005336846571243553\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  7.264512744316688 acc:  0.004465980394346068\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  7.202344964100765 acc:  0.03257932697675457\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.13540938818987527 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'SiLU', 'activation_transformers': 'Hardtanh', 'batch_size': 32, 'concatenate_features': False, 'd_model': 600, 'dropout': 0.4561195515382004, 'dropout_StationIdEmbedding': 0.9417777919007059, 'dropout_timeStampEmbedding': 0.173417021958978, 'dropout_transformers': 0.6362686627523595, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 64, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.13540938818987527, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 4, 'lr': 1.7055595170546112e-05, 'momentum': 0.3830425719103891, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 2.398977035981425e-09, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  8.112507612345604 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  8.110068529011818 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  8.110512269941788 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  8.108899462822428 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'Hardsigmoid', 'batch_size': 64, 'concatenate_features': True, 'd_model': 936, 'dropout': 0.2799242543542327, 'dropout_StationIdEmbedding': 0.0386155069040552, 'dropout_timeStampEmbedding': 0.37828697971916725, 'dropout_transformers': 0.117164417380415, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 68, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 21, 'eta_min': 9.15341714389996e-06, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8635109006281019, 'beta_2': 0.9564845063877521, 'eps': 9.256577028073751e-08, 'lr': 0.0007266834770204199, 'optimizer': 'AdamW', 'weight_decay': 4.8192916351369436e-08, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  0 loss :  7.501266737907163 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  1 loss :  7.347215046421174 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  2 loss :  7.298147155392554 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  3 loss :  7.290869875877134 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  4 loss :  7.289702397008096 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m epoch:  5 loss :  7.269704852565642 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2697655062947037 and num_layers=1\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=147718)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Softmin', 'batch_size': 128, 'concatenate_features': True, 'd_model': 840, 'dropout': 0.623608665410623, 'dropout_StationIdEmbedding': 0.1564696070676831, 'dropout_timeStampEmbedding': 0.1023317236799374, 'dropout_transformers': 0.15929640622293462, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 78, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.6433319532151986, 'scheduler': 'StepLR', 'step_size': 10, 'dropout_lstm': 0.2697655062947037, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'RReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 36, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.9612234038792027, 'beta_2': 0.9623928139360105, 'eps': 9.72651506065075e-09, 'lr': 0.025347935396950034, 'optimizer': 'AdamW', 'weight_decay': 3.2666748081056375e-07, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Tanh', 'dropout_gcn': 0.9894712551099907, 'hidden_channels': 512, 'layer_type': 'GraphSAGE', 'norm': 'LayerNorm', 'num_layers_gcn': 2, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=147718)\u001b[0m loss is undifined\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-07 02:41:01,860\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-07 02:41:16,737\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-07 02:41:16,738\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_3        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_3\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_3`\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.38031071144816575 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'ReLU6', 'batch_size': 64, 'concatenate_features': False, 'd_model': 456, 'dropout': 0.0012980306555653853, 'dropout_StationIdEmbedding': 0.10456912036775422, 'dropout_timeStampEmbedding': 0.5254810160750509, 'dropout_transformers': 0.4484911681975673, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 8, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.38031071144816575, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 1, 'lr': 0.011109562390026308, 'momentum': 0.09445856056773963, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 8.65962469596313e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.140402308965134 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  8.12464802952136 acc:  0.0006029073532367192\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  8.115220829591912 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  8.098896172087072 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  8.08287504163839 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  8.073831040980453 acc:  0.0014737735301342026\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  8.06185421701205 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  8.04357148833194 acc:  0.0023669696090034163\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.484035234259669 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'Sigmoid', 'batch_size': 128, 'concatenate_features': True, 'd_model': 504, 'dropout': 0.08794049450462536, 'dropout_StationIdEmbedding': 0.20865629698406893, 'dropout_timeStampEmbedding': 0.1322336147870886, 'dropout_transformers': 0.2581026005192587, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 31, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 0.0009871197075032479, 'max_lr': 0.07635522331092133, 'mode': 'triangular2', 'scheduler': 'CyclicLR', 'step_size_up': 9, 'dropout_lstm': 0.484035234259669, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'LeakyReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 4, 'amsgrad': False, 'beta_1': 0.885400090675543, 'beta_2': 0.9673373506900346, 'eps': 4.858946471998355e-09, 'lr': 0.00017703926163231486, 'optimizer': 'AdamW', 'weight_decay': 0.0005334299895800831, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  7.459433671047813 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.379312183982448 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.406506970054225 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  7.37391132053576 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  7.395470513795551 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  7.407405747865376 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  7.475888493186549 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'LeakyReLU', 'activation_transformers': 'CELU', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1128, 'dropout': 0.3794736514078232, 'dropout_StationIdEmbedding': 0.2831505455159269, 'dropout_timeStampEmbedding': 0.46072963792558486, 'dropout_transformers': 0.7762152524010135, 'early_stopping': 5, 'encoder_only': False, 'epochs_classifcation_only': 37, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 48, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 3, 'alpha': 0.9520295006346334, 'centered': True, 'eps': 1.3224368591670442e-08, 'lr': 0.011778938887567073, 'momentum': 0.4630914023055783, 'optimizer': 'RMSprop', 'weight_decay': 0.3136568930440908, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'RReLU', 'dropout_gcn': 0.5700377284419751, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 5, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.305441014310146 acc:  0.003103856374070518\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  8.019828370276917 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  8.019319595174586 acc:  0.0005359176473215282\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  8.019433914346898 acc:  0.000781546569010562\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  8.010004439252489 acc:  0.0036397740213920463\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  7.87381082900027 acc:  0.002009691177455731\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  7.818281589670384 acc:  0.0016077529419645847\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  7.850242959692123 acc:  0.0005359176473215282\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  7.875070835681671 acc:  0.00017863921577384274\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  7.882081468054589 acc:  0.0009378558828126744\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'GELU', 'batch_size': 64, 'concatenate_features': False, 'd_model': 216, 'dropout': 0.3261789051355585, 'dropout_StationIdEmbedding': 0.6700150419413546, 'dropout_timeStampEmbedding': 0.08239052987264821, 'dropout_transformers': 0.3741388122684646, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 45, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 20, 'eta_min': 0.3018332817437602, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.5866157179760317, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 5, 'amsgrad': False, 'beta_1': 0.9151395203704776, 'beta_2': 0.9723896620714999, 'eps': 1.4729887865514226e-06, 'lr': 2.7781862337894983e-05, 'optimizer': 'AdamW', 'weight_decay': 3.2043231485854965e-09, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5866157179760317 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.169504349973021 acc:  0.0008931960788692137\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.3936777956345505 acc:  0.03097157403478999\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  6.798426992752972 acc:  0.18652167116986357\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  5.878395024467917 acc:  0.2126923162807315\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  5.279356479644775 acc:  0.22434852510997477\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  5.082611893405433 acc:  0.21514860549762185\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.23713305944913743 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softmin', 'activation_transformers': 'Tanhshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 696, 'dropout': 0.541300256763616, 'dropout_StationIdEmbedding': 0.32440931108325116, 'dropout_timeStampEmbedding': 0.3325215479777458, 'dropout_transformers': 0.34721351770104, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 62, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.5687294813995353, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.23713305944913743, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardswish', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 24, 'num_layers_transformer': 1, 'lr': 3.3272871705909365e-06, 'momentum': 0.39267756241044116, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 0.016307081781112134, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Tanhshrink', 'dropout_gcn': 0.7702759895140173, 'hidden_channels': 2048, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 2, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.020785875409564 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  8.020754778496572 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  8.020906733575268 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4727041511204779 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Hardswish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1392, 'dropout': 0.23486814482870977, 'dropout_StationIdEmbedding': 0.012869712632046393, 'dropout_timeStampEmbedding': 0.20990781763196148, 'dropout_transformers': 0.39982692079034005, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 71, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.4727041511204779, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8640403458355227, 'beta_2': 0.9594584700146656, 'eps': 2.2922850836244756e-08, 'lr': 2.8664721598807104e-05, 'optimizer': 'AdamW', 'weight_decay': 1.0190736848145976e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  7.789859985138153 acc:  0.00714556863095371\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.233793135289545 acc:  0.02407163432552531\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.008068741618336 acc:  0.05515485787017395\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  6.7622723312644695 acc:  0.10533014760065203\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  6.51518282523522 acc:  0.14228613536386575\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  6.258257008932687 acc:  0.16651407900319318\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  6.02914826853292 acc:  0.1955206216644709\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  5.771256636906337 acc:  0.2108389344170779\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  5.5140855062258 acc:  0.224661143737579\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  5.273860084426986 acc:  0.24015809570595986\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  5.030562254098745 acc:  0.25147935600562715\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  4.859471702909136 acc:  0.2648996270906371\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  4.687740674385657 acc:  0.2761762275863609\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  4.50453299409026 acc:  0.28660429180715896\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  4.330409243390276 acc:  0.2956032423017663\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  4.246118345460691 acc:  0.3054283991693276\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  4.079436895730612 acc:  0.31302056583971594\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  3.981575810825908 acc:  0.3221758256481254\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  3.8570581099370145 acc:  0.32760199182725586\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  3.7754533957768155 acc:  0.33791840653819527\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  3.6884113375123566 acc:  0.34363486144295824\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  3.616556892861853 acc:  0.3497532545832124\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  3.5382127178298846 acc:  0.3542415648795302\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  3.454745659461388 acc:  0.36011432909809526\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  3.4004125345003353 acc:  0.36679096978764264\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  3.3090630794738556 acc:  0.3709889913583279\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  3.279325710309969 acc:  0.3758345800861934\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  3.2380669933932644 acc:  0.38027823057856774\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  3.184783682122931 acc:  0.3848781903847442\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  3.1586065008923723 acc:  0.3863519639148784\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  3.104966655477777 acc:  0.39101891342697004\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  3.034396828471364 acc:  0.3945247080365317\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  3.0126920063178857 acc:  0.3955518835272313\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  2.995948316334011 acc:  0.39881204921510394\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  2.941933000004375 acc:  0.39992854431369046\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  2.9042393360938226 acc:  0.4021838644128352\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  2.9031692218113614 acc:  0.4052877207869057\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  2.8988946617900075 acc:  0.40618091686577495\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  2.851872355787904 acc:  0.4109595158877253\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  2.8063282749869605 acc:  0.41124980461335775\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  2.8020380810424164 acc:  0.4138847330460219\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  2.778101442577122 acc:  0.41395172275193715\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  2.7421786384982663 acc:  0.4164303418707992\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  2.7391292215227248 acc:  0.4181274144206507\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  2.7210593156881266 acc:  0.41971283746064353\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  2.705372720331579 acc:  0.42154388942232546\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  2.664134585774028 acc:  0.42333028158006386\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  2.643217408573711 acc:  0.4236652301096398\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  2.660680919260412 acc:  0.4238661992273854\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  2.6102714655282613 acc:  0.4261661791304736\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  2.6034009840104964 acc:  0.4261885090324454\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  51 loss :  2.5914232947609643 acc:  0.42723801442511666\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  52 loss :  2.5476577265279277 acc:  0.4292477056025724\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  53 loss :  2.5623279901651235 acc:  0.4306991492307349\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  54 loss :  2.539593400655093 acc:  0.43150302570171717\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  55 loss :  2.5357964639063484 acc:  0.4318826340352366\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  56 loss :  2.512460733627106 acc:  0.43308844874171004\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  57 loss :  2.513147949338793 acc:  0.43429426344818345\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  58 loss :  2.4920465329310275 acc:  0.4324855413884733\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  59 loss :  2.472854191606695 acc:  0.43532143893888303\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  60 loss :  2.4527669036305033 acc:  0.4343165933501552\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  61 loss :  2.453039115125483 acc:  0.43717482080253667\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  62 loss :  2.433956771463781 acc:  0.43893888305830336\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  63 loss :  2.4211488987182403 acc:  0.43737578992028225\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  64 loss :  2.419457296391467 acc:  0.4391845119799924\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  65 loss :  2.4051424298253092 acc:  0.43994372864703124\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  66 loss :  2.397324968884875 acc:  0.4407252752160418\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  67 loss :  2.4438954833504205 acc:  0.43916218207802066\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  68 loss :  2.3810851207146277 acc:  0.44010003796083336\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  69 loss :  2.3627810886689833 acc:  0.4421320590402608\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  70 loss :  2.342743725209803 acc:  0.442199048746176\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.748792574455236 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Hardswish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1368, 'dropout': 0.21633943701146666, 'dropout_StationIdEmbedding': 0.06006319940401399, 'dropout_timeStampEmbedding': 0.24817736841872917, 'dropout_transformers': 0.21707783273911457, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 75, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.748792574455236, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.858421889895238, 'beta_2': 0.9604391015598143, 'eps': 1.7901681033341197e-08, 'lr': 2.7271004275205055e-05, 'optimizer': 'AdamW', 'weight_decay': 1.3929402018660987e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  7.761166375833792 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.21886828896049 acc:  0.010896992162204407\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  6.990608191990352 acc:  0.07007123238728982\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  6.717974219288859 acc:  0.11160485005470826\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  6.444596333937212 acc:  0.15079382801509503\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  6.134482076951674 acc:  0.17863921577384276\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  5.842527079415488 acc:  0.19922738539177812\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  5.57309541502199 acc:  0.22012817363731774\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  5.2695352681033265 acc:  0.23821539423441931\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  5.034614089485649 acc:  0.2555657280664538\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  4.8478559547370965 acc:  0.26900832905343547\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  4.652619268510725 acc:  0.2802402697452158\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  4.4633120623501865 acc:  0.28801107563137796\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  4.296869429675016 acc:  0.2999575731862537\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  4.155021492417876 acc:  0.3114798026036666\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  4.020986520327055 acc:  0.31911662907799837\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  3.927044406637445 acc:  0.3276243217292276\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  3.7921922023479757 acc:  0.3362659937922873\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  3.685011240152212 acc:  0.34539892369872494\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  3.609226396867445 acc:  0.351852265368555\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  3.552526357290628 acc:  0.3590424938034522\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  3.438462175689377 acc:  0.3641337114530067\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  3.3955271944299446 acc:  0.3682424134158051\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  3.27430958514447 acc:  0.3738918786146529\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  3.235975175470739 acc:  0.3767724359690061\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  3.208812418517533 acc:  0.37922872518589645\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  3.1283457579312626 acc:  0.3836723756782708\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  3.1064315475783983 acc:  0.389701449210638\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  3.0463202483170515 acc:  0.39316258401625614\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  2.9981753776123474 acc:  0.39521693499765537\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  2.9470426436070793 acc:  0.39823147176383894\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  2.93679954455449 acc:  0.4014469776477681\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  2.904672809414097 acc:  0.4053100506888775\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  2.8686966762676107 acc:  0.4072974119643615\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  2.8394709150274315 acc:  0.4102449590246299\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  2.8303734739343605 acc:  0.4127012482415202\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  2.7665437985133456 acc:  0.41435366098742826\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  2.742644331671975 acc:  0.4158274345175625\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  2.7164121757854116 acc:  0.41964584775472835\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  2.725618679206688 acc:  0.42015943550007817\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  2.6670160743740055 acc:  0.42105263157894735\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  2.654204523646748 acc:  0.42250407520710986\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  2.6434967184400224 acc:  0.4244244467766787\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  2.61495135047219 acc:  0.4254292923654065\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  2.5956946219597663 acc:  0.4271486948172298\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  2.5899147653913164 acc:  0.4295156644262332\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  2.5517906584106127 acc:  0.429448674720318\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  2.570274716490632 acc:  0.4312573967800281\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  2.5146345425319003 acc:  0.4324632114865016\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  2.500080387075464 acc:  0.4327981600160775\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  2.515171895493994 acc:  0.43349038697720116\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  51 loss :  2.4754315256238817 acc:  0.4336020364870598\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  52 loss :  2.46196579016172 acc:  0.4352767791349396\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  53 loss :  2.448159948095575 acc:  0.436482593841413\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  54 loss :  2.428837540266397 acc:  0.43679521246901726\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  55 loss :  2.4162742899848033 acc:  0.4374204497242257\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  56 loss :  2.4499026845385146 acc:  0.4388272335484447\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  57 loss :  2.4047069949703617 acc:  0.43721948060648014\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  58 loss :  2.3844250815731662 acc:  0.4403679967844941\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  59 loss :  2.3876260444000885 acc:  0.43996605854900295\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  60 loss :  2.3519400674980004 acc:  0.4409709041377308\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  61 loss :  2.3594328406807428 acc:  0.44173012080476964\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  62 loss :  2.331864842168101 acc:  0.44076993501998524\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7504566065967552 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Hardswish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1392, 'dropout': 0.2132341800917258, 'dropout_StationIdEmbedding': 0.055069988690378546, 'dropout_timeStampEmbedding': 0.28081094060414435, 'dropout_transformers': 0.40518460279781665, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 75, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7504566065967552, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8643925986139109, 'beta_2': 0.9503202673300509, 'eps': 1.7039507347789476e-08, 'lr': 4.313868257479694e-06, 'optimizer': 'AdamW', 'weight_decay': 1.4980452342895956e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.04369711042284 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.9989510816294 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.930481737310236 acc:  0.003572784315476855\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  7.799376447717626 acc:  0.006051403434338924\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  7.6176435563947775 acc:  0.005738784806734698\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  7.480077450092022 acc:  0.006230042650112766\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  7.378244413362516 acc:  0.00710090882701025\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  7.303836759153779 acc:  0.007949445101936002\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  7.249109108131249 acc:  0.008641672063059644\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  7.196235546698937 acc:  0.009266909318268093\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  7.162779537947862 acc:  0.010941651966147868\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  7.121152691074185 acc:  0.013509590692896858\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  7.083504726836732 acc:  0.01632315834133488\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  7.058888171936249 acc:  0.01998526226469866\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  7.030744899402965 acc:  0.023870665207779737\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  7.005885230911361 acc:  0.029721099524373087\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  6.980486069525872 acc:  0.03592881227251413\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  6.931820315914554 acc:  0.040729741196436145\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  6.911795879577423 acc:  0.05006364022061943\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  6.871668482160235 acc:  0.05448496081102204\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  6.839020538996984 acc:  0.05796842551861197\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  6.800483790310946 acc:  0.06426545787463993\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  6.761667755100277 acc:  0.06864211866109908\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  6.742568799665758 acc:  0.07688185248866758\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  6.703404696671279 acc:  0.08204005984413729\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  6.676980562143393 acc:  0.08585847308130318\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  6.653921047290722 acc:  0.08987785543621464\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  6.598452581392301 acc:  0.09776031083223545\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  6.566219319830408 acc:  0.10539713730656722\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  6.536267377279855 acc:  0.10993010740682849\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  6.515979513421759 acc:  0.11647276868454548\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  6.472448969220782 acc:  0.12372998682535784\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  6.437140644847096 acc:  0.1303842976129335\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  6.39841984702157 acc:  0.1367259897729049\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  6.37135923492325 acc:  0.14154924859879864\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  6.324007411103149 acc:  0.14771230154299622\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  6.295336629960921 acc:  0.15427729272268495\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  6.276513983319689 acc:  0.15633164370408414\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  6.240123361974329 acc:  0.16396847017841593\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  6.20797907222401 acc:  0.1706451108679633\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  6.156252044064182 acc:  0.1742625549873836\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  6.100105889193662 acc:  0.17676350400821741\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  6.105501178261283 acc:  0.181765402049885\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  6.072158986871893 acc:  0.18283723734452806\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  5.998035707673826 acc:  0.18748185695464797\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  6.009256079480364 acc:  0.19002746577942523\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  5.91262826052579 acc:  0.19217113636871133\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  5.902180154840429 acc:  0.19424781725208226\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  5.890824591363226 acc:  0.19833418931290892\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  5.838671864329518 acc:  0.19978563294107138\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  5.76276978912887 acc:  0.20210794274613134\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  51 loss :  5.759845243467318 acc:  0.2047205412768238\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  52 loss :  5.747130327291422 acc:  0.20947681039680235\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  53 loss :  5.693706385739199 acc:  0.21115155304468214\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  54 loss :  5.656135399024803 acc:  0.21387580108523324\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  55 loss :  5.63071741757693 acc:  0.21626510059620838\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  56 loss :  5.5603718457522096 acc:  0.21809615255789028\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  57 loss :  5.561756190720138 acc:  0.21963691579393965\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  58 loss :  5.496971760596429 acc:  0.22329901971730345\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  59 loss :  5.481420446942736 acc:  0.22374561775673804\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  60 loss :  5.402080359158816 acc:  0.2270951030524976\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  61 loss :  5.4183094184715435 acc:  0.23028827903445503\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  62 loss :  5.340414380693769 acc:  0.23162807315275885\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  63 loss :  5.33481757624166 acc:  0.2343746510952817\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  64 loss :  5.294821859239699 acc:  0.23526784717415092\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  65 loss :  5.271759199929404 acc:  0.23792510550878682\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  66 loss :  5.227302711326759 acc:  0.23971149766652525\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  67 loss :  5.21412628680676 acc:  0.2421677868834156\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  68 loss :  5.155590347476773 acc:  0.24355224080566287\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  69 loss :  5.113035158677534 acc:  0.24605318982649665\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  70 loss :  5.081414407783455 acc:  0.24708036531719627\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  71 loss :  5.066733180226146 acc:  0.2498492731616908\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  72 loss :  5.075920735205804 acc:  0.25185896433914656\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  73 loss :  5.00823871239082 acc:  0.2532210883594221\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  74 loss :  5.00560574764972 acc:  0.2559453363999732\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9180716770322955 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Hardswish', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1320, 'dropout': 0.2937572585945335, 'dropout_StationIdEmbedding': 0.14329899549129785, 'dropout_timeStampEmbedding': 0.24963007890636787, 'dropout_transformers': 0.22764380063168463, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 71, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.9180716770322955, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8502403679384819, 'beta_2': 0.9593259292545429, 'eps': 1.653602809193038e-08, 'lr': 1.999907349664514e-05, 'optimizer': 'AdamW', 'weight_decay': 8.133674275845041e-07, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  7.9972980960722895 acc:  0.0032824955898443607\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.609290538295623 acc:  0.004867918629837215\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.247208256875315 acc:  0.00777080588616216\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  7.151393785784322 acc:  0.01685907598865641\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  7.055059549885412 acc:  0.030257017171694617\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  6.939060543429467 acc:  0.042315164236429004\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  6.791184917573006 acc:  0.07335372797713419\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  6.708912452574699 acc:  0.09318268092803073\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  6.57174345754808 acc:  0.10689324073867315\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  6.460606633463214 acc:  0.12763771967041065\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  6.3763051740584835 acc:  0.1504142196815756\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  6.257902837568714 acc:  0.1616684902753277\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  6.144585107987927 acc:  0.17678583391018912\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  6.056237602233887 acc:  0.19228278587857\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  5.963803638950471 acc:  0.19822253980305027\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  5.792663217359974 acc:  0.21166514079003193\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  5.661623705587079 acc:  0.21890002902887257\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  5.57521895439394 acc:  0.22640287609137397\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  5.471447507796749 acc:  0.23569211531161377\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  5.371974249809019 acc:  0.24346292119777593\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  5.2359994334559286 acc:  0.2481968604157828\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  5.1166578861974905 acc:  0.25797735747940065\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  5.089159439456078 acc:  0.2636714824821919\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  4.96719954552189 acc:  0.27164325748609963\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  4.901047454341765 acc:  0.2752160418015765\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  4.840900711859426 acc:  0.2800839604314137\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  4.707283655289681 acc:  0.2870732197485653\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  4.664731456387427 acc:  0.29323627269276287\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  4.532947974051199 acc:  0.2962731393609182\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  4.461942698878627 acc:  0.30221289328539847\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  4.392212010968116 acc:  0.30491481142397786\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  4.287079029698526 acc:  0.31000602907353236\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  4.2064117508549845 acc:  0.31610209231181474\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  4.169799104813607 acc:  0.3170176182926557\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  4.098773700960221 acc:  0.32094768103968024\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  4.0327196382707164 acc:  0.32697675457204745\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  3.971272102479012 acc:  0.3288747962396445\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  3.90148882250632 acc:  0.33284951879061253\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  3.9039692509558894 acc:  0.3372038496750999\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  3.852085768791937 acc:  0.3381417055579126\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  3.7806279843853368 acc:  0.3427639952660608\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  3.7555653264445645 acc:  0.34629211977759417\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  3.637196134751843 acc:  0.347631913895898\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  3.6567395733248804 acc:  0.350445481544336\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  3.6410971195467057 acc:  0.35292410066319807\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  3.572996513305172 acc:  0.357323091351629\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  3.4995269221644247 acc:  0.3584619163521872\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  3.497734491286739 acc:  0.36127548400062526\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  3.468598634965958 acc:  0.36310653596230713\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  3.4460732813804382 acc:  0.3676618359645401\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  3.4139856000100415 acc:  0.3695822075341089\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  51 loss :  3.3708189241347775 acc:  0.37165888841747985\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  52 loss :  3.3568147059409847 acc:  0.3736909094969073\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  53 loss :  3.3550301213418283 acc:  0.37556662126253265\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  54 loss :  3.3092157456182663 acc:  0.37750932273407317\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  55 loss :  3.272810782155683 acc:  0.3805238595002568\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  56 loss :  3.23346473170865 acc:  0.3813054060692674\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  57 loss :  3.2111288301406367 acc:  0.38338208695263826\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  58 loss :  3.202272424390239 acc:  0.3851908090123484\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  59 loss :  3.226569172643846 acc:  0.3866422526405109\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  60 loss :  3.159577598879414 acc:  0.38784806734698435\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  61 loss :  3.164966244851389 acc:  0.3887635933278253\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  62 loss :  3.081305233124764 acc:  0.39218006832950003\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  63 loss :  3.092796867124496 acc:  0.3939441305852667\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  64 loss :  3.070429039001465 acc:  0.395328584507514\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  65 loss :  3.0158747442307012 acc:  0.39740526539088494\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  66 loss :  3.00064323025365 acc:  0.39970524529397317\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  67 loss :  3.0048147924484745 acc:  0.39952660607819934\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  68 loss :  2.99553853542574 acc:  0.40071009088270104\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  69 loss :  3.00506837906376 acc:  0.4027644418641002\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  70 loss :  2.940231796233885 acc:  0.40497510215930155\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.626037335541467 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Hardswish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1392, 'dropout': 0.39893655006454704, 'dropout_StationIdEmbedding': 0.03937634339952692, 'dropout_timeStampEmbedding': 0.1959278187612626, 'dropout_transformers': 0.5318811969083641, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 65, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.626037335541467, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8733727036005741, 'beta_2': 0.9530214850132347, 'eps': 5.326440476770292e-09, 'lr': 8.43784429544044e-07, 'optimizer': 'AdamW', 'weight_decay': 6.936782733401724e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.059258349114955 acc:  0.0004465980394346069\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  8.050868822875636 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  8.043586096950083 acc:  0.0005582475492932586\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  8.033892879272972 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  8.026288093801318 acc:  0.0006922269611236406\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  8.0189220625595 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  8.007996551151383 acc:  0.0008485362749257531\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  8.000217325860561 acc:  0.0009601857847844048\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  7.990629206822571 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  7.979012958164321 acc:  0.0012504745104168992\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  7.96689948289754 acc:  0.0016077529419645847\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  7.954280011480747 acc:  0.0021213406873143827\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  7.939680112806778 acc:  0.0024786191188620682\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  7.9230216868096885 acc:  0.0031708460799857088\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  7.90249638584073 acc:  0.003974722550968001\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  7.877990315080355 acc:  0.004756269119978563\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  7.850945435422759 acc:  0.005761114708706429\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  7.816894904195263 acc:  0.007011589219123328\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  7.776869491491904 acc:  0.007860125494049082\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  7.732452730892756 acc:  0.008664001965031374\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  7.6892439373378645 acc:  0.008820311278833487\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  7.637118507363943 acc:  0.008038764709822925\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  7.598053940181626 acc:  0.007681486278275238\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8650828676885423 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'Hardswish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1272, 'dropout': 0.7045868659766934, 'dropout_StationIdEmbedding': 0.25997619658098026, 'dropout_timeStampEmbedding': 0.41481858473695277, 'dropout_transformers': 0.47035257882989645, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 69, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.8650828676885423, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8869391769498158, 'beta_2': 0.9619919102981779, 'eps': 2.86454790422147e-08, 'lr': 4.426161064719228e-06, 'optimizer': 'AdamW', 'weight_decay': 1.0093987311866204e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.193831010298295 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.98452549380856 acc:  0.002099010785342652\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.804132458213326 acc:  0.003505794609561664\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  7.654387897544807 acc:  0.00582810441462162\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  7.52007217340536 acc:  0.009959136279391734\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  7.405759754714432 acc:  0.015251323046691825\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  7.2973979403088975 acc:  0.019739633343009624\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  7.205116195278568 acc:  0.024384252953129536\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  7.123983993396893 acc:  0.030614295603242303\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  7.059707908363609 acc:  0.03952392648996271\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  6.978105138231824 acc:  0.04899180492597637\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  6.869955523030741 acc:  0.05933054953888752\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  6.800519376367956 acc:  0.07790902797936718\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  6.639256133899822 acc:  0.09416519661478687\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  6.518308302739284 acc:  0.11196212848625595\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  6.365806126094364 acc:  0.12643190496393722\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  6.261917571087817 acc:  0.14338030056048054\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  6.138119340776564 acc:  0.15628698390014067\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  6.008663317540309 acc:  0.16928298684768775\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  5.965897383389773 acc:  0.1799343500882031\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  5.876844452811288 acc:  0.18826340352365853\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  5.765164765444669 acc:  0.19786526137150257\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  5.698098976295311 acc:  0.2053011187280888\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  5.5758055373505275 acc:  0.21184378000580578\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  5.499960055718055 acc:  0.21865440010718354\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  5.465997685919275 acc:  0.22403590648237054\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  5.3724246925407355 acc:  0.23035526874037024\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  5.3297090697121785 acc:  0.23598240403724627\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  5.258578233785562 acc:  0.24111828149074427\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  5.17224671290471 acc:  0.2449813545318536\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  5.149807616547271 acc:  0.25190362414309003\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  5.05241031580038 acc:  0.2556550476743407\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  5.025502474991591 acc:  0.26029966728446063\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  4.95429174716656 acc:  0.26541321483598684\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  4.915015610781583 acc:  0.2696112364066722\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  4.856685113239955 acc:  0.27298305160440345\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  4.815384534689096 acc:  0.27760534131255166\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  4.77999794566548 acc:  0.28077618739253735\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  4.664185964144194 acc:  0.2849295491592792\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  4.631623061386855 acc:  0.28818971484715183\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  4.59428752385653 acc:  0.29245472612375234\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  4.526866791131613 acc:  0.29562557220373803\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  4.535291039860332 acc:  0.29908670700935625\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  4.493817894608824 acc:  0.30301676975638075\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  4.40348241379211 acc:  0.30674586338565973\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  4.385483216572474 acc:  0.30902351338677625\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  4.324248979141662 acc:  0.312105039858875\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  4.2807586276447855 acc:  0.3148292878994261\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  4.277197215940569 acc:  0.318893330058281\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  4.228598299560013 acc:  0.32112632025545407\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  4.219744899056175 acc:  0.3234932898644575\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  51 loss :  4.1519778325007515 acc:  0.32447580555121364\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  52 loss :  4.12198223934307 acc:  0.32874081682781414\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  53 loss :  4.077938211547745 acc:  0.3312194359466762\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  54 loss :  4.0662646960545255 acc:  0.3329611683004712\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  55 loss :  4.052043046151008 acc:  0.33595337516468304\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  56 loss :  4.014022610404274 acc:  0.33738248889087374\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  57 loss :  3.995102675644668 acc:  0.34021838644128355\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  58 loss :  3.9410479985750637 acc:  0.34258535605028695\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  59 loss :  3.894575210718008 acc:  0.34508630507112076\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  60 loss :  3.895405129119233 acc:  0.34606882075787687\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  61 loss :  3.877370727645767 acc:  0.3475649241899828\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  62 loss :  3.8250658628823873 acc:  0.3511600384074314\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  63 loss :  3.8217050145556044 acc:  0.3528794408592546\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  64 loss :  3.782446356086464 acc:  0.3552240805662863\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  65 loss :  3.74231090245547 acc:  0.35663086439050534\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  66 loss :  3.7483627529411048 acc:  0.3585065761561307\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  67 loss :  3.712662760194365 acc:  0.3596230712547172\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  68 loss :  3.7135429665758894 acc:  0.36174441194203155\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7497613309921367 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'PReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1320, 'dropout': 0.13591267613015862, 'dropout_StationIdEmbedding': 0.021355715096755643, 'dropout_timeStampEmbedding': 0.3141526519353306, 'dropout_transformers': 0.4009762494255571, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 80, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7497613309921367, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8574505286970864, 'beta_2': 0.9680012490499907, 'eps': 5.882632088579272e-09, 'lr': 1.88043262679481e-05, 'optimizer': 'AdamW', 'weight_decay': 2.236058148816801e-08, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  7.826978847270704 acc:  0.010651363240515374\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.245127495918565 acc:  0.03838510148940446\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  6.829291827806079 acc:  0.0947011142621084\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  6.429971880585183 acc:  0.15429962262465668\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  5.989630011201815 acc:  0.1936002500949021\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  5.631206876449003 acc:  0.22381260746265325\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  5.284807634717636 acc:  0.2449813545318536\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  5.01849681184492 acc:  0.267802514346962\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  4.773312783423271 acc:  0.2830315074916821\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  4.549835601835761 acc:  0.2991536967152714\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  4.388028301355493 acc:  0.3118817408391577\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  4.222109885616157 acc:  0.3243864859433267\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  4.037503368071928 acc:  0.33523881830158764\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  3.896965678411586 acc:  0.3435902016390148\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  3.7952021132898697 acc:  0.35178527566263984\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  3.6312586988201576 acc:  0.36165509233414467\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  3.558746965786883 acc:  0.36681329968961435\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  3.4619933521474593 acc:  0.37536565214478707\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  3.381863168177714 acc:  0.37907241587209434\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  3.3341892839388083 acc:  0.3836053859723556\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  3.2345065287961305 acc:  0.38836165509233417\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  3.2011353095979183 acc:  0.39278297568273673\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  3.1366140241841323 acc:  0.39506062568385325\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  3.076412537625728 acc:  0.3997275751959449\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  3.063149863527021 acc:  0.40033048254918163\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  2.9961149383137244 acc:  0.4036353080409977\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  2.9434583751299908 acc:  0.40749838108210706\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  2.8983846402350273 acc:  0.4086148761806936\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  2.864870200630363 acc:  0.4096197217694214\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  2.8493504160233125 acc:  0.4133264854967287\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  2.812076979921064 acc:  0.4136837639282764\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  2.7536593775712808 acc:  0.41734586785164013\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  2.767591028723098 acc:  0.4176361565772726\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  2.7267370151199457 acc:  0.418931290891633\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  2.704424008158327 acc:  0.42065069334345623\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  2.665090064056047 acc:  0.4224147555992229\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  2.6445371904445967 acc:  0.4234419310899225\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  2.6541002178920134 acc:  0.4228836835406293\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  2.616549147904374 acc:  0.42375454971752674\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  2.591568721159724 acc:  0.4241788178549896\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  2.545623519038426 acc:  0.4259428801107563\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  2.546181536812819 acc:  0.42603219971864326\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  2.5231783235346086 acc:  0.42824286001384454\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  2.4923512162143036 acc:  0.4283098497197597\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  2.4679784019484776 acc:  0.4279972310921555\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  2.459050568005511 acc:  0.428287519817788\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  2.45885095341515 acc:  0.4303418707991872\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  2.435841559453775 acc:  0.4294710046222897\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  2.4181292284535996 acc:  0.432262242368756\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  2.423323021590255 acc:  0.4313913761918585\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  2.3897803002641402 acc:  0.4327758301141058\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  51 loss :  2.4019859501423726 acc:  0.4352767791349396\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  52 loss :  2.3369054575912824 acc:  0.43366902619297504\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  53 loss :  2.343203895874606 acc:  0.4347408614876181\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  54 loss :  2.335099613393536 acc:  0.43547774825268515\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  55 loss :  2.2977704892631707 acc:  0.43699618158676284\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  56 loss :  2.3084097409066353 acc:  0.4361699752138088\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  57 loss :  2.2966776722260103 acc:  0.4359243462921198\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5846808633499627 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Hardswish', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1032, 'dropout': 0.04206424576531012, 'dropout_StationIdEmbedding': 0.1969024329519362, 'dropout_timeStampEmbedding': 0.04114825969916583, 'dropout_transformers': 0.16057233067894292, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 76, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 0.000444893292663368, 'max_lr': 0.0840626223586766, 'mode': 'triangular', 'scheduler': 'CyclicLR', 'step_size_up': 23, 'dropout_lstm': 0.5846808633499627, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8725586338848395, 'beta_2': 0.9789791962061831, 'eps': 3.153177313595816e-09, 'lr': 0.00015376743967275276, 'optimizer': 'AdamW', 'weight_decay': 0.003587017774694503, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  6.860105525181946 acc:  0.15662193242971664\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  6.1241057571752116 acc:  0.19458276578165823\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  5.270810139245827 acc:  0.18335082508987785\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'SiLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 48, 'dropout': 0.23682486461811514, 'dropout_StationIdEmbedding': 0.06454844274578705, 'dropout_timeStampEmbedding': 0.37825295767676975, 'dropout_transformers': 0.20653395037789302, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 58, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.5359932806822093, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Softplus', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8450229318380051, 'beta_2': 0.9556847894445372, 'eps': 1.289920689646375e-08, 'lr': 3.457327757156022e-05, 'optimizer': 'AdamW', 'weight_decay': 8.691594068762584e-09, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5359932806822093 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.052920606059413 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.996616926500874 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.9272722151971635 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  7.850446845639136 acc:  0.0014514436281624723\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  7.764176485615392 acc:  0.0016970725498515061\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  7.683235746814359 acc:  0.0032155058839291695\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  7.601924914698447 acc:  0.004443650492374339\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  7.524986870058121 acc:  0.005091217649554518\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  7.470863508409069 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  7.422743274319556 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7127836068323692 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'ReLU6', 'activation_transformers': 'LogSigmoid', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1440, 'dropout': 0.339943726147545, 'dropout_StationIdEmbedding': 0.1603580960400004, 'dropout_timeStampEmbedding': 0.22571627244022935, 'dropout_transformers': 0.32817603885627034, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 73, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7127836068323692, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.824913797527221, 'beta_2': 0.9521636125462684, 'eps': 2.488835076286207e-08, 'lr': 4.628359076506855e-06, 'optimizer': 'AdamW', 'weight_decay': 2.1676028811482392e-07, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.102220404239102 acc:  0.0015407632360493937\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.797550518094129 acc:  0.0036621039233637764\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.57164721088555 acc:  0.004979568139695867\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  7.435675391714081 acc:  0.0064756715718018\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  7.311198467516717 acc:  0.009244579416296363\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  7.209133479431386 acc:  0.012616394614027644\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  7.138787979388055 acc:  0.019784293146953087\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  7.071776532034837 acc:  0.028961882857334257\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  6.983794150461677 acc:  0.04276176227586361\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  6.891665960996206 acc:  0.055757765223410666\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  6.7955169131737625 acc:  0.07221490297657593\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  6.677001418048189 acc:  0.0894089274948083\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  6.574361906706832 acc:  0.10177969318714691\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  6.442450308617745 acc:  0.11537860348793069\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  6.358292106453699 acc:  0.13080856575039634\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  6.213607049170341 acc:  0.14398320791371727\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  6.143537637841611 acc:  0.15418797311479804\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  6.067200245748039 acc:  0.1651296250809459\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  5.999759240914847 acc:  0.17466449322287475\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  5.900902227591012 acc:  0.18350713440367997\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  5.818135727452868 acc:  0.19165754862336154\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  5.729675016330399 acc:  0.19864680794051315\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  5.658253214741481 acc:  0.2061273251010428\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  5.578253425714624 acc:  0.21430006922269612\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  5.507889598380518 acc:  0.22041846236295023\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  5.4835900168382485 acc:  0.2268718040327803\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  5.377209539631851 acc:  0.2333474756045821\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  5.303266110311028 acc:  0.23980081727441216\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  5.293340624743745 acc:  0.24598620012058148\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  5.178075746725534 acc:  0.25092110845633386\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  5.150179757416703 acc:  0.2565035839492665\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  5.069865459704217 acc:  0.2611035437554429\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  5.025374856613975 acc:  0.2637161422861354\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  4.946124459040984 acc:  0.2693656074849831\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  4.876551231355157 acc:  0.27293839180046\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  4.820922735083194 acc:  0.2780072795480428\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  4.786648368107453 acc:  0.28329946631534286\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  4.719147296352241 acc:  0.2869392403367349\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  4.681048496988893 acc:  0.2909586226916464\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  4.6326308723624425 acc:  0.294553736909095\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  4.538588487464963 acc:  0.2966750775964094\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  4.5346298072174305 acc:  0.300761449657236\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  4.504514808873184 acc:  0.30520510014961033\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  4.449439174346342 acc:  0.3077953687783311\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  4.375855808039658 acc:  0.3112118437800058\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  4.361860326228251 acc:  0.3144050197619632\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  4.331524670579051 acc:  0.3159457829980126\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  4.245093154543229 acc:  0.31898264966616796\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  4.239444454207675 acc:  0.3215952481968604\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  4.215016294071693 acc:  0.3245651251591006\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  4.139260579611509 acc:  0.3278476207489449\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  51 loss :  4.062061781191644 acc:  0.32972333251457026\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  52 loss :  4.120364007149034 acc:  0.3320233124176585\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  53 loss :  4.067282108860161 acc:  0.334546591340464\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  54 loss :  4.019171274345339 acc:  0.33731549918495857\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  55 loss :  3.967358398073502 acc:  0.3378514168322801\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  56 loss :  3.998799580654115 acc:  0.34216108791282407\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  57 loss :  3.894318018250793 acc:  0.3442600986981667\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  58 loss :  3.8697449942581525 acc:  0.3460911506598486\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  59 loss :  3.828314036813401 acc:  0.3487484089944845\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  60 loss :  3.8185607986595795 acc:  0.3491503472299757\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  61 loss :  3.817745265159898 acc:  0.35267847174150907\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  62 loss :  3.7889407063258513 acc:  0.35236585311390484\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  63 loss :  3.723232893543389 acc:  0.35567067860572094\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  64 loss :  3.7232071479768245 acc:  0.357323091351629\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  65 loss :  3.636473424562061 acc:  0.3600250094902083\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  66 loss :  3.6236799673269724 acc:  0.36129781390259696\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  67 loss :  3.626205012998508 acc:  0.36230265949132484\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  68 loss :  3.595308056314483 acc:  0.3641783712569502\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  69 loss :  3.576813115418412 acc:  0.36522787664962153\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  70 loss :  3.5371281809479225 acc:  0.3673492173369359\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  71 loss :  3.548742807548465 acc:  0.3682200835138334\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  72 loss :  3.5328590414906276 acc:  0.3696268673380524\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6344844716753606 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'SELU', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1200, 'dropout': 0.10530849752579123, 'dropout_StationIdEmbedding': 0.004648442177951087, 'dropout_timeStampEmbedding': 0.4944041311982972, 'dropout_transformers': 0.058679930965360616, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 51, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.19425452486587952, 'scheduler': 'StepLR', 'step_size': 22, 'dropout_lstm': 0.6344844716753606, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.9157157310582519, 'beta_2': 0.9607815413789651, 'eps': 7.839760337087477e-09, 'lr': 1.3483795641215889e-05, 'optimizer': 'AdamW', 'weight_decay': 2.888055197848342e-05, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.063434221073539 acc:  0.005247526963356631\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  7.513524215378447 acc:  0.013576580398812049\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  7.183442749663027 acc:  0.03108322354464864\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  6.913594659930932 acc:  0.07297411964361476\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  6.604136789630273 acc:  0.11394948976173995\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  6.300416692288336 acc:  0.1484268584060916\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  5.984659823115001 acc:  0.17997900989214657\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  5.729282941646918 acc:  0.20536810843400397\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  5.427889566935465 acc:  0.22796596922939508\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  5.121976852416992 acc:  0.2472590045329701\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  4.927328625125085 acc:  0.26326954424670074\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  4.703776222503114 acc:  0.2795033829801487\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  4.56498358064069 acc:  0.2927896746533283\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  4.354734924738993 acc:  0.30591965701270574\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  4.192635242096678 acc:  0.31936225799968737\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  4.055711286510536 acc:  0.3272447133957082\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  3.9450706407695475 acc:  0.3362213339883438\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  3.7964529762724917 acc:  0.3443047585021102\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  3.733836837871346 acc:  0.35294643056516983\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  3.6329609933727514 acc:  0.35761338007726146\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  3.5279890748555074 acc:  0.3641337114530067\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  3.523602121604417 acc:  0.369827836455798\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  3.4238555517025335 acc:  0.37185985753522544\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  23 loss :  3.3726726691879914 acc:  0.3731549918495858\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  24 loss :  3.416445836335599 acc:  0.3742714869481723\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  25 loss :  3.3967370872725984 acc:  0.37498604381126766\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  26 loss :  3.3678757744634935 acc:  0.37637049773351494\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  27 loss :  3.3644045641322338 acc:  0.377442333028158\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  28 loss :  3.356224248509207 acc:  0.3788267869504053\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  29 loss :  3.312804153579438 acc:  0.379184065381953\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  30 loss :  3.2771127038373207 acc:  0.38056851930420027\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  31 loss :  3.3225142327611317 acc:  0.3807024987160306\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  32 loss :  3.2790501196227386 acc:  0.3817743340106737\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  33 loss :  3.2913618073491993 acc:  0.38239957126588214\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  34 loss :  3.2469799818392997 acc:  0.3826452001875712\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  35 loss :  3.236793947790911 acc:  0.38394033450193155\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  36 loss :  3.2442618173039603 acc:  0.38534711832615054\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  37 loss :  3.225149286007453 acc:  0.38644128352276536\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  38 loss :  3.2528988130078345 acc:  0.3866199227385392\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  39 loss :  3.181352112821476 acc:  0.3873568095036063\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  40 loss :  3.212723559248233 acc:  0.3883839849943059\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  41 loss :  3.1380892028351743 acc:  0.3893218408771186\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  42 loss :  3.2060028521600596 acc:  0.3901033874461291\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  43 loss :  3.1609732119623057 acc:  0.3912198825447156\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  44 loss :  3.1602057811028943 acc:  0.3916218207802068\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  45 loss :  3.1083764216143215 acc:  0.3916218207802068\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  46 loss :  3.1509039287795564 acc:  0.3916664805841502\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  47 loss :  3.1456712555742548 acc:  0.3918674497018958\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  48 loss :  3.1058450516112552 acc:  0.3920907487216131\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  49 loss :  3.1446643832201016 acc:  0.391934439407811\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  50 loss :  3.1619079926770604 acc:  0.3922470580354152\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9999884604343798 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'RReLU', 'batch_size': 64, 'concatenate_features': False, 'd_model': 1344, 'dropout': 0.44031019592152043, 'dropout_StationIdEmbedding': 0.13560210590149924, 'dropout_timeStampEmbedding': 0.18474624694175346, 'dropout_transformers': 0.25764684879082145, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 61, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 7, 'factor': 0.23634776462298565, 'patience': 3, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.024117131735883056, 'dropout_lstm': 0.9999884604343798, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8547843145683762, 'beta_2': 0.9647424675391205, 'eps': 1.3947327261047364e-07, 'lr': 3.807521477669775e-07, 'optimizer': 'AdamW', 'weight_decay': 2.6331477326483065e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.472260315101463 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  8.42440763886992 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  8.37315059875275 acc:  0.0004912578433780676\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  8.31939817308546 acc:  0.0005805774512649889\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  8.287404107047127 acc:  0.000714556863095371\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  8.252193630992116 acc:  0.0007368867650671013\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  8.240209652827335 acc:  0.0007592166670388317\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  8.233344718292877 acc:  0.0007592166670388317\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  8.225445480613441 acc:  0.0007592166670388317\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  8.218463997740846 acc:  0.000781546569010562\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  8.197439353782814 acc:  0.0008038764709822924\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  8.200930201923931 acc:  0.0008038764709822924\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  8.175795214993137 acc:  0.0008708661768974834\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  8.170217123898594 acc:  0.0008931960788692137\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  8.16491758906758 acc:  0.0008931960788692137\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  8.153946202951712 acc:  0.0009378558828126744\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  8.145949560445505 acc:  0.0009601857847844048\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  8.13765832260772 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  8.144528809127275 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  19 loss :  8.142099260450243 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  20 loss :  8.137890759047929 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  21 loss :  8.134763754331148 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  22 loss :  8.133424322088281 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Hardswish', 'batch_size': 16, 'concatenate_features': True, 'd_model': 1392, 'dropout': 0.26994384071160216, 'dropout_StationIdEmbedding': 0.34490922319938555, 'dropout_timeStampEmbedding': 0.025699444898742818, 'dropout_transformers': 0.09594281093674678, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 56, 'input_size': 2, 'learnable_pos_encoding': False, 'T_max': 7, 'eta_min': 0.00011531436667335876, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8690907919429511, 'beta_2': 0.9718100178402834, 'eps': 2.129778984661896e-07, 'lr': 7.804132949827923e-05, 'optimizer': 'AdamW', 'weight_decay': 0.0002913967594942029, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Mish', 'dropout_gcn': 0.8694470040881741, 'hidden_channels': 256, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 7, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  7.592907971098223 acc:  0.04662483531697296\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  6.780105325101896 acc:  0.1473326932094768\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  2 loss :  5.864943509793464 acc:  0.20161668490275328\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  3 loss :  5.115409896573947 acc:  0.24326195208003037\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  4 loss :  4.6627985848725295 acc:  0.2665966996404886\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  5 loss :  4.347649661639265 acc:  0.29852845946006296\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  6 loss :  4.1242246245609895 acc:  0.3056293682870732\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  7 loss :  3.900954532259293 acc:  0.3282718888864078\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  8 loss :  3.8060125121633517 acc:  0.3403523658531139\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  9 loss :  3.621855160662236 acc:  0.33907956144072526\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  10 loss :  3.42607288142197 acc:  0.3574794006654311\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  11 loss :  3.3720174509150382 acc:  0.3664560212580667\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  12 loss :  3.322783501093624 acc:  0.3825112207757408\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  13 loss :  3.2906171583947335 acc:  0.3903043565638747\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  14 loss :  3.0715025836274825 acc:  0.3875354487193801\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  15 loss :  3.207074724990903 acc:  0.39077328450528104\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  16 loss :  3.0732414167345934 acc:  0.3970926467632807\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  17 loss :  3.0864451786943974 acc:  0.3953509144094857\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  18 loss :  3.048430757668182 acc:  0.3901033874461291\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8860686225732174 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Tanhshrink', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1152, 'dropout': 0.06670883122058396, 'dropout_StationIdEmbedding': 0.23220397307259277, 'dropout_timeStampEmbedding': 0.5666722094324743, 'dropout_transformers': 0.6111331274044083, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 70, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.8860686225732174, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardtanh', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 2, 'lr': 3.734533104079965e-05, 'momentum': 0.4964813497282795, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 1.2190803582560416e-06, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  0 loss :  8.070689743864323 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m epoch:  1 loss :  8.069126466077245 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.42729860920615326 and num_layers=1\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=183394)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'Tanh', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1272, 'dropout': 0.5230184003523776, 'dropout_StationIdEmbedding': 0.08866550197650655, 'dropout_timeStampEmbedding': 0.13104041522686613, 'dropout_transformers': 0.0382403423304013, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 67, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 0.03014888867201773, 'max_lr': 0.35456745319479893, 'mode': 'triangular2', 'scheduler': 'CyclicLR', 'step_size_up': 12, 'dropout_lstm': 0.42729860920615326, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 3, 'alpha': 0.9216496905883678, 'centered': False, 'eps': 5.0370463085408695e-09, 'lr': 0.12927564949871456, 'momentum': 0.24688802877421226, 'optimizer': 'RMSprop', 'weight_decay': 1.1819073094071588e-07, 'positive_function': 'exp', 'epochs_complete_problem': 50, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=183394)\u001b[0m loss is undifined\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-07 06:14:49,636\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-07 06:15:04,347\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-07 06:15:04,348\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_4        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_4\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_4`\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.48496243866799893 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'LeakyReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 264, 'dropout': 0.18921447977489503, 'dropout_StationIdEmbedding': 0.8783451120581629, 'dropout_timeStampEmbedding': 0.4411108462953692, 'dropout_transformers': 0.4326701753045321, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 48, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.48496243866799893, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8027430683522431, 'beta_2': 0.9689786117017483, 'eps': 4.353000287425135e-08, 'lr': 1.751970616992533e-06, 'optimizer': 'AdamW', 'weight_decay': 6.286975977639781e-07, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.512047232868515 acc:  0.00037960833351941584\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.490784858988825 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  8.46561635988895 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  8.450360619019126 acc:  0.00042426813746287653\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  8.425761704132936 acc:  0.0004465980394346069\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  8.408574968854957 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  8.387044175762997 acc:  0.0005805774512649889\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  8.36542661613393 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  8.348945029428073 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  8.328465060652974 acc:  0.0008485362749257531\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  8.308605104963355 acc:  0.0009601857847844048\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  8.292262496235214 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  8.27447828845443 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  8.255166882666472 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  8.225517174907933 acc:  0.0012281446084451688\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  8.214366725672072 acc:  0.0012951343143603599\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  8.197309556408463 acc:  0.0014737735301342026\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  8.181826029982522 acc:  0.001630082843936315\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  8.164688743163492 acc:  0.0016524127459080454\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  8.150320320485909 acc:  0.0016970725498515061\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  8.13969636186261 acc:  0.0018310519616818882\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  8.119869633255718 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  8.102746214822075 acc:  0.0018533818636536185\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  8.090905635156364 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  8.079669667181568 acc:  0.0019873612754840006\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  8.06558785928744 acc:  0.002009691177455731\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  8.047477980640448 acc:  0.0021213406873143827\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  8.034938821168703 acc:  0.002143670589286113\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  28 loss :  8.018942209047692 acc:  0.0021660004912578434\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  29 loss :  8.015428596567885 acc:  0.002277650001116495\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  30 loss :  7.984750533772406 acc:  0.0024562892168903377\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  31 loss :  7.9859827612047996 acc:  0.002523278922805529\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  32 loss :  7.9782242151064295 acc:  0.002545608824777259\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  33 loss :  7.960391463520371 acc:  0.002545608824777259\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  34 loss :  7.948203171525046 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  35 loss :  7.937493172761436 acc:  0.002679588236607641\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  36 loss :  7.935162874025719 acc:  0.002724248040551102\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  37 loss :  7.908718603793706 acc:  0.002791237746466293\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  38 loss :  7.907359399528147 acc:  0.0028358975504097538\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  39 loss :  7.9032672543392 acc:  0.0029028872563249446\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  40 loss :  7.887448845622695 acc:  0.0029475470602684053\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  41 loss :  7.871259542269128 acc:  0.002925217158296675\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  42 loss :  7.858081510133832 acc:  0.0030368666681553267\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  43 loss :  7.850083257550391 acc:  0.0030591965701270572\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  44 loss :  7.848430058666479 acc:  0.0030145367661835966\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  45 loss :  7.840044057257821 acc:  0.002969876962240136\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9507139578736439 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'ReLU', 'batch_size': 64, 'concatenate_features': False, 'd_model': 1224, 'dropout': 0.5831576399529182, 'dropout_StationIdEmbedding': 0.2990453422552868, 'dropout_timeStampEmbedding': 0.271409010561531, 'dropout_transformers': 0.6896369677433796, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 77, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.19222104919759714, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.9507139578736439, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.9835849021157268, 'beta_2': 0.963711335805969, 'eps': 7.199556483162474e-08, 'lr': 6.9435874884811615e-06, 'optimizer': 'AdamW', 'weight_decay': 6.642973029331114e-08, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.071886517924646 acc:  0.0009825156867561352\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.002387339068997 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.985435538138113 acc:  0.0012504745104168992\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  7.983993951735958 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  7.983418224703881 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  7.981033417486375 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  7.983908133352957 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'Hardshrink', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1080, 'dropout': 0.01818215807187984, 'dropout_StationIdEmbedding': 0.11732043996121594, 'dropout_timeStampEmbedding': 0.34546239200053797, 'dropout_transformers': 0.311123427646334, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 74, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8915785285274265, 'beta_2': 0.9989488772635665, 'eps': 2.385510423325791e-08, 'lr': 0.00021897164080009272, 'optimizer': 'AdamW', 'weight_decay': 4.926066601088997e-05, 'positive_function': 'abs', 'epochs_complete_problem': 0, 'reg': True, 'transformers_model': True, 'activation_gcn': 'ReLU6', 'dropout_gcn': 0.1302383445414188, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'LayerNorm', 'num_layers_gcn': 2, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  7.466196364715319 acc:  0.03637541031194873\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  6.822341834797578 acc:  0.10941651966147868\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  6.1197423173599885 acc:  0.1715383069468325\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  5.3092862457788295 acc:  0.22088739030435656\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  4.752447857576258 acc:  0.2577763883616551\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  4.408903973443167 acc:  0.29234307661389364\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  4.058050924990358 acc:  0.3242971663354398\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  3.8425048499548136 acc:  0.34664939820914187\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  3.648299858349712 acc:  0.3565192148806467\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  3.4488252190982593 acc:  0.3698501663577697\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  3.3930744844324447 acc:  0.37578992028224995\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  3.2546352518706763 acc:  0.3824442310698256\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  3.1845424896528742 acc:  0.39213540852555656\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  3.137018842857425 acc:  0.40006252372552087\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  3.015986136027745 acc:  0.40542170019873613\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  3.0640053288275455 acc:  0.4033450193153652\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  2.921164931369429 acc:  0.41256726882968986\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  2.9203695469543716 acc:  0.4127682379474354\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  2.9024412451671955 acc:  0.41493423843869326\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  2.879628720403719 acc:  0.41803809481276377\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  2.8584926789548217 acc:  0.41980215706853047\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  2.7811884168817214 acc:  0.4222361163834491\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  2.777144235723159 acc:  0.4239555188352723\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  2.7581377229770694 acc:  0.42214679677556216\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  2.7296639029719247 acc:  0.42105263157894735\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  2.766214597125013 acc:  0.42310698256034657\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  2.6982378869497476 acc:  0.42779626197440995\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  2.7056620932426774 acc:  0.4273719938369471\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  28 loss :  2.652778600444313 acc:  0.4285108188375053\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  29 loss :  2.674916804337702 acc:  0.43114574727016947\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  30 loss :  2.6433039502937254 acc:  0.428287519817788\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  31 loss :  2.595924302309501 acc:  0.42906906638679854\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  32 loss :  2.585906369345529 acc:  0.42784092177835337\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  33 loss :  2.5894976153093228 acc:  0.42877877766116607\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7719569484872553 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Sigmoid', 'activation_transformers': 'Hardtanh', 'batch_size': 64, 'concatenate_features': True, 'd_model': 984, 'dropout': 0.29896751712271796, 'dropout_StationIdEmbedding': 0.3931028815211049, 'dropout_timeStampEmbedding': 0.6232814553531789, 'dropout_transformers': 0.4870216677368089, 'early_stopping': 1, 'encoder_only': False, 'epochs_classifcation_only': 80, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7719569484872553, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8798492922675848, 'beta_2': 0.9811305851691509, 'eps': 1.2869855665917602e-08, 'lr': 4.774731308457301e-05, 'optimizer': 'AdamW', 'weight_decay': 0.0015817620111417982, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  7.500497995246767 acc:  0.007301877944755822\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  7.2017967838267385 acc:  0.021704664716521896\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.103982116539441 acc:  0.045843288747962396\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  6.992234162635204 acc:  0.05859366277382042\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  6.903273073166453 acc:  0.07004890248531809\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  6.7911562520171955 acc:  0.07281781032981265\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  6.666176526334273 acc:  0.08518857602215127\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  6.546968881996515 acc:  0.0950807225956278\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  6.440686865002697 acc:  0.10407967309023514\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  6.324582025018662 acc:  0.11196212848625595\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  6.174012928108894 acc:  0.12241252260902574\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  6.036722015960054 acc:  0.1299600294754706\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  5.870483043930293 acc:  0.1427550633052721\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  5.724061446664221 acc:  0.15302681821226805\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  5.610616316970106 acc:  0.16265100596208382\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  5.4720680626275024 acc:  0.17444119420315746\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  5.3689366110956485 acc:  0.18158676283411115\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  5.245611877341545 acc:  0.19313132215349574\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  5.13596062385599 acc:  0.20235357166782036\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  4.974525921007726 acc:  0.21101757363285176\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  4.856901538309627 acc:  0.21992720451957215\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  4.793773591206336 acc:  0.22622423687560012\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  4.6663573437336225 acc:  0.2338610633499319\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  4.5911414485951365 acc:  0.24248040551101982\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  4.493648021008956 acc:  0.24830850992564144\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  4.402573895079927 acc:  0.2540249648304044\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  4.3243581774347115 acc:  0.25699484179264453\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  4.269374485415314 acc:  0.26378313199205056\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  28 loss :  4.198275459998565 acc:  0.2701471540539937\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  29 loss :  4.112214616455957 acc:  0.27689078444945625\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  30 loss :  4.058264610030888 acc:  0.2824062702364737\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  31 loss :  3.99011934739757 acc:  0.28705088984659355\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  32 loss :  3.950845373862701 acc:  0.29341491190853675\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  33 loss :  3.8925659743903194 acc:  0.2972556550476743\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  34 loss :  3.8245886395739013 acc:  0.30085076926512294\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  35 loss :  3.7731105494873685 acc:  0.30359734720764575\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  36 loss :  3.729328865780256 acc:  0.3116807717214121\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  37 loss :  3.6691482915928226 acc:  0.3144050197619632\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  38 loss :  3.645101207713182 acc:  0.3192506084898287\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  39 loss :  3.600764113571007 acc:  0.325525310943885\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  40 loss :  3.549146616022 acc:  0.3265748163365563\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  41 loss :  3.513623730674464 acc:  0.33186700310385636\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  42 loss :  3.4862686676504726 acc:  0.3327601991827256\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  43 loss :  3.4434895665233674 acc:  0.3387892727150928\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  44 loss :  3.4177344282260114 acc:  0.34104459281423755\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  45 loss :  3.399569181871664 acc:  0.3438804903646473\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  46 loss :  3.3377289759551045 acc:  0.3485251099747672\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  47 loss :  3.326367574212439 acc:  0.3494629658575799\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  48 loss :  3.270780361135593 acc:  0.35243284281982\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  49 loss :  3.2556050035966004 acc:  0.35709979233191164\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  50 loss :  3.2474080355379593 acc:  0.3625706183149856\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  51 loss :  3.2185511776290014 acc:  0.36274925753075943\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  52 loss :  3.2005568224722176 acc:  0.36714824821919034\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  53 loss :  3.159863800278509 acc:  0.3678181452783422\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  54 loss :  3.1553674056267864 acc:  0.36911327959270257\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  55 loss :  3.1184641016715484 acc:  0.37121229037804526\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  56 loss :  3.1101379481909786 acc:  0.3746510952816917\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  57 loss :  3.0741608080439544 acc:  0.37538798204675883\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  58 loss :  3.055022343291038 acc:  0.37925105508786816\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  59 loss :  3.043424135727408 acc:  0.37929571489181163\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  60 loss :  3.0440088042414 acc:  0.3809704575396914\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  61 loss :  2.9965493654081334 acc:  0.38570439675769824\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  62 loss :  3.008272594182279 acc:  0.38740146930754976\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  63 loss :  2.9803476795476143 acc:  0.39072862470133757\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  64 loss :  2.9727589222773205 acc:  0.3903266864658464\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7105133455623256 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'PReLU', 'activation_transformers': 'Mish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1416, 'dropout': 0.3709266463526089, 'dropout_StationIdEmbedding': 0.5398274661428972, 'dropout_timeStampEmbedding': 0.3955817697346448, 'dropout_transformers': 0.3637237029431211, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 63, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 3, 'factor': 0.897258964909519, 'patience': 6, 'scheduler': 'ReduceLROnPlateau', 'threshold': 8.080584549844031e-06, 'dropout_lstm': 0.7105133455623256, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Tanhshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 4, 'lr': 0.00039835427073885897, 'momentum': 0.05340950004697964, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.06994766340182877, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.337263000594985 acc:  0.00037960833351941584\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.307118555882594 acc:  0.00042426813746287653\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  8.274600929313607 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  8.24173864618048 acc:  0.0005359176473215282\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  8.20883445472984 acc:  0.000714556863095371\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  8.175687136349978 acc:  0.0008485362749257531\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  8.152125198524315 acc:  0.001027175490699596\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  8.112987164850836 acc:  0.0010718352946430564\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  8.087325302870957 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  8.060667251373504 acc:  0.0015407632360493937\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  8.032926292686195 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  8.010881030476178 acc:  0.002143670589286113\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  7.9853972955183545 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  7.956760299789322 acc:  0.0030368666681553267\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  7.933464727201662 acc:  0.0036621039233637764\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  7.911459946132206 acc:  0.004086372060826653\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  7.8898544978428555 acc:  0.00453297010026126\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  7.862157728288557 acc:  0.004890248531808946\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  7.838482283212088 acc:  0.005225197061384901\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  7.823553728890586 acc:  0.005716454904762968\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  7.801479046161358 acc:  0.006140723042225845\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  7.780130996570721 acc:  0.006319362257999687\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  7.7684126267066365 acc:  0.006564991179688721\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  7.744041236130507 acc:  0.006721300493490834\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  7.74301941078026 acc:  0.006810620101377755\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  7.716902165979772 acc:  0.006899939709264676\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  7.69566145810214 acc:  0.007011589219123328\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  7.688194494981032 acc:  0.0071678985329254406\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  28 loss :  7.678085317144861 acc:  0.007368867650671014\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  29 loss :  7.6617501032102355 acc:  0.007413527454614474\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  30 loss :  7.653081440425419 acc:  0.007391197552642744\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  31 loss :  7.63649312266103 acc:  0.007458187258557935\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  32 loss :  7.619432552711113 acc:  0.007346537748699283\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  33 loss :  7.614075760741334 acc:  0.0075028470625013955\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  34 loss :  7.602969209631007 acc:  0.007659156376303508\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  35 loss :  7.6026564478040575 acc:  0.007681486278275238\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  36 loss :  7.57018719186316 acc:  0.0077261460822186994\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  37 loss :  7.578632137992165 acc:  0.007949445101936002\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  38 loss :  7.560411813375834 acc:  0.007927115199964273\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  39 loss :  7.5481946151573345 acc:  0.008105754415738116\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  40 loss :  7.547400754648489 acc:  0.008239733827568497\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  41 loss :  7.542301888232465 acc:  0.00850769265122926\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  42 loss :  7.53143107474267 acc:  0.008597012259116183\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  43 loss :  7.520526022344202 acc:  0.008775651474890026\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  44 loss :  7.51599774660764 acc:  0.008753321572918294\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  45 loss :  7.507624319383314 acc:  0.008864971082776946\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  46 loss :  7.514554904057429 acc:  0.008931960788692137\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  47 loss :  7.489987946890451 acc:  0.009132929906437711\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  48 loss :  7.475537990356659 acc:  0.009199919612352902\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  49 loss :  7.482407206421965 acc:  0.009356228926155015\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  50 loss :  7.472871813740763 acc:  0.009512538239957126\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  51 loss :  7.468307851911424 acc:  0.009847486769533082\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  52 loss :  7.4637347501474665 acc:  0.009802826965589621\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  53 loss :  7.46223775156728 acc:  0.009892146573476543\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  54 loss :  7.4604026087514175 acc:  0.010048455887278655\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  55 loss :  7.461336736078863 acc:  0.010115445593193845\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  56 loss :  7.464146207262586 acc:  0.010539713730656722\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  57 loss :  7.425332702956833 acc:  0.010807672554317487\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  58 loss :  7.438827841431944 acc:  0.010696023044458835\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  59 loss :  7.449637416359428 acc:  0.010785342652345755\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  60 loss :  7.428875713081626 acc:  0.010896992162204407\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  61 loss :  7.442656947182608 acc:  0.01103097157403479\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  62 loss :  7.417001207391698 acc:  0.01103097157403479\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3566787514886083 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Softplus', 'batch_size': 32, 'concatenate_features': True, 'd_model': 912, 'dropout': 0.1543828633140444, 'dropout_StationIdEmbedding': 0.1915331084813851, 'dropout_timeStampEmbedding': 0.21685185271604293, 'dropout_transformers': 0.15329681094697212, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 20, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.6773168859133127, 'scheduler': 'StepLR', 'step_size': 3, 'dropout_lstm': 0.3566787514886083, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 1, 'alpha': 0.9429014819244373, 'centered': False, 'eps': 5.4517615313317995e-08, 'lr': 2.3035100635708954e-06, 'momentum': 9.186930080154476e-05, 'optimizer': 'RMSprop', 'weight_decay': 0.010984999828302106, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.040915456436972 acc:  0.002657258334635911\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.000836376015467 acc:  0.0027689078444945625\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.96505323439154 acc:  0.002858227452381484\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  7.93165348140338 acc:  0.002858227452381484\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  7.907843961060502 acc:  0.002858227452381484\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  7.890013705683119 acc:  0.002925217158296675\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  7.867895421181016 acc:  0.0028805573543532145\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  7.8542284856315785 acc:  0.002925217158296675\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'SiLU', 'activation_transformers': 'Hardsigmoid', 'batch_size': 128, 'concatenate_features': True, 'd_model': 120, 'dropout': 0.26181015714226535, 'dropout_StationIdEmbedding': 0.24841424792751132, 'dropout_timeStampEmbedding': 0.081298679338424, 'dropout_transformers': 0.26875295260934684, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 59, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 30, 'eta_min': 0.00600322394822414, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.5663061603452897, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8121711918173192, 'beta_2': 0.9869372623089461, 'eps': 4.997066548310436e-07, 'lr': 0.0014979404787010763, 'optimizer': 'AdamW', 'weight_decay': 8.1472503649447e-06, 'positive_function': 'exp', 'epochs_complete_problem': 6, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5663061603452897 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  7.259811984575712 acc:  0.05310050688877476\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  6.188090698535626 acc:  0.16238304713842305\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  5.191818420703594 acc:  0.22055244177478062\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  4.436965441703796 acc:  0.26385012169796573\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  3.928424211648794 acc:  0.30382064622736304\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  3.590215800358699 acc:  0.3307951678092133\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  3.346536452953632 acc:  0.34843579036688027\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  3.1650021663078896 acc:  0.362124020275551\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  3.040695260121272 acc:  0.3763928276354867\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  2.933615726691026 acc:  0.38257821048165597\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  2.8478934856561513 acc:  0.3907509546033093\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  2.779959805195148 acc:  0.3939441305852667\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  2.712554847277128 acc:  0.40012951343143605\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  2.6636774613307073 acc:  0.4035013286291673\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  2.62176639850323 acc:  0.40926244333787376\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  2.582916853978084 acc:  0.4088158452984391\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'Softshrink', 'batch_size': 128, 'concatenate_features': False, 'd_model': 384, 'dropout': 0.41470194870064825, 'dropout_StationIdEmbedding': 0.4746244819494172, 'dropout_timeStampEmbedding': 0.7207617239408708, 'dropout_transformers': 0.5259967874782697, 'early_stopping': 7, 'encoder_only': True, 'epochs_classifcation_only': 72, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.9981087774916896, 'beta_2': 0.9964612903860228, 'eps': 3.3245299374898253e-09, 'lr': 0.0006324927249037488, 'optimizer': 'AdamW', 'weight_decay': 0.00012980940840553882, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'ReLU', 'dropout_gcn': 0.679969928167736, 'hidden_channels': 64, 'layer_type': 'GraphSAGE', 'norm': 'InstanceNorm', 'num_layers_gcn': 4, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  7.476247345108583 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  7.2142416482948395 acc:  0.0201639014804725\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.097275958003768 acc:  0.029966728446062123\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  6.962122262242329 acc:  0.037916173547998124\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  6.863709202731949 acc:  0.04861219659245696\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  6.700064498257924 acc:  0.06149655003014537\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  6.525685844651187 acc:  0.07205859366277383\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  6.409473108958049 acc:  0.08389344170779091\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  6.26287368981235 acc:  0.09284773239845477\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  6.12891983124147 acc:  0.10117678583391018\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  6.0191734095653855 acc:  0.11238639662371883\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  5.895706981061453 acc:  0.1251144407476051\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  5.806999057172293 acc:  0.12748141035660854\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  5.727470196873309 acc:  0.1404104235982404\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  5.628140966576266 acc:  0.14349195007033919\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  5.540899305458528 acc:  0.14554630105173838\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  5.461383584034012 acc:  0.1538083647812786\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  5.377164116824966 acc:  0.15847531429337025\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  5.27212880031172 acc:  0.1645490476296809\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  5.235567483557276 acc:  0.1685907598865641\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  5.158604512731713 acc:  0.17421789518344014\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  5.061263785304793 acc:  0.17718777214568027\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  4.971537187875035 acc:  0.1755130294978005\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  4.940033079629921 acc:  0.18366344371748208\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  4.894394127719374 acc:  0.18513721724761628\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  4.819597295967929 acc:  0.19063037313266196\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  4.758268735494958 acc:  0.19054105352477502\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  4.732884510453925 acc:  0.19715070450840722\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  28 loss :  4.671891545674887 acc:  0.1957439206841882\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  29 loss :  4.611979317952351 acc:  0.1961235290177076\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  30 loss :  4.571298875004413 acc:  0.1988477770582587\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  31 loss :  4.505983769175518 acc:  0.20547975794386264\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  32 loss :  4.465643204838397 acc:  0.20427394323738918\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  33 loss :  4.396932645016406 acc:  0.2086282741218766\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  34 loss :  4.391899870102664 acc:  0.2108389344170779\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  35 loss :  4.339478992554079 acc:  0.2089855525534243\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  36 loss :  4.286973941757019 acc:  0.21336221333988345\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  37 loss :  4.244601588651358 acc:  0.2122233883393252\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  38 loss :  4.228582715413657 acc:  0.2159971417725476\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  39 loss :  4.172519675220352 acc:  0.21769421432239913\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  40 loss :  4.157536555485553 acc:  0.21713596677310587\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  41 loss :  4.110434693026256 acc:  0.2180514927539468\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  42 loss :  4.089790981936168 acc:  0.2203961324609785\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  43 loss :  4.0328944935856095 acc:  0.21986021481365697\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  44 loss :  4.026536927165755 acc:  0.22640287609137397\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  45 loss :  3.97712147379496 acc:  0.2246834736395507\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  46 loss :  3.9683544291071144 acc:  0.22472813344349418\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  47 loss :  3.9099974833339095 acc:  0.23069021726994618\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  48 loss :  3.9127399375639764 acc:  0.2290824643279816\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  49 loss :  3.876062005399221 acc:  0.2308465265837483\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  50 loss :  3.8400946065603967 acc:  0.2248174530513811\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  51 loss :  3.850069738296141 acc:  0.22562132952236338\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  52 loss :  3.796581314270755 acc:  0.2312931246231829\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  53 loss :  3.7951438628047347 acc:  0.23185137217247617\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  54 loss :  3.764015068490821 acc:  0.2320076814862783\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  55 loss :  3.739170720778316 acc:  0.2336377643302146\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  56 loss :  3.737209012709468 acc:  0.23609405354710492\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  57 loss :  3.709188800260245 acc:  0.23582609472344415\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  58 loss :  3.6779387428099852 acc:  0.23138244423106982\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  59 loss :  3.644451345305845 acc:  0.2315834133488154\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  60 loss :  3.6585967339665055 acc:  0.23301252707500614\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  61 loss :  3.6109675154628524 acc:  0.23397271285979054\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  62 loss :  3.6157804454665587 acc:  0.23513386776232054\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  63 loss :  3.6149936440479324 acc:  0.2355581358997834\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  64 loss :  3.5999088086277604 acc:  0.23712122903780453\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  65 loss :  3.5825369472963264 acc:  0.23450863050711207\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  66 loss :  3.561262848865555 acc:  0.23455329031105554\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  67 loss :  3.5389901356524733 acc:  0.2371882187437197\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  68 loss :  3.516667664769184 acc:  0.23732219815555008\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  69 loss :  3.5160003943615648 acc:  0.2392425697251189\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  70 loss :  3.496018605059888 acc:  0.23707656923386106\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  71 loss :  3.4804182483489257 acc:  0.23774646629301296\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3936608039544467 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Tanhshrink', 'activation_transformers': 'Sigmoid', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1248, 'dropout': 0.3378364401700178, 'dropout_StationIdEmbedding': 0.6271969873068627, 'dropout_timeStampEmbedding': 0.2435602882649876, 'dropout_transformers': 0.571146930296943, 'early_stopping': 4, 'encoder_only': False, 'epochs_classifcation_only': 67, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 1.0212463224330208e-05, 'max_lr': 0.20911668943063563, 'mode': 'triangular', 'scheduler': 'CyclicLR', 'step_size_up': 2, 'dropout_lstm': 0.3936608039544467, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 6, 'amsgrad': False, 'beta_1': 0.8447917293198515, 'beta_2': 0.9595394544281279, 'eps': 3.536604334770625e-08, 'lr': 1.4432643295325132e-05, 'optimizer': 'AdamW', 'weight_decay': 1.525851714934616e-06, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.135195872362923 acc:  0.003126186276042248\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5147370506522114 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'Softmin', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1104, 'dropout': 0.22555105167793071, 'dropout_StationIdEmbedding': 0.057793441448606354, 'dropout_timeStampEmbedding': 0.11579851747214598, 'dropout_transformers': 0.19413105209070972, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 53, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.5147370506522114, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 24, 'num_layers_transformer': 3, 'alpha': 0.9787207652151018, 'centered': True, 'eps': 2.5263767063959107e-07, 'lr': 3.394482902763441e-05, 'momentum': 0.33355430267137515, 'optimizer': 'RMSprop', 'weight_decay': 1.036799336866004e-06, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  7.015030369391808 acc:  0.1607083044905433\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  5.287693698589618 acc:  0.27023647366188064\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  4.328094405394334 acc:  0.3299019717303441\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  3.7455423538501447 acc:  0.3635977938056852\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  3.3674745853130634 acc:  0.38514614920840495\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  3.1079805465844963 acc:  0.40035281245115334\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  2.923987399614774 acc:  0.4103789384364603\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  2.7815703226969792 acc:  0.4176361565772726\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  2.6703746758974516 acc:  0.4207846727552866\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  2.5818268262423003 acc:  0.4249603644240002\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  2.4971510520348184 acc:  0.4279302413862403\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  2.4329840605075543 acc:  0.43114574727016947\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  2.370580394451435 acc:  0.4325525310943885\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  2.317272265140827 acc:  0.4317486546234062\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  2.2673883511469914 acc:  0.43346805707522945\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  2.218391749492058 acc:  0.4340263046245227\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  2.1783484844061043 acc:  0.4344059129580421\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  2.137234467726487 acc:  0.4343165933501552\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  2.098140770655412 acc:  0.4343612531540987\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  2.0622311372023363 acc:  0.4355893977625438\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  2.0283646684426526 acc:  0.4349864904093071\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  1.99365280683224 acc:  0.43391465511466404\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  1.9632729814602778 acc:  0.43413795413438133\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  1.9321171127832852 acc:  0.4324185516825581\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  1.9032197209504935 acc:  0.43290980952593616\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.456481359098961 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Softshrink', 'activation_transformers': 'ReLU6', 'batch_size': 16, 'concatenate_features': True, 'd_model': 1176, 'dropout': 0.5070853730140505, 'dropout_StationIdEmbedding': 0.16587801342984249, 'dropout_timeStampEmbedding': 0.29205325348036293, 'dropout_transformers': 0.12497726841988827, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 78, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.7103881081913198, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.456481359098961, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.935144399029113, 'beta_2': 0.9656618270983361, 'eps': 4.8574369406385944e-08, 'lr': 0.00010406861423571612, 'optimizer': 'AdamW', 'weight_decay': 1.6717006529001255e-07, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  7.844898043180767 acc:  0.015764910792041623\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  7.288737668489155 acc:  0.04530737110064087\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  6.967266584697523 acc:  0.0794721211173883\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  6.74008960723877 acc:  0.1091262309358462\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  6.430073110680831 acc:  0.13058526673067905\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  6.369275886134098 acc:  0.14061139271598597\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  6.21610285608392 acc:  0.1515307147801621\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  5.938185556311357 acc:  0.15936851037223945\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  6.028225175957931 acc:  0.1635888618448965\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  5.957670570674695 acc:  0.16602282115981512\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  5.993946040304084 acc:  0.16854610008262064\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  5.886453513095254 acc:  0.16995288390683966\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  5.789887789676064 acc:  0.17017618292655695\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  5.785146479857596 acc:  0.17091306969162406\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  5.8208005102057205 acc:  0.17198490498626712\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  5.955370054746929 acc:  0.17227519371189962\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  5.967831410859761 acc:  0.17269946184936247\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  5.880429626766004 acc:  0.1729227608690798\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  5.9303835642965215 acc:  0.17287810106513632\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  5.943599204013222 acc:  0.17296742067302326\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  5.820853434110942 acc:  0.17314605988879708\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  5.875408473767732 acc:  0.1733023692025992\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  5.904096545671162 acc:  0.17341401871245785\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  5.891531467437744 acc:  0.17341401871245785\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  5.969725899947317 acc:  0.17343634861442958\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  5.975935589639764 acc:  0.17345867851640132\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  5.848273417824193 acc:  0.17345867851640132\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  5.825634200949418 acc:  0.17343634861442958\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'GELU', 'batch_size': 64, 'concatenate_features': False, 'd_model': 1296, 'dropout': 0.12256149644960407, 'dropout_StationIdEmbedding': 0.004789601402590812, 'dropout_timeStampEmbedding': 0.006715849422762182, 'dropout_transformers': 0.386798439479885, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 29, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 3, 'factor': 0.18525657105878968, 'patience': 1, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0009261398986921457, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 1, 'lr': 3.059789487637234e-07, 'momentum': 0.167309414056598, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 7.819027376326033e-05, 'positive_function': 'exp', 'epochs_complete_problem': 50, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.109095770553504 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.111121135051024 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  8.109406529858125 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  8.109154120503858 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6479502864819442 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'Hardswish', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1344, 'dropout': 0.19405620811699947, 'dropout_StationIdEmbedding': 0.12029553523734873, 'dropout_timeStampEmbedding': 0.33094222517985805, 'dropout_transformers': 0.7549187860890786, 'early_stopping': 1, 'encoder_only': False, 'epochs_classifcation_only': 42, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'dropout_lstm': 0.6479502864819442, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'SELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 4, 'amsgrad': False, 'beta_1': 0.9006984236608723, 'beta_2': 0.9742574372367876, 'eps': 7.950082241770404e-08, 'lr': 7.348729687445295e-06, 'optimizer': 'AdamW', 'weight_decay': 2.493938682682645e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Sigmoid', 'dropout_gcn': 0.5299446358194301, 'hidden_channels': 2048, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 9, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.143794032889353 acc:  0.0009378558828126744\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  7.98302495983285 acc:  0.001630082843936315\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.840869339419083 acc:  0.0026125985306924503\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  7.699779087389019 acc:  0.004599959806176451\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  7.561500999289499 acc:  0.005694125002791238\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  7.474786221141547 acc:  0.005091217649554518\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  7.4243793756189485 acc:  0.0057834446106781595\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  7.38256746614483 acc:  0.007190228434897171\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  7.295765312624649 acc:  0.008105754415738116\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  7.320246454695581 acc:  0.006207712748141036\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.10470707377267989 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'LeakyReLU', 'activation_transformers': 'Tanhshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1056, 'dropout': 0.6293800358023907, 'dropout_StationIdEmbedding': 0.024550087264256136, 'dropout_timeStampEmbedding': 0.5318561847851061, 'dropout_transformers': 0.8262808500245482, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 65, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.2041077143474148, 'scheduler': 'StepLR', 'step_size': 23, 'dropout_lstm': 0.10470707377267989, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 12, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.830053753345513, 'beta_2': 0.9540680567416152, 'eps': 1.3428144563166173e-07, 'lr': 6.600719555550121e-07, 'optimizer': 'AdamW', 'weight_decay': 3.1402550329808313e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.238884074871356 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.20557924417349 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  8.173183954679049 acc:  0.0006029073532367192\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  8.143801579108604 acc:  0.000781546569010562\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  8.112053401653583 acc:  0.0008708661768974834\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  8.080414948096642 acc:  0.0009155259808409441\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  8.051134571662317 acc:  0.001049505392671326\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  8.023255979097806 acc:  0.001027175490699596\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  7.997594176805936 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  7.971620163550743 acc:  0.0012951343143603599\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  7.94566090290363 acc:  0.001362124020275551\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  7.9216103113614595 acc:  0.0013844539222472813\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  7.897769165039063 acc:  0.0014514436281624723\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  7.8714996888087345 acc:  0.0014737735301342026\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  7.850752562742967 acc:  0.0015854230399928544\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  7.82670032794659 acc:  0.0016747426478797758\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  7.807738340817965 acc:  0.0018087220597101578\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  7.785339524195744 acc:  0.0019650313735122705\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  7.764625589664166 acc:  0.002076680883370922\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  7.744835343727699 acc:  0.002143670589286113\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  7.7262352576622595 acc:  0.0022553200991447648\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  7.707079175802377 acc:  0.002232990197173034\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  7.689559467022235 acc:  0.0021883303932295735\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Softmin', 'activation_transformers': 'SiLU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 624, 'dropout': 0.46235501041049154, 'dropout_StationIdEmbedding': 0.09031967887517603, 'dropout_timeStampEmbedding': 0.5952318343725602, 'dropout_transformers': 0.299104380959662, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 39, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.1643022465642971, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8798163581990404, 'beta_2': 0.9616400310192474, 'eps': 2.005068622312937e-08, 'lr': 2.6322560422675635e-06, 'optimizer': 'Adam', 'weight_decay': 0.00022895110865179532, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1643022465642971 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.021060694181003 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.020760345458985 acc:  0.0006922269611236406\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  8.020595594552846 acc:  0.000781546569010562\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  8.020387854942909 acc:  0.0008931960788692137\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  8.020163858853854 acc:  0.0009378558828126744\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  8.020004573235145 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  8.019789006159856 acc:  0.0011388250005582475\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  8.019576835632325 acc:  0.0012281446084451688\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  8.019430586007925 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  8.019220586923453 acc:  0.0012728044123886295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  8.0190230516287 acc:  0.0013174642163320902\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  8.018864881075345 acc:  0.0014067838242190116\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  8.018657823709342 acc:  0.0014737735301342026\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  8.018465746366061 acc:  0.0014737735301342026\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  8.018297422849216 acc:  0.0015184333340776633\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  8.01806403673612 acc:  0.0015407632360493937\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  8.017904582390418 acc:  0.0016747426478797758\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  8.017711866818942 acc:  0.0016970725498515061\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  8.017560305962196 acc:  0.0017194024518232365\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  8.017369571098914 acc:  0.0017863921577384275\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  8.01720308157114 acc:  0.0017640622557666972\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  8.017002413823054 acc:  0.0018757117656253489\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  8.016806448422946 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  8.016616718585675 acc:  0.002054350981399192\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  8.01646387393658 acc:  0.0021883303932295735\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  8.01628836118258 acc:  0.0021883303932295735\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  8.016127219566933 acc:  0.0022553200991447648\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  8.015915870666504 acc:  0.0022553200991447648\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  28 loss :  8.015734973320594 acc:  0.002277650001116495\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  29 loss :  8.015552219977746 acc:  0.0023223098050599556\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  30 loss :  8.01539068222046 acc:  0.002411629412946877\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  31 loss :  8.015187417543851 acc:  0.0024786191188620682\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  32 loss :  8.01503123503465 acc:  0.002523278922805529\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  33 loss :  8.014872925098127 acc:  0.0025902686287207198\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  34 loss :  8.014669234936054 acc:  0.002657258334635911\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  35 loss :  8.01450493152325 acc:  0.002679588236607641\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  36 loss :  8.01432914000291 acc:  0.0027019181385793717\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  37 loss :  8.014095115661622 acc:  0.0027689078444945625\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  38 loss :  8.013974079718956 acc:  0.0027689078444945625\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8633564207247018 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'CELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 528, 'dropout': 0.794232607505785, 'dropout_StationIdEmbedding': 0.4210913947066786, 'dropout_timeStampEmbedding': 0.17741066482794376, 'dropout_transformers': 0.22589320678744376, 'early_stopping': 9, 'encoder_only': True, 'epochs_classifcation_only': 46, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.8633564207247018, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 4, 'alpha': 0.9001533125079709, 'centered': True, 'eps': 2.2772687555266254e-06, 'lr': 0.00029935152808090805, 'momentum': 0.19280486949470169, 'optimizer': 'RMSprop', 'weight_decay': 1.9016187326978194e-05, 'positive_function': 'exp', 'epochs_complete_problem': 16, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  7.41056944499506 acc:  0.013777549516557623\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  6.801354078488929 acc:  0.09851952749927428\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  5.957427390267916 acc:  0.15610834468436682\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  5.220211020139891 acc:  0.2064846035325905\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  4.6831383370907504 acc:  0.23124846481923944\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  4.352227950764593 acc:  0.2566822231650403\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  4.029932525670417 acc:  0.2805305584708483\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  3.823168571864333 acc:  0.304736172208204\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  3.6198019402049413 acc:  0.30654489426791415\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  3.5022385142673955 acc:  0.3206573923140477\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  3.415679851425028 acc:  0.3343902820266619\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  3.31493900201031 acc:  0.3430542839916933\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  3.225737368949106 acc:  0.34700667664068957\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  3.1502468184890033 acc:  0.36047160752964297\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  14 loss :  3.093958754405797 acc:  0.36489292812004553\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  15 loss :  3.051630381111787 acc:  0.37107831096621485\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  16 loss :  2.9968772758947355 acc:  0.374048187928455\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  17 loss :  2.984667771330504 acc:  0.378022910479423\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  18 loss :  2.904477785680896 acc:  0.3803228903825112\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  19 loss :  2.8755516404303436 acc:  0.3828015095013733\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  20 loss :  2.846548312178282 acc:  0.38878592322979705\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  21 loss :  2.836874393659217 acc:  0.38746845901346494\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  22 loss :  2.7662691156440804 acc:  0.3934752026438604\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  23 loss :  2.792728929876167 acc:  0.39503829578188154\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  24 loss :  2.7517681344647276 acc:  0.39622178058638324\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  25 loss :  2.7512848533202554 acc:  0.3986780698032736\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  26 loss :  2.706367922720508 acc:  0.4034120090212804\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  27 loss :  2.6871322761072176 acc:  0.40088873009847487\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  28 loss :  2.65635585562091 acc:  0.4039479266686019\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  29 loss :  2.622455280517863 acc:  0.4065158653953509\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  30 loss :  2.5991238389059763 acc:  0.4096197217694214\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  31 loss :  2.593056068242153 acc:  0.4091954536319586\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  32 loss :  2.574626410118887 acc:  0.40845856686689147\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  33 loss :  2.568911106786995 acc:  0.41049058794631893\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  34 loss :  2.540756895163349 acc:  0.4100439899068843\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  35 loss :  2.5127753587526698 acc:  0.41475559922291944\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  36 loss :  2.4844539143214717 acc:  0.41227698010405733\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  37 loss :  2.4744649256501243 acc:  0.41198669137842486\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  38 loss :  2.489161471340144 acc:  0.4168769399102338\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  39 loss :  2.462638750254551 acc:  0.41571578500770384\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  40 loss :  2.445159142262468 acc:  0.41529151687024096\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  41 loss :  2.4225472991711627 acc:  0.41618471294911014\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  42 loss :  2.4282472880087167 acc:  0.4141526918696827\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  43 loss :  2.4372056978885257 acc:  0.4177924658910747\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  44 loss :  2.395603510820977 acc:  0.41589442422347767\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  45 loss :  2.3805800208421513 acc:  0.4193555590290959\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  46 loss :  3.5308023568625764 acc:  0.4170109193220642\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  47 loss :  3.4588596352906986 acc:  0.4148672487327781\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  48 loss :  3.417522138524278 acc:  0.4160507335372798\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  49 loss :  3.3657329706388097 acc:  0.4195565281468414\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  50 loss :  3.3520261171822234 acc:  0.41770314628318783\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  51 loss :  3.3196546652606713 acc:  0.4178371256950182\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  52 loss :  3.327566503364349 acc:  0.41765848647924436\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  53 loss :  3.277559275939086 acc:  0.41868566196994395\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  54 loss :  3.281975255948361 acc:  0.4215215595203537\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  55 loss :  3.277527548442377 acc:  0.4177924658910747\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  56 loss :  3.2543512437945212 acc:  0.4201371055981064\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  57 loss :  3.261006491206517 acc:  0.4207623428533149\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  58 loss :  3.1991812781752826 acc:  0.4201147756961347\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  59 loss :  3.2188147161608542 acc:  0.42000312618627605\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  60 loss :  3.1813519379802955 acc:  0.4207623428533149\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  61 loss :  3.166047185380882 acc:  0.42163320903021234\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'ReLU6', 'activation_transformers': 'Hardswish', 'batch_size': 32, 'concatenate_features': False, 'd_model': 192, 'dropout': 0.3790077124974481, 'dropout_StationIdEmbedding': 0.211855333048019, 'dropout_timeStampEmbedding': 0.06763651735033571, 'dropout_transformers': 0.34639986451800575, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 69, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 19, 'eta_min': 4.7733323352901595e-05, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 2, 'lr': 0.01591651318321042, 'momentum': 0.32672622245520316, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 9.558377001205394e-08, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.061816663049452 acc:  0.0009155259808409441\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  7.958005225858209 acc:  0.0038630730411093497\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.863549666697753 acc:  0.005962083826452002\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  7.744618453127046 acc:  0.005113547551526249\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  7.662357434214161 acc:  0.005314516669271822\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  7.572128293234543 acc:  0.005582475492932586\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  7.502763681571577 acc:  0.007033919121095058\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  7.428222810755895 acc:  0.006386351963914879\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  7.406939626406025 acc:  0.00911060000446598\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  7.377618118371378 acc:  0.009802826965589621\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  7.347550421453721 acc:  0.010026125985306925\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  7.344926098871498 acc:  0.0072795480427840925\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  7.338717252848535 acc:  0.007391197552642744\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  7.3197237845905665 acc:  0.007413527454614474\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3281658900891189 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'PReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 960, 'dropout': 0.0709098155711059, 'dropout_StationIdEmbedding': 0.30945856059094634, 'dropout_timeStampEmbedding': 0.9294054882340348, 'dropout_transformers': 0.012087171029021349, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 76, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 1.5884807139281368e-05, 'max_lr': 0.07091185906973357, 'mode': 'triangular', 'scheduler': 'CyclicLR', 'step_size_up': 5, 'dropout_lstm': 0.3281658900891189, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 24, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.839472343702114, 'beta_2': 0.9517423218372957, 'eps': 1.992314286269397e-07, 'lr': 3.805466860391014e-05, 'optimizer': 'AdamW', 'weight_decay': 4.559441511587964e-07, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'PReLU', 'dropout_gcn': 0.919152667349318, 'hidden_channels': 1024, 'layer_type': 'GCNConv', 'norm': 'LayerNorm', 'num_layers_gcn': 7, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.8104073483011 acc:  0.002523278922805529\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.79557939197706 acc:  0.02409396422749704\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.810966429503067 acc:  0.017930911283299468\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.28580213789945247 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'LogSigmoid', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1440, 'dropout': 0.30684583624769574, 'dropout_StationIdEmbedding': 0.2655651355688701, 'dropout_timeStampEmbedding': 0.13842146147581633, 'dropout_transformers': 0.42152062585316596, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 12, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.28580213789945247, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'ELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.9228975514913261, 'beta_2': 0.9580603442310848, 'eps': 7.153323289813479e-09, 'lr': 0.003161554982354393, 'optimizer': 'AdamW', 'weight_decay': 1.5957521151853145e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  9.729491505256066 acc:  0.0032378357859009\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  8.93027351819552 acc:  0.025098809816224907\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  8.171822177446806 acc:  0.03974722550968001\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  7.449608028852023 acc:  0.07346537748699283\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  7.07744401051448 acc:  0.0789585333720385\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  6.890328810765193 acc:  0.0861710917089074\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  6.776701277952927 acc:  0.10215930152066632\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  6.693147450227004 acc:  0.095147712301543\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  6.6483891303722675 acc:  0.10698256034656008\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  6.570549289996808 acc:  0.1122970770158319\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  6.447203947947576 acc:  0.12216689368733671\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  6.343081279901358 acc:  0.13366679320277783\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m {'activation': 'Hardshrink', 'activation_transformers': 'SELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 696, 'dropout': 0.9065393445926437, 'dropout_StationIdEmbedding': 0.055148270186013496, 'dropout_timeStampEmbedding': 0.2071029054944667, 'dropout_transformers': 0.4883602010689975, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 61, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.42046869798079084, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.6104251002465972, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8528235842538338, 'beta_2': 0.9547362483269606, 'eps': 3.5770231179550053e-08, 'lr': 0.00029221588143049513, 'optimizer': 'Adam', 'weight_decay': 0.20523053066253258, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6104251002465972 and num_layers=1\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  0 loss :  8.03023234535666 acc:  0.0913962887702923\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  1 loss :  7.705863131194556 acc:  0.14230846526583749\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  2 loss :  7.6544465938536055 acc:  0.14672978585624008\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  3 loss :  7.654393504647648 acc:  0.14789094075877007\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  4 loss :  7.6493695523558545 acc:  0.14827054909228948\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  5 loss :  7.654211388916528 acc:  0.14844918830806333\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  6 loss :  7.647995111321201 acc:  0.1483598687001764\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  7 loss :  7.636520105249741 acc:  0.148225889288346\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  8 loss :  7.645323436801173 acc:  0.14831520889623295\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  9 loss :  7.646690697229209 acc:  0.1483375387982047\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  10 loss :  7.645583469326756 acc:  0.1483598687001764\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  11 loss :  7.64380584845022 acc:  0.1483375387982047\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  12 loss :  7.639350855049967 acc:  0.1483375387982047\n",
            "\u001b[36m(eval_config pid=236102)\u001b[0m epoch:  13 loss :  7.6415513623662354 acc:  0.1483598687001764\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-07 09:24:55,526\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-07 09:25:10,292\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-07 09:25:10,293\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_5        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_5\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_5`\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6676595277172814 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'RReLU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1008, 'dropout': 0.16821195820533483, 'dropout_StationIdEmbedding': 0.14841863350246676, 'dropout_timeStampEmbedding': 0.3632658260507174, 'dropout_transformers': 0.5511763209829913, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 73, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.6676595277172814, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 24, 'num_layers_transformer': 2, 'amsgrad': True, 'beta_1': 0.9093179418325481, 'beta_2': 0.9564538859282321, 'eps': 4.289354539422935e-07, 'lr': 6.059474741974039e-05, 'optimizer': 'AdamW', 'weight_decay': 1.0038778269415936e-07, 'positive_function': 'exp', 'epochs_complete_problem': 23, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.37901013814486 acc:  0.07773038876359332\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  6.164171006129338 acc:  0.1942031574481388\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  5.360526202275203 acc:  0.244534756492419\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  4.875951084723839 acc:  0.2785208672933926\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  4.523144160784208 acc:  0.3000245628921689\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  4.254289766458364 acc:  0.3206127325101043\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  4.035704689759474 acc:  0.335462117321305\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  3.8658965807694656 acc:  0.34685036732688745\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  3.723159749691303 acc:  0.3568988232141661\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  3.6009683994146493 acc:  0.36225799968738137\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  3.5041546711554896 acc:  0.36759484625862493\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  3.4061297545066247 acc:  0.37556662126253265\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  3.332135360057537 acc:  0.37989862224504833\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  3.259919045521663 acc:  0.38646361342473706\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  3.1967013615828295 acc:  0.38858495411205146\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  3.1345741308652437 acc:  0.39220239823147174\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  3.0881219973930945 acc:  0.3950159658799098\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  3.0364886173835166 acc:  0.39765089431257394\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  2.9927958800242496 acc:  0.40035281245115334\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  2.9515833121079664 acc:  0.404684813433669\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  2.917008720911466 acc:  0.4053323805908492\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  2.881198338361887 acc:  0.40673916441506824\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  2.8475327235001786 acc:  0.40964205167139317\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  2.8152969965567958 acc:  0.41167407275082063\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  2.7833609764392557 acc:  0.41243328941785945\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  2.7582550599024844 acc:  0.4136614340263046\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  2.7333662913395806 acc:  0.4125449389277181\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  2.7082460531821617 acc:  0.4144429805953152\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  2.685627891467168 acc:  0.41801576491079206\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  2.658628401389489 acc:  0.41772547618515954\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  2.6415828503095184 acc:  0.41962351785275664\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  2.617176921551044 acc:  0.4217001987361276\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  32 loss :  2.5927831172943114 acc:  0.42156621932429716\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  33 loss :  2.5755061553074765 acc:  0.42161087912824063\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  34 loss :  2.554794909403874 acc:  0.4213652502065516\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  35 loss :  2.5378578406113843 acc:  0.4242011477569613\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  36 loss :  2.524883301441486 acc:  0.42257106491302504\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  37 loss :  2.5051547307234543 acc:  0.42522832324766097\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  38 loss :  2.487354696713961 acc:  0.42549628207132173\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  39 loss :  2.4730089939557587 acc:  0.4261661791304736\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  40 loss :  2.457288621022151 acc:  0.42647879775807784\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  41 loss :  2.442684105726389 acc:  0.42670209677779514\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  42 loss :  2.4279673062838043 acc:  0.4292923654065159\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  43 loss :  2.4134487610596875 acc:  0.42900207668088336\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  44 loss :  2.3990478680684015 acc:  0.43018556148538506\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  45 loss :  2.3854991390154914 acc:  0.4295379943282049\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  46 loss :  2.373472837301401 acc:  0.42958265413214836\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  47 loss :  2.362603492003221 acc:  0.43038653060313065\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  48 loss :  2.345123026004204 acc:  0.43002925217158294\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  49 loss :  2.3305983919363755 acc:  0.4316593350155193\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  50 loss :  2.320332171366765 acc:  0.4302525511913003\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  51 loss :  2.307474481142484 acc:  0.4313690462898868\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  52 loss :  2.296289573265956 acc:  0.429448674720318\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'ReLU', 'batch_size': 64, 'concatenate_features': False, 'd_model': 1368, 'dropout': 0.5624398422114418, 'dropout_StationIdEmbedding': 0.35466211127264585, 'dropout_timeStampEmbedding': 0.48452930264041183, 'dropout_transformers': 0.18011858070552153, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 44, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 8, 'factor': 0.43227729393732117, 'patience': 6, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.027386100625346726, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'alpha': 0.9570807031054822, 'centered': False, 'eps': 7.349415435916268e-09, 'lr': 1.4219526344956858e-06, 'momentum': 0.44916068013737803, 'optimizer': 'RMSprop', 'weight_decay': 0.0015297187478221102, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  8.00710559801291 acc:  0.0009378558828126744\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  7.922955276401899 acc:  0.0030368666681553267\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  7.825460193721392 acc:  0.004711609316035103\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  7.725067571829293 acc:  0.004934908335752406\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  7.636036516145896 acc:  0.005113547551526249\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  7.5622240750844245 acc:  0.0041310318647701134\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  7.50487728701293 acc:  0.004108701962798384\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  7.455378998326891 acc:  0.004756269119978563\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  7.423642646265394 acc:  0.004488310296317799\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  7.409595460382127 acc:  0.004175691668713575\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9790457332190934 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Tanhshrink', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1176, 'dropout': 0.2572423849012085, 'dropout_StationIdEmbedding': 0.7291587657077927, 'dropout_timeStampEmbedding': 0.653891060993218, 'dropout_transformers': 0.09311329840059308, 'early_stopping': 1, 'encoder_only': False, 'epochs_classifcation_only': 48, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.5125915866913783, 'scheduler': 'StepLR', 'step_size': 9, 'dropout_lstm': 0.9790457332190934, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 12, 'num_layers_transformer': 5, 'lr': 0.0025721432851743384, 'momentum': 0.16851136418441132, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 2.44946659037716e-09, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'activation_gcn': 'CELU', 'dropout_gcn': 0.6476946755203802, 'hidden_channels': 128, 'layer_type': 'GraphSAGE', 'norm': 'BatchNorm', 'num_layers_gcn': 3, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  8.35224014081453 acc:  0.003304825491816091\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  8.136648077713815 acc:  0.0049572382377241365\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  7.92219335656417 acc:  0.005470825983073934\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  7.8320473721152855 acc:  0.005426166179130474\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  7.737622025138453 acc:  0.006966929415179867\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  7.680378979130795 acc:  0.00966884755375924\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  7.58886345311215 acc:  0.011477569613469397\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  7.589624098727578 acc:  0.013420271085009938\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  7.53044985219052 acc:  0.012504745104168992\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  7.510919435400712 acc:  0.015854230399928546\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  7.51047110808523 acc:  0.01933769510751848\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  7.457834926404451 acc:  0.02199495344215439\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  7.4620928513376334 acc:  0.02286581961905187\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  7.404537818306371 acc:  0.025344438737913942\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  7.401557204597875 acc:  0.02320076814862783\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  7.3629434635764675 acc:  0.027019181385793716\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  7.389090282038638 acc:  0.03592881227251413\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  7.346259287783974 acc:  0.04068508139249269\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  7.347825035295989 acc:  0.04428019560994127\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  7.318964802591425 acc:  0.04508407208092356\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  7.366483352058812 acc:  0.04539669070852779\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  7.276342798534192 acc:  0.04387825737445013\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  7.270138434359902 acc:  0.045619989728245096\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  7.320340071226421 acc:  0.04287341178572226\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7974220468877724 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Tanh', 'batch_size': 128, 'concatenate_features': True, 'd_model': 768, 'dropout': 0.14314346166955466, 'dropout_StationIdEmbedding': 0.17727136502200258, 'dropout_timeStampEmbedding': 0.10401683259798905, 'dropout_transformers': 0.4469283567262584, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 55, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7974220468877724, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.86529213639288, 'beta_2': 0.9659153731645884, 'eps': 2.6910702033406116e-07, 'lr': 0.00011208769197689375, 'optimizer': 'AdamW', 'weight_decay': 6.27327345792811e-09, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.528120492054866 acc:  0.03644240001786392\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  6.68277802834144 acc:  0.13737355693008507\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  6.010977198527409 acc:  0.20518946921823014\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  5.386584197557889 acc:  0.24589688051269454\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  4.851180498416607 acc:  0.2804858986669049\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  4.401645357792194 acc:  0.31007301877944754\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  4.043458714851966 acc:  0.3333184467320188\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  3.7538298826951246 acc:  0.3521202241922158\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  3.531991773385268 acc:  0.36641136145412323\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  3.3436224662340606 acc:  0.37867047763660316\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  3.1953474613336414 acc:  0.38941116048500546\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  3.0731226976101214 acc:  0.3981198222539803\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  2.9612736848684458 acc:  0.4051314114731036\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  2.8759795665740966 acc:  0.4106915570640645\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  2.797330669256357 acc:  0.41573811490967555\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  2.7261640512026273 acc:  0.41848469285219836\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  2.6653798268391538 acc:  0.4216555389321841\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  2.613181284757761 acc:  0.42411182814907444\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  2.561364692908067 acc:  0.4268137462876538\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  2.5161571741104125 acc:  0.42980595315186565\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  2.4712567879603458 acc:  0.4326418507022754\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  2.432030859360328 acc:  0.43384766540874886\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  2.3973787087660567 acc:  0.43462921197775944\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  2.3638155817985536 acc:  0.43487484089944844\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  2.3321573284956125 acc:  0.4360136659000067\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  2.30139669913512 acc:  0.43777772815577337\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  2.269852751951951 acc:  0.43715249090056496\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  2.2457884091597338 acc:  0.4381350065873211\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  2.220008949133066 acc:  0.4376660786459147\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  2.1938284589694095 acc:  0.4384476252149253\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  2.16821132348134 acc:  0.4392068418819641\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  2.1446945217939524 acc:  0.43780005805774513\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  32 loss :  2.1235268666194034 acc:  0.4388495634504165\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  33 loss :  2.102924725642571 acc:  0.43972042962731395\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  34 loss :  2.078352934580583 acc:  0.43876024384252954\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  35 loss :  2.0614928383093614 acc:  0.4390282026661903\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  36 loss :  2.041391545992631 acc:  0.43855927472478395\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  37 loss :  2.021635314134451 acc:  0.438804903646473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9173466203406977 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'Hardswish', 'batch_size': 128, 'concatenate_features': True, 'd_model': 336, 'dropout': 0.09855337001364473, 'dropout_StationIdEmbedding': 0.5748062449705604, 'dropout_timeStampEmbedding': 0.0894199242841272, 'dropout_transformers': 0.605230251754783, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 36, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 8, 'eta_min': 0.03614893051563325, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.9173466203406977, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'GELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8290402123059228, 'beta_2': 0.9775379365381849, 'eps': 1.3310756323321625e-08, 'lr': 0.00012974228292840007, 'optimizer': 'Adam', 'weight_decay': 7.749580630890602e-09, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.66655413554265 acc:  0.008284393631511958\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  6.097377384625949 acc:  0.23971149766652525\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  4.546730223068824 acc:  0.27291606189848827\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  4.169201940756578 acc:  0.26088024473572563\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  4.0869096554242645 acc:  0.25272983051604403\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  3.8596809754004844 acc:  0.20733313980751625\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.806981315373564 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Tanh', 'batch_size': 32, 'concatenate_features': True, 'd_model': 768, 'dropout': 0.04103687552901361, 'dropout_StationIdEmbedding': 0.4395743172978708, 'dropout_timeStampEmbedding': 0.036792508767713644, 'dropout_transformers': 0.6496266306883016, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 56, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.806981315373564, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8150818212122708, 'beta_2': 0.9731752652478046, 'eps': 1.1805766476111629e-07, 'lr': 0.00021509996998583062, 'optimizer': 'AdamW', 'weight_decay': 0.025623470556296005, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.652510905934271 acc:  0.030346336779581536\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  6.823022165031077 acc:  0.10291851818770516\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  6.062534924979522 acc:  0.18078288636312886\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  5.4056768907564825 acc:  0.22361163834490766\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  4.9486077277459835 acc:  0.25730746042024877\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  4.731089641000623 acc:  0.2730947011142621\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  4.380782020426242 acc:  0.2937721903400844\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  4.088587119200519 acc:  0.3103856374070518\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  3.934735926512246 acc:  0.3184244021168747\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  3.74057451586857 acc:  0.3399727575195945\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  3.5956318356166377 acc:  0.343902820266619\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  3.4729883715371104 acc:  0.3504231516423643\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  3.3691105062716473 acc:  0.366098742826519\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  3.2874245933283155 acc:  0.37246276488846214\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  3.147414773424095 acc:  0.3754996315566175\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  3.076879109177634 acc:  0.3835383962664404\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  3.1168579832415713 acc:  0.38742379920952147\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  2.989197450263478 acc:  0.3867092423464261\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  3.000370257368712 acc:  0.391688810486122\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  2.958389242118764 acc:  0.3977402139204609\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  2.8933407770139037 acc:  0.396154790880468\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  2.8871622887727257 acc:  0.4017372663734006\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  2.784985816367319 acc:  0.40499743206127325\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  2.795154492431712 acc:  0.40484112274747114\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  2.7890258927211584 acc:  0.407833329611683\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  2.702565233284068 acc:  0.41167407275082063\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  2.6880828665795726 acc:  0.40926244333787376\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  2.6369535365951395 acc:  0.41165174284884887\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  2.6221843155745033 acc:  0.41256726882968986\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  2.639531373977661 acc:  0.4185516825581136\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  2.5619035613871066 acc:  0.4208516624612018\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  2.5947865281149607 acc:  0.41651966147868613\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  32 loss :  2.495570904740663 acc:  0.41897595069557647\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  33 loss :  2.5363438731042023 acc:  0.41919924971529376\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  34 loss :  2.5247556060274072 acc:  0.4193332291271241\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  35 loss :  2.4794490170256 acc:  0.42140991001049505\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  36 loss :  2.478516702340028 acc:  0.42074001295134317\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  37 loss :  2.4676408043531612 acc:  0.4196681776567001\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  38 loss :  2.4175131009003827 acc:  0.421499229618382\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  39 loss :  2.379376912785468 acc:  0.4241564879530179\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  40 loss :  2.3923777854331187 acc:  0.42589822030681285\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  41 loss :  2.3878622311297977 acc:  0.4252506531496327\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  42 loss :  2.3329174908522132 acc:  0.42216912667753387\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  43 loss :  2.32983860679876 acc:  0.4269700556014559\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  44 loss :  2.3313538381986527 acc:  0.4244244467766787\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  45 loss :  2.353910399374561 acc:  0.42435745707076344\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  46 loss :  2.3116658825740637 acc:  0.4269030658955407\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  47 loss :  2.279818969352223 acc:  0.42705937520934284\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  48 loss :  2.2377627486380463 acc:  0.42663510707187996\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  49 loss :  2.3102053682380745 acc:  0.42551861197329344\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  50 loss :  2.2553055319830637 acc:  0.4257642408949825\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  51 loss :  2.211718553694609 acc:  0.42589822030681285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5555528912478276 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Tanh', 'batch_size': 16, 'concatenate_features': False, 'd_model': 288, 'dropout': 0.1397743165799917, 'dropout_StationIdEmbedding': 0.18181095374153725, 'dropout_timeStampEmbedding': 0.8256580178311448, 'dropout_transformers': 0.7200530998825372, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 54, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.5555528912478276, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8632594503835318, 'beta_2': 0.9701106869242241, 'eps': 1.1557901385771028e-06, 'lr': 0.000459284499588945, 'optimizer': 'AdamW', 'weight_decay': 3.910575904830193e-09, 'positive_function': 'sig', 'epochs_complete_problem': 32, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.959890730240765 acc:  0.008463032847285801\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  7.369899184763932 acc:  0.015050353928946252\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  7.19243544089694 acc:  0.027153160797624098\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  7.042164502023649 acc:  0.0351026058995601\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  6.945774935874619 acc:  0.04825491816090927\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  6.871201992034912 acc:  0.06194314806957998\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  6.751658720128677 acc:  0.07415760444811648\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  6.539598256600003 acc:  0.08391577160976263\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  6.5043103594739895 acc:  0.10416899269812206\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  6.337864170555307 acc:  0.1147310363307505\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  6.25124067018012 acc:  0.11526695397807203\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  6.069851490629821 acc:  0.12710180202308913\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  6.096021856580462 acc:  0.13549784516445973\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  5.9111795104852245 acc:  0.14572494026751223\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  5.8210592830882355 acc:  0.1558627157626778\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  5.728694663328283 acc:  0.16258401625616864\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  5.592591986936681 acc:  0.1639461402764442\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  5.595485737343796 acc:  0.16905968782797043\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  5.424655559684048 acc:  0.1814081236183373\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  5.491846240869089 acc:  0.18672264028760913\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  5.347805952825466 acc:  0.1979545809793895\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  5.257801618896613 acc:  0.20092445794162964\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  5.190738311334818 acc:  0.20139338588303599\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  5.132144479190602 acc:  0.21077194471116273\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  5.155342649011051 acc:  0.21948060648013756\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  4.934195514486618 acc:  0.21867673000915527\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  4.980125206859172 acc:  0.22738539177813008\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  4.938170180601232 acc:  0.23372708393810152\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  4.888425742878633 acc:  0.23593774423330283\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  4.888134249118196 acc:  0.24176584864792444\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  4.768020303309465 acc:  0.247102695219168\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  4.658825645927622 acc:  0.2532880780653373\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  32 loss :  4.757352804937282 acc:  0.2527521604180158\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  33 loss :  4.6501423551254915 acc:  0.2554317486546234\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  34 loss :  4.6306591735166664 acc:  0.2651452560123261\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  35 loss :  4.673311508002401 acc:  0.2648772971886653\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  36 loss :  4.451880420957293 acc:  0.26773552464104683\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  37 loss :  4.492489089484976 acc:  0.2722684947413081\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  38 loss :  4.358714386194694 acc:  0.2733180001339794\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  39 loss :  4.503323514922326 acc:  0.2756626398410111\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  40 loss :  4.30188340098918 acc:  0.27898979523479894\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  41 loss :  4.431302916102049 acc:  0.2813121050398589\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  42 loss :  4.24461909702846 acc:  0.2833217962173146\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  43 loss :  4.298718023700874 acc:  0.2855994462184311\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  44 loss :  4.290163472920907 acc:  0.28604604425786573\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  45 loss :  4.196686438151768 acc:  0.2911149320054485\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  46 loss :  4.185215110538387 acc:  0.29283433445727175\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  47 loss :  4.110116405647342 acc:  0.2947323761248688\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  48 loss :  4.042341444672656 acc:  0.29709934573387226\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  49 loss :  4.2077120692790055 acc:  0.3007391197552643\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  50 loss :  4.128952463133996 acc:  0.30286046044257864\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  51 loss :  4.082419145007093 acc:  0.3062099457383382\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  52 loss :  3.993441665873808 acc:  0.30676819328763144\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  53 loss :  4.031078617111976 acc:  0.31123417368197753\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  54 loss :  5.17314903676009 acc:  0.30942545162226737\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  55 loss :  5.020356691184164 acc:  0.3112118437800058\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  56 loss :  4.901320048740932 acc:  0.3132438648594333\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  57 loss :  4.900315823675204 acc:  0.3122390192707054\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  58 loss :  4.992988181715252 acc:  0.3151642364290021\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  59 loss :  5.001511405496037 acc:  0.3174642163320903\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  60 loss :  4.876992470076104 acc:  0.3188486702543376\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  61 loss :  4.866027172874002 acc:  0.32072438201996295\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  62 loss :  5.015563530080459 acc:  0.3219748565303798\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  63 loss :  4.922408859268958 acc:  0.32387289819797693\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  64 loss :  4.807199075442402 acc:  0.32472143447290264\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  65 loss :  4.810210560550209 acc:  0.3269097648661322\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  66 loss :  4.8383618222565214 acc:  0.3282048991804926\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  67 loss :  4.8469049008954475 acc:  0.33133108545653484\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  68 loss :  4.831178006003885 acc:  0.3331621374182167\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  69 loss :  4.837575934514278 acc:  0.33383203447736864\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  70 loss :  4.775772950228522 acc:  0.3360873545765134\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  71 loss :  4.73126447300951 acc:  0.3374271486948172\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  72 loss :  4.759783304038168 acc:  0.3366232722238349\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  73 loss :  4.807933925580578 acc:  0.33787374673425186\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  74 loss :  4.64681306005526 acc:  0.3391912109505839\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  75 loss :  4.660507678985596 acc:  0.3403970256570574\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  76 loss :  4.69404740894542 acc:  0.33986110800973585\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  77 loss :  4.6462924260051315 acc:  0.3422727374226827\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  78 loss :  4.640161253824956 acc:  0.3430542839916933\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  79 loss :  4.548954114192674 acc:  0.3430542839916933\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  80 loss :  4.505985650695672 acc:  0.3435902016390148\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  81 loss :  4.732529359705308 acc:  0.346805707522944\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  82 loss :  4.515127722956553 acc:  0.3449746555612621\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  83 loss :  4.626270027721629 acc:  0.34843579036688027\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  84 loss :  4.481697515279305 acc:  0.3499765536029297\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  85 loss :  4.636897063054958 acc:  0.3504901413482795\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Tanh', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1128, 'dropout': 0.02872526563508271, 'dropout_StationIdEmbedding': 0.48168654612734446, 'dropout_timeStampEmbedding': 0.10396939670496755, 'dropout_transformers': 0.5882554780053237, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 50, 'input_size': 2, 'learnable_pos_encoding': False, 'base_lr': 0.007060092348477847, 'max_lr': 0.1959699540419694, 'mode': 'exp_range', 'scheduler': 'CyclicLR', 'step_size_up': 22, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8676448243058204, 'beta_2': 0.9671430898993892, 'eps': 8.427482768295967e-07, 'lr': 0.08118068981591323, 'optimizer': 'AdamW', 'weight_decay': 2.8666606362955618e-08, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'GELU', 'dropout_gcn': 0.2788434631677166, 'hidden_channels': 512, 'layer_type': 'GCNConv', 'norm': 'InstanceNorm', 'num_layers_gcn': 8, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6925845555252007 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Tanh', 'batch_size': 64, 'concatenate_features': True, 'd_model': 888, 'dropout': 0.23038104592408168, 'dropout_StationIdEmbedding': 0.37824620541798815, 'dropout_timeStampEmbedding': 0.556171862073551, 'dropout_transformers': 0.6258576086909705, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 59, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.12889488100863017, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.6925845555252007, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'alpha': 0.9380614590332889, 'centered': True, 'eps': 2.7652696346231813e-08, 'lr': 0.004579984015833657, 'momentum': 0.08043679108743235, 'optimizer': 'RMSprop', 'weight_decay': 8.291479566103543e-05, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  6.35236905003322 acc:  0.17001987361275483\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  4.600183215760093 acc:  0.2178281937342295\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  4.0196848734644535 acc:  0.23774646629301296\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  3.8878046610883175 acc:  0.23919790992117546\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  3.864528852564688 acc:  0.23922023982314716\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  3.864498409606118 acc:  0.2392425697251189\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  3.861455637080069 acc:  0.23922023982314716\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m loss is undifined\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7793811260972364 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'LeakyReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 432, 'dropout': 0.3447829323125935, 'dropout_StationIdEmbedding': 0.23056574861372098, 'dropout_timeStampEmbedding': 0.39104866879312883, 'dropout_transformers': 0.4656398232078552, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 63, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7793811260972364, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 1, 'lr': 1.9836763426450555e-06, 'momentum': 0.410560820313398, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.000352682807235384, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  8.062653736895825 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  8.066188168812948 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  8.068040537546915 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  8.06241151510951 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  8.063443804361734 acc:  0.0004689279414063372\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7493994813432109 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Sigmoid', 'activation_transformers': 'Mish', 'batch_size': 128, 'concatenate_features': True, 'd_model': 72, 'dropout': 0.2081011915750497, 'dropout_StationIdEmbedding': 0.10584970923744956, 'dropout_timeStampEmbedding': 0.23603122081229674, 'dropout_transformers': 0.6769297756991051, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 33, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 9, 'factor': 0.8002633200469347, 'patience': 9, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.0024560988206451776, 'dropout_lstm': 0.7493994813432109, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 12, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8410972314233909, 'beta_2': 0.963365380453122, 'eps': 3.589701122163516e-09, 'lr': 8.81995693471003e-05, 'optimizer': 'Adam', 'weight_decay': 0.0005390963526518113, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.968245520958534 acc:  0.0031708460799857088\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  7.754098719816941 acc:  0.003416475001674743\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  7.571145226405217 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  7.430386748680702 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  7.355628791222205 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  7.319050821891198 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  7.29811272254357 acc:  0.0033271553937878214\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  7.286491478406466 acc:  0.00326016568787263\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  7.278920943920429 acc:  0.003126186276042248\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'SiLU', 'activation_transformers': 'Hardtanh', 'batch_size': 64, 'concatenate_features': True, 'd_model': 480, 'dropout': 0.003950246091029563, 'dropout_StationIdEmbedding': 0.01970301287523811, 'dropout_timeStampEmbedding': 0.3051400563604007, 'dropout_transformers': 0.44323202055637817, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 58, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.9753335494552355, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Mish', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8223854928426003, 'beta_2': 0.9685017556445132, 'eps': 2.862543172425694e-07, 'lr': 0.0024804393797994563, 'optimizer': 'AdamW', 'weight_decay': 2.152267734971646e-09, 'positive_function': 'sig', 'epochs_complete_problem': 46, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9753335494552355 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  5.732600860253066 acc:  0.22524172118884397\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  3.6409438955569695 acc:  0.2976575932831655\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  3.1208380467877417 acc:  0.33186700310385636\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  2.83414418540315 acc:  0.3586628854699328\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  2.636648019630752 acc:  0.36761717616059664\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  2.4689917585806933 acc:  0.37829086930308375\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  2.332380659565954 acc:  0.3740928477323985\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  2.2372386284217147 acc:  0.3795860036174441\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  2.0898773427494985 acc:  0.38659759283656747\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  2.003453396037667 acc:  0.38161802469687156\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  1.916038296893685 acc:  0.380010271754907\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m GraphSAGE\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'PReLU', 'activation_transformers': 'Softplus', 'batch_size': 32, 'concatenate_features': False, 'd_model': 1224, 'dropout': 0.43946636613546325, 'dropout_StationIdEmbedding': 0.04361604974666561, 'dropout_timeStampEmbedding': 0.26364882585276483, 'dropout_transformers': 0.5098832538607492, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 39, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.783495839207017, 'scheduler': 'StepLR', 'step_size': 18, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8083991339632046, 'beta_2': 0.9660630414962572, 'eps': 6.196229293156739e-08, 'lr': 0.000787029105982129, 'optimizer': 'AdamW', 'weight_decay': 6.903816400561451e-09, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softmin', 'dropout_gcn': 0.4036868286560899, 'hidden_channels': 256, 'layer_type': 'GraphSAGE', 'norm': 'GraphNorm', 'num_layers_gcn': 5, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.586122708571585 acc:  0.008172744121653306\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  7.243910222304494 acc:  0.011097961279949982\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  6.962121827978837 acc:  0.08255364758948708\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  6.44713884654798 acc:  0.14748900252327893\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  5.828361671849301 acc:  0.18623138244423107\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  5.4065143685591845 acc:  0.21682334814550164\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  5.085841653221532 acc:  0.229462072661501\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  4.892646741867066 acc:  0.2512337270839381\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  4.697303681624563 acc:  0.27369760846749885\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  4.612124832052934 acc:  0.27157626778018445\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  4.331747167988827 acc:  0.29153920014291135\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  4.214358894448531 acc:  0.2935488913203671\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  4.169599902002435 acc:  0.2980595315186566\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  4.074130848834389 acc:  0.31159145211352524\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  4.048389530181884 acc:  0.3109215550543733\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  4.025516816189414 acc:  0.3115021325056383\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  3.866986550782856 acc:  0.32168456780474736\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  3.9556271502846165 acc:  0.324609784963044\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  3.8719887934233013 acc:  0.33255923006498\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  3.7776247024536134 acc:  0.33573007614496575\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  3.7925282628912673 acc:  0.3423173972266262\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  3.7361657669669706 acc:  0.347966862425474\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  3.6649373054504393 acc:  0.3423397271285979\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  3.71373206941705 acc:  0.3438135006587321\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  3.678711464530543 acc:  0.3455775629144988\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  3.6651424433055677 acc:  0.3512493580153183\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  3.5610847422951144 acc:  0.3501105330147601\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  3.5899603241368343 acc:  0.34901636781814527\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  3.5592934407685934 acc:  0.3537949668400956\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  3.5757617097151906 acc:  0.35701047272402475\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  3.527234212975753 acc:  0.3593327825290847\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  3.5285270590531197 acc:  0.35774735948909187\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  32 loss :  3.543047799562153 acc:  0.3591094835093674\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  33 loss :  3.489634591654727 acc:  0.35821628743049816\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  34 loss :  3.483486466658743 acc:  0.35893084429359357\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  35 loss :  3.4426969201941238 acc:  0.36174441194203155\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  36 loss :  3.4961767196655273 acc:  0.36525020655159324\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  37 loss :  3.441528666646857 acc:  0.36346381439385483\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  38 loss :  3.4141221447994834 acc:  0.3657414643949713\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8371274296683918 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'Hardsigmoid', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1296, 'dropout': 0.0850547714230167, 'dropout_StationIdEmbedding': 0.2826457984476207, 'dropout_timeStampEmbedding': 0.4324662733899777, 'dropout_transformers': 0.24037013476907143, 'early_stopping': 7, 'encoder_only': False, 'epochs_classifcation_only': 52, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.8371274296683918, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8587365476729337, 'beta_2': 0.9607861888411813, 'eps': 1.0703283003693741e-08, 'lr': 3.063295314996003e-07, 'optimizer': 'AdamW', 'weight_decay': 3.735596068166369e-08, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  8.124269798022357 acc:  0.0002456289216890338\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  8.100787771850072 acc:  0.0004465980394346069\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  8.08082518858068 acc:  0.0006252372552084496\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  8.0570474993281 acc:  0.0008262063729540227\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  8.035624339800922 acc:  0.0010048455887278656\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  8.014211290022907 acc:  0.001116495098586517\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  7.994096531587489 acc:  0.001362124020275551\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  7.974068721803296 acc:  0.0015854230399928544\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  7.954323956946365 acc:  0.00194270147154054\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  7.934571053801465 acc:  0.002344639707031686\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  7.915822814492619 acc:  0.002523278922805529\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  7.896197583495068 acc:  0.002969876962240136\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  7.880285647736878 acc:  0.0032378357859009\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  7.863223528661647 acc:  0.0033271553937878214\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  7.843031863204572 acc:  0.0034611348056182035\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  7.828117210324071 acc:  0.003684433825335507\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  7.810836836069572 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  7.793325324018462 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  7.779361340178161 acc:  0.00390773284505281\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  7.757903952558501 acc:  0.003840743139137619\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  7.7444922783795525 acc:  0.003818413237165889\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  7.727046008871383 acc:  0.00388540294308108\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  7.7123386719647575 acc:  0.003974722550968001\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  7.695933935021152 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  7.682989749587884 acc:  0.003997052452939732\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  7.664646365061528 acc:  0.004019382354911462\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  7.6506313436171585 acc:  0.004019382354911462\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  7.635270715761585 acc:  0.004064042158854923\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  7.6216956026413865 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  7.607788995534432 acc:  0.004064042158854923\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  7.593648614001875 acc:  0.004086372060826653\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  7.578851435364795 acc:  0.0041310318647701134\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  32 loss :  7.567772544732614 acc:  0.0041310318647701134\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  33 loss :  7.5513775769402 acc:  0.004220351472657035\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  34 loss :  7.541078811934014 acc:  0.004332000982515687\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  35 loss :  7.52400122169687 acc:  0.004332000982515687\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  36 loss :  7.511785535251393 acc:  0.004332000982515687\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  37 loss :  7.500596807784393 acc:  0.004309671080543956\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  38 loss :  7.489213911425166 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  39 loss :  7.475768321702461 acc:  0.004398990688430878\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  40 loss :  7.465324165440407 acc:  0.004376660786459147\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  41 loss :  7.457306661525695 acc:  0.004398990688430878\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  42 loss :  7.445411950600247 acc:  0.004443650492374339\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  43 loss :  7.43416553785821 acc:  0.004465980394346068\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  44 loss :  7.423625501264043 acc:  0.00457762990420472\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  45 loss :  7.409797644414821 acc:  0.00457762990420472\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  46 loss :  7.4019801636703875 acc:  0.004599959806176451\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  47 loss :  7.394074215608485 acc:  0.004711609316035103\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  48 loss :  7.382258258947806 acc:  0.004778599021950294\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  49 loss :  7.37747994591208 acc:  0.004867918629837215\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  50 loss :  7.370840866024754 acc:  0.004890248531808946\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  51 loss :  7.362342285508869 acc:  0.005024227943639327\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4519857731800705 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Softshrink', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1416, 'dropout': 0.057200208245928724, 'dropout_StationIdEmbedding': 0.07242704379271506, 'dropout_timeStampEmbedding': 0.06121609413872914, 'dropout_transformers': 0.4132641473989971, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 70, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 25, 'eta_min': 0.0027280909463625025, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.4519857731800705, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 24, 'num_layers_transformer': 2, 'alpha': 0.9912484729167507, 'centered': False, 'eps': 6.198526123708583e-07, 'lr': 5.636483529357195e-05, 'momentum': 0.346409223583691, 'optimizer': 'RMSprop', 'weight_decay': 3.129572597498784e-08, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.055309264459343 acc:  0.10937185985753523\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  6.029104370937169 acc:  0.16937230645557466\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  5.359456699585246 acc:  0.2287921756023491\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  4.68026710447864 acc:  0.26744523591541436\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  4.143817322276463 acc:  0.29325860259473463\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  3.7896478265245386 acc:  0.30591965701270574\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  3.8234189759905095 acc:  0.2913382310251658\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9111685055502249 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'Hardshrink', 'batch_size': 64, 'concatenate_features': True, 'd_model': 672, 'dropout': 0.11608960109022144, 'dropout_StationIdEmbedding': 0.3313240807004375, 'dropout_timeStampEmbedding': 0.01888233950632834, 'dropout_transformers': 0.9462678346226849, 'early_stopping': 8, 'encoder_only': True, 'epochs_classifcation_only': 67, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.9111685055502249, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 1, 'lr': 0.10625506248268199, 'momentum': 0.33939904054899034, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 2.8476802917942196e-06, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  7.561540163480318 acc:  0.05794609561664024\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  6.827279304290985 acc:  0.11671839760623451\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  6.280373970111767 acc:  0.16530826429671974\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  5.848354022819679 acc:  0.1973740035281245\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  5.323691321419669 acc:  0.22374561775673804\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  4.771129541463785 acc:  0.2607909251278387\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  4.290862066762431 acc:  0.2890605810240493\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  3.930374654022964 acc:  0.31022932809324966\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  3.7091156602739455 acc:  0.32920974476922044\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  3.5372371123387265 acc:  0.3451979545809794\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  3.3866796826982832 acc:  0.3633298349820244\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  3.287244506649204 acc:  0.37203849675099926\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  3.193807706966267 acc:  0.37654913694928877\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  3.111520210346142 acc:  0.38157336489292815\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  3.0404762738234514 acc:  0.3819083134225041\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  2.9857724980040863 acc:  0.39454703793850343\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  2.9242259822525343 acc:  0.38829466538641894\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  2.873757274000795 acc:  0.3929839448004823\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  2.8367726369337602 acc:  0.3980305026460934\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  2.809334241426908 acc:  0.39463635754639037\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  2.747709687773164 acc:  0.39608780117455283\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  2.755343047055331 acc:  0.3990800080387647\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  2.6879488521522577 acc:  0.40171493647142886\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  2.6627687224141368 acc:  0.40816827814125894\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  2.640250187653762 acc:  0.4020275550990331\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  2.6291926807456916 acc:  0.40108969921622045\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  2.5918660480659326 acc:  0.40539937029676437\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  2.540571177756036 acc:  0.4035013286291673\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  2.5409925125695607 acc:  0.40455083402183867\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  2.5336600550404795 acc:  0.40582363843422725\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  2.4818505040415517 acc:  0.4064712055914075\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  2.482294700362466 acc:  0.4052430609829623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.49918763640664365 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Tanhshrink', 'activation_transformers': 'Softmin', 'batch_size': 128, 'concatenate_features': True, 'd_model': 864, 'dropout': 0.14917661543852637, 'dropout_StationIdEmbedding': 0.8238426599593924, 'dropout_timeStampEmbedding': 0.19787889535725717, 'dropout_transformers': 0.3308422612571747, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 64, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 0.00036922382080356434, 'max_lr': 0.10734852933029443, 'mode': 'triangular2', 'scheduler': 'CyclicLR', 'step_size_up': 25, 'dropout_lstm': 0.49918763640664365, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8767780074760605, 'beta_2': 0.9591230180008555, 'eps': 2.1258327457565617e-09, 'lr': 0.0012521059176644326, 'optimizer': 'Adam', 'weight_decay': 1.1560643392418431e-06, 'positive_function': 'relu', 'epochs_complete_problem': 21, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  6.865080129183256 acc:  0.25487350110533014\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  5.160398501616258 acc:  0.2851528481789965\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  5.341884345274705 acc:  0.2906683339660139\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  6.191592986767109 acc:  0.26769086483710336\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m GAT\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'ReLU6', 'batch_size': 32, 'concatenate_features': False, 'd_model': 744, 'dropout': 0.31927197226046844, 'dropout_StationIdEmbedding': 0.7711524576324957, 'dropout_timeStampEmbedding': 0.5135006518462428, 'dropout_transformers': 0.3828635507478771, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 60, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.43806953847537455, 'scheduler': 'ExponentialLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8465602810945871, 'beta_2': 0.9762751442290083, 'eps': 1.04226439326792e-07, 'lr': 0.0049170284704256545, 'optimizer': 'AdamW', 'weight_decay': 7.057879568166178e-05, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Sigmoid', 'dropout_gcn': 0.48352739696165487, 'hidden_channels': 512, 'layer_type': 'GAT', 'norm': 'InstanceNorm', 'num_layers_gcn': 1, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  9.240909625637915 acc:  0.00042426813746287653\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  7.689574613878804 acc:  0.0003126186276042248\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  7.426230636719734 acc:  0.003706763727307237\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  7.324171146269768 acc:  0.0033941450997030122\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6932986004019901 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'Sigmoid', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1344, 'dropout': 0.28831747197292956, 'dropout_StationIdEmbedding': 0.6773714722636933, 'dropout_timeStampEmbedding': 0.16480278848759067, 'dropout_transformers': 0.28781813764833997, 'early_stopping': 10, 'encoder_only': True, 'epochs_classifcation_only': 49, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.6932986004019901, 'lstm_layer_with_layer_norm': True, 'activation_lstm': 'Sigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8909514190846709, 'beta_2': 0.9696918477138865, 'eps': 2.4547074716140227e-06, 'lr': 1.0282087948458245e-05, 'optimizer': 'AdamW', 'weight_decay': 1.687709251382846e-08, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  8.118472669377674 acc:  0.003550454413505125\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  7.721066914457182 acc:  0.005024227943639327\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  7.488974323485817 acc:  0.005135877453497979\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  7.400655376178593 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  7.363688082668368 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  7.343554723196189 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  7.321392653374699 acc:  0.005716454904762968\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  7.31814096493428 acc:  0.004198021570685304\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  7.29989674371048 acc:  0.0056271352968760464\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  7.29796991667934 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  7.288969796463098 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  7.290754153075831 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  7.284071008586351 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  7.278865124260247 acc:  0.0041310318647701134\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  7.2660140511709885 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  7.271398893281734 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  7.263927598239324 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m {'activation': 'Softshrink', 'activation_transformers': 'Softsign', 'batch_size': 16, 'concatenate_features': True, 'd_model': 552, 'dropout': 0.3912300220586564, 'dropout_StationIdEmbedding': 0.13071550179170677, 'dropout_timeStampEmbedding': 0.47109212130984746, 'dropout_transformers': 0.3517978813456328, 'early_stopping': 4, 'encoder_only': False, 'epochs_classifcation_only': 75, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.6113946715141367, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': True, 'beta_1': 0.8332348672283685, 'beta_2': 0.9794007557844995, 'eps': 6.607239911852249e-07, 'lr': 2.1657567195736806e-05, 'optimizer': 'AdamW', 'weight_decay': 0.012074339319989844, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6113946715141367 and num_layers=1\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  0 loss :  8.277191999579678 acc:  0.0013174642163320902\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  1 loss :  8.07288722831662 acc:  0.0022999799030882255\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  2 loss :  7.8712977802052215 acc:  0.0038630730411093497\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  3 loss :  7.714090395374458 acc:  0.004934908335752406\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  4 loss :  7.596345248342562 acc:  0.004823258825893754\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  5 loss :  7.495501922960041 acc:  0.005582475492932586\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  6 loss :  7.495824833877948 acc:  0.0056271352968760464\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  7 loss :  7.431319561325202 acc:  0.006810620101377755\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  8 loss :  7.412263994457341 acc:  0.007636826474331778\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  9 loss :  7.4263102066617055 acc:  0.00908827010249425\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  10 loss :  7.3797762213634845 acc:  0.009490208337985397\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  11 loss :  7.357156713469689 acc:  0.011901837750932273\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  12 loss :  7.34809085701694 acc:  0.01299600294754706\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  13 loss :  7.318172026081245 acc:  0.015764910792041623\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  14 loss :  7.272289332221536 acc:  0.01748431324386486\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  15 loss :  7.2508926471742265 acc:  0.01819887010696023\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  16 loss :  7.227732097401338 acc:  0.01958332402920751\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  17 loss :  7.219577428673496 acc:  0.024428912757072995\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  18 loss :  7.193964946169813 acc:  0.026929861777906794\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  19 loss :  7.207409245627267 acc:  0.0278900475626912\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  20 loss :  7.133773515204422 acc:  0.033360873545765134\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  21 loss :  7.06953240242325 acc:  0.036598709331666035\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  22 loss :  7.049219948904855 acc:  0.031328852466337674\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  23 loss :  7.063889154866964 acc:  0.0437889377665632\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  24 loss :  7.041716980333088 acc:  0.04633454659134047\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  25 loss :  6.954221252633744 acc:  0.04841122747471139\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  26 loss :  6.9877364134588165 acc:  0.052497599535538036\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  27 loss :  6.936017461183693 acc:  0.05839269365607485\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  28 loss :  6.938850218508424 acc:  0.0632829421878838\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  29 loss :  6.877643845662349 acc:  0.06640912846392605\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  30 loss :  6.867399135557544 acc:  0.07187995444699997\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  31 loss :  6.819087445235052 acc:  0.07482750150726838\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  32 loss :  6.779358919929056 acc:  0.08000803876470983\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  33 loss :  6.774567752325234 acc:  0.08324587455061072\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  34 loss :  6.71765945739105 acc:  0.08784583435678717\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  35 loss :  6.730360179388223 acc:  0.09115065984860327\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  36 loss :  6.687249420069847 acc:  0.09595158877252528\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  37 loss :  6.6441775570396615 acc:  0.09970301230377598\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  38 loss :  6.604246552250967 acc:  0.10564276622825626\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  39 loss :  6.549760858551795 acc:  0.1083223544648639\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  40 loss :  6.562506535473992 acc:  0.11234173681977536\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  41 loss :  6.527557433152399 acc:  0.11504365495835474\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  42 loss :  6.559963126142486 acc:  0.11776790299890584\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  43 loss :  6.478328937242011 acc:  0.11968827456847464\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  44 loss :  6.4819270983463575 acc:  0.12466784270817051\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  45 loss :  6.413134895452933 acc:  0.12772703927829757\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  46 loss :  6.364965579088996 acc:  0.1300270191813858\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  47 loss :  6.442174683098032 acc:  0.13424737065404282\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  48 loss :  6.385013115506212 acc:  0.13737355693008507\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  49 loss :  6.255057815744095 acc:  0.14105799075542058\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  50 loss :  6.234223954817828 acc:  0.1437152490900565\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  51 loss :  6.198696813663514 acc:  0.1463725074246924\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  52 loss :  6.166843590616178 acc:  0.14876180693566754\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  53 loss :  6.152408074931938 acc:  0.15039188977960385\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  54 loss :  6.196759917155034 acc:  0.15434428242860013\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  55 loss :  6.123852737811434 acc:  0.15776075743027487\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  56 loss :  6.060742646706204 acc:  0.1618247995891298\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  57 loss :  6.1688064967884735 acc:  0.1652189446888328\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  58 loss :  5.993569730710583 acc:  0.16466069713953957\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  59 loss :  6.070902415684292 acc:  0.16604515106178683\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  60 loss :  6.114887734421161 acc:  0.16854610008262064\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  61 loss :  6.009519512913808 acc:  0.1716052966527477\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  62 loss :  6.00667566010932 acc:  0.1735256682223165\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  63 loss :  5.9007803592361325 acc:  0.1762275863608959\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  64 loss :  5.932224053294719 acc:  0.17948775204876852\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  65 loss :  5.922632630131826 acc:  0.18091686577495925\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  66 loss :  5.947219528070017 acc:  0.18247995891298038\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  67 loss :  5.796790259225028 acc:  0.18486925842395552\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  68 loss :  5.7993361008267446 acc:  0.18605274322845722\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  69 loss :  5.864260825790277 acc:  0.18855369224929103\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  70 loss :  5.664940621672558 acc:  0.19100998146618137\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  71 loss :  5.763645484667866 acc:  0.19382354911461938\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  72 loss :  5.762721354220094 acc:  0.19614585891967934\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  73 loss :  5.644063677106585 acc:  0.1985351584306545\n",
            "\u001b[36m(eval_config pid=282961)\u001b[0m epoch:  74 loss :  5.7302801128195116 acc:  0.19947301431346717\n",
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-07 12:15:48,311\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-07 12:16:03,790\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-07 12:16:03,792\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_6        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_6\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_6`\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7285644466682207 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'Hardswish', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1080, 'dropout': 0.6982788545664582, 'dropout_StationIdEmbedding': 0.621152124953612, 'dropout_timeStampEmbedding': 0.1175394105594772, 'dropout_transformers': 0.8938253628636652, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 54, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 0, 'factor': 0.5839932175187988, 'patience': 4, 'scheduler': 'ReduceLROnPlateau', 'threshold': 2.6073916866169487e-05, 'dropout_lstm': 0.7285644466682207, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 72, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 6, 'alpha': 0.976122219792859, 'centered': False, 'eps': 2.0314563783269275e-06, 'lr': 0.0007494851094505996, 'momentum': 0.05318765580598933, 'optimizer': 'RMSprop', 'weight_decay': 0.047018482691796405, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  7.635608679811719 acc:  0.03032400687760981\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.5060004180585835 acc:  0.01958332402920751\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  7.50230104822508 acc:  0.016591117164995645\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4022500782633984 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Softmin', 'activation_transformers': 'PReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1032, 'dropout': 0.17382228778505543, 'dropout_StationIdEmbedding': 0.0012113331169516517, 'dropout_timeStampEmbedding': 0.28275818844789413, 'dropout_transformers': 0.4712260195060145, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 57, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.21555833725608312, 'scheduler': 'StepLR', 'step_size': 27, 'dropout_lstm': 0.4022500782633984, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 192, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 2, 'lr': 8.330030652151283e-06, 'momentum': 0.4577617790866179, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 5.823050087710426e-08, 'positive_function': 'abs', 'epochs_complete_problem': 42, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.020934224752855 acc:  0.00017863921577384274\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.0206249696422 acc:  0.00017863921577384274\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  8.020755283495518 acc:  0.00017863921577384274\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  8.02079319579439 acc:  0.00017863921577384274\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m GCNConv\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'LeakyReLU', 'activation_transformers': 'CELU', 'batch_size': 64, 'concatenate_features': False, 'd_model': 1248, 'dropout': 0.41857592377860064, 'dropout_StationIdEmbedding': 0.9992517453967049, 'dropout_timeStampEmbedding': 0.3326713947103078, 'dropout_transformers': 0.2615123560582785, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 41, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 12, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8843581897050788, 'beta_2': 0.9734899088637147, 'eps': 3.5627483414665463e-06, 'lr': 2.7590007495743843e-06, 'optimizer': 'AdamW', 'weight_decay': 0.0010418247918815352, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'activation_gcn': 'swish', 'dropout_gcn': 0.2030528186185816, 'hidden_channels': 64, 'layer_type': 'GCNConv', 'norm': 'BatchNorm', 'num_layers_gcn': 6, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.07227906379991 acc:  0.0004689279414063372\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.00277702316983 acc:  0.0005582475492932586\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  7.9398366622342404 acc:  0.000714556863095371\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  7.876204679940493 acc:  0.0009601857847844048\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  7.823907040457689 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  7.777820496158745 acc:  0.0011834848045017081\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  7.73022641480424 acc:  0.0014067838242190116\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  7.693378528565851 acc:  0.0015854230399928544\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  7.650017159585734 acc:  0.001630082843936315\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  7.618457732309822 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  7.585296583539657 acc:  0.002054350981399192\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  7.556407895706992 acc:  0.002344639707031686\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  7.529347022981134 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  7.51051716040109 acc:  0.002858227452381484\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  7.486667895135079 acc:  0.003103856374070518\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  7.469088874700415 acc:  0.0031708460799857088\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  7.452187625506452 acc:  0.003304825491816091\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  7.4254415927042485 acc:  0.0035281245115333943\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  7.416641162551996 acc:  0.0035951142174485856\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  7.405448302057863 acc:  0.0037514235312506978\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  7.393718319084808 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  7.385013394683372 acc:  0.004175691668713575\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  7.365415529440377 acc:  0.004041712256883192\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  7.3591573620570525 acc:  0.003997052452939732\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  7.355781613415434 acc:  0.004086372060826653\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  25 loss :  7.341273366039946 acc:  0.003997052452939732\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  26 loss :  7.3468797989474 acc:  0.004019382354911462\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  27 loss :  7.331814383732453 acc:  0.004198021570685304\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  28 loss :  7.338896369206086 acc:  0.004287341178572226\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  29 loss :  7.33028180362614 acc:  0.004220351472657035\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  30 loss :  7.3317299835554515 acc:  0.004198021570685304\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  31 loss :  7.316594396838705 acc:  0.004265011276600496\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  32 loss :  7.31571517099861 acc:  0.004332000982515687\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  33 loss :  7.320146243990832 acc:  0.004354330884487417\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  34 loss :  7.310133213305291 acc:  0.004443650492374339\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  35 loss :  7.301768051758978 acc:  0.004443650492374339\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  36 loss :  7.308032330665879 acc:  0.004465980394346068\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  37 loss :  7.303676266706627 acc:  0.0045106401982895295\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  38 loss :  7.300301726537806 acc:  0.00455530000223299\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  39 loss :  7.302349956891009 acc:  0.004599959806176451\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  40 loss :  7.297743429664437 acc:  0.004644619610119911\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8529788182319261 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'GELU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 600, 'dropout': 0.7737635793072686, 'dropout_StationIdEmbedding': 0.17519000252911243, 'dropout_timeStampEmbedding': 0.4092491115886116, 'dropout_transformers': 0.21433876172616176, 'early_stopping': 3, 'encoder_only': False, 'epochs_classifcation_only': 62, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.8529788182319261, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8511654164329339, 'beta_2': 0.9709067987954072, 'eps': 3.1628252883785764e-08, 'lr': 9.136265427713556e-07, 'optimizer': 'Adam', 'weight_decay': 3.602206475017811e-07, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.608632095043475 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.597873981182392 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  8.583227715125451 acc:  0.000513587745349798\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  8.569037254040058 acc:  0.0005805774512649889\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  8.554274074847882 acc:  0.0006698970591519103\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  8.540134716033936 acc:  0.000714556863095371\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  8.527931528825027 acc:  0.0008485362749257531\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  8.513457826467661 acc:  0.0009601857847844048\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  8.503212686685416 acc:  0.001116495098586517\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  8.48953888966487 acc:  0.0012281446084451688\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  8.477287710629977 acc:  0.0014067838242190116\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  8.463767770620493 acc:  0.001429113726190742\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  8.450555236522968 acc:  0.0015184333340776633\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  8.440495879833515 acc:  0.0016747426478797758\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  8.426772631131685 acc:  0.0017863921577384275\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  8.409799319047194 acc:  0.0019203715695688096\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  8.400479096632738 acc:  0.0021883303932295735\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  8.385743911449726 acc:  0.002411629412946877\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  8.372831359276404 acc:  0.0024786191188620682\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  8.359439923213078 acc:  0.0027019181385793717\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  8.345665660271278 acc:  0.0028358975504097538\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  8.336934874607966 acc:  0.0030368666681553267\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  8.324930191040039 acc:  0.0032155058839291695\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  8.31149649986854 acc:  0.0033494852957595515\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  8.296150552309477 acc:  0.003416475001674743\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  25 loss :  8.28384986290565 acc:  0.003572784315476855\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  26 loss :  8.27200243289654 acc:  0.0037290936292789676\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  27 loss :  8.261968979468712 acc:  0.0037737534332224283\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  28 loss :  8.247681559049166 acc:  0.0038630730411093497\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  29 loss :  8.23603908098661 acc:  0.003930062747024541\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  30 loss :  8.222937804002028 acc:  0.00390773284505281\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  31 loss :  8.21129542864286 acc:  0.003997052452939732\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  32 loss :  8.197839186741756 acc:  0.004108701962798384\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  33 loss :  8.186121955284705 acc:  0.004086372060826653\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  34 loss :  8.172355402432956 acc:  0.004242681374628765\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  35 loss :  8.16205942447369 acc:  0.004242681374628765\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  36 loss :  8.14926763681265 acc:  0.004287341178572226\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  37 loss :  8.138149364177997 acc:  0.004265011276600496\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  38 loss :  8.123905996175912 acc:  0.004376660786459147\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  39 loss :  8.110068284548246 acc:  0.004421320590402608\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  40 loss :  8.102926107553335 acc:  0.0045106401982895295\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  41 loss :  8.087553497461172 acc:  0.004599959806176451\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  42 loss :  8.075886264214148 acc:  0.00457762990420472\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  43 loss :  8.066306679065411 acc:  0.004666949512091642\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  44 loss :  8.055154495972854 acc:  0.004666949512091642\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  45 loss :  8.043417978286744 acc:  0.004778599021950294\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  46 loss :  8.03571331684406 acc:  0.004778599021950294\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  47 loss :  8.018451422911424 acc:  0.0048455887278654845\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  48 loss :  8.009976695134089 acc:  0.004890248531808946\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  49 loss :  8.002928748497595 acc:  0.0049572382377241365\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  50 loss :  7.989854464164147 acc:  0.005024227943639327\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  51 loss :  7.984621066313523 acc:  0.005046557845611058\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  52 loss :  7.9741256016951345 acc:  0.005091217649554518\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  53 loss :  7.963624928547786 acc:  0.005269856865328361\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  54 loss :  7.953743017636812 acc:  0.005269856865328361\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  55 loss :  7.94444272334759 acc:  0.0051805372574414395\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  56 loss :  7.937461713644175 acc:  0.00520286715941317\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8131903964889645 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'ReLU6', 'activation_transformers': 'Hardswish', 'batch_size': 32, 'concatenate_features': True, 'd_model': 936, 'dropout': 0.9981029205156808, 'dropout_StationIdEmbedding': 0.2033947301535048, 'dropout_timeStampEmbedding': 0.45583779151986803, 'dropout_transformers': 0.7113105692359017, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 65, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 17, 'eta_min': 0.00021331642528687843, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.8131903964889645, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'LogSigmoid', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8716061299135273, 'beta_2': 0.962573402846431, 'eps': 4.079150547378938e-09, 'lr': 1.622441544049761e-05, 'optimizer': 'AdamW', 'weight_decay': 4.015798996419644e-06, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  7.891806999329598 acc:  0.00453297010026126\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.455219610275761 acc:  0.010740682848402296\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  7.2825483229852495 acc:  0.02594734609115066\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  7.152921673559374 acc:  0.04405689659022397\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  6.95252419440977 acc:  0.08550119464975549\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  6.655070637118432 acc:  0.1348056182033361\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  6.1541990034041865 acc:  0.17988969028425966\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  5.4675010927261845 acc:  0.224661143737579\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  4.867289978458035 acc:  0.2514123662997119\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  4.396007622441938 acc:  0.27709175356720184\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  4.03514756079643 acc:  0.30299443985440905\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  3.769619471027005 acc:  0.3227564030993904\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  3.6029381290558846 acc:  0.32523502221825246\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  3.3887616449786773 acc:  0.33972712859790544\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  3.2433624313723657 acc:  0.3512270281133466\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  3.099061259915752 acc:  0.35678717370430746\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  3.0386641794635403 acc:  0.3709443315543845\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  2.9879417996252737 acc:  0.3746734251836634\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  2.845999103976834 acc:  0.3768394256749213\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  2.7882882010552192 acc:  0.37967532322533104\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  2.7086722958472467 acc:  0.3889868923475426\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  2.589812478711528 acc:  0.3951276153897684\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  2.553274826849661 acc:  0.4002411629412947\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  2.4951776427607384 acc:  0.40682848402295513\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  2.44038651527897 acc:  0.41140611392715987\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  25 loss :  2.3960695958906606 acc:  0.41102650559364046\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  26 loss :  2.3500873634892123 acc:  0.4181944041265659\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  27 loss :  2.335082240258494 acc:  0.4204720541276824\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  28 loss :  2.2662330419786514 acc:  0.42158854922626887\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  29 loss :  2.260192370414734 acc:  0.4236652301096398\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  30 loss :  2.2550223450506888 acc:  0.4267467565817386\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  31 loss :  2.210863208770752 acc:  0.4297612933479222\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  32 loss :  2.2156534825601883 acc:  0.42897974677891165\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  33 loss :  2.197940267285993 acc:  0.4307214791327066\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  34 loss :  2.171776923825664 acc:  0.43014090168144165\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  35 loss :  2.129755776928317 acc:  0.43112341736819776\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  36 loss :  2.1698394483135592 acc:  0.4300739119755264\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  37 loss :  2.131392393573638 acc:  0.43023022128932853\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'SiLU', 'batch_size': 128, 'concatenate_features': True, 'd_model': 168, 'dropout': 0.19423095381915984, 'dropout_StationIdEmbedding': 0.40575419834473314, 'dropout_timeStampEmbedding': 0.04755486230518166, 'dropout_transformers': 0.5322875429640814, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 71, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 4.3181652588848135e-07, 'max_lr': 0.06352084361072614, 'mode': 'exp_range', 'scheduler': 'CyclicLR', 'step_size_up': 16, 'dropout_lstm': 0.5321682289139822, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 48, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.9056036179761526, 'beta_2': 0.9532761994201512, 'eps': 8.280690093245792e-08, 'lr': 0.009946581026051383, 'optimizer': 'AdamW', 'weight_decay': 1.6541841611454316e-07, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5321682289139822 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.112470403630683 acc:  0.00037960833351941584\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.3238808550733205 acc:  0.036531719625750844\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  6.431374367247236 acc:  0.14916374517115868\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  4.973196557227602 acc:  0.22758636089587567\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  4.123810671745463 acc:  0.2691423084652658\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  3.855647508134233 acc:  0.2763995266060782\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  3.7233513010309096 acc:  0.27488109327200055\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  3.7441188223818513 acc:  0.28041890896098964\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  3.7895849714887904 acc:  0.27374226827144227\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  3.821441650390625 acc:  0.2801509501373289\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  3.9687378660161445 acc:  0.2666190295424603\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  4.035825942425018 acc:  0.2628899359131813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7999477690446972 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'ELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 96, 'dropout': 0.4916265034111731, 'dropout_StationIdEmbedding': 0.5093633260296117, 'dropout_timeStampEmbedding': 0.23955233203332024, 'dropout_transformers': 0.7369323958193752, 'early_stopping': 6, 'encoder_only': True, 'epochs_classifcation_only': 68, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.7999477690446972, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 5, 'alpha': 0.9190346505446495, 'centered': True, 'eps': 2.8605143217570725e-08, 'lr': 1.3991877761774403e-05, 'momentum': 0.14893249973402697, 'optimizer': 'RMSprop', 'weight_decay': 5.429837114510171e-07, 'positive_function': 'abs', 'epochs_complete_problem': 37, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.055588368769293 acc:  0.0004912578433780676\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.015623125996623 acc:  0.0007368867650671013\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  7.943168026584011 acc:  0.0007368867650671013\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  7.803447499975458 acc:  0.0007592166670388317\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  7.660284559209864 acc:  0.001027175490699596\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  7.558984686444689 acc:  0.0013174642163320902\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  7.493149317227877 acc:  0.0016970725498515061\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  7.444392027554812 acc:  0.0018980416675970792\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  7.408065025623028 acc:  0.0022999799030882255\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  7.376175997140524 acc:  0.0024339593149186075\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  7.349439864392047 acc:  0.002657258334635911\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  7.326759031602553 acc:  0.0029475470602684053\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  7.308372921043342 acc:  0.0030368666681553267\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  7.285624537434611 acc:  0.0036174441194203157\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  7.286596614997704 acc:  0.004376660786459147\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  7.269393917563912 acc:  0.004644619610119911\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  7.259697097164768 acc:  0.004756269119978563\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  7.251459014999283 acc:  0.004823258825893754\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  7.241925386282114 acc:  0.004934908335752406\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  7.244145956906405 acc:  0.004912578433780675\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  7.231064069521177 acc:  0.004912578433780675\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  7.232057938208947 acc:  0.0048455887278654845\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  7.224491236093161 acc:  0.00455530000223299\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  7.2257289119533725 acc:  0.004242681374628765\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  7.223019943370685 acc:  0.004332000982515687\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Hardswish', 'activation_transformers': 'Tanh', 'batch_size': 32, 'concatenate_features': False, 'd_model': 240, 'dropout': 0.3565262322345777, 'dropout_StationIdEmbedding': 0.5408675033441696, 'dropout_timeStampEmbedding': 0.1434803658242147, 'dropout_transformers': 0.5576083230456509, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 43, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 2, 'lr': 0.00016479515113953583, 'momentum': 0.06016097756926758, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 0.06382469242115874, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.075600062575296 acc:  0.00042426813746287653\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.075218646325798 acc:  0.00042426813746287653\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6726020993135349 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'ELU', 'activation_transformers': 'LogSigmoid', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1128, 'dropout': 0.869414927497665, 'dropout_StationIdEmbedding': 0.23601656858465375, 'dropout_timeStampEmbedding': 0.0032633030339148383, 'dropout_transformers': 0.05808652889000432, 'early_stopping': 2, 'encoder_only': False, 'epochs_classifcation_only': 24, 'input_size': 2, 'learnable_pos_encoding': False, 'gamma': 0.5933965392884422, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.6726020993135349, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.8009660756300333, 'beta_2': 0.985718006037852, 'eps': 2.5686189920203655e-09, 'lr': 6.894282909620469e-05, 'optimizer': 'AdamW', 'weight_decay': 0.006158378750125147, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'SiLU', 'dropout_gcn': 0.8741515301478306, 'hidden_channels': 1024, 'layer_type': 'GAT', 'norm': 'InstanceNorm', 'num_layers_gcn': 10, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  7.697440791676063 acc:  0.006743630395462564\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.235400181690245 acc:  0.07857892503851908\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  6.841804406115117 acc:  0.12017953241185271\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.8865671310543172 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'CELU', 'activation_transformers': 'SELU', 'batch_size': 16, 'concatenate_features': True, 'd_model': 1368, 'dropout': 0.6402435004726911, 'dropout_StationIdEmbedding': 0.26657991713671453, 'dropout_timeStampEmbedding': 0.2171513140168797, 'dropout_transformers': 0.13938663616720098, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 79, 'input_size': 2, 'learnable_pos_encoding': True, 'cooldown': 4, 'factor': 0.12105520029752503, 'patience': 10, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.00036144919635293046, 'dropout_lstm': 0.8865671310543172, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': True, 'beta_1': 0.9416612409919667, 'beta_2': 0.9646237783716565, 'eps': 2.1929131358148982e-08, 'lr': 4.240603395805862e-05, 'optimizer': 'AdamW', 'weight_decay': 2.1799650013456092e-06, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  7.888871428841039 acc:  0.01230377598642342\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.319431360144364 acc:  0.0395909161958779\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  6.933335655613949 acc:  0.09034678337762098\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  6.540802910453395 acc:  0.13328718486925842\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  6.156086111068726 acc:  0.16606748096375856\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  5.741147774144223 acc:  0.19239443538842865\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  5.466975786811427 acc:  0.215550543733113\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  5.266258761757299 acc:  0.2374785074693522\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  4.961720318543284 acc:  0.2545385525757542\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  4.923160309540598 acc:  0.27260344327088404\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  4.721554756164551 acc:  0.286849920728848\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  4.574913986105668 acc:  0.2975236138713351\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  4.473066159298545 acc:  0.30205658397159635\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  4.354498506847181 acc:  0.3058303374048188\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  4.365443954969708 acc:  0.30926914230846525\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  4.342020438846789 acc:  0.31125650358394924\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  4.318356850272731 acc:  0.31159145211352524\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  4.397396822979576 acc:  0.31337784427126364\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  4.216263535148219 acc:  0.31485161780139787\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  4.36044182024504 acc:  0.31719625750842956\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  4.29108879942643 acc:  0.3176875153518076\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  4.131228572443912 acc:  0.31862537123462026\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  4.310887833645469 acc:  0.3197195364312351\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  4.154408362037257 acc:  0.3204117633923587\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  4.154315712577418 acc:  0.3215952481968604\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  25 loss :  4.216529509895726 acc:  0.32273407319741865\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  26 loss :  4.26050601507488 acc:  0.32449813545318534\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  27 loss :  4.264824287514937 acc:  0.3244534756492419\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  28 loss :  4.2088831148649515 acc:  0.3243194962374115\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  29 loss :  4.1819335862209925 acc:  0.3245204653551571\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  30 loss :  4.071342139495046 acc:  0.32483308398276134\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  31 loss :  3.9886120520139996 acc:  0.3249670633945917\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  32 loss :  4.190494424418399 acc:  0.3252796820221959\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  33 loss :  4.0555835799167035 acc:  0.32536900163008287\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  34 loss :  4.044993458296124 acc:  0.32523502221825246\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  35 loss :  4.185111643138685 acc:  0.3253913315320546\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  36 loss :  4.155032536858006 acc:  0.3257486099636023\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  37 loss :  4.143062912790399 acc:  0.3257486099636023\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  38 loss :  4.2099103927612305 acc:  0.3261058883951499\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  39 loss :  4.2527567512110656 acc:  0.3259049192774044\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  40 loss :  4.153973662225824 acc:  0.32603889868923475\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  41 loss :  4.205052413438496 acc:  0.326351517316839\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  42 loss :  4.103642556541844 acc:  0.32632918741486727\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  43 loss :  4.139974594116211 acc:  0.32632918741486727\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  44 loss :  4.0266000973550895 acc:  0.3263738472188107\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  45 loss :  4.160906716396934 acc:  0.3264408369247259\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  46 loss :  4.0414808925829435 acc:  0.3263068575128955\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  47 loss :  4.263396647101954 acc:  0.32641850702275416\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6530071716302206 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Softplus', 'activation_transformers': 'Hardswish', 'batch_size': 128, 'concatenate_features': True, 'd_model': 840, 'dropout': 0.07991189862618484, 'dropout_StationIdEmbedding': 0.3012024150538483, 'dropout_timeStampEmbedding': 0.365765290576924, 'dropout_transformers': 0.3641449571799821, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 47, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.5149354217727644, 'scheduler': 'StepLR', 'step_size': 14, 'dropout_lstm': 0.6530071716302206, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'Hardshrink', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 5, 'max_len': 100, 'nb_batchs': 60, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8958303830087536, 'beta_2': 0.9567207186565265, 'eps': 2.4698628288045266e-07, 'lr': 5.945264842115342e-06, 'optimizer': 'AdamW', 'weight_decay': 1.1846883967484644e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.395894826468774 acc:  0.0008708661768974834\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.076363054372496 acc:  0.00390773284505281\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  7.799881943201615 acc:  0.01239309559431034\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  7.57802891327163 acc:  0.024987160306366257\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  7.383161189192433 acc:  0.037960833351941586\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  7.18906417135465 acc:  0.052117991202018626\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  7.046799627401061 acc:  0.0687984279749012\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  6.884860717644126 acc:  0.08579148337538799\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  6.770003480426336 acc:  0.0988321461268785\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  6.629430803201966 acc:  0.11162717995667999\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  6.466703641212593 acc:  0.12301542996226246\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  6.349244521836103 acc:  0.13509590692896858\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  6.2400361400539595 acc:  0.1475559922291941\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  6.145626553034378 acc:  0.15977044860773063\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  6.007125151359428 acc:  0.16716164616037335\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  5.964721259424242 acc:  0.17453051381104437\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  5.81174903804973 acc:  0.17988969028425966\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  5.7729547145002975 acc:  0.18819641381774335\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  5.721742120839782 acc:  0.19355559029095862\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  5.662115484981213 acc:  0.20036621039233637\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  5.596541404724121 acc:  0.2064176138266753\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  5.517854836027501 acc:  0.21244668735904249\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  5.452577566696426 acc:  0.21887769912690083\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  5.3452461856906694 acc:  0.2228300917758971\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  5.3179674471839 acc:  0.22910479422995333\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  25 loss :  5.232471692360054 acc:  0.23421834178147957\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  26 loss :  5.161088401988401 acc:  0.23897461090145813\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  27 loss :  5.113589804051286 acc:  0.2414532300203202\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  28 loss :  5.089410022153693 acc:  0.2452716432574861\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  29 loss :  5.083899174706411 acc:  0.24833083982761317\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  30 loss :  5.044931322841321 acc:  0.2499832525735212\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  31 loss :  5.032787573539604 acc:  0.2517919746332314\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  32 loss :  4.955861180515613 acc:  0.25400263492843267\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  33 loss :  4.987363475864217 acc:  0.25657057365518166\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  34 loss :  4.906226384437691 acc:  0.259026862872072\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  35 loss :  4.906835749997931 acc:  0.260433646696291\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  36 loss :  4.867233106645487 acc:  0.2619520800303687\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  37 loss :  4.886245929588706 acc:  0.2637384721881071\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  38 loss :  4.8407754493972 acc:  0.2664180604247147\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  39 loss :  4.835165629952641 acc:  0.26731125650358395\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  40 loss :  4.769801766185437 acc:  0.26871804032780294\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  41 loss :  4.7404976618492 acc:  0.27141995846638234\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  42 loss :  4.742968696658894 acc:  0.2724694638590537\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  43 loss :  4.77638474157301 acc:  0.2728937319965165\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  44 loss :  4.742511830087436 acc:  0.2746577942522832\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  45 loss :  4.705596657122596 acc:  0.275171381997633\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  46 loss :  4.709491794392214 acc:  0.27642185650804996\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5716689527608813 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'RReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1200, 'dropout': 0.24879735946171516, 'dropout_StationIdEmbedding': 0.8680940493691933, 'dropout_timeStampEmbedding': 0.18624064977254484, 'dropout_transformers': 0.6641887724303077, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 72, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.5716689527608813, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 84, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 4, 'amsgrad': False, 'beta_1': 0.818161808879219, 'beta_2': 0.9665781557047406, 'eps': 4.982400883965902e-08, 'lr': 0.0004017445004897519, 'optimizer': 'Adam', 'weight_decay': 8.868956217206096e-07, 'positive_function': 'relu', 'epochs_complete_problem': 6, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  7.478825368076922 acc:  0.008753321572918294\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.154862288969109 acc:  0.0345890181542103\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  6.599725424525249 acc:  0.10662528191501239\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  6.031697261764343 acc:  0.15436661233057186\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  5.546584858951799 acc:  0.1957439206841882\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  5.115219541342862 acc:  0.23529017707612263\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  4.834146005561553 acc:  0.2555657280664538\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  4.5838456010243975 acc:  0.2761985574883326\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  4.368814543069127 acc:  0.28977513788714465\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  4.213315394987543 acc:  0.2989973874014693\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  4.045381324837007 acc:  0.3175312060380055\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  3.93970692301371 acc:  0.32273407319741865\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  3.797778497259301 acc:  0.33501551928187034\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  3.6629366012940925 acc:  0.3425406962463435\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  3.6193883849913817 acc:  0.3489270482102584\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  3.521318136927593 acc:  0.3495522854654668\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  3.466646487454334 acc:  0.35625125605698593\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  3.435248593249953 acc:  0.35998034968626486\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  3.440981198506183 acc:  0.36703659870933164\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  3.3144096782408563 acc:  0.36759484625862493\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  3.3506642134792832 acc:  0.36795212469017263\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  3.2727413120039976 acc:  0.3699841457696001\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  3.204956594719944 acc:  0.37496371390929595\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  3.14355402682201 acc:  0.37771029185181876\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  3.1947034956460976 acc:  0.3790500859701226\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  25 loss :  3.1596555106611137 acc:  0.37793359087153605\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  26 loss :  3.132223040224558 acc:  0.3809481276377197\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  27 loss :  3.130234726940293 acc:  0.38195297322644756\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  28 loss :  3.041358416339001 acc:  0.38161802469687156\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  29 loss :  3.021183642996363 acc:  0.38409664381573366\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  30 loss :  2.9940829908991433 acc:  0.38467722126699866\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  31 loss :  2.9922835884324037 acc:  0.38724515999374765\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  32 loss :  2.9954403394676117 acc:  0.3901703771520443\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  33 loss :  2.932603540190731 acc:  0.3908626041131679\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  34 loss :  2.9529919509428093 acc:  0.3895228099948641\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  35 loss :  2.9205867870744453 acc:  0.39159949087823503\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  36 loss :  2.933302732835333 acc:  0.39077328450528104\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  37 loss :  2.9069572069558753 acc:  0.39191210950583927\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  38 loss :  2.832436555839447 acc:  0.3920460889176697\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  39 loss :  2.851975538644446 acc:  0.3930286046044258\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  40 loss :  2.818880856755268 acc:  0.39251501685907597\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  41 loss :  2.8378003993666314 acc:  0.3916664805841502\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  42 loss :  2.820403799953231 acc:  0.3938548109773798\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  43 loss :  2.7891463141843498 acc:  0.3924256972511891\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  44 loss :  2.8506736410669533 acc:  0.39508295558582496\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  45 loss :  2.8034143620226755 acc:  0.39483732666413596\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  46 loss :  2.786918433315783 acc:  0.394167429604984\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  47 loss :  2.7642616593694114 acc:  0.39470334725230555\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  48 loss :  2.7578632055994974 acc:  0.39727128597905453\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  49 loss :  2.714644305677299 acc:  0.39758390460665877\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  50 loss :  2.7455669052629585 acc:  0.39506062568385325\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  51 loss :  2.711237999330084 acc:  0.3987003997052453\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  52 loss :  2.672905864485775 acc:  0.3944130585266731\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  53 loss :  2.6871443139501365 acc:  0.39794118303820647\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  54 loss :  2.679396749979042 acc:  0.3975169149007436\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'SELU', 'activation_transformers': 'Hardshrink', 'batch_size': 128, 'concatenate_features': False, 'd_model': 384, 'dropout': 0.03149136586843543, 'dropout_StationIdEmbedding': 0.15861903802321128, 'dropout_timeStampEmbedding': 0.0922427553815913, 'dropout_transformers': 0.31007141622070783, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 1, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 25, 'eta_min': 0.1360615059611034, 'scheduler': 'CosineAnnealingLR', 'lstm_model': False, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'alpha': 0.9016259578112159, 'centered': True, 'eps': 1.7128503423118e-07, 'lr': 7.615727857893126e-07, 'momentum': 0.4261493850531214, 'optimizer': 'RMSprop', 'weight_decay': 0.0022328080173205315, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.179859300760123 acc:  0.0004465980394346069\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m GraphSAGE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7276233255277926 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'ReLU', 'activation_transformers': 'ReLU', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1416, 'dropout': 0.22228639024716934, 'dropout_StationIdEmbedding': 0.10971197929876407, 'dropout_timeStampEmbedding': 0.3222707154597041, 'dropout_transformers': 0.39271621728371553, 'early_stopping': 1, 'encoder_only': False, 'epochs_classifcation_only': 52, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'dropout_lstm': 0.7276233255277926, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 12, 'num_layers_transformer': 1, 'lr': 0.033617422096279076, 'momentum': 0.19225390445007326, 'nesterov': True, 'optimizer': 'SGD', 'weight_decay': 8.263987570378271e-07, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softplus', 'dropout_gcn': 0.07404274949465145, 'hidden_channels': 2048, 'layer_type': 'GraphSAGE', 'norm': 'LayerNorm', 'num_layers_gcn': 9, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.02365252786054 acc:  0.005001898041667597\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.7146798293747585 acc:  0.003438804903646473\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3510214778367106 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Sigmoid', 'activation_transformers': 'Hardtanh', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1296, 'dropout': 0.11903135394656124, 'dropout_StationIdEmbedding': 0.0878556598123226, 'dropout_timeStampEmbedding': 0.6977913756529432, 'dropout_transformers': 0.4441800721114561, 'early_stopping': 9, 'encoder_only': True, 'epochs_classifcation_only': 74, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 0.0023825539308145183, 'max_lr': 0.2417778788122593, 'mode': 'triangular', 'scheduler': 'CyclicLR', 'step_size_up': 8, 'dropout_lstm': 0.3510214778367106, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 4, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 24, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8670226986493064, 'beta_2': 0.971778584754757, 'eps': 1.7233729079557205e-07, 'lr': 0.00015359095307671152, 'optimizer': 'AdamW', 'weight_decay': 6.255719186284025e-08, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  7.204725081179323 acc:  0.10642431279726682\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.23974620679000014 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'PReLU', 'activation_transformers': 'LeakyReLU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1152, 'dropout': 0.00020035101392942845, 'dropout_StationIdEmbedding': 0.03822377555890902, 'dropout_timeStampEmbedding': 0.9945591160555007, 'dropout_transformers': 0.11062839502517029, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 34, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.23974620679000014, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.889898900589533, 'beta_2': 0.9614086272391106, 'eps': 1.9514414505961705e-06, 'lr': 3.386664318693659e-06, 'optimizer': 'AdamW', 'weight_decay': 3.602725746465123e-05, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  12.310759176755084 acc:  0.0007592166670388317\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  12.171656406125543 acc:  0.0011611549025299778\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  12.041409087580675 acc:  0.001630082843936315\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  11.896340945579487 acc:  0.0021213406873143827\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  11.763379070345916 acc:  0.0025009490208337984\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  11.647745793092184 acc:  0.0030815264720987874\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  11.54173001230762 acc:  0.0036621039233637764\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  11.417050628022775 acc:  0.004153361766741844\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  11.317675606498506 acc:  0.0049572382377241365\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  11.225316276763404 acc:  0.006029073532367193\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  11.124488649421565 acc:  0.006877609807292946\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  11.069531456718233 acc:  0.007569836768416586\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  10.991354670604514 acc:  0.008664001965031374\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  10.9136723406488 acc:  0.009356228926155015\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  10.862026092060452 acc:  0.010160105397137307\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  10.766172611513618 acc:  0.011432909809525936\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  16 loss :  10.737626406067577 acc:  0.012482415202197263\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  17 loss :  10.685361036375248 acc:  0.013866869124444544\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  18 loss :  10.648367658007745 acc:  0.015251323046691825\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  19 loss :  10.560888684661695 acc:  0.016658106870910835\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  20 loss :  10.532568169705694 acc:  0.01824352991090369\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  21 loss :  10.465247894798576 acc:  0.019806623048924814\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  22 loss :  10.387484449248074 acc:  0.02201728334412612\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  23 loss :  10.328211725757109 acc:  0.023558046580175514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  24 loss :  10.301717321299973 acc:  0.02588035638523547\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  25 loss :  10.232382683780607 acc:  0.02871625393564522\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  26 loss :  10.198892870428843 acc:  0.03188710001563093\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  27 loss :  10.14201649074448 acc:  0.03396378089900185\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  28 loss :  10.083801940832725 acc:  0.03713462697898756\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  29 loss :  10.040347978389462 acc:  0.04102002992206864\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  30 loss :  9.960502017143718 acc:  0.04398990688430878\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  31 loss :  9.93475238437759 acc:  0.04791996963133332\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  32 loss :  9.87001599679446 acc:  0.05102382600540384\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  33 loss :  9.819377377046553 acc:  0.05457428041890896\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5984912469305215 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Hardtanh', 'activation_transformers': 'Softplus', 'batch_size': 64, 'concatenate_features': True, 'd_model': 792, 'dropout': 0.554402115029295, 'dropout_StationIdEmbedding': 0.9708884108804527, 'dropout_timeStampEmbedding': 0.5518884414527024, 'dropout_transformers': 0.15775173013744181, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 29, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.7436442480648722, 'scheduler': 'ExponentialLR', 'dropout_lstm': 0.5984912469305215, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'RReLU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': True, 'beta_1': 0.8556593759718351, 'beta_2': 0.9500484337849381, 'eps': 6.627517235019971e-09, 'lr': 2.9000965635502274e-05, 'optimizer': 'AdamW', 'weight_decay': 0.0003587457439928636, 'positive_function': 'exp', 'epochs_complete_problem': 45, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  7.701443038620315 acc:  0.002679588236607641\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  7.361327554796126 acc:  0.004086372060826653\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  7.292015862631631 acc:  0.004108701962798384\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  7.269559143306492 acc:  0.004666949512091642\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  7.2588652764166985 acc:  0.004309671080543956\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  7.262927138722026 acc:  0.004421320590402608\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Hardsigmoid', 'activation_transformers': 'Hardsigmoid', 'batch_size': 128, 'concatenate_features': False, 'd_model': 24, 'dropout': 0.5902320465531741, 'dropout_StationIdEmbedding': 0.00018276572767558785, 'dropout_timeStampEmbedding': 0.7468873506630933, 'dropout_transformers': 0.1898212356144767, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 55, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 132, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.9111107450965328, 'beta_2': 0.9944199895378764, 'eps': 1.5568698597669735e-08, 'lr': 1.7533979987754298e-06, 'optimizer': 'AdamW', 'weight_decay': 2.1083346392857033e-05, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.07105781848614 acc:  0.00013397941183038205\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.068403574136587 acc:  0.0001563093138021124\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  8.066063154660739 acc:  0.00017863921577384274\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  8.063596762143648 acc:  0.00022329901971730344\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  8.060939803490271 acc:  0.0002456289216890338\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  8.058512130150428 acc:  0.0002679588236607641\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  8.055475891553439 acc:  0.0002456289216890338\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  8.05338186117319 acc:  0.0002679588236607641\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  8 loss :  8.050981925084042 acc:  0.00029028872563249445\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  9 loss :  8.048899008677555 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  10 loss :  8.046702487652118 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  11 loss :  8.044802632698646 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  12 loss :  8.042721326534565 acc:  0.0003572784315476855\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  13 loss :  8.040656984769381 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  14 loss :  8.038812523621779 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  15 loss :  8.036236271491418 acc:  0.00033494852957595514\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m GAT\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.41775579499746784 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'SiLU', 'activation_transformers': 'Hardswish', 'batch_size': 32, 'concatenate_features': True, 'd_model': 1008, 'dropout': 0.2801632291540205, 'dropout_StationIdEmbedding': 0.4591933754349636, 'dropout_timeStampEmbedding': 0.2643943131049443, 'dropout_transformers': 0.5806967525231004, 'early_stopping': 6, 'encoder_only': False, 'epochs_classifcation_only': 60, 'input_size': 2, 'learnable_pos_encoding': False, 'cooldown': 1, 'factor': 0.36347237537102384, 'patience': 7, 'scheduler': 'ReduceLROnPlateau', 'threshold': 0.08994590584201007, 'dropout_lstm': 0.41775579499746784, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 108, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 2, 'alpha': 0.9588217578087365, 'centered': False, 'eps': 2.206698910102262e-09, 'lr': 0.04569972124520867, 'momentum': 0.27889369622408344, 'optimizer': 'RMSprop', 'weight_decay': 7.545111873315641e-05, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'Softsign', 'dropout_gcn': 0.7258942541676093, 'hidden_channels': 128, 'layer_type': 'GAT', 'norm': 'GraphNorm', 'num_layers_gcn': 1, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6329342251839823 and num_layers=1\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=325129)\u001b[0m {'activation': 'Tanhshrink', 'activation_transformers': 'Mish', 'batch_size': 64, 'concatenate_features': True, 'd_model': 1368, 'dropout': 0.31169826214676205, 'dropout_StationIdEmbedding': 0.13160859652748813, 'dropout_timeStampEmbedding': 0.29653531065964284, 'dropout_transformers': 0.27556631911990104, 'early_stopping': 1, 'encoder_only': True, 'epochs_classifcation_only': 77, 'input_size': 2, 'learnable_pos_encoding': True, 'gamma': 0.0011398799035657325, 'scheduler': 'StepLR', 'step_size': 7, 'dropout_lstm': 0.6329342251839823, 'lstm_layer_with_layer_norm': True, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 180, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 3, 'num_layers_transformer': 2, 'lr': 0.0017645426016499638, 'momentum': 0.4264976557791812, 'nesterov': False, 'optimizer': 'SGD', 'weight_decay': 1.178830112551214e-08, 'positive_function': 'relu', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  0 loss :  8.261014330986491 acc:  0.0004019382354911462\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  1 loss :  8.230989541421389 acc:  0.0007368867650671013\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  2 loss :  8.20589766688853 acc:  0.0013174642163320902\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  3 loss :  8.174270304887655 acc:  0.0023223098050599556\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  4 loss :  8.14517664243389 acc:  0.003930062747024541\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  5 loss :  8.109410544347497 acc:  0.006230042650112766\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  6 loss :  8.082139446748702 acc:  0.010539713730656722\n",
            "\u001b[36m(eval_config pid=325129)\u001b[0m epoch:  7 loss :  8.062283641133229 acc:  0.010539713730656722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-07 14:01:59,436\tWARNING experiment_state.py:323 -- Experiment checkpoint syncing has been triggered multiple times in the last 30.0 seconds. A sync will be triggered whenever a trial has checkpointed more than `num_to_keep` times since last sync or if 300 seconds have passed since last sync. If you have set `num_to_keep` in your `CheckpointConfig`, consider increasing the checkpoint frequency or keeping more checkpoints. You can supress this warning by changing the `TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S` environment variable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-48-a5533dccde58>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "2024-03-07 14:02:26,262\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-07 14:02:40,934\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-07 14:02:40,935\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     xp_num_7        |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 20              |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/tuning/xp_num_7\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/xp_num_7`\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m   warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.47297946496673576 and num_layers=1\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m {'activation': 'Tanh', 'activation_transformers': 'Tanh', 'batch_size': 128, 'concatenate_features': True, 'd_model': 960, 'dropout': 0.45725375589169126, 'dropout_StationIdEmbedding': 0.019039350643153267, 'dropout_timeStampEmbedding': 0.8800060290461524, 'dropout_transformers': 0.07657673102719267, 'early_stopping': 4, 'encoder_only': True, 'epochs_classifcation_only': 80, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.47297946496673576, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 6, 'max_len': 100, 'nb_batchs': 120, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 6, 'num_layers_transformer': 5, 'amsgrad': False, 'beta_1': 0.8357575159138049, 'beta_2': 0.9638104898314652, 'eps': 8.467320827795803e-09, 'lr': 0.0005690481895893917, 'optimizer': 'AdamW', 'weight_decay': 0.00011981508075403076, 'positive_function': 'abs', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  0 loss :  6.723028375321076 acc:  0.23301252707500614\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  1 loss :  3.9705037830256615 acc:  0.35002121340687314\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  2 loss :  3.0944251813808408 acc:  0.3811044369515218\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  3 loss :  2.7828931267521964 acc:  0.4011343590201639\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  4 loss :  2.6095740755065147 acc:  0.41676529040037513\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  5 loss :  2.496331273006792 acc:  0.4221244668735904\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  6 loss :  2.3693840684009198 acc:  0.4270370453073711\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  7 loss :  2.2955926997320995 acc:  0.4277739320724382\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  8 loss :  2.2066391816660134 acc:  0.43014090168144165\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  9 loss :  2.128504810213041 acc:  0.43170399481946276\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  10 loss :  2.0638400115886655 acc:  0.4310340977603108\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  11 loss :  1.9991301548581164 acc:  0.4283098497197597\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  12 loss :  1.9453760004844987 acc:  0.4302525511913003\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  13 loss :  1.8811956533864767 acc:  0.4321952526628408\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  14 loss :  1.826164040244928 acc:  0.42913605609271377\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  15 loss :  1.7842054156696094 acc:  0.42692539579751243\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5106880460589658 and num_layers=1\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m {'activation': 'GELU', 'activation_transformers': 'Softshrink', 'batch_size': 64, 'concatenate_features': True, 'd_model': 888, 'dropout': 0.519654137315249, 'dropout_StationIdEmbedding': 0.07186650563184206, 'dropout_timeStampEmbedding': 0.16745292021889577, 'dropout_transformers': 0.4974269998477895, 'early_stopping': 5, 'encoder_only': True, 'epochs_classifcation_only': 68, 'input_size': 2, 'learnable_pos_encoding': True, 'T_max': 9, 'eta_min': 0.013620024809027471, 'scheduler': 'CosineAnnealingLR', 'dropout_lstm': 0.5106880460589658, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 24, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 1, 'amsgrad': False, 'beta_1': 0.9223522397587934, 'beta_2': 0.9588926512353584, 'eps': 4.1642074338271694e-08, 'lr': 0.0002467082635844658, 'optimizer': 'Adam', 'weight_decay': 6.5846585921587555e-06, 'positive_function': 'exp', 'epochs_complete_problem': 19, 'reg': True, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  0 loss :  7.907570320626964 acc:  0.0064533416698300695\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  1 loss :  7.293242205744204 acc:  0.05761114708706429\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  2 loss :  6.6332628623298975 acc:  0.14958801330862156\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  3 loss :  5.773975890615712 acc:  0.1751557510662528\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  4 loss :  5.266205207161281 acc:  0.18879932117098006\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  5 loss :  4.909868344016697 acc:  0.1979545809793895\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  6 loss :  4.880870383718739 acc:  0.18837505303351718\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  7 loss :  4.5835496239040205 acc:  0.19389053882053459\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  8 loss :  4.586005304170691 acc:  0.19880311725431526\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  9 loss :  4.595715833746868 acc:  0.18888864077886697\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  10 loss :  4.496070571567701 acc:  0.19214880646673962\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  11 loss :  4.289145065390545 acc:  0.18681195989549607\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  12 loss :  4.168992726699166 acc:  0.18759350646450662\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  13 loss :  4.160383753154589 acc:  0.20056717951008196\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  14 loss :  3.8710368094236953 acc:  0.2076680883370922\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  15 loss :  3.8197137998498003 acc:  0.2192126476564768\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  16 loss :  3.5724744693092676 acc:  0.22667083491503473\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  17 loss :  3.6008027636486553 acc:  0.23441931089922516\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  18 loss :  3.4698829028917397 acc:  0.2382823839403345\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  19 loss :  3.454410729200944 acc:  0.24451242659044728\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  20 loss :  3.5538847653762153 acc:  0.24127459080454636\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  21 loss :  3.5225115444349204 acc:  0.23042225844628542\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  22 loss :  3.549601585968681 acc:  0.22198155550097134\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  23 loss :  3.6395280672156294 acc:  0.20219726235401828\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  24 loss :  3.7538525436235513 acc:  0.1967710961748878\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m {'activation': 'Mish', 'activation_transformers': 'Softmin', 'batch_size': 32, 'concatenate_features': False, 'd_model': 1224, 'dropout': 0.10375445633112801, 'dropout_StationIdEmbedding': 0.2162202583477274, 'dropout_timeStampEmbedding': 0.12507717185144088, 'dropout_transformers': 0.5177438239980385, 'early_stopping': 2, 'encoder_only': True, 'epochs_classifcation_only': 19, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'lstm_model': False, 'max_len': 100, 'nb_batchs': 96, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 24, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.8252846138933905, 'beta_2': 0.9677885247286593, 'eps': 2.8453859938423464e-08, 'lr': 8.130473767265106e-06, 'optimizer': 'AdamW', 'weight_decay': 1.0836672927347303e-07, 'positive_function': 'sig', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmin has been deprecated. Change the call to include dim=X as an argument.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m   return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  0 loss :  8.05355120207134 acc:  0.0005359176473215282\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  1 loss :  7.944669362118369 acc:  0.0013397941183038206\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  2 loss :  7.851455723611932 acc:  0.002545608824777259\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  3 loss :  7.7583009719848635 acc:  0.004064042158854923\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  4 loss :  7.6610903237995345 acc:  0.007078578925038519\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  5 loss :  7.550861900731137 acc:  0.009333899024183284\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  6 loss :  7.486959261643259 acc:  0.010227095103052497\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  7 loss :  7.393215184462698 acc:  0.010673693142487105\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  8 loss :  7.34486003173025 acc:  0.012929013241631869\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  9 loss :  7.299941479532342 acc:  0.01491637451711587\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  10 loss :  7.2638982321086685 acc:  0.017640622557666973\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  11 loss :  7.240370554673044 acc:  0.01875711765625349\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  12 loss :  7.210776800858347 acc:  0.02139204608891767\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  13 loss :  7.201557410390754 acc:  0.026505593640443918\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  14 loss :  7.17464852082102 acc:  0.02909586226916464\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  15 loss :  7.134135100716039 acc:  0.03369582207534109\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  16 loss :  7.132151468176591 acc:  0.0366433691356095\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  17 loss :  7.1070913415206105 acc:  0.042516133354174576\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  18 loss :  7.087165576533267 acc:  0.045642319630216824\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m GCNConv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.7663443880786416 and num_layers=1\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m {'activation': 'Softsign', 'activation_transformers': 'GELU', 'batch_size': 64, 'concatenate_features': True, 'd_model': 504, 'dropout': 0.1611289472808294, 'dropout_StationIdEmbedding': 0.1905259635966413, 'dropout_timeStampEmbedding': 0.0742556468227499, 'dropout_transformers': 0.7958061391173161, 'early_stopping': 10, 'encoder_only': False, 'epochs_classifcation_only': 46, 'input_size': 2, 'learnable_pos_encoding': False, 'scheduler': None, 'dropout_lstm': 0.7663443880786416, 'lstm_layer_with_layer_norm': False, 'activation_lstm': 'GELU', 'lstm_layer_with_perceptron': True, 'lstm_model': True, 'num_layers_lstm': 3, 'max_len': 100, 'nb_batchs': 156, 'nb_of_pos_ids': 3043, 'normalize_features': 'before', 'num_heads': 12, 'num_layers_transformer': 2, 'amsgrad': False, 'beta_1': 0.8414817558186823, 'beta_2': 0.9555569901737565, 'eps': 1.134557639865927e-08, 'lr': 0.052657538045344315, 'optimizer': 'AdamW', 'weight_decay': 2.556462947812415e-07, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'activation_gcn': 'SELU', 'dropout_gcn': 0.2034016205880249, 'hidden_channels': 256, 'layer_type': 'GCNConv', 'norm': 'InstanceNorm', 'num_layers_gcn': 4, 'use_gcn': True}\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3771393692002015 and num_layers=1\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m {'activation': 'LogSigmoid', 'activation_transformers': 'Sigmoid', 'batch_size': 128, 'concatenate_features': True, 'd_model': 1320, 'dropout': 0.051143762192593006, 'dropout_StationIdEmbedding': 0.37233317315623277, 'dropout_timeStampEmbedding': 0.34507038426161707, 'dropout_transformers': 0.41721749831833316, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 66, 'input_size': 2, 'learnable_pos_encoding': True, 'base_lr': 0.04016026631212392, 'max_lr': 0.16261453880976653, 'mode': 'triangular2', 'scheduler': 'CyclicLR', 'step_size_up': 11, 'dropout_lstm': 0.3771393692002015, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 168, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 4, 'alpha': 0.9316564273067711, 'centered': False, 'eps': 3.5253345000905124e-06, 'lr': 8.264875019620823e-05, 'momentum': 0.2079035196341558, 'optimizer': 'RMSprop', 'weight_decay': 2.2636268490910553e-09, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m loss is undifined\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m /usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.04863489924196046 and num_layers=1\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m   warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(eval_config pid=351644)\u001b[0m {'activation': 'RReLU', 'activation_transformers': 'Tanhshrink', 'batch_size': 128, 'concatenate_features': True, 'd_model': 744, 'dropout': 0.18397777982436023, 'dropout_StationIdEmbedding': 0.14949347815709124, 'dropout_timeStampEmbedding': 0.15176209383182665, 'dropout_transformers': 0.3343336144074971, 'early_stopping': 3, 'encoder_only': True, 'epochs_classifcation_only': 70, 'input_size': 2, 'learnable_pos_encoding': True, 'scheduler': None, 'dropout_lstm': 0.04863489924196046, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 1, 'max_len': 100, 'nb_batchs': 144, 'nb_of_pos_ids': 3043, 'normalize_features': None, 'num_heads': 3, 'num_layers_transformer': 3, 'amsgrad': False, 'beta_1': 0.9665370508517013, 'beta_2': 0.9575836482210145, 'eps': 3.5296129260855e-07, 'lr': 1.2253304990255833e-05, 'optimizer': 'AdamW', 'weight_decay': 1.4896539037074301e-06, 'positive_function': 'exp', 'reg': False, 'transformers_model': True, 'use_gcn': False}\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m CUDA is available. Using GPU.\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  0 loss :  8.089272557772123 acc:  0.002277650001116495\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  1 loss :  7.710772749093863 acc:  0.006564991179688721\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  2 loss :  7.41291251549354 acc:  0.011455239711497667\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  3 loss :  7.201604975186862 acc:  0.02197262354018266\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  4 loss :  7.033450794219971 acc:  0.039992854431369046\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  5 loss :  6.862494956530058 acc:  0.0610052921867673\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  6 loss :  6.675756483811599 acc:  0.08503226670834915\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  7 loss :  6.489254958813007 acc:  0.10908157113190273\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  8 loss :  6.292531028160682 acc:  0.13054060692673558\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  9 loss :  6.1079884859231806 acc:  0.1514413951722752\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  10 loss :  5.933213883179885 acc:  0.17093539959359577\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  11 loss :  5.756353469995352 acc:  0.18982649666167967\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  12 loss :  5.593198273732112 acc:  0.206774892258223\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  13 loss :  5.4394403897798975 acc:  0.21961458589196794\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  14 loss :  5.282137276576115 acc:  0.23144943393698503\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  15 loss :  5.142485468204205 acc:  0.2418774981577831\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  16 loss :  5.009320259094238 acc:  0.2519706138490052\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  17 loss :  4.880906974352323 acc:  0.26032199718643234\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  18 loss :  4.763159685868484 acc:  0.27041511287765446\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  19 loss :  4.64594438626216 acc:  0.2775383516066364\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  20 loss :  4.537965103296133 acc:  0.28604604425786573\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  21 loss :  4.433954442464389 acc:  0.29446441730120804\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  22 loss :  4.343534453098591 acc:  0.3025478418149744\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  23 loss :  4.2555549786641045 acc:  0.30880021436705896\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  24 loss :  4.171270073377169 acc:  0.3153205457428042\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  25 loss :  4.093192997345557 acc:  0.3210146707455954\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  26 loss :  4.018307671180138 acc:  0.326351517316839\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  27 loss :  3.946608147254357 acc:  0.33168836388808254\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  28 loss :  3.8811045738366934 acc:  0.3353504678114463\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  29 loss :  3.8184705789272604 acc:  0.3401290668333966\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  30 loss :  3.765076129253094 acc:  0.34450572761985576\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  31 loss :  3.7006334744966947 acc:  0.3488823884063149\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  32 loss :  3.65959186737354 acc:  0.35232119330996137\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  33 loss :  3.605106289570148 acc:  0.35607261684121205\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  34 loss :  3.560711062871493 acc:  0.35951142174485856\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  35 loss :  3.513338013795706 acc:  0.3633075050800527\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  36 loss :  3.474230880003709 acc:  0.36556282517919747\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  37 loss :  3.437323893033541 acc:  0.37027443449523256\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  38 loss :  3.401331830024719 acc:  0.3727977134180381\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  39 loss :  3.360711227930509 acc:  0.3766831163611192\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  40 loss :  3.3224487121288595 acc:  0.37851416832280105\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  41 loss :  3.290991335648757 acc:  0.3817296742067302\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  42 loss :  3.2604286744044377 acc:  0.384811200678829\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  43 loss :  3.231949470593379 acc:  0.38769175803318223\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  44 loss :  3.2048762431511513 acc:  0.38983542862246834\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  45 loss :  3.1747662305831907 acc:  0.39135386195654603\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  46 loss :  3.1512229937773486 acc:  0.3934305428399169\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  47 loss :  3.1271816272002 acc:  0.39526159480159884\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  48 loss :  3.103031459221473 acc:  0.3980305026460934\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  49 loss :  3.0805426120758055 acc:  0.3992586472545386\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  50 loss :  3.058027746127202 acc:  0.40071009088270104\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  51 loss :  3.0342871134097757 acc:  0.40151396735368333\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  52 loss :  3.0113418304003203 acc:  0.4037022977469129\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  53 loss :  2.9958691101807813 acc:  0.40477413304155596\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  54 loss :  2.9769372976743256 acc:  0.40696246343478554\n",
            "\u001b[36m(eval_config pid=351644)\u001b[0m epoch:  55 loss :  2.954783588189345 acc:  0.40718576245450283\n"
          ]
        }
      ],
      "source": [
        "run_all_xp(xps_name=\"hyperparameter_tuning_projet_long\", algo=None, xp_size=20, xps_number=10, accuracy_target=0.98, max_num_epochs=None, storage_path='/content/tuning',drive_path=\"/content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN5SCJEQbdUL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the tuner from the pickle file\n",
        "with open('/content/drive/MyDrive/hyperparameter_tuning_projet_long/xp_num_9/tuner.pkl', 'rb') as f:\n",
        "    tuner = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yHiQRawJBHs"
      },
      "outputs": [],
      "source": [
        "torch.tensor([1,2,3]).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2HKIDeUJOYr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo8GkX4lh116",
        "outputId": "f0bf337d-3e2a-4469-94b8-c91fba198b7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tuner['_tune_config'].search_alg._hpopt_trials.best_trial['misc']['vals']['num_layers']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5K5mhJLjWxb",
        "outputId": "aa018970-ceb2-4401-e397-6525d68f350d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_dynamic_trials', '_exp_key', '_ids', '_insert_trial_docs', '_trials', 'aname', 'argmin', 'assert_valid_trial', 'asynchronous', 'attachments', 'average_best_error', 'best_trial', 'count_by_state_synced', 'count_by_state_unsynced', 'delete_all', 'fmin', 'idxs', 'idxs_vals', 'insert_trial_doc', 'insert_trial_docs', 'losses', 'miscs', 'new_trial_docs', 'new_trial_ids', 'refresh', 'results', 'source_trial_docs', 'specs', 'statuses', 'tids', 'trial_attachments', 'trials', 'vals', 'view']\n"
          ]
        }
      ],
      "source": [
        "\"print(dir(tuner['_tune_config'].search_alg._hpopt_trials))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXU5DJC8p169"
      },
      "source": [
        "## check performance on new station"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HClPVCKb59Zl"
      },
      "outputs": [],
      "source": [
        "def evaluate_repeat(model,dataloader,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    repeat=0\n",
        "    not_repeat=0\n",
        "    correct_not_repeat=0\n",
        "    correct_repeat=0\n",
        "    incorrect_not_repeat_as_repeat=0\n",
        "    incorrect_not_repeat=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "        pred=out[\"next_station\"].data.argmax(dim=1)\n",
        "        pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        for i in range(len(target_pos_ids.data)):\n",
        "          if target_pos_ids.data[i]==pos_ids.data[i]:\n",
        "            repeat+=1\n",
        "\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_repeat+=1\n",
        "          else:\n",
        "            not_repeat+=1\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_not_repeat+=1\n",
        "            if target_pos_ids.data[i]!=pred[i]:\n",
        "              incorrect_not_repeat+=1\n",
        "\n",
        "          if pred[i]==pos_ids.data[i] and target_pos_ids.data[i]!=pos_ids.data[i]:\n",
        "            incorrect_not_repeat_as_repeat+=1\n",
        "    print(nb_points,\"repeat: \",repeat,\" not_repeat: \",not_repeat,\" correct_repeat/repeat: \",correct_repeat/repeat,\" correct_not_repeat/not_repeat: \",correct_not_repeat/not_repeat,incorrect_not_repeat_as_repeat/incorrect_not_repeat)\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqswVm-Flq",
        "outputId": "4f8a2d24-1d1d-4d5e-da84-30957736cbd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6509191238701378  correct_not_repeat/not_repeat:  0.2762021385930769 0.4746223564954683\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=768,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=0,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufqL-c1e27Vm",
        "outputId": "5e436410-45a5-4c71-da92-f5b40a030112"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.572683570872406  correct_not_repeat/not_repeat:  0.24545712973693992 0.38929461542920074\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=12,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqLMoo9hpA82",
        "outputId": "cc34ad4e-47d5-4efd-d99e-5c06837a607e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5703307491790515  correct_not_repeat/not_repeat:  0.22293411471430757 0.40792435839711844\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=10,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYc14HpTkU2z",
        "outputId": "36544e2a-7d0b-4787-df90-4160a18680cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.44894038389925184  correct_not_repeat/not_repeat:  0.29502962979160746 0.2873538261112317\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "tI1PRax29Fqw",
        "outputId": "d9d5ea9f-4378-49ec-be25-2fe2ce5e37f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d5fdb97ca0b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-17df1714a1da>\u001b[0m in \u001b[0;36mevaluate_repeat\u001b[0;34m(model, dataloader, device, reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpos_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrepeat\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucdMSE-0T3-s",
        "outputId": "4f766291-2702-431e-d059-241a1a1ccfb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7462507193879279  correct_not_repeat/not_repeat:  0.23080623646979073 0.5669490561746645\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEt-Bw9FgvDq",
        "outputId": "70b7fed7-4cc6-495a-de11-57721bf5d02d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4839957344527574  correct_not_repeat/not_repeat:  0.35011261507511315 0.2974764468371467\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaphByRZOIo",
        "outputId": "13cfb863-5bb6-43a5-f104-7ce207f93c1e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6290835844138258  correct_not_repeat/not_repeat:  0.26852681988148086 0.44345460524349045\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGE1rXtIIEpE",
        "outputId": "775055f6-3dae-4158-fdc4-078b4adba7b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5781001387995531  correct_not_repeat/not_repeat:  0.3046073779274453 0.4137291280148423\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "QPBs0PUrKUra",
        "outputId": "196fe7cd-25e3-479d-8c4b-b6a83ccd3977"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.linear1.weight\", \"transformer_model.transformer.decoder.layers.5.linear1.bias\", \"transformer_model.transformer.decoder.layers.5.linear2.weight\", \"transformer_model.transformer.decoder.layers.5.linear2.bias\", \"transformer_model.transformer.decoder.layers.5.norm1.weight\", \"transformer_model.transformer.decoder.layers.5.norm1.bias\", \"transformer_model.transformer.decoder.layers.5.norm2.weight\", \"transformer_model.transformer.decoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.norm3.weight\", \"transformer_model.transformer.decoder.layers.5.norm3.bias\", \"transformer_lstm__list.2.layer_normalisation.weight\", \"transformer_lstm__list.2.layer_normalisation.bias\", \"transformer_lstm__list.2.lstm.weight_ih_l0\", \"transformer_lstm__list.2.lstm.weight_hh_l0\", \"transformer_lstm__list.2.lstm.bias_ih_l0\", \"transformer_lstm__list.2.lstm.bias_hh_l0\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.bias\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.bias\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-10bf83ce1050>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                          ).to(device)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5...."
          ]
        }
      ],
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=3,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cIy9KQFjv8",
        "outputId": "3ea5d475-a56f-4118-d568-2c020806ed64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8564101696062832  correct_not_repeat/not_repeat:  0.09422492401215805 0.8021341316208778\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.643730131126935, 2.2387092113494873, 4.385555267333984)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06MhCDWcaF9",
        "outputId": "4a84b9d7-b594-41b9-bca7-acacf62010b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6269508107925116  correct_not_repeat/not_repeat:  0.24020904856661782 0.4275024463247568\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5190344566683142, 3.329756021499634, 2.0473709106445312)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonRxU5b96oc",
        "outputId": "baba1be5-861e-44f1-95fb-808d9cd6b5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7019025694844104  correct_not_repeat/not_repeat:  0.2555815529946863 0.5174044590664747\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5773612306040138, 2.5278093814849854, 2.7385761737823486)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz2zFYSsAroq",
        "outputId": "10b15668-6b3b-44d9-eb4f-c28a02cbd09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7179745421307424  correct_not_repeat/not_repeat:  0.2436203013273272 0.5562012142237641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5856108172094187, 2.1441447734832764, 1.2037302255630493)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNlGehwEQtfH",
        "outputId": "378de12f-1ebf-44f6-b4b6-e57d19a2bb65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8789227800534886  correct_not_repeat/not_repeat:  0.11254947409853272 0.8136211314803864\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6650741059388481, 1.82258939743042, 2.1341636180877686)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejz7LOwNTZ-k",
        "outputId": "5d16ae15-48ea-4ff8-a2e7-718c1be73c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8187565591252243  correct_not_repeat/not_repeat:  0.1462683956178522 0.7372573126376722\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.6311055788439596, 2.027265787124634, 2.6414260864257812)"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9yHv6HjJ-N",
        "outputId": "ba6f7b9e-b88f-407c-d22b-1e6913f0e56b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.3789143166661024  correct_not_repeat/not_repeat:  0.3284642802475345 0.28316509280364704\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.3648367472709855, 2.700303077697754, 3.602348566055298)"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLWBTILrc94",
        "outputId": "c04e1913-1724-4df4-e563-1801d24ca813"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4562527506009005  correct_not_repeat/not_repeat:  0.30379829874702063 0.3240153275959545\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4137118868488654, 2.806699752807617, 3.5753602981567383)"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCR7Ctsbp_nT"
      },
      "source": [
        "## clean cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtscCJTUkD0R",
        "outputId": "5818aefb-3793-4163-c256-e9ba8ec98e3f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "730344960"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_allocated()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "IV_LSRYqGMO2",
        "f5D9IoBtGX6R",
        "dDvAwpD4GrJu",
        "S_mzoE-MHqLa",
        "7oeWr0HDhJfo",
        "bcCkeqmkhRnT",
        "3Bbp1dVXWQs3",
        "zZpbR8rG8kBn",
        "QXU5DJC8p169"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}