{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_LSRYqGMO2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qh5nnjRIFe0h"
      },
      "outputs": [],
      "source": [
        "from os import makedirs\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_x(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[0])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def get_y(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[1])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def read_dataframe(name):\n",
        "  if not os.path.exists(name+\".pkl\"):\n",
        "    print(\"reading dataframe: \"+name+\".xlsx\")\n",
        "    df=pd.read_excel(name+\".xlsx\")\n",
        "    df.to_pickle(name+\".pkl\")\n",
        "  else:\n",
        "    print(\"using already read daframe\")\n",
        "\n",
        "def get_vocab(poses,vocab):\n",
        "  for pos in poses:\n",
        "    if pos not in vocab and not any(isinstance(n, float) and math.isnan(n) for n in pos):\n",
        "        vocab[pos]=len(vocab)+1\n",
        "  return vocab\n",
        "\n",
        "def get_fix_time_encoding(df):\n",
        "\n",
        "  df['month_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "  df['month_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "\n",
        "  df['day_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "  df['day_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "\n",
        "  df['hour_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "  df['hour_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "\n",
        "  df['minute_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "  df['minute_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "\n",
        "  df['second_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "  df['second_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "def get_time_data(df):\n",
        "  df['month'] =  df[\"start time\"].dt.month\n",
        "  df['day'] =  df[\"start time\"].dt.day\n",
        "  df['hour'] =  df[\"start time\"].dt.hour\n",
        "  df['minute'] = df[\"start time\"].dt.minute\n",
        "  df['second'] = df[\"start time\"].dt.second\n",
        "  return df\n",
        "\n",
        "\n",
        "def tokenize_pos(pos,vocab):\n",
        "\n",
        "  if math.isnan(pos[0]) and math.isnan(pos[1]):\n",
        "    return len(vocab)\n",
        "  else:\n",
        "    return vocab[pos]\n",
        "\n",
        "def get_coordinates(df,input_position,full_dataset):\n",
        "\n",
        "  if full_dataset:\n",
        "    df['x'] = df['latitude']\n",
        "    df['y'] = df['longitude']\n",
        "  else:\n",
        "    df['x'] = df['location(latitude/lontitude)'].apply(get_x)\n",
        "    df['y'] = df['location(latitude/lontitude)'].apply(get_y)\n",
        "\n",
        "\n",
        "  if input_position:\n",
        "    df['x_normalised']=(df['x']-df['x'].mean())/(df['x'].std())\n",
        "    df['y_normalised']=(df['y']-df['y'].mean())/df['y'].std()\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_joined_coordinates(df):\n",
        "\n",
        "  df['pos']= list(zip(df['x'],df['y']))\n",
        "  poses=df['pos'].unique()\n",
        "\n",
        "  return poses\n",
        "\n",
        "def get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset):\n",
        "  col_to_drop_in_df=['date', 'end time','pos']\n",
        "  col_to_drop_in_dict=['x','y', 'time_to_end', 'time_to_next','start time', 'user id']\n",
        "  col_to_add_to_dict=[]\n",
        "  col_in_input=[]\n",
        "  if not full_dataset:\n",
        "    col_to_drop_in_df+=['location(latitude/lontitude)']\n",
        "  else:\n",
        "    col_to_drop_in_df+=['latitude','longitude']\n",
        "  if fixed_time_encoding:\n",
        "    col_to_drop_in_df+=[]\n",
        "    col_to_drop_in_dict+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "    col_in_input+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "  else:\n",
        "    col_to_add_to_dict+=['month','day','hour','minute','second']\n",
        "  if input_position:\n",
        "    col_to_drop_in_dict += ['x_normalised', 'y_normalised']\n",
        "    col_in_input+=['x_normalised', 'y_normalised']\n",
        "  return col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict\n",
        "\n",
        "def process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict):\n",
        "  #get the time to next connection\n",
        "  df_user[\"time_to_next\"] =  df_user[\"start time\"].diff(-1).dt.total_seconds()\n",
        "  dict_user=df_user.to_dict('list')\n",
        "  #create input\n",
        "  dict_user[\"pos_id\"],dict_user[\"pos_id_target\"]=torch.tensor(dict_user[\"pos_id\"][:-1]),torch.tensor(dict_user[\"pos_id\"][1:])\n",
        "\n",
        "  if col_in_input:\n",
        "    dict_user[\"input\"]=torch.tensor([dict_user[col] for col in col_in_input]).T\n",
        "    dict_user[\"input\"][:-1]\n",
        "  if col_to_add_to_dict:\n",
        "    for col in col_to_add_to_dict:\n",
        "      dict_user[col]=torch.tensor(dict_user[col])\n",
        "      dict_user[col][:-1]\n",
        "\n",
        "  #print(dict_user[\"input\"].shape,dict_user[\"input\"].T.shape)\n",
        "  dict_user[\"time_target\"]=torch.tensor([dict_user[\"time_to_end\"],dict_user[\"time_to_next\"]]).T\n",
        "  dict_user[\"time_target\"]=dict_user[\"time_target\"][:-1]\n",
        "  #clean dictionnary\n",
        "  for e in col_to_drop_in_dict:\n",
        "    dict_user.pop(e)\n",
        "  return dict_user\n",
        "\n",
        "def normalize_output(list_users):\n",
        "  #get means and stds\n",
        "  time_targets=torch.cat([dict_user[\"time_target\"] for dict_user in list_users],dim=0)\n",
        "  time_targets_mean=time_targets.mean(dim=0)\n",
        "  time_targets_std=time_targets.std(dim=0)\n",
        "  #normalize\n",
        "  for i in range(len(list_users)):\n",
        "    list_users[i][\"time_target\"]=(list_users[i][\"time_target\"]-time_targets_mean)/time_targets_std\n",
        "  return list_users\n",
        "\n",
        "\n",
        "\n",
        "def process_dataframe(name,vocab,fixed_time_encoding=False,input_position=False,full_dataset=True,format=\".pkl\"):\n",
        "  df= pd.read_pickle(name+format)\n",
        "  df=df.sort_values('start time')\n",
        "  df=df.drop(['month'],axis=1)\n",
        "\n",
        "  df=get_coordinates(df,input_position,full_dataset)\n",
        "\n",
        "  poses=get_joined_coordinates(df)\n",
        "  vocab=get_vocab(poses,vocab)\n",
        "  df['pos_id'] = df['pos'].apply(lambda pos: tokenize_pos(pos,vocab))\n",
        "\n",
        "  df['time_to_end']=df['end time']-df['start time']\n",
        "  df['time_to_end']=df['time_to_end'].dt.total_seconds()\n",
        "  if fixed_time_encoding:\n",
        "    df=get_fix_time_encoding(df)\n",
        "  else:\n",
        "    df=get_time_data(df)\n",
        "\n",
        "  col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict=get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset)\n",
        "  df=df.drop(col_to_drop_in_df, axis=1)\n",
        "\n",
        "  df_user_group = df.groupby('user id')\n",
        "  list_users=[]\n",
        "  for user, df_user in df_user_group:\n",
        "    if len(df_user)>=2 and not df_user['x'].isnull().values.any():\n",
        "        list_users.append(process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict))\n",
        "  list_users=normalize_output(list_users)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "def get_raw_data(directory,src_directory,full_dataset):\n",
        "  if  full_dataset:\n",
        "    shutil.copytree(src_directory,directory)#telecomDataset6mont\n",
        "  else:\n",
        "    runcmd('wget http://sguangwang.com/dataset/telecom.zip', verbose = False)\n",
        "    runcmd('unzip /content/telecom.zip')\n",
        "\n",
        "def get_processed_dataset(load_dataset_path):\n",
        "  saved_list_user_path = os.path.join(load_dataset_path,\"list_users\")\n",
        "  saved_vocab_path = os.path.join(load_dataset_path,\"vocab\")\n",
        "  print(\"loading already preprocessed data: \")\n",
        "  print(saved_list_user_path)\n",
        "  print(saved_vocab_path)\n",
        "  list_users=torch.load(saved_list_user_path)\n",
        "  vocab=torch.load(saved_vocab_path)\n",
        "  return list_users,vocab\n",
        "\n",
        "def process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset):\n",
        "  list_users=[]\n",
        "  vocab={}\n",
        "  if not os.path.exists(directory_raw_data):\n",
        "    print('getting raw data at: '+src_directory_raw_data)\n",
        "    get_raw_data(directory_raw_data,src_directory_raw_data,full_dataset)\n",
        "  for name in os.listdir(directory_raw_data):\n",
        "    if not name.endswith(\".pkl\"):\n",
        "      complete_name=os.path.join(directory_raw_data,\".\".join(name.split(\".\")[:-1]))\n",
        "      print(\"processing dataframe: \"+complete_name)\n",
        "      read_dataframe(complete_name)\n",
        "      new_list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset)\n",
        "      list_users+=new_list_users\n",
        "  return list_users,vocab\n",
        "\n",
        "def split_long_sequences(list_users,max_sequence_length):\n",
        "  new_list_users=[]\n",
        "  for i in range(len(list_users)):\n",
        "    seq_length=list_users[i][\"input\"].shape[0]\n",
        "    if seq_length>=max_sequence_length:\n",
        "      nb_of_seq=seq_length//max_sequence_length\n",
        "      rest=seq_length%max_sequence_length\n",
        "      list_splitted_seq=nb_of_seq*[{}]\n",
        "      rest_splitted={}\n",
        "      for key in list_users[i]:\n",
        "        for j in range(nb_of_seq):\n",
        "          list_splitted_seq[j][key]=list_users[i][key][max_sequence_length*j:max_sequence_length*(j+1)]\n",
        "        if rest>2:\n",
        "          rest_splitted[key]= list_users[i][key][-rest:]\n",
        "      new_list_users=new_list_users+list_splitted_seq\n",
        "      if len(rest_splitted)>0:\n",
        "        new_list_users+=[rest_splitted]\n",
        "    else:\n",
        "      new_list_users.append(list_users[i])\n",
        "\n",
        "  return new_list_users\n",
        "\n",
        "def save_processed_data(list_users,vocab,path_to_save_dataset):\n",
        "    print(\"creating directory: \"+path_to_save_dataset)\n",
        "    os.makedirs(path_to_save_dataset,exist_ok=True)\n",
        "    print(\"saving processed data at: \")\n",
        "    save_list_user_path = os.path.join(path_to_save_dataset,\"list_users\")\n",
        "    save_vocab_path = os.path.join(path_to_save_dataset,\"vocab\")\n",
        "    print(save_list_user_path)\n",
        "    print(save_vocab_path)\n",
        "    torch.save(list_users,save_list_user_path)\n",
        "    torch.save(vocab,save_vocab_path)\n",
        "\n",
        "def get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",directory_raw_data='/content/dataset-telecom',fixed_time_encoding=False,input_position=False,full_dataset=True,spliting_long_sequences=True,max_sequence_length=100,min_sequence_length=3,save=False,path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month\",download=False,load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month\"):\n",
        "  if not download:\n",
        "    list_users,vocab = get_processed_dataset(load_dataset_path)\n",
        "  else:\n",
        "    list_users,vocab=process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset)\n",
        "  if spliting_long_sequences:\n",
        "    print(\"spliting sequences longuer than : \"+str(max_sequence_length)+ \" steps\")\n",
        "    list_users=split_long_sequences(list_users,max_sequence_length)\n",
        "  if save:\n",
        "    save_processed_data(list_users,vocab,path_to_save_dataset)\n",
        "  return list_users,vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRA1WX8UcsV",
        "outputId": "3de2e97c-1b46-444e-963c-9d64120943f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100/vocab\n",
            "spliting sequences longuer than : 100 steps\n",
            "creating directory: /content/drive/MyDrive/telecomDataset6month-splited-100\n",
            "saving processed data at: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100/vocab\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=True,\n",
        "                                    spliting_long_sequences=True,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_length=3,\n",
        "                                    save=True,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100\",\n",
        "                                    download=False,\n",
        "                                    load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100\",)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SixWrOzR8Qt7"
      },
      "outputs": [],
      "source": [
        "for user in list_users:\n",
        "  if user[\"input\"].shape[0]>100:\n",
        "    print(user[\"input\"].shape[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZG4hw4YVkyPd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f048b41-7e85-4876-a9b7-4fd0653ec483"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(31.098701, 121.582178): 1,\n",
              " (31.159304, 121.358718): 2,\n",
              " (31.401391, 121.463758): 3,\n",
              " (31.059755, 121.389723): 4,\n",
              " (31.149677, 121.729053): 5,\n",
              " (31.252264, 121.38269): 6,\n",
              " (31.111396, 121.379353): 7,\n",
              " (31.124501, 121.32791): 8,\n",
              " (30.731903, 121.334173): 9,\n",
              " (31.274417, 121.257132): 10,\n",
              " (31.121127, 121.243829): 11,\n",
              " (31.158515, 121.1384): 12,\n",
              " (31.01217, 121.265575): 13,\n",
              " (31.176807, 121.251063): 14,\n",
              " (31.19382, 121.49524): 15,\n",
              " (31.140384, 121.620758): 16,\n",
              " (31.163432, 121.354451): 17,\n",
              " (31.145387, 121.61623): 18,\n",
              " (31.127217, 121.335543): 19,\n",
              " (31.199721, 121.325753): 20,\n",
              " (30.887807, 121.346419): 21,\n",
              " (31.202883, 121.476296): 22,\n",
              " (31.051898, 121.769904): 23,\n",
              " (31.159756, 121.525433): 24,\n",
              " (31.021462, 121.321255): 25,\n",
              " (31.203987, 121.520591): 26,\n",
              " (31.339114, 121.321261): 27,\n",
              " (31.277512, 121.287969): 28,\n",
              " (30.844397, 121.528845): 29,\n",
              " (31.246097, 121.39928): 30,\n",
              " (31.346801, 121.577398): 31,\n",
              " (31.2676, 121.541803): 32,\n",
              " (31.036623, 121.377512): 33,\n",
              " (31.171101, 121.507145): 34,\n",
              " (31.295347, 121.515155): 35,\n",
              " (31.324905, 121.29416): 36,\n",
              " (31.223175, 121.184646): 37,\n",
              " (31.311329, 121.495756): 38,\n",
              " (30.894923, 121.319564): 39,\n",
              " (31.263762, 121.481215): 40,\n",
              " (32.902077, 109.42358): 41,\n",
              " (31.048845, 121.231719): 42,\n",
              " (31.231585, 121.517218): 43,\n",
              " (46.777465, 131.812182): 44,\n",
              " (31.307268, 121.521916): 45,\n",
              " (31.249162, 121.487899): 46,\n",
              " (31.285944, 121.44442): 47,\n",
              " (31.246527, 121.534982): 48,\n",
              " (31.148773, 121.387158): 49,\n",
              " (31.180897, 121.463904): 50,\n",
              " (31.104981, 121.360938): 51,\n",
              " (30.999098, 121.447826): 52,\n",
              " (31.247519, 121.322265): 53,\n",
              " (31.209025, 121.616514): 54,\n",
              " (31.192915, 121.682688): 55,\n",
              " (31.248482, 121.53958): 56,\n",
              " (31.142865, 121.330843): 57,\n",
              " (31.037985, 121.752781): 58,\n",
              " (31.252404, 121.435035): 59,\n",
              " (31.158843, 121.127268): 60,\n",
              " (31.087306, 121.275545): 61,\n",
              " (31.378631, 121.277965): 62,\n",
              " (31.155297, 121.573504): 63,\n",
              " (31.252105, 121.56398): 64,\n",
              " (31.116103, 121.43571): 65,\n",
              " (31.149689, 121.531301): 66,\n",
              " (31.157241, 121.418919): 67,\n",
              " (31.27975, 121.497791): 68,\n",
              " (31.277766, 121.433047): 69,\n",
              " (31.353347, 121.441038): 70,\n",
              " (31.22121, 121.411568): 71,\n",
              " (31.10366, 121.420217): 72,\n",
              " (31.00948, 121.4998): 73,\n",
              " (31.285698, 121.612262): 74,\n",
              " (30.989121, 121.127308): 75,\n",
              " (31.31394, 121.382812): 76,\n",
              " (31.254111, 121.408424): 77,\n",
              " (30.899117, 121.175731): 78,\n",
              " (31.387987, 121.495216): 79,\n",
              " (31.122623, 121.433813): 80,\n",
              " (31.091117, 121.549904): 81,\n",
              " (31.204179, 121.538588): 82,\n",
              " (31.306089, 121.474725): 83,\n",
              " (31.337218, 121.465181): 84,\n",
              " (30.771941, 121.373524): 85,\n",
              " (31.319806, 121.526248): 86,\n",
              " (31.2355, 121.417315): 87,\n",
              " (31.230933, 121.347295): 88,\n",
              " (31.13756, 121.339005): 89,\n",
              " (31.092058, 121.38753): 90,\n",
              " (31.155384, 121.771585): 91,\n",
              " (31.099085, 121.386207): 92,\n",
              " (31.13264, 121.32443): 93,\n",
              " (31.155403, 121.57642): 94,\n",
              " (31.32719, 121.53013): 95,\n",
              " (30.954604, 121.333773): 96,\n",
              " (31.293561, 121.362372): 97,\n",
              " (30.834815, 121.197836): 98,\n",
              " (31.25007, 121.546845): 99,\n",
              " (31.116791, 121.166516): 100,\n",
              " (31.2077, 121.669382): 101,\n",
              " (30.901978, 121.452316): 102,\n",
              " (31.282237, 121.42091): 103,\n",
              " (31.069261, 121.375484): 104,\n",
              " (31.213728, 121.386102): 105,\n",
              " (31.425786, 121.422506): 106,\n",
              " (31.228726, 121.482748): 107,\n",
              " (31.236405, 121.432864): 108,\n",
              " (31.144212, 121.50825): 109,\n",
              " (31.121152, 121.141999): 110,\n",
              " (31.291143, 121.472012): 111,\n",
              " (31.324066, 121.536866): 112,\n",
              " (31.254912, 121.371838): 113,\n",
              " (31.342417, 121.40922): 114,\n",
              " (31.239726, 121.139516): 115,\n",
              " (31.035335, 121.117893): 116,\n",
              " (31.158559, 121.319801): 117,\n",
              " (31.318369, 121.407014): 118,\n",
              " (31.125695, 121.46401): 119,\n",
              " (31.190389, 121.457678): 120,\n",
              " (31.276982, 121.460649): 121,\n",
              " (31.283959, 121.416644): 122,\n",
              " (31.151539, 121.41109): 123,\n",
              " (31.355737, 121.403678): 124,\n",
              " (30.808567, 121.29074): 125,\n",
              " (31.232857, 121.475333): 126,\n",
              " (31.08111, 121.263537): 127,\n",
              " (26.139329, 103.078562): 128,\n",
              " (38.052584, 114.484137): 129,\n",
              " (31.211485, 121.403238): 130,\n",
              " (31.348418, 121.427796): 131,\n",
              " (31.273019, 121.307617): 132,\n",
              " (31.268642, 121.332479): 133,\n",
              " (35.379598, 116.072359): 134,\n",
              " (31.141982, 121.459872): 135,\n",
              " (31.069676, 121.773361): 136,\n",
              " (31.17665, 121.132941): 137,\n",
              " (31.189701, 121.543143): 138,\n",
              " (30.852446, 121.262447): 139,\n",
              " (31.160355, 121.454785): 140,\n",
              " (31.292519, 121.549839): 141,\n",
              " (31.181485, 121.520026): 142,\n",
              " (31.015724, 121.40907): 143,\n",
              " (31.250674, 121.47282): 144,\n",
              " (31.286022, 121.497436): 145,\n",
              " (30.923134, 121.481921): 146,\n",
              " (31.395915, 121.363062): 147,\n",
              " (30.904648, 121.173227): 148,\n",
              " (31.338059, 121.289933): 149,\n",
              " (31.227933, 121.45361): 150,\n",
              " (31.013227, 121.552205): 151,\n",
              " (31.245242, 121.479617): 152,\n",
              " (31.140855, 121.360032): 153,\n",
              " (31.108567, 121.766873): 154,\n",
              " (30.967774, 121.447499): 155,\n",
              " (31.209644, 121.56447): 156,\n",
              " (31.143754, 121.434357): 157,\n",
              " (30.910136, 121.843044): 158,\n",
              " (31.23423, 121.119889): 159,\n",
              " (30.939177, 121.223182): 160,\n",
              " (30.892415, 121.023553): 161,\n",
              " (31.235929, 121.449652): 162,\n",
              " (31.102, 121.462298): 163,\n",
              " (31.215428, 121.660427): 164,\n",
              " (30.982046, 121.207872): 165,\n",
              " (31.19206, 121.438501): 166,\n",
              " (31.183857, 121.637321): 167,\n",
              " (31.152157, 121.514858): 168,\n",
              " (30.97935, 121.815249): 169,\n",
              " (31.423448, 121.433415): 170,\n",
              " (31.242157, 121.568785): 171,\n",
              " (31.265054, 121.456413): 172,\n",
              " (31.244102, 121.437744): 173,\n",
              " (31.113744, 121.341878): 174,\n",
              " (31.270279, 121.531241): 175,\n",
              " (31.276178, 121.553996): 176,\n",
              " (31.388641, 121.444174): 177,\n",
              " (31.190301, 121.566862): 178,\n",
              " (31.116021, 121.287621): 179,\n",
              " (31.174053, 121.282683): 180,\n",
              " (31.154923, 121.36045): 181,\n",
              " (31.009423, 121.245898): 182,\n",
              " (31.195613, 121.40425): 183,\n",
              " (31.299749, 121.170321): 184,\n",
              " (31.284385, 121.511361): 185,\n",
              " (31.031203, 121.65009): 186,\n",
              " (30.941447, 121.293964): 187,\n",
              " (31.301749, 121.457406): 188,\n",
              " (31.294194, 121.316828): 189,\n",
              " (31.234496, 121.521516): 190,\n",
              " (31.307215, 121.486962): 191,\n",
              " (31.020065, 121.23311): 192,\n",
              " (31.291242, 121.489761): 193,\n",
              " (31.120602, 121.581473): 194,\n",
              " (31.463063, 121.345403): 195,\n",
              " (31.412982, 121.496282): 196,\n",
              " (31.171686, 121.441221): 197,\n",
              " (31.268106, 121.49354): 198,\n",
              " (31.08093, 121.159291): 199,\n",
              " (31.23593, 121.220028): 200,\n",
              " (31.272151, 121.375814): 201,\n",
              " (31.098359, 121.073778): 202,\n",
              " (30.93817, 121.865652): 203,\n",
              " (31.120753, 121.381579): 204,\n",
              " (30.832683, 121.481868): 205,\n",
              " (31.217516, 121.452872): 206,\n",
              " (31.196135, 121.558528): 207,\n",
              " (31.155848, 121.383402): 208,\n",
              " (31.148848, 121.314851): 209,\n",
              " (31.246106, 121.443663): 210,\n",
              " (31.240478, 121.526884): 211,\n",
              " (31.280867, 121.656939): 212,\n",
              " (30.719537, 121.355948): 213,\n",
              " (30.947965, 121.640593): 214,\n",
              " (30.90387, 121.433071): 215,\n",
              " (31.045313, 121.844791): 216,\n",
              " (31.142941, 121.443942): 217,\n",
              " (30.860668, 121.470193): 218,\n",
              " (31.285059, 121.406807): 219,\n",
              " (31.044667, 121.46191): 220,\n",
              " (31.109998, 121.164842): 221,\n",
              " (31.046945, 121.764826): 222,\n",
              " (31.07195, 121.150279): 223,\n",
              " (31.214209, 121.659493): 224,\n",
              " (31.195131, 121.418788): 225,\n",
              " (31.105808, 121.038742): 226,\n",
              " (31.184888, 121.510295): 227,\n",
              " (31.420016, 121.44113): 228,\n",
              " (31.075231, 121.436269): 229,\n",
              " (31.282514, 121.343407): 230,\n",
              " (31.324464, 121.387532): 231,\n",
              " (31.370992, 121.177459): 232,\n",
              " (31.138371, 121.416975): 233,\n",
              " (31.120864, 121.389141): 234,\n",
              " (31.382771, 121.291756): 235,\n",
              " (31.256559, 121.43557): 236,\n",
              " (31.331242, 121.225906): 237,\n",
              " (31.025227, 121.44251): 238,\n",
              " (30.98383, 121.235077): 239,\n",
              " (31.312825, 121.491112): 240,\n",
              " (30.874563, 121.067902): 241,\n",
              " (31.273024, 121.42572): 242,\n",
              " (31.37273, 121.546117): 243,\n",
              " (31.038201, 121.216468): 244,\n",
              " (31.112344, 121.480257): 245,\n",
              " (31.367578, 121.31858): 246,\n",
              " (31.282271, 121.524511): 247,\n",
              " (31.176813, 121.439114): 248,\n",
              " (31.492475, 121.328439): 249,\n",
              " (31.121932, 121.33481): 250,\n",
              " (30.995864, 121.568444): 251,\n",
              " (31.187254, 121.536259): 252,\n",
              " (31.211591, 121.499553): 253,\n",
              " (31.134786, 121.444969): 254,\n",
              " (31.173163, 121.356571): 255,\n",
              " (31.150483, 121.491401): 256,\n",
              " (31.253601, 121.4782): 257,\n",
              " (30.866245, 121.343018): 258,\n",
              " (31.296656, 121.483976): 259,\n",
              " (31.355465, 121.412867): 260,\n",
              " (31.244479, 121.429372): 261,\n",
              " (31.284042, 121.583676): 262,\n",
              " (31.078791, 121.422082): 263,\n",
              " (31.2317, 121.546506): 264,\n",
              " (30.889897, 121.320413): 265,\n",
              " (31.230109, 121.434393): 266,\n",
              " (31.430211, 121.28239): 267,\n",
              " (31.13107, 121.298444): 268,\n",
              " (31.112548, 121.538798): 269,\n",
              " (30.846647, 121.431405): 270,\n",
              " (31.344291, 121.487858): 271,\n",
              " (31.031459, 121.300841): 272,\n",
              " (30.74139, 121.380141): 273,\n",
              " (31.19868, 121.485666): 274,\n",
              " (31.069946, 121.719641): 275,\n",
              " (31.189335, 121.781953): 276,\n",
              " (31.263651, 121.573401): 277,\n",
              " (31.236856, 121.40554): 278,\n",
              " (30.860242, 121.79388): 279,\n",
              " (31.201237, 121.669336): 280,\n",
              " (31.357773, 121.508693): 281,\n",
              " (31.208346, 121.544892): 282,\n",
              " (31.303478, 121.55082): 283,\n",
              " (31.280262, 121.364633): 284,\n",
              " (31.402222, 121.493387): 285,\n",
              " (31.281567, 121.557955): 286,\n",
              " (31.186628, 121.372116): 287,\n",
              " (31.26484, 121.498029): 288,\n",
              " (31.473557, 121.331298): 289,\n",
              " (31.123394, 121.278015): 290,\n",
              " (31.018376, 121.40507): 291,\n",
              " (31.191674, 121.699103): 292,\n",
              " (31.19143, 121.411411): 293,\n",
              " (31.344675, 121.443963): 294,\n",
              " (30.983162, 121.054057): 295,\n",
              " (31.313065, 121.417971): 296,\n",
              " (31.198952, 121.70244): 297,\n",
              " (31.255942, 121.38141): 298,\n",
              " (30.766975, 121.362906): 299,\n",
              " (31.237872, 121.470259): 300,\n",
              " (31.28386, 121.530324): 301,\n",
              " (31.370388, 121.187891): 302,\n",
              " (31.11794, 121.457471): 303,\n",
              " (31.233761, 121.670586): 304,\n",
              " (31.143297, 121.132883): 305,\n",
              " (30.957633, 121.343012): 306,\n",
              " (30.900013, 121.049649): 307,\n",
              " (31.228847, 121.338653): 308,\n",
              " (31.253657, 121.443859): 309,\n",
              " (31.273451, 121.681542): 310,\n",
              " (31.213659, 121.504661): 311,\n",
              " (31.175318, 121.181299): 312,\n",
              " (31.261563, 121.376836): 313,\n",
              " (31.198048, 121.604048): 314,\n",
              " (36.406412, 102.003965): 315,\n",
              " (31.146522, 121.352501): 316,\n",
              " (31.238561, 121.455041): 317,\n",
              " (31.074013, 121.407487): 318,\n",
              " (31.202075, 121.187355): 319,\n",
              " (31.191847, 121.381117): 320,\n",
              " (31.251239, 121.428578): 321,\n",
              " (31.42225, 121.355698): 322,\n",
              " (31.174661, 121.52822): 323,\n",
              " (31.010124, 121.538644): 324,\n",
              " (31.062583, 121.44986): 325,\n",
              " (31.111821, 121.385462): 326,\n",
              " (31.226966, 121.53174): 327,\n",
              " (31.300693, 121.496182): 328,\n",
              " (31.356238, 121.568226): 329,\n",
              " (31.263678, 121.468663): 330,\n",
              " (31.222168, 121.378951): 331,\n",
              " (31.19232, 121.142889): 332,\n",
              " (31.17762, 121.511042): 333,\n",
              " (31.318661, 121.194317): 334,\n",
              " (31.128227, 121.45441): 335,\n",
              " (31.127879, 121.452728): 336,\n",
              " (31.270626, 121.5908): 337,\n",
              " (31.3397, 121.298473): 338,\n",
              " (31.224452, 121.511537): 339,\n",
              " (31.217968, 121.41467): 340,\n",
              " (31.305269, 121.397452): 341,\n",
              " (30.983141, 121.540945): 342,\n",
              " (31.279012, 121.514374): 343,\n",
              " (31.290887, 121.435064): 344,\n",
              " (31.267654, 121.473299): 345,\n",
              " (30.955208, 121.732358): 346,\n",
              " (31.33167, 121.440751): 347,\n",
              " (31.302746, 121.376994): 348,\n",
              " (31.406017, 121.240101): 349,\n",
              " (31.200432, 121.447381): 350,\n",
              " (31.370457, 121.470415): 351,\n",
              " (31.132311, 121.362691): 352,\n",
              " (31.166046, 121.642137): 353,\n",
              " (31.176379, 121.397706): 354,\n",
              " (31.042056, 121.720903): 355,\n",
              " (31.269929, 121.453419): 356,\n",
              " (31.032565, 121.409407): 357,\n",
              " (31.2798, 121.459715): 358,\n",
              " (31.242808, 121.420927): 359,\n",
              " (31.284148, 121.481157): 360,\n",
              " (31.114131, 121.394475): 361,\n",
              " (31.241274, 121.36272): 362,\n",
              " (31.24448, 121.37062): 363,\n",
              " (31.115176, 121.36579): 364,\n",
              " (31.125467, 121.581832): 365,\n",
              " (31.011339, 121.296295): 366,\n",
              " (31.403259, 121.447841): 367,\n",
              " (31.042021, 121.204506): 368,\n",
              " (31.301602, 121.487289): 369,\n",
              " (31.319501, 121.584673): 370,\n",
              " (31.203031, 121.443586): 371,\n",
              " (31.241036, 121.374063): 372,\n",
              " (31.195139, 121.385973): 373,\n",
              " (31.056648, 121.404326): 374,\n",
              " (31.157362, 121.221594): 375,\n",
              " (31.156874, 121.348458): 376,\n",
              " (31.216752, 121.476401): 377,\n",
              " (31.231508, 121.577691): 378,\n",
              " (30.919334, 121.475563): 379,\n",
              " (31.213701, 121.41802): 380,\n",
              " (31.4163, 121.496785): 381,\n",
              " (31.177858, 121.346956): 382,\n",
              " (31.232639, 121.705254): 383,\n",
              " (31.011886, 121.234971): 384,\n",
              " (31.03333, 121.427385): 385,\n",
              " (31.284953, 121.489071): 386,\n",
              " (31.303224, 121.308726): 387,\n",
              " (31.259633, 121.594477): 388,\n",
              " (31.177067, 121.370844): 389,\n",
              " (31.204999, 121.471193): 390,\n",
              " (31.223004, 121.465956): 391,\n",
              " (31.285741, 121.626244): 392,\n",
              " (31.182483, 121.5762): 393,\n",
              " (31.243338, 121.253922): 394,\n",
              " (31.131231, 121.398758): 395,\n",
              " (31.054506, 121.460002): 396,\n",
              " (31.273183, 121.488391): 397,\n",
              " (31.316557, 121.516135): 398,\n",
              " (30.880562, 121.486561): 399,\n",
              " (31.232893, 121.380242): 400,\n",
              " (31.250332, 121.594504): 401,\n",
              " (31.157677, 121.424531): 402,\n",
              " (31.310614, 121.476617): 403,\n",
              " (31.225266, 121.444323): 404,\n",
              " (31.102445, 121.413911): 405,\n",
              " (31.311942, 121.54204): 406,\n",
              " (31.111607, 121.324796): 407,\n",
              " (31.245663, 121.449725): 408,\n",
              " (31.188821, 121.387044): 409,\n",
              " (31.233201, 121.469528): 410,\n",
              " (31.182189, 121.394088): 411,\n",
              " (31.314038, 121.509762): 412,\n",
              " (31.17023, 121.337413): 413,\n",
              " (31.132785, 121.413457): 414,\n",
              " (31.224117, 121.4732): 415,\n",
              " (31.194194, 121.456589): 416,\n",
              " (31.254981, 121.623923): 417,\n",
              " (31.11674, 121.586334): 418,\n",
              " (31.131416, 121.429704): 419,\n",
              " (31.078365, 121.404647): 420,\n",
              " (31.217476, 121.494788): 421,\n",
              " (31.209905, 121.37472): 422,\n",
              " (30.730296, 121.342643): 423,\n",
              " (30.816155, 121.298392): 424,\n",
              " (31.296619, 121.497055): 425,\n",
              " (31.209904, 121.234203): 426,\n",
              " (31.235072, 121.518805): 427,\n",
              " (31.016895, 121.42971): 428,\n",
              " (31.099642, 121.449578): 429,\n",
              " (31.266743, 121.481644): 430,\n",
              " (30.92668, 121.466784): 431,\n",
              " (31.086916, 121.387535): 432,\n",
              " (31.276317, 121.524794): 433,\n",
              " (31.228755, 121.523626): 434,\n",
              " (31.192975, 121.449755): 435,\n",
              " (31.19706, 121.095538): 436,\n",
              " (31.085146, 121.589124): 437,\n",
              " (31.236104, 121.375177): 438,\n",
              " (31.227795, 121.254172): 439,\n",
              " (31.368461, 121.372259): 440,\n",
              " (31.206547, 121.306923): 441,\n",
              " (31.201251, 121.561484): 442,\n",
              " (30.915686, 121.462098): 443,\n",
              " (31.157536, 121.203337): 444,\n",
              " (30.956624, 121.334663): 445,\n",
              " (31.436901, 121.294601): 446,\n",
              " (30.750457, 121.376839): 447,\n",
              " (31.12408, 121.716759): 448,\n",
              " (30.799332, 121.268825): 449,\n",
              " (31.159771, 121.58896): 450,\n",
              " (31.137509, 121.40768): 451,\n",
              " (31.213015, 121.547755): 452,\n",
              " (31.150797, 121.728702): 453,\n",
              " (31.164195, 121.330502): 454,\n",
              " (31.072138, 121.661831): 455,\n",
              " (31.094914, 121.43682): 456,\n",
              " (31.038291, 121.194794): 457,\n",
              " (31.024417, 121.471297): 458,\n",
              " (30.716187, 121.349498): 459,\n",
              " (31.175621, 121.317835): 460,\n",
              " (31.180175, 121.422303): 461,\n",
              " (30.904757, 121.467941): 462,\n",
              " (31.24822, 121.296297): 463,\n",
              " (30.79706, 121.431713): 464,\n",
              " (31.177646, 121.555355): 465,\n",
              " (31.318281, 121.447698): 466,\n",
              " (31.359311, 121.310645): 467,\n",
              " (31.280389, 121.467635): 468,\n",
              " (31.241014, 121.48529): 469,\n",
              " (31.384742, 121.277311): 470,\n",
              " (31.341417, 121.271089): 471,\n",
              " (31.224731, 121.43464): 472,\n",
              " (31.120822, 121.29001): 473,\n",
              " (30.938138, 121.327668): 474,\n",
              " (31.358014, 121.37617): 475,\n",
              " (31.221325, 121.448881): 476,\n",
              " (30.909885, 121.195459): 477,\n",
              " (31.276567, 121.483483): 478,\n",
              " (30.991472, 121.430726): 479,\n",
              " (31.133956, 121.526289): 480,\n",
              " (31.378285, 121.436137): 481,\n",
              " (31.294057, 121.171316): 482,\n",
              " (31.214958, 121.527054): 483,\n",
              " (31.08532, 121.237002): 484,\n",
              " (30.889354, 121.488288): 485,\n",
              " (31.116903, 121.595022): 486,\n",
              " (31.198093, 121.126168): 487,\n",
              " (31.313954, 121.499987): 488,\n",
              " (31.371973, 121.278978): 489,\n",
              " (30.829105, 121.180127): 490,\n",
              " (30.918205, 121.509992): 491,\n",
              " (31.211574, 121.366182): 492,\n",
              " (30.735389, 121.358248): 493,\n",
              " (30.935837, 121.561311): 494,\n",
              " (31.117005, 121.271194): 495,\n",
              " (31.242627, 121.730079): 496,\n",
              " (31.262941, 121.602924): 497,\n",
              " (31.114038, 121.415096): 498,\n",
              " (31.220649, 121.35994): 499,\n",
              " (31.204349, 121.693531): 500,\n",
              " (31.134254, 121.280533): 501,\n",
              " (31.287814, 121.355886): 502,\n",
              " (31.053362, 121.431405): 503,\n",
              " (31.366638, 121.43543): 504,\n",
              " (31.289282, 121.378597): 505,\n",
              " (30.960682, 121.447242): 506,\n",
              " (30.723654, 121.330778): 507,\n",
              " (30.978506, 121.447689): 508,\n",
              " (31.288583, 121.428686): 509,\n",
              " (31.278008, 121.492049): 510,\n",
              " (31.25901, 121.65797): 511,\n",
              " (31.280452, 121.480426): 512,\n",
              " (30.956973, 121.097333): 513,\n",
              " (31.216371, 121.601926): 514,\n",
              " (31.384186, 121.484871): 515,\n",
              " (31.297281, 121.505124): 516,\n",
              " (31.193409, 121.393885): 517,\n",
              " (31.301877, 121.353206): 518,\n",
              " (31.267673, 121.346341): 519,\n",
              " (31.179835, 121.429502): 520,\n",
              " (31.205759, 121.26316): 521,\n",
              " (31.370336, 121.265507): 522,\n",
              " (31.082701, 121.448694): 523,\n",
              " (31.051731, 121.423756): 524,\n",
              " (31.260835, 121.366953): 525,\n",
              " (31.25563, 121.508513): 526,\n",
              " (31.231687, 121.32576): 527,\n",
              " (31.180466, 121.441251): 528,\n",
              " (31.200882, 121.446511): 529,\n",
              " (31.217261, 121.391532): 530,\n",
              " (31.197573, 121.37641): 531,\n",
              " (30.843004, 121.438792): 532,\n",
              " (30.874727, 121.250912): 533,\n",
              " (31.166408, 121.569203): 534,\n",
              " (31.120714, 121.42693): 535,\n",
              " (31.158202, 121.389411): 536,\n",
              " (31.069757, 121.399847): 537,\n",
              " (31.196266, 121.537132): 538,\n",
              " (31.045516, 121.216817): 539,\n",
              " (31.18026, 121.406339): 540,\n",
              " (30.895699, 121.478289): 541,\n",
              " (31.214561, 121.298904): 542,\n",
              " (31.22258, 121.428599): 543,\n",
              " (30.898595, 121.030128): 544,\n",
              " (31.447176, 121.385936): 545,\n",
              " (31.225207, 121.452948): 546,\n",
              " (31.32712, 121.254377): 547,\n",
              " (31.27431, 121.448589): 548,\n",
              " (31.233674, 121.498294): 549,\n",
              " (31.488248, 121.349833): 550,\n",
              " (31.045842, 121.756555): 551,\n",
              " (31.040378, 121.255563): 552,\n",
              " (30.912581, 121.465142): 553,\n",
              " (31.268263, 121.394134): 554,\n",
              " (31.242948, 121.608734): 555,\n",
              " (31.036429, 121.324198): 556,\n",
              " (31.109649, 121.402441): 557,\n",
              " (30.72456, 121.344043): 558,\n",
              " (31.225275, 121.47164): 559,\n",
              " (31.265902, 121.537589): 560,\n",
              " (31.396687, 121.378844): 561,\n",
              " (31.17247, 121.404203): 562,\n",
              " (30.715526, 121.351073): 563,\n",
              " (31.125764, 121.562057): 564,\n",
              " (31.220416, 121.529149): 565,\n",
              " (31.273242, 121.499598): 566,\n",
              " (31.271472, 121.494721): 567,\n",
              " (31.459469, 121.412299): 568,\n",
              " (31.218812, 121.401309): 569,\n",
              " (31.096325, 121.341209): 570,\n",
              " (31.036987, 121.386554): 571,\n",
              " (31.100465, 121.401931): 572,\n",
              " (31.310235, 121.437455): 573,\n",
              " (31.265, 121.48958): 574,\n",
              " (31.24143, 121.475172): 575,\n",
              " (31.039158, 121.239632): 576,\n",
              " (31.255248, 121.416487): 577,\n",
              " (30.741272, 121.3348): 578,\n",
              " (31.296405, 121.392803): 579,\n",
              " (31.214683, 121.551574): 580,\n",
              " (31.267478, 121.5468): 581,\n",
              " (31.18851, 121.403115): 582,\n",
              " (31.326648, 121.304442): 583,\n",
              " (31.16346, 121.405127): 584,\n",
              " (31.163705, 121.503747): 585,\n",
              " (31.318646, 121.53899): 586,\n",
              " (31.230395, 121.46599): 587,\n",
              " (31.045834, 121.408322): 588,\n",
              " (30.910239, 121.419143): 589,\n",
              " (31.268663, 121.428101): 590,\n",
              " (31.183323, 121.447822): 591,\n",
              " (31.300493, 121.323029): 592,\n",
              " (31.139657, 121.298915): 593,\n",
              " (31.168296, 121.588625): 594,\n",
              " (30.918282, 121.639066): 595,\n",
              " (31.209704, 121.368486): 596,\n",
              " (31.201259, 121.40143): 597,\n",
              " (30.741693, 121.367052): 598,\n",
              " (31.172757, 121.425965): 599,\n",
              " (31.238593, 121.450062): 600,\n",
              " (31.26748, 121.57859): 601,\n",
              " (31.183064, 121.407773): 602,\n",
              " (31.251358, 121.485526): 603,\n",
              " (31.300119, 121.532041): 604,\n",
              " (31.162714, 121.772242): 605,\n",
              " (31.344366, 121.51915): 606,\n",
              " (31.247009, 121.47942): 607,\n",
              " (31.265326, 121.394456): 608,\n",
              " (31.202395, 121.6278): 609,\n",
              " (31.05799, 121.626154): 610,\n",
              " (31.269517, 121.498812): 611,\n",
              " (31.354948, 121.496863): 612,\n",
              " (31.051247, 121.480621): 613,\n",
              " (31.204614, 121.432408): 614,\n",
              " (31.444014, 121.25661): 615,\n",
              " (31.299261, 121.578516): 616,\n",
              " (31.146311, 121.399951): 617,\n",
              " (31.155148, 121.597536): 618,\n",
              " (31.178088, 121.413515): 619,\n",
              " (31.224758, 121.446392): 620,\n",
              " (31.247155, 121.23112): 621,\n",
              " (31.109271, 121.617421): 622,\n",
              " (31.327049, 121.443238): 623,\n",
              " (31.129955, 121.336848): 624,\n",
              " (31.134484, 121.42764): 625,\n",
              " (31.241762, 121.50497): 626,\n",
              " (30.823953, 121.528367): 627,\n",
              " (31.246059, 121.362706): 628,\n",
              " (31.334449, 121.536876): 629,\n",
              " (31.184319, 121.230946): 630,\n",
              " (31.039301, 121.593594): 631,\n",
              " (31.233629, 121.532011): 632,\n",
              " (31.152182, 121.564661): 633,\n",
              " (30.935874, 121.692581): 634,\n",
              " (31.233973, 121.41641): 635,\n",
              " (31.280339, 121.402957): 636,\n",
              " (31.012108, 121.074488): 637,\n",
              " (31.337262, 121.473928): 638,\n",
              " (31.423181, 121.387917): 639,\n",
              " (31.052639, 121.761696): 640,\n",
              " (31.210887, 121.315496): 641,\n",
              " (31.295335, 121.555781): 642,\n",
              " (31.305522, 121.483135): 643,\n",
              " (31.260429, 121.426787): 644,\n",
              " (31.06623, 121.321933): 645,\n",
              " (30.841016, 121.482504): 646,\n",
              " (31.323091, 121.364299): 647,\n",
              " (31.25393, 121.455087): 648,\n",
              " (30.891157, 121.186557): 649,\n",
              " (31.081475, 121.523285): 650,\n",
              " (31.248146, 121.441241): 651,\n",
              " (31.273276, 121.1303): 652,\n",
              " (31.460409, 121.401649): 653,\n",
              " (30.87706, 121.856429): 654,\n",
              " (31.326619, 121.493433): 655,\n",
              " (31.145439, 121.417433): 656,\n",
              " (31.195907, 121.410659): 657,\n",
              " (31.216359, 121.397227): 658,\n",
              " (31.19979, 121.389906): 659,\n",
              " (31.30647, 121.492925): 660,\n",
              " (31.197114, 121.442853): 661,\n",
              " (31.242893, 121.465382): 662,\n",
              " (31.24641, 121.548541): 663,\n",
              " (31.292718, 121.410291): 664,\n",
              " (31.230287, 121.557116): 665,\n",
              " (31.086136, 121.507054): 666,\n",
              " (30.935314, 121.733322): 667,\n",
              " (31.274169, 121.221302): 668,\n",
              " (31.209913, 121.425292): 669,\n",
              " (31.306306, 121.28988): 670,\n",
              " (31.045319, 121.846169): 671,\n",
              " (31.260555, 121.410883): 672,\n",
              " (31.272774, 121.436412): 673,\n",
              " (30.960195, 121.398632): 674,\n",
              " (31.162282, 121.426688): 675,\n",
              " (30.796356, 121.199264): 676,\n",
              " (30.950067, 121.449083): 677,\n",
              " (30.960478, 121.057529): 678,\n",
              " (30.7653, 121.292527): 679,\n",
              " (31.147783, 121.366475): 680,\n",
              " (31.257553, 121.311711): 681,\n",
              " (31.232729, 121.566935): 682,\n",
              " (31.395973, 121.258897): 683,\n",
              " (31.228017, 121.444777): 684,\n",
              " (31.33454, 121.46611): 685,\n",
              " (31.134548, 121.39095): 686,\n",
              " (31.335151, 121.590721): 687,\n",
              " (30.897085, 121.617832): 688,\n",
              " (30.850119, 121.499982): 689,\n",
              " (31.140979, 121.586245): 690,\n",
              " (31.207139, 121.449578): 691,\n",
              " (31.165339, 121.427732): 692,\n",
              " (31.151421, 121.279419): 693,\n",
              " (31.327488, 121.437837): 694,\n",
              " (31.260814, 121.490337): 695,\n",
              " (31.221662, 121.539905): 696,\n",
              " (31.176978, 121.499522): 697,\n",
              " (31.020849, 121.759757): 698,\n",
              " (31.163393, 121.381735): 699,\n",
              " (31.207679, 121.489691): 700,\n",
              " (31.143965, 121.573343): 701,\n",
              " (31.405231, 121.506638): 702,\n",
              " (31.406426, 121.476433): 703,\n",
              " (31.203552, 121.371544): 704,\n",
              " (31.280857, 121.169253): 705,\n",
              " (31.39034, 121.272165): 706,\n",
              " (31.262756, 121.625447): 707,\n",
              " (31.045101, 121.223676): 708,\n",
              " (31.382812, 121.242351): 709,\n",
              " (31.152045, 121.420017): 710,\n",
              " (31.253405, 121.091261): 711,\n",
              " (31.186465, 121.429887): 712,\n",
              " (31.176998, 121.423586): 713,\n",
              " (31.334186, 121.50238): 714,\n",
              " (31.088099, 121.430111): 715,\n",
              " (31.150398, 121.537521): 716,\n",
              " (31.218201, 121.487151): 717,\n",
              " (31.240601, 121.442307): 718,\n",
              " (31.222119, 121.454078): 719,\n",
              " (31.258656, 121.472538): 720,\n",
              " (31.124812, 121.508263): 721,\n",
              " (31.426865, 121.351198): 722,\n",
              " (31.162868, 121.448422): 723,\n",
              " (31.239454, 121.46057): 724,\n",
              " (31.068209, 121.373076): 725,\n",
              " (31.415226, 121.349387): 726,\n",
              " (31.04822, 121.231928): 727,\n",
              " (31.327299, 121.452936): 728,\n",
              " (31.254534, 121.34772): 729,\n",
              " (31.239025, 121.551622): 730,\n",
              " (31.327319, 121.462047): 731,\n",
              " (31.339206, 121.453188): 732,\n",
              " (31.132833, 121.17281): 733,\n",
              " (31.121363, 121.369095): 734,\n",
              " (31.182725, 121.41492): 735,\n",
              " (31.274564, 121.51979): 736,\n",
              " (31.283403, 121.541263): 737,\n",
              " (30.929412, 121.460403): 738,\n",
              " (31.23543, 121.447652): 739,\n",
              " (31.251894, 121.464026): 740,\n",
              " (31.251883, 121.418603): 741,\n",
              " (31.207348, 121.70185): 742,\n",
              " (31.275929, 121.473484): 743,\n",
              " (31.197909, 121.426542): 744,\n",
              " (30.73407, 121.350673): 745,\n",
              " (31.288777, 121.390078): 746,\n",
              " (31.088801, 121.536584): 747,\n",
              " (31.005717, 121.202014): 748,\n",
              " (30.939279, 121.074396): 749,\n",
              " (31.143774, 121.519955): 750,\n",
              " (31.130891, 121.472384): 751,\n",
              " (31.302355, 121.515812): 752,\n",
              " (31.221001, 121.731159): 753,\n",
              " (31.221777, 121.558508): 754,\n",
              " (31.098558, 121.377147): 755,\n",
              " (30.852916, 121.364997): 756,\n",
              " (31.456258, 121.413525): 757,\n",
              " (31.057069, 121.599172): 758,\n",
              " (31.1744, 121.285083): 759,\n",
              " (31.168853, 121.394657): 760,\n",
              " (31.205813, 121.413544): 761,\n",
              " (30.844717, 121.528288): 762,\n",
              " (31.235247, 121.383101): 763,\n",
              " (30.746286, 121.344028): 764,\n",
              " (31.267266, 121.385292): 765,\n",
              " (31.10114, 121.181649): 766,\n",
              " (31.234259, 121.52572): 767,\n",
              " (31.343106, 121.455682): 768,\n",
              " (30.975655, 121.228194): 769,\n",
              " (30.956486, 121.40597): 770,\n",
              " (31.087507, 121.398717): 771,\n",
              " (31.30446, 121.426575): 772,\n",
              " (31.159569, 121.145966): 773,\n",
              " (31.118723, 121.248449): 774,\n",
              " (31.253808, 121.483756): 775,\n",
              " (31.304576, 121.522177): 776,\n",
              " (31.313444, 121.309439): 777,\n",
              " (31.124664, 121.323455): 778,\n",
              " (31.321416, 121.215): 779,\n",
              " (31.201279, 121.709485): 780,\n",
              " (30.889377, 121.176142): 781,\n",
              " (31.30835, 121.657013): 782,\n",
              " (31.276268, 121.683198): 783,\n",
              " (31.04139, 121.741651): 784,\n",
              " (31.145641, 121.449009): 785,\n",
              " (31.304081, 121.326517): 786,\n",
              " (31.192275, 121.462826): 787,\n",
              " (31.200224, 121.316463): 788,\n",
              " (31.22135, 121.391319): 789,\n",
              " (31.276349, 121.39526): 790,\n",
              " (31.427978, 121.433301): 791,\n",
              " (30.912905, 121.801126): 792,\n",
              " (31.250734, 121.487731): 793,\n",
              " (31.017385, 121.715431): 794,\n",
              " (31.255173, 121.403569): 795,\n",
              " (31.141245, 121.500503): 796,\n",
              " (31.058407, 121.330724): 797,\n",
              " (31.222233, 121.623547): 798,\n",
              " (30.993883, 121.129758): 799,\n",
              " (31.212113, 121.444193): 800,\n",
              " (31.234004, 121.276012): 801,\n",
              " (31.160199, 121.557134): 802,\n",
              " (31.050708, 121.217094): 803,\n",
              " (31.136777, 121.546762): 804,\n",
              " (31.207125, 121.299924): 805,\n",
              " (31.227762, 121.495598): 806,\n",
              " (31.17939, 121.754087): 807,\n",
              " (25.222206, 117.086322): 808,\n",
              " (31.281813, 121.354285): 809,\n",
              " (31.173856, 121.350522): 810,\n",
              " (31.2716, 121.465223): 811,\n",
              " (30.903508, 121.331287): 812,\n",
              " (31.021822, 121.590566): 813,\n",
              " (31.1634, 121.396418): 814,\n",
              " (30.919796, 121.459621): 815,\n",
              " (31.052986, 121.506423): 816,\n",
              " (31.387989, 121.396749): 817,\n",
              " (31.242514, 121.493696): 818,\n",
              " (31.254308, 121.459288): 819,\n",
              " (30.876032, 121.466173): 820,\n",
              " (31.327041, 121.661258): 821,\n",
              " (31.190185, 121.443475): 822,\n",
              " (31.250271, 121.406561): 823,\n",
              " (31.207247, 121.572933): 824,\n",
              " (30.798179, 121.426469): 825,\n",
              " (31.385543, 121.469415): 826,\n",
              " (31.370654, 121.459272): 827,\n",
              " (30.933365, 121.873737): 828,\n",
              " (31.260816, 121.36983): 829,\n",
              " (30.900763, 121.246509): 830,\n",
              " (31.181785, 121.529159): 831,\n",
              " (31.311631, 121.354326): 832,\n",
              " (31.260465, 121.461228): 833,\n",
              " (31.219455, 121.098597): 834,\n",
              " (31.337091, 121.543621): 835,\n",
              " (30.8406, 121.251681): 836,\n",
              " (31.387389, 121.431922): 837,\n",
              " (31.171972, 121.140685): 838,\n",
              " (31.163459, 121.574025): 839,\n",
              " (31.33775, 121.306733): 840,\n",
              " (31.244859, 121.489305): 841,\n",
              " (31.201303, 121.434409): 842,\n",
              " (30.932475, 121.692651): 843,\n",
              " (31.168519, 121.804669): 844,\n",
              " (31.396351, 121.44222): 845,\n",
              " (31.360977, 121.444195): 846,\n",
              " (31.402828, 121.48779): 847,\n",
              " (30.998611, 121.216212): 848,\n",
              " (31.211617, 121.486703): 849,\n",
              " (31.379455, 121.372876): 850,\n",
              " (31.258903, 121.441965): 851,\n",
              " (31.217682, 121.499709): 852,\n",
              " (29.263844, 115.023159): 853,\n",
              " (31.278808, 121.419722): 854,\n",
              " (31.283733, 121.149832): 855,\n",
              " (31.155342, 121.8105): 856,\n",
              " (31.26604, 121.399004): 857,\n",
              " (30.739797, 121.32739): 858,\n",
              " (31.414938, 121.486809): 859,\n",
              " (31.27369, 121.537119): 860,\n",
              " (31.146551, 121.235394): 861,\n",
              " (30.705762, 121.33512): 862,\n",
              " (31.235453, 121.653548): 863,\n",
              " (31.046524, 121.749155): 864,\n",
              " (30.967758, 121.08302): 865,\n",
              " (31.455174, 121.413981): 866,\n",
              " (30.920156, 121.488943): 867,\n",
              " (30.751838, 121.360056): 868,\n",
              " (31.32755, 121.15343): 869,\n",
              " (31.015452, 121.154814): 870,\n",
              " (30.900529, 121.246804): 871,\n",
              " (30.88883, 121.130607): 872,\n",
              " (31.236828, 121.514196): 873,\n",
              " (31.129482, 121.192266): 874,\n",
              " (31.283201, 121.665402): 875,\n",
              " (31.38336, 121.420672): 876,\n",
              " (31.298177, 121.473198): 877,\n",
              " (31.244229, 121.475231): 878,\n",
              " (31.149628, 121.43924): 879,\n",
              " (31.24708, 121.419525): 880,\n",
              " (31.441487, 121.179013): 881,\n",
              " (31.177248, 121.51249): 882,\n",
              " (31.201965, 121.452457): 883,\n",
              " (31.26338, 121.517162): 884,\n",
              " (31.03732, 121.18963): 885,\n",
              " (30.972355, 121.610007): 886,\n",
              " (31.073706, 121.680729): 887,\n",
              " (31.224226, 121.43839): 888,\n",
              " (31.230748, 121.485488): 889,\n",
              " (31.27795, 121.538718): 890,\n",
              " (31.256071, 121.462096): 891,\n",
              " (31.237022, 121.546184): 892,\n",
              " (31.234796, 121.533464): 893,\n",
              " (31.243013, 121.416207): 894,\n",
              " (31.346682, 121.49352): 895,\n",
              " (31.127837, 121.380654): 896,\n",
              " (31.248587, 121.277434): 897,\n",
              " (31.284675, 121.516006): 898,\n",
              " (31.265918, 121.428644): 899,\n",
              " (31.154875, 121.131226): 900,\n",
              " (31.23301, 121.442524): 901,\n",
              " (30.747024, 121.287991): 902,\n",
              " (31.072213, 121.658885): 903,\n",
              " (31.107362, 121.020176): 904,\n",
              " (30.873326, 121.284335): 905,\n",
              " (31.130862, 121.091425): 906,\n",
              " (31.383625, 121.268069): 907,\n",
              " (31.244555, 121.5099): 908,\n",
              " (31.414397, 121.481621): 909,\n",
              " (31.000658, 121.659325): 910,\n",
              " (31.236862, 121.355595): 911,\n",
              " (31.227167, 121.633295): 912,\n",
              " (31.22622, 121.506261): 913,\n",
              " (31.20172, 121.743118): 914,\n",
              " (31.297887, 121.62077): 915,\n",
              " (31.43737, 121.432451): 916,\n",
              " (30.734301, 121.25722): 917,\n",
              " (31.193811, 121.373792): 918,\n",
              " (31.030356, 121.535809): 919,\n",
              " (30.984047, 121.726879): 920,\n",
              " (31.252877, 121.148181): 921,\n",
              " (31.225318, 121.437806): 922,\n",
              " (31.315825, 121.532151): 923,\n",
              " (30.946702, 121.624426): 924,\n",
              " (31.280348, 121.592755): 925,\n",
              " (31.358732, 121.508333): 926,\n",
              " (31.209857, 121.644884): 927,\n",
              " (31.16584, 121.530742): 928,\n",
              " (31.253346, 121.448039): 929,\n",
              " (31.039193, 121.604274): 930,\n",
              " (31.317294, 121.460019): 931,\n",
              " (31.303373, 121.4429): 932,\n",
              " (31.219437, 121.374523): 933,\n",
              " (31.090076, 121.406999): 934,\n",
              " (31.475425, 121.356951): 935,\n",
              " (31.122449, 121.582926): 936,\n",
              " (31.290434, 121.425041): 937,\n",
              " (30.774911, 121.259927): 938,\n",
              " (31.242039, 121.687875): 939,\n",
              " (31.273982, 121.512661): 940,\n",
              " (30.988441, 121.87486): 941,\n",
              " (31.255025, 121.199554): 942,\n",
              " (31.294393, 121.497298): 943,\n",
              " (31.209872, 121.409627): 944,\n",
              " (31.032045, 121.783775): 945,\n",
              " (30.86683, 121.83302): 946,\n",
              " (31.19124, 121.507961): 947,\n",
              " (31.033659, 121.577846): 948,\n",
              " (31.026816, 121.426094): 949,\n",
              " (31.373432, 121.490334): 950,\n",
              " (31.413754, 121.202263): 951,\n",
              " (31.10888, 121.390919): 952,\n",
              " (31.025763, 121.541983): 953,\n",
              " (31.042149, 121.539262): 954,\n",
              " (31.177169, 121.445517): 955,\n",
              " (31.242991, 121.515376): 956,\n",
              " (31.453478, 121.256429): 957,\n",
              " (31.312672, 121.593011): 958,\n",
              " (30.867598, 121.198612): 959,\n",
              " (30.905367, 121.832974): 960,\n",
              " (31.15702, 121.106352): 961,\n",
              " (30.733903, 121.341904): 962,\n",
              " (31.320837, 121.542217): 963,\n",
              " (30.924981, 121.710847): 964,\n",
              " (31.224385, 121.143509): 965,\n",
              " (31.221876, 121.687381): 966,\n",
              " (31.071955, 121.234457): 967,\n",
              " (31.066368, 121.2082): 968,\n",
              " (31.108876, 121.345256): 969,\n",
              " (31.281363, 121.459935): 970,\n",
              " (31.17605, 121.431225): 971,\n",
              " (31.217372, 121.460724): 972,\n",
              " (22.522803, 114.218796): 973,\n",
              " (30.953548, 121.636227): 974,\n",
              " (31.323113, 121.616609): 975,\n",
              " (30.838466, 121.228977): 976,\n",
              " (30.800042, 121.417303): 977,\n",
              " (30.865062, 121.662776): 978,\n",
              " (31.253394, 121.389864): 979,\n",
              " (31.182371, 121.526439): 980,\n",
              " (30.893975, 121.016922): 981,\n",
              " (31.103853, 121.810034): 982,\n",
              " (31.236009, 121.478941): 983,\n",
              " (31.397749, 121.267217): 984,\n",
              " (31.100034, 121.663489): 985,\n",
              " (31.242841, 121.391309): 986,\n",
              " (31.155349, 121.375527): 987,\n",
              " (31.170097, 121.608906): 988,\n",
              " (31.301954, 121.54744): 989,\n",
              " (31.188696, 121.181232): 990,\n",
              " (31.198799, 121.383701): 991,\n",
              " (31.010331, 121.381275): 992,\n",
              " (31.210843, 121.491168): 993,\n",
              " (31.285861, 121.507007): 994,\n",
              " (30.908583, 121.656169): 995,\n",
              " (31.030308, 121.650922): 996,\n",
              " (31.018176, 121.418004): 997,\n",
              " (31.396658, 121.496972): 998,\n",
              " (31.206668, 121.449678): 999,\n",
              " (31.123621, 121.266561): 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5D9IoBtGX6R"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvAwpD4GrJu"
      },
      "source": [
        "## Reproducibility seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8p5d4mp9GDah"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import string\n",
        "import random\n",
        "def get_reproducible_seeds(name=\"ProjectLong\",nb_seeds=100):\n",
        "    # Calculate SHA-256 hash\n",
        "    sha256_hash = hashlib.sha256(name.encode()).hexdigest()\n",
        "    # Define character sets\n",
        "    digits = string.digits\n",
        "    # Use the hash to seed the random number generator\n",
        "    hash_as_int = int(sha256_hash, 16)\n",
        "    random.seed(hash_as_int)\n",
        "    # Generate a random list of seed of desired length\n",
        "    reproducibility_seeds = [random.randint(0,10000) for _ in range(nb_seeds)]\n",
        "\n",
        "    return reproducibility_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Mn8U0p9TGJXK"
      },
      "outputs": [],
      "source": [
        "reproducibility_seed=get_reproducible_seeds()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mzoE-MHqLa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8nhxCSLNHsd9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class VariableLengthDatasetWithPosID(Dataset):\n",
        "    def __init__(self, time_series, transform=None):\n",
        "        self.times_series=time_series\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.times_series)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_dict=self.times_series[idx]\n",
        "\n",
        "        return  user_dict['input'],user_dict['month'],user_dict['day'],user_dict['hour'],user_dict['minute'],user_dict['second'],user_dict['pos_id'],user_dict['pos_id_target'], user_dict['time_target']\n",
        "\n",
        "def create_dataset(list_users,split=[0.8,0.1,0.1]):\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "  return dataset_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGgl2XnIhDQ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nke01dG-KJxO"
      },
      "outputs": [],
      "source": [
        "def collate_fn_padd(batch):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "    ## get sequence lengths\n",
        "    inputs,month,day,hour,minute,second,pos_ids,pos_id_targets,time_targets=zip(*batch)\n",
        "    lengths = torch.tensor([ input.shape[0] for input in inputs ])\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs,batch_first=True,padding_value=0)\n",
        "    month = torch.nn.utils.rnn.pad_sequence(month,batch_first=True,padding_value=13)\n",
        "    day = torch.nn.utils.rnn.pad_sequence(day,batch_first=True,padding_value=32)\n",
        "    hour = torch.nn.utils.rnn.pad_sequence(hour,batch_first=True,padding_value=25)\n",
        "    minute = torch.nn.utils.rnn.pad_sequence(minute,batch_first=True,padding_value=60)\n",
        "    second = torch.nn.utils.rnn.pad_sequence(second,batch_first=True,padding_value=60)\n",
        "\n",
        "\n",
        "    time_targets = torch.nn.utils.rnn.pad_sequence(time_targets,batch_first=True,padding_value=-1)\n",
        "    pos_ids = torch.nn.utils.rnn.pad_sequence(pos_ids,batch_first=True,padding_value=len(vocab))\n",
        "    pos_id_targets = torch.nn.utils.rnn.pad_sequence(pos_id_targets,batch_first=True,padding_value=len(vocab))\n",
        "\n",
        "\n",
        "    return inputs,month,day,hour,minute,second, pos_ids, time_targets, pos_id_targets, lengths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Yl1E6gY8_P"
      },
      "source": [
        "## Instanciate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GHpriSipY7Kz"
      },
      "outputs": [],
      "source": [
        "dataset_list=create_dataset(list_users)\n",
        "train_dataset=dataset_list[0]\n",
        "valid_dataset=dataset_list[1]\n",
        "test_dataset=dataset_list[2]\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=64,collate_fn=collate_fn_padd,shuffle=True)\n",
        "valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HodJbvKeMe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptycyS7FWE4b"
      },
      "source": [
        "## Transformer Encoder followed by LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2yFXZxqeHMwi"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class VanillaPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = (x.transpose(0,1) + self.pe[:x.transpose(0,1).size(0)]).transpose(0,1)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dX-kk1ScG-_n"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.positional_embedding=nn.Embedding(num_embeddings=max_len,embedding_dim= d_model)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size,seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        #print(x.shape[1])\n",
        "        x = x  + self.positional_embedding(torch.arange(0,x.shape[1]).cuda())\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JzKYvfaWG6cH"
      },
      "outputs": [],
      "source": [
        "def get_PositionalEncoding(d_model: int, dropout: float = 0.1, max_len: int = 2000, learnable=False):\n",
        "  if learnable:\n",
        "    return LearnablePositionalEncoding(d_model, dropout, max_len)\n",
        "  else:\n",
        "    return VanillaPositionalEncoding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8SZQnLdN4UxY"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,  dropout=dropout, batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only=False,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,batch_first=batch_first)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers,nhead,dropout,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GSt_zuJRKgBh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "def get_mask(bath_size,sequence_length,lengths):\n",
        "  mask=torch.zeros(bath_size,sequence_length).cuda()\n",
        "  for i, length in enumerate(lengths):\n",
        "    mask[i,length:]=1\n",
        "  return mask.bool()\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(F.relu(self.linear_perceptron_in(x)))\n",
        "\n",
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=d_model)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=d_model)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=d_model)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=60,embedding_dim=d_model)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=60,embedding_dim=d_model)\n",
        "\n",
        "  def forward(self,x,month,day,hour,minute,second):\n",
        "    return x\n",
        "    + self.month_embedding(month)\n",
        "    + self.day_embedding(day)\n",
        "    + self.hour_embedding(hour)\n",
        "    + self.minute_embedding(minute)\n",
        "    + self.second_embedding(second)\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,embedding_dim,num_layers,num_heads,dropout=0.1,batch_first=True):\n",
        "    super().__init__()\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.mlp=MLP(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "    x=self.layer_normalisation(x)\n",
        "    x=self.dropout(x)\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    x=x.data\n",
        "    x=self.mlp(x)+x\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "    x=self.layer_normalisation(x)\n",
        "    x=self.dropout(x)\n",
        "    return torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,d_model,nb_of_pos_ids,output_regression_size,output_classfication_size,embedding_dim,num_layers_lstm,num_layers_transformer,encoder_only,nhead,learnable_pos_encoding,new_station_binary_classification,use_gcn,vocab,hidden_dim1, hidden_dim2,max_len=500,dropout=0.1,batch_first=True,concatenate_feature=True):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.station_embeddings=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "    self.timeEmbedding=TimeStampEmbedding(d_model)\n",
        "    self.num_feature=2\n",
        "    if num_layers_transformer>0:\n",
        "      self.pos_encoder = get_PositionalEncoding(d_model, dropout, max_len,learnable_pos_encoding)\n",
        "      self.transformer_model=get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,nhead,dropout,batch_first)\n",
        "    if num_layers_lstm>0:\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(d_model,output_regression_size,output_classfication_size,embedding_dim,1,nhead,dropout=dropout,batch_first=batch_first) for layer in range(num_layers_lstm)])\n",
        "    self.linear_reg=nn.Linear(d_model,output_regression_size)\n",
        "    self.classifier=nn.Linear(d_model,output_classfication_size)\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.binary_classifier=nn.Linear(d_model,1)\n",
        "    self.num_layers_transformer=num_layers_transformer\n",
        "    self.num_layers_lstm=num_layers_lstm\n",
        "    if self.use_gcn:\n",
        "      self.num_feature+=1\n",
        "      self.gcn = GCN(hidden_dim1, hidden_dim2, d_model,vocab)\n",
        "\n",
        "  def forward(self,x,month,day,hour,minute,second,pos_id,lengths,reg=True):\n",
        "    #BEFORE: x.shape=(batch_size, max_sequence_length,input_size); pos_id.shape=(batch_size,max_sequence_length)\n",
        "    x=self.station_embeddings(pos_id)\n",
        "    x=self.timeEmbedding(x,month,day,hour,minute,second)\n",
        "    #print(self.gcn(pos_id).isnan().sum())\n",
        "    #flf\n",
        "    x+=self.gcn(pos_id)\n",
        "    if self.num_layers_transformer>0:\n",
        "      self.pos_encoder(x)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],lengths)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device= torch.device('cuda'))\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "      x=self.timeEmbedding(x,month,day,hour,minute,second)\n",
        "      x+=self.station_embeddings(pos_id)\n",
        "      x+=self.gcn(pos_id)\n",
        "    #print(x.shape)\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.num_layers_lstm>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,lengths)\n",
        "    x=F.relu(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(torch.exp(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.new_station_binary_classification:\n",
        "      out[\"new_station\"]=  torch.nn.utils.rnn.PackedSequence( self.binary_classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1FQrA5vg1Px"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bbp1dVXWQs3"
      },
      "source": [
        "## graph_deepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW174iJrhFC",
        "outputId": "180f0a48-7090-49f1-da78-c84c1df75a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpysal\n",
            "  Downloading libpysal-4.9.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m0.3/2.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[91m\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.12.3)\n",
            "Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from libpysal) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.25.2)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (23.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.11.4)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.10->libpysal) (2.5)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (1.9.5)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (67.7.2)\n",
            "Installing collected packages: libpysal\n",
            "Successfully installed libpysal-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install libpysal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "r-HKmoZiWP4S",
        "outputId": "b599b585-fa18-431c-f1cc-b2ec98c64d1c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'voronoi_frames' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-1e4c49373c8c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcoordinates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcells\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoronoi_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"convex hull\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdelaunay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcells\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdelaunay_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelaunay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_networkx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'voronoi_frames' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "x_array=[key[0] for key in vocab]\n",
        "y_array=[key[1] for key in vocab]\n",
        "coordinates=np.column_stack((x_array,y_array))\n",
        "cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "delaunay = weights.Rook.from_dataframe(cells)\n",
        "delaunay_graph = delaunay.to_networkx()\n",
        "positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SQvAPDZyWNtg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea476b75-6073-42dc-ef37-49c0aa6e43cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=23ace6746cb212a0fabf2ec30ba6e1d5cd726d2639911f752504e9d4cb356118\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "sdSTBOc3sKsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50a8457-367b-4aae-ca11-df12672ee0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-73b66cb624ec>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/convert.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  data_dict[key] = torch.as_tensor(value)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, hidden_dim1, hidden_dim2, output_dim,vocab):\n",
        "        super(GCN, self).__init__()\n",
        "        net=get_net(vocab)\n",
        "        self.edge_index=edge_index = net.edge_index.long().cuda()\n",
        "        self.distance= net.distance.float().cuda()\n",
        "        self.coordinates=net.coordinates.float().cuda()\n",
        "        mean_distance=self.distance.mean()\n",
        "        std_distance=self.distance.std()\n",
        "        self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "        mean_coordinates=self.coordinates.mean(dim=0)\n",
        "        std_coordinates=self.coordinates.std(dim=0)\n",
        "        #print(self.coordinates.shape,mean_coordinates.unsqueeze(0).shape)\n",
        "        self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "        #print(self.distance.isnan().sum(),\"eer\",self.coordinates.isnan().sum())\n",
        "\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
        "        self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
        "        self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
        "\n",
        "    def forward(self, pos_id):\n",
        "        x = self.conv1(self.coordinates, self.edge_index,)#self.distance)\n",
        "        #print(\"test\",x.isnan().sum())\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, self.edge_index,self.distance)\n",
        "        #print(x.isnan().sum())\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv3(x, self.edge_index,self.distance)\n",
        "        x=torch.cat((x,torch.zeros(1,x.shape[1]).cuda()),dim=0)\n",
        "        #print(x.isnan().sum())\n",
        "        return x[pos_id]\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_dim = 2  # Latitude and Longitude\n",
        "hidden_dim1 = 128\n",
        "hidden_dim2 = 256\n",
        "output_dim = 768\n",
        "model = GCN( hidden_dim1, hidden_dim2, output_dim,vocab).cuda()\n",
        "# Assuming you have your graph data in appropriate format\n",
        "\n",
        "output = model(torch.randint(len(vocab)+1,size=[64,100]).cuda())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6bAo3VDOelX"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yaclkV0OkjJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "predicted=torch.randint(2,(100,))\n",
        "pos_ids_target=torch.randint(2,(100,))\n",
        "index_1=pos_ids_target.nonzero()\n",
        "index_0=(pos_ids_target==0).nonzero()\n",
        "slice_index_O=torch.randperm(index_0.shape[0])[:index_1.shape[0]]\n",
        "predicted[index]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28s2GCFETdYS"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loss"
      ],
      "metadata": {
        "id": "nX3QuxcYmRJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "\n",
        "  def forward(self,x,target_pos_ids,lengths,target):\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(target_pos_ids, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    index_repeat=target.nonzero()\n",
        "    index_non_repeat=(target==0).nonzero()\n",
        "    nb_non_repeat=index_non_repeat.shape[0]\n",
        "    slice_repeat=index_repeat[torch.randperm(index_repeat.shape[0])[:int(nb_non_repeat/3)]]\n",
        "    loss_classification=self.criterion(x.data[index_non_repeat.squeeze()],target_pos_ids.data[index_non_repeat.squeeze()])\n",
        "    #loss_classification+=self.criterion(x.data[slice_repeat.squeeze()],target_pos_ids.data[slice_repeat.squeeze()])\n",
        "    #loss_classification=self.criterion(x.data,target_pos_ids.data)\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,y,time_targets,lengths):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(time_targets, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(y.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Loss_new_station_binary_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion =  nn.BCEWithLogitsLoss()\n",
        "  def forward(self,z,target_pos_ids,pos_ids,lengths):\n",
        "    #print(target_pos_ids)\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(target_pos_ids, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    pos_ids=torch.nn.utils.rnn.pack_padded_sequence(pos_ids, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    target=(target_pos_ids.data==pos_ids.data).type(torch.LongTensor).cuda()\n",
        "    loss_classification=self.criterion(z.data.squeeze(),target.float())\n",
        "    return loss_classification,target\n",
        "\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self,new_station_binary_classification=False) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.loss_new_station_binary_classification=Loss_new_station_binary_classification()\n",
        "\n",
        "\n",
        "  def forward(self, out, target_pos_ids, time_targets, pos_ids, lengths, reg=False):\n",
        "    loss={}\n",
        "    if self.new_station_binary_classification:\n",
        "      loss[\"new_station\"],target=self.loss_new_station_binary_classification(out[\"new_station\"],target_pos_ids,pos_ids,lengths)\n",
        "      #loss[\"total\"]=loss[\"new_station\"]\n",
        "    loss[\"classification\"]=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids,lengths,target)\n",
        "    loss[\"total\"]=loss[\"classification\"]\n",
        "\n",
        "    if reg:\n",
        "      loss[\"time_regression\"]=self.loss_time_regression(out[\"time_regression\"],time_targets,lengths)\n",
        "      loss[\"total\"]+=loss[\"time_regression\"]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_ujoc4c2mQh_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## evalution"
      ],
      "metadata": {
        "id": "3Namj1ah1Erg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IHCyYC32ToKU"
      },
      "outputs": [],
      "source": [
        "def evaluate(model,dataloader,criterion):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    for x, month, day, hour, minute, second, pos_ids, time_targets, target_pos_ids, lengths in dataloader:\n",
        "      batch_size=x.shape[0]\n",
        "      x=x.float().cuda()\n",
        "      pos_ids=pos_ids.cuda()\n",
        "      time_targets = time_targets.cuda()\n",
        "      target_pos_ids = target_pos_ids.cuda()\n",
        "      with autocast(device_type=\"cuda\"):\n",
        "        out=model(x,month,day,hour,minute,second,pos_ids,lengths,reg=True)\n",
        "        loss=criterion(out, target_pos_ids, time_targets, pos_ids, lengths,reg=True)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(target_pos_ids, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    return acc.item()/(nb_points),loss[\"classification\"].item(),loss[\"time_regression\"].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##training"
      ],
      "metadata": {
        "id": "Ul43R9go1HzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "yLu25E-eTcbT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "def train(\n",
        "          epochs_classifcation_only,\n",
        "          epochs_complete_problem,\n",
        "          input_size,\n",
        "          num_heads,\n",
        "          d_model,\n",
        "          nb_of_pos_ids,\n",
        "          num_layers_lstm,\n",
        "          num_layers_transformer,\n",
        "          encoder_only,\n",
        "          output_regression_size,\n",
        "          output_classfication_size,\n",
        "          nb_batchs,\n",
        "          dropout,\n",
        "          max_len,\n",
        "          weight_decay,\n",
        "          lr,\n",
        "          learnable_pos_encoding,\n",
        "          new_station_binary_classification,\n",
        "          vocab,hidden_dim1, hidden_dim2\n",
        "          ):\n",
        "\n",
        "  embedding_dim=d_model\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n",
        "                                         nb_of_pos_ids=nb_of_pos_ids,\n",
        "                                         output_regression_size=output_regression_size,\n",
        "                                         output_classfication_size=output_classfication_size,\n",
        "                                         embedding_dim=embedding_dim,\n",
        "                                         num_layers_lstm=num_layers_lstm,\n",
        "                                         num_layers_transformer=num_layers_transformer,\n",
        "                                         encoder_only=encoder_only,\n",
        "                                         nhead=num_heads,\n",
        "                                         learnable_pos_encoding=learnable_pos_encoding,\n",
        "                                         new_station_binary_classification=new_station_binary_classification,\n",
        "                                         vocab=vocab,hidden_dim1=hidden_dim1, hidden_dim2=hidden_dim2,\n",
        "                                         max_len=max_len,\n",
        "                                         dropout=dropout,\n",
        "                                         ).cuda()\n",
        "  optimizer_encoder = optim.Adam(model.parameters(),lr=lr,weight_decay=weight_decay)\n",
        "  criterion= Total_loss(new_station_binary_classification=new_station_binary_classification)\n",
        "  train_losses=[]\n",
        "  valid_accs=[]\n",
        "  valid_losses_classification=[]\n",
        "  valid_losses_regression=[]\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_losses=[]\n",
        "    i=0\n",
        "    for x, month, day, hour, minute, second,pos_ids, time_targets, target_pos_ids, lengths in train_dataloader:\n",
        "      #print(x.shape)\n",
        "      i+=1\n",
        "      if i>=nb_batchs:\n",
        "        break\n",
        "      optimizer_encoder.zero_grad()\n",
        "      x=x.float().cuda()\n",
        "      pos_ids=pos_ids.cuda()\n",
        "      time_targets = time_targets.cuda()\n",
        "      target_pos_ids = target_pos_ids.cuda()\n",
        "      with autocast(device_type=\"cuda\"):\n",
        "        reg=epoch >= epochs_classifcation_only\n",
        "        #print(x)\n",
        "        out=model(x,month,day,hour,minute,second,pos_ids,lengths, reg)\n",
        "        #print(out)\n",
        "        #yv\n",
        "        loss=criterion(out, target_pos_ids, time_targets, pos_ids, lengths, reg)\n",
        "        loss[\"total\"].backward()\n",
        "        optimizer_encoder.step()\n",
        "      epoch_losses.append(loss[\"total\"].cpu().item())\n",
        "      del loss\n",
        "      del x\n",
        "      del pos_ids\n",
        "      del target_pos_ids\n",
        "    loss_epoch=np.mean(epoch_losses)\n",
        "    train_losses.append(loss_epoch)\n",
        "    valid_acc,valid_loss_classification,valid_loss_regression=evaluate(model,valid_dataloader,criterion)\n",
        "    valid_accs.append(valid_acc)\n",
        "    valid_losses_classification.append(valid_loss_classification)\n",
        "    valid_losses_regression.append(valid_loss_regression)\n",
        "    print(\"epoch: \"+str(epoch)+\" train loss: \"+str(loss_epoch)+\" valid_acc: \"+str(valid_acc),\" valid_loss_classification: \"+str(valid_loss_classification)+\" valid_loss_regression: \"+str(valid_loss_regression))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instance of training"
      ],
      "metadata": {
        "id": "QF4FEU_h1YtX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL0AZ-YJChyE",
        "outputId": "c2c217fc-d19b-40e4-b40c-d4bf421f0278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "<ipython-input-19-73b66cb624ec>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 train loss: 7.65563462437063 valid_acc: 0.002446235722510971  valid_loss_classification: 9.432498931884766 valid_loss_regression: 2.30960750579834\n",
            "epoch: 1 train loss: 7.284949129906254 valid_acc: 0.02267592045065513  valid_loss_classification: 10.262377738952637 valid_loss_regression: 3.6408164501190186\n",
            "epoch: 2 train loss: 6.868906332098919 valid_acc: 0.07460085275901777  valid_loss_classification: 7.939478397369385 valid_loss_regression: 2.449186325073242\n",
            "epoch: 3 train loss: 6.207157314687535 valid_acc: 0.13517164109427032  valid_loss_classification: 7.274097442626953 valid_loss_regression: 2.5340797901153564\n",
            "epoch: 4 train loss: 5.54513268539871 valid_acc: 0.17883041299679436  valid_loss_classification: 6.845264434814453 valid_loss_regression: 2.5623276233673096\n",
            "epoch: 5 train loss: 4.899852531543677 valid_acc: 0.227344309234073  valid_loss_classification: 6.284029006958008 valid_loss_regression: 2.514657974243164\n",
            "epoch: 6 train loss: 4.565951654876488 valid_acc: 0.2448663284678348  valid_loss_classification: 5.8652663230896 valid_loss_regression: 2.391078472137451\n",
            "epoch: 7 train loss: 4.235529692276664 valid_acc: 0.26183436556596434  valid_loss_classification: 5.738829612731934 valid_loss_regression: 2.546616792678833\n",
            "epoch: 8 train loss: 3.9841380084770313 valid_acc: 0.2683389872708599  valid_loss_classification: 5.688628196716309 valid_loss_regression: 2.6013925075531006\n",
            "epoch: 9 train loss: 3.719081861385401 valid_acc: 0.28761009616880895  valid_loss_classification: 5.621701717376709 valid_loss_regression: 2.6485233306884766\n",
            "epoch: 10 train loss: 3.6243886083796406 valid_acc: 0.27299492701752204  valid_loss_classification: 5.637372016906738 valid_loss_regression: 2.7770869731903076\n"
          ]
        }
      ],
      "source": [
        "model=train(\n",
        "          epochs_classifcation_only=11,\n",
        "          epochs_complete_problem =0,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=888,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=6,\n",
        "          num_layers_transformer=8,\n",
        "          encoder_only=False,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=70,\n",
        "          dropout=0.1,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=1e-4,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=True,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          #concatenate_feature=True\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFPw9qZ4QUTa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58420fa9-8ad1-4eab-e6ba-174c6dc2b3c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 train loss: 7.329884394621238 valid_acc: 0.19729233450561762  valid_loss_classification: 7.7301788330078125 valid_loss_regression: 1.5886894464492798\n",
            "epoch: 1 train loss: 5.976874155875964 valid_acc: 0.4469141950141608  valid_loss_classification: 6.434479713439941 valid_loss_regression: 1.7368848323822021\n",
            "epoch: 2 train loss: 4.723571055974716 valid_acc: 0.5731972238648034  valid_loss_classification: 5.147871971130371 valid_loss_regression: 1.7278903722763062\n",
            "epoch: 3 train loss: 3.8915583231510262 valid_acc: 0.6259998132644486  valid_loss_classification: 4.435739994049072 valid_loss_regression: 1.745749831199646\n",
            "epoch: 4 train loss: 3.27751027009426 valid_acc: 0.6592574149575177  valid_loss_classification: 4.031399726867676 valid_loss_regression: 1.8383245468139648\n",
            "epoch: 5 train loss: 2.878502729611519 valid_acc: 0.6748934051227786  valid_loss_classification: 3.7951579093933105 valid_loss_regression: 1.8549515008926392\n",
            "epoch: 6 train loss: 2.607596727517935 valid_acc: 0.6838318135134294  valid_loss_classification: 3.688779354095459 valid_loss_regression: 1.8658480644226074\n",
            "epoch: 7 train loss: 2.362287830083798 valid_acc: 0.6920855248825122  valid_loss_classification: 3.5738885402679443 valid_loss_regression: 1.8497941493988037\n",
            "epoch: 8 train loss: 2.245436023443173 valid_acc: 0.6948741091158072  valid_loss_classification: 3.4427571296691895 valid_loss_regression: 1.8906102180480957\n",
            "epoch: 9 train loss: 2.0994652265157456 valid_acc: 0.6980548380069093  valid_loss_classification: 3.4704904556274414 valid_loss_regression: 1.981113076210022\n",
            "epoch: 10 train loss: 1.9607851352447119 valid_acc: 0.7002085213656594  valid_loss_classification: 3.3994016647338867 valid_loss_regression: 1.9533582925796509\n",
            "epoch: 11 train loss: 1.8769998856079884 valid_acc: 0.7018393451806666  valid_loss_classification: 3.3339266777038574 valid_loss_regression: 1.9301677942276\n",
            "epoch: 12 train loss: 1.8639242068315163 valid_acc: 0.7030842488562448  valid_loss_classification: 3.3824939727783203 valid_loss_regression: 2.0446696281433105\n",
            "epoch: 13 train loss: 1.7856174126649513 valid_acc: 0.7041673150539977  valid_loss_classification: 3.3268089294433594 valid_loss_regression: 2.0345871448516846\n",
            "epoch: 14 train loss: 1.7487526490138128 valid_acc: 0.7045594597118048  valid_loss_classification: 3.2475709915161133 valid_loss_regression: 1.9765311479568481\n",
            "epoch: 15 train loss: 1.69864415205442 valid_acc: 0.7058728330895397  valid_loss_classification: 3.177165985107422 valid_loss_regression: 1.9331605434417725\n",
            "epoch: 16 train loss: 1.6624637261415138 valid_acc: 0.705860384052784  valid_loss_classification: 3.2759032249450684 valid_loss_regression: 2.0540597438812256\n",
            "epoch: 17 train loss: 1.6038648807085478 valid_acc: 0.7071488593570072  valid_loss_classification: 3.254763126373291 valid_loss_regression: 2.054865598678589\n",
            "epoch: 18 train loss: 1.5666838303590431 valid_acc: 0.7078460054153309  valid_loss_classification: 3.20133638381958 valid_loss_regression: 2.0152153968811035\n",
            "epoch: 19 train loss: 1.5573170276788564 valid_acc: 0.7079767203012667  valid_loss_classification: 3.213383197784424 valid_loss_regression: 2.046142816543579\n",
            "epoch: 20 train loss: 1.5033726692199707 valid_acc: 0.7092278484952227  valid_loss_classification: 3.3029792308807373 valid_loss_regression: 2.150639295578003\n",
            "epoch: 21 train loss: 1.4886803535314708 valid_acc: 0.7094145840465594  valid_loss_classification: 3.125117778778076 valid_loss_regression: 2.008413791656494\n",
            "epoch: 22 train loss: 1.4517560188586895 valid_acc: 0.7094457066384489  valid_loss_classification: 3.263676643371582 valid_loss_regression: 2.1466782093048096\n",
            "epoch: 23 train loss: 1.4361392167898326 valid_acc: 0.7092714201238679  valid_loss_classification: 3.083261013031006 valid_loss_regression: 1.9692368507385254\n",
            "epoch: 24 train loss: 1.4328739245732625 valid_acc: 0.7091407052379323  valid_loss_classification: 3.1498241424560547 valid_loss_regression: 2.049075126647949\n",
            "epoch: 25 train loss: 1.4185636456196125 valid_acc: 0.7094145840465594  valid_loss_classification: 3.1740591526031494 valid_loss_regression: 2.0891876220703125\n",
            "epoch: 26 train loss: 1.3882853297086863 valid_acc: 0.7094643801935825  valid_loss_classification: 3.0358593463897705 valid_loss_regression: 1.983586311340332\n",
            "epoch: 27 train loss: 1.430586692614433 valid_acc: 0.7102797921010862  valid_loss_classification: 3.0541656017303467 valid_loss_regression: 2.0098485946655273\n",
            "epoch: 28 train loss: 1.3651313659472344 valid_acc: 0.7100432604027264  valid_loss_classification: 3.0027694702148438 valid_loss_regression: 1.9580750465393066\n",
            "epoch: 29 train loss: 1.3600162145419 valid_acc: 0.7105661199464691  valid_loss_classification: 3.0898475646972656 valid_loss_regression: 2.061476707458496\n",
            "epoch: 30 train loss: 1.3491449356079102 valid_acc: 0.7102486695091967  valid_loss_classification: 3.0148797035217285 valid_loss_regression: 1.989415168762207\n",
            "epoch: 31 train loss: 1.3619949939923408 valid_acc: 0.7097444835205876  valid_loss_classification: 2.9977073669433594 valid_loss_regression: 1.974471092224121\n",
            "epoch: 32 train loss: 1.316266038478949 valid_acc: 0.7114002054091064  valid_loss_classification: 3.0179853439331055 valid_loss_regression: 2.0115253925323486\n",
            "epoch: 33 train loss: 1.3060871836466668 valid_acc: 0.7114562260745075  valid_loss_classification: 2.9803452491760254 valid_loss_regression: 1.987246036529541\n",
            "epoch: 34 train loss: 1.330869693022508 valid_acc: 0.7110018362329215  valid_loss_classification: 2.8896403312683105 valid_loss_regression: 1.900090217590332\n",
            "epoch: 35 train loss: 1.2995747572336442 valid_acc: 0.7113441847437054  valid_loss_classification: 2.960745096206665 valid_loss_regression: 1.9807376861572266\n",
            "epoch: 36 train loss: 1.290322916629987 valid_acc: 0.7112383679312814  valid_loss_classification: 3.037285804748535 valid_loss_regression: 2.0583269596099854\n",
            "epoch: 37 train loss: 1.2902401380049877 valid_acc: 0.711599389997199  valid_loss_classification: 2.8984251022338867 valid_loss_regression: 1.929636001586914\n",
            "epoch: 38 train loss: 1.2561318553411043 valid_acc: 0.7116242880707105  valid_loss_classification: 3.0035643577575684 valid_loss_regression: 2.031362295150757\n",
            "epoch: 39 train loss: 1.2797660705370781 valid_acc: 0.7120413308020291  valid_loss_classification: 2.9393773078918457 valid_loss_regression: 1.9649921655654907\n",
            "epoch: 40 train loss: 1.2720735164789052 valid_acc: 0.7116367371074663  valid_loss_classification: 2.944155216217041 valid_loss_regression: 1.9718362092971802\n",
            "epoch: 41 train loss: 1.2631624600826166 valid_acc: 0.7124957206436152  valid_loss_classification: 2.8251771926879883 valid_loss_regression: 1.8635404109954834\n",
            "epoch: 42 train loss: 1.2725189924240112 valid_acc: 0.7123214341290343  valid_loss_classification: 2.878107786178589 valid_loss_regression: 1.9300082921981812\n",
            "epoch: 43 train loss: 1.2352195519667406 valid_acc: 0.7117985745852915  valid_loss_classification: 2.956785202026367 valid_loss_regression: 2.005892753601074\n",
            "epoch: 44 train loss: 1.2200508270508204 valid_acc: 0.7115744919236874  valid_loss_classification: 2.8209166526794434 valid_loss_regression: 1.8712258338928223\n",
            "epoch: 45 train loss: 1.2361813202882423 valid_acc: 0.7114500015561296  valid_loss_classification: 2.897171974182129 valid_loss_regression: 1.9548996686935425\n",
            "epoch: 46 train loss: 1.195386712367718 valid_acc: 0.7117799010301578  valid_loss_classification: 2.9791669845581055 valid_loss_regression: 2.0265398025512695\n",
            "epoch: 47 train loss: 1.2186762461295495 valid_acc: 0.7117238803647568  valid_loss_classification: 2.8915653228759766 valid_loss_regression: 1.957707405090332\n",
            "epoch: 48 train loss: 1.2040755718182294 valid_acc: 0.7115807164420653  valid_loss_classification: 2.959044933319092 valid_loss_regression: 2.0208399295806885\n",
            "epoch: 49 train loss: 1.1799243474617982 valid_acc: 0.7114562260745075  valid_loss_classification: 2.8204526901245117 valid_loss_regression: 1.9051142930984497\n",
            "epoch: 50 train loss: 2.3788735438615847 valid_acc: 0.7103793843951324  valid_loss_classification: 1.8673360347747803 valid_loss_regression: 0.9347753524780273\n",
            "epoch: 51 train loss: 2.2883355770355616 valid_acc: 0.7100308113659706  valid_loss_classification: 1.8481955528259277 valid_loss_regression: 0.9246847629547119\n",
            "epoch: 52 train loss: 2.1972392828036575 valid_acc: 0.7119604120631167  valid_loss_classification: 1.8388700485229492 valid_loss_regression: 0.923212468624115\n",
            "epoch: 53 train loss: 2.2129175601861415 valid_acc: 0.7118234726588031  valid_loss_classification: 1.8457322120666504 valid_loss_regression: 0.9207108616828918\n",
            "epoch: 54 train loss: 2.222466007257119 valid_acc: 0.7102299959540631  valid_loss_classification: 1.8434932231903076 valid_loss_regression: 0.9208166003227234\n",
            "epoch: 55 train loss: 2.1212230126063027 valid_acc: 0.7111076530453456  valid_loss_classification: 1.8412059545516968 valid_loss_regression: 0.920855700969696\n",
            "epoch: 56 train loss: 2.173314161789723 valid_acc: 0.7117550029566462  valid_loss_classification: 1.8448671102523804 valid_loss_regression: 0.9208605885505676\n",
            "epoch: 57 train loss: 2.1296665913019424 valid_acc: 0.7109209174940089  valid_loss_classification: 1.8396542072296143 valid_loss_regression: 0.9196029305458069\n",
            "epoch: 58 train loss: 2.1187783968754306 valid_acc: 0.7123401076841679  valid_loss_classification: 1.835322618484497 valid_loss_regression: 0.9186170697212219\n",
            "epoch: 59 train loss: 2.149143332090133 valid_acc: 0.7109956117145436  valid_loss_classification: 1.8226659297943115 valid_loss_regression: 0.9183821082115173\n",
            "epoch: 60 train loss: 2.1137729638662095 valid_acc: 0.7110703059350783  valid_loss_classification: 1.8197437524795532 valid_loss_regression: 0.9175175428390503\n",
            "epoch: 61 train loss: 2.2004724404750724 valid_acc: 0.7110391833431888  valid_loss_classification: 1.8235054016113281 valid_loss_regression: 0.9176680445671082\n",
            "epoch: 62 train loss: 2.1488492152629752 valid_acc: 0.7116865332544894  valid_loss_classification: 1.8342077732086182 valid_loss_regression: 0.9130244851112366\n",
            "epoch: 63 train loss: 2.106760798356472 valid_acc: 0.7111636737107466  valid_loss_classification: 1.8133974075317383 valid_loss_regression: 0.9176170825958252\n",
            "epoch: 64 train loss: 2.128112847988422 valid_acc: 0.7117425539198905  valid_loss_classification: 1.8079960346221924 valid_loss_regression: 0.9163634777069092\n",
            "epoch: 65 train loss: 2.117413471906613 valid_acc: 0.7114437770377517  valid_loss_classification: 1.8023518323898315 valid_loss_regression: 0.9132631421089172\n",
            "epoch: 66 train loss: 2.175914321190272 valid_acc: 0.7123276586474122  valid_loss_classification: 1.8047330379486084 valid_loss_regression: 0.912869930267334\n",
            "epoch: 67 train loss: 2.1645633563017235 valid_acc: 0.7122529644268775  valid_loss_classification: 1.8040921688079834 valid_loss_regression: 0.9091820120811462\n",
            "epoch: 68 train loss: 2.1070452470045824 valid_acc: 0.7125206187171268  valid_loss_classification: 1.8027013540267944 valid_loss_regression: 0.9099318981170654\n",
            "epoch: 69 train loss: 2.1258999414933033 valid_acc: 0.7112321434129034  valid_loss_classification: 1.8044312000274658 valid_loss_regression: 0.913191556930542\n",
            "epoch: 70 train loss: 2.172079156606625 valid_acc: 0.7104727521708007  valid_loss_classification: 1.8191301822662354 valid_loss_regression: 0.9131483435630798\n",
            "epoch: 71 train loss: 2.0474219413904042 valid_acc: 0.7104852012075565  valid_loss_classification: 1.8088347911834717 valid_loss_regression: 0.9131134152412415\n",
            "epoch: 72 train loss: 2.1819375661703257 valid_acc: 0.7107715290529395  valid_loss_classification: 1.8081918954849243 valid_loss_regression: 0.9128170013427734\n",
            "epoch: 73 train loss: 2.07061789280329 valid_acc: 0.7111636737107466  valid_loss_classification: 1.8136727809906006 valid_loss_regression: 0.9129401445388794\n",
            "epoch: 74 train loss: 2.153321091945355 valid_acc: 0.7105349973545797  valid_loss_classification: 1.8126840591430664 valid_loss_regression: 0.9095620512962341\n",
            "epoch: 75 train loss: 2.0485508105693717 valid_acc: 0.7121844947247207  valid_loss_classification: 1.8034467697143555 valid_loss_regression: 0.9099165201187134\n",
            "epoch: 76 train loss: 2.105321868872031 valid_acc: 0.711250816968037  valid_loss_classification: 1.8005181550979614 valid_loss_regression: 0.9037860631942749\n",
            "epoch: 77 train loss: 2.072640504592504 valid_acc: 0.7116242880707105  valid_loss_classification: 1.803985595703125 valid_loss_regression: 0.9039105176925659\n",
            "epoch: 78 train loss: 2.081646665548667 valid_acc: 0.7114375525193738  valid_loss_classification: 1.7943370342254639 valid_loss_regression: 0.9050130844116211\n",
            "epoch: 79 train loss: 2.1041819804754014 valid_acc: 0.7119666365814945  valid_loss_classification: 1.7868001461029053 valid_loss_regression: 0.8933272361755371\n",
            "epoch: 80 train loss: 2.147507835657169 valid_acc: 0.7119417385079829  valid_loss_classification: 1.8113700151443481 valid_loss_regression: 0.9075978994369507\n",
            "epoch: 81 train loss: 2.085176944732666 valid_acc: 0.7113068376334382  valid_loss_classification: 1.8283259868621826 valid_loss_regression: 0.9183274507522583\n",
            "epoch: 82 train loss: 2.1419617335001626 valid_acc: 0.7112694905231708  valid_loss_classification: 1.8117311000823975 valid_loss_regression: 0.9116718769073486\n",
            "epoch: 83 train loss: 2.1634896290607943 valid_acc: 0.7121533721328313  valid_loss_classification: 1.8105332851409912 valid_loss_regression: 0.9033840298652649\n",
            "epoch: 84 train loss: 2.115830274728628 valid_acc: 0.7120475553204071  valid_loss_classification: 1.79611074924469 valid_loss_regression: 0.9041645526885986\n",
            "epoch: 85 train loss: 2.1055876108316274 valid_acc: 0.7116616351809779  valid_loss_classification: 1.7821776866912842 valid_loss_regression: 0.9021055698394775\n",
            "epoch: 86 train loss: 2.1237876812616983 valid_acc: 0.7117799010301578  valid_loss_classification: 1.7887934446334839 valid_loss_regression: 0.9043729901313782\n",
            "epoch: 87 train loss: 2.007014201237605 valid_acc: 0.7117052068096231  valid_loss_classification: 1.7880923748016357 valid_loss_regression: 0.900674045085907\n",
            "epoch: 88 train loss: 2.065596968699724 valid_acc: 0.7122654134636333  valid_loss_classification: 1.7982330322265625 valid_loss_regression: 0.9056625366210938\n",
            "epoch: 89 train loss: 2.0409459120188003 valid_acc: 0.7120662288755407  valid_loss_classification: 1.8030633926391602 valid_loss_regression: 0.9029229283332825\n",
            "epoch: 90 train loss: 2.0893780910051785 valid_acc: 0.7120288817652735  valid_loss_classification: 1.7976329326629639 valid_loss_regression: 0.8979069590568542\n",
            "epoch: 91 train loss: 2.0315199417945666 valid_acc: 0.7111823472658803  valid_loss_classification: 1.7859432697296143 valid_loss_regression: 0.9006630182266235\n",
            "epoch: 92 train loss: 2.0560168761473436 valid_acc: 0.7105536709097133  valid_loss_classification: 1.805633783340454 valid_loss_regression: 0.9067031145095825\n",
            "epoch: 93 train loss: 2.1275706871961937 valid_acc: 0.711088979490212  valid_loss_classification: 1.7978880405426025 valid_loss_regression: 0.9110788106918335\n",
            "epoch: 94 train loss: 2.0744521740155344 valid_acc: 0.7106345896486259  valid_loss_classification: 1.8105871677398682 valid_loss_regression: 0.911277711391449\n",
            "epoch: 95 train loss: 2.082128264965155 valid_acc: 0.7098440758146338  valid_loss_classification: 1.8026769161224365 valid_loss_regression: 0.9025325179100037\n",
            "epoch: 96 train loss: 2.063080310821533 valid_acc: 0.7111138775637235  valid_loss_classification: 1.810049295425415 valid_loss_regression: 0.9052987694740295\n",
            "epoch: 97 train loss: 2.0218676114693666 valid_acc: 0.7100245868475926  valid_loss_classification: 1.7929739952087402 valid_loss_regression: 0.8988935947418213\n",
            "epoch: 98 train loss: 2.0251665237622385 valid_acc: 0.7106657122405153  valid_loss_classification: 1.7957470417022705 valid_loss_regression: 0.9058712124824524\n",
            "epoch: 99 train loss: 2.021016692503905 valid_acc: 0.7120102082101397  valid_loss_classification: 1.7858080863952637 valid_loss_regression: 0.9010905623435974\n"
          ]
        }
      ],
      "source": [
        "model=train(\n",
        "          epochs_classifcation_only=50,\n",
        "          epochs_complete_problem =50,\n",
        "          input_size=2,\n",
        "          num_heads=10,\n",
        "          d_model=300,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=1,\n",
        "          num_layers_transformer=0,\n",
        "          encoder_only=False,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=40,\n",
        "          dropout=0,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=5e-4,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=False\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "y2VdXRTrCVSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b03fbcc8-ba9e-4ae1-a8b1-b5418ffb4745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "<ipython-input-45-73b66cb624ec>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 train loss: 14.458231829214787 valid_acc: 0.27099063209984126  valid_loss_classification: 13.874046325683594 valid_loss_regression: 1.8129009008407593\n",
            "epoch: 1 train loss: 11.706424934276637 valid_acc: 0.4846223273474215  valid_loss_classification: 10.610663414001465 valid_loss_regression: 1.8791038990020752\n",
            "epoch: 2 train loss: 9.63578942893208 valid_acc: 0.5779029597584887  valid_loss_classification: 9.373394012451172 valid_loss_regression: 2.0248842239379883\n",
            "epoch: 3 train loss: 8.174956107485121 valid_acc: 0.6242133764899941  valid_loss_classification: 8.772051811218262 valid_loss_regression: 1.958383321762085\n",
            "epoch: 4 train loss: 7.144522929537123 valid_acc: 0.6485948149761912  valid_loss_classification: 7.653692245483398 valid_loss_regression: 2.0124785900115967\n",
            "epoch: 5 train loss: 6.341517552085545 valid_acc: 0.6618219165292085  valid_loss_classification: 7.010973930358887 valid_loss_regression: 1.9042747020721436\n",
            "epoch: 6 train loss: 5.905943448992743 valid_acc: 0.6706980797360804  valid_loss_classification: 6.557224273681641 valid_loss_regression: 1.9187488555908203\n",
            "epoch: 7 train loss: 5.53874895538109 valid_acc: 0.675217080078429  valid_loss_classification: 6.236551761627197 valid_loss_regression: 1.793091893196106\n",
            "epoch: 8 train loss: 5.163910133251245 valid_acc: 0.6782172979365721  valid_loss_classification: 6.393878936767578 valid_loss_regression: 1.9565210342407227\n",
            "epoch: 9 train loss: 4.928085416987322 valid_acc: 0.6748436089757555  valid_loss_classification: 5.95673131942749 valid_loss_regression: 1.8298887014389038\n",
            "epoch: 10 train loss: 4.775297731593035 valid_acc: 0.6821760916249106  valid_loss_classification: 5.886410236358643 valid_loss_regression: 1.8633538484573364\n",
            "epoch: 11 train loss: 4.612032917962558 valid_acc: 0.6823628271762472  valid_loss_classification: 5.691585063934326 valid_loss_regression: 1.7817155122756958\n",
            "epoch: 12 train loss: 4.479403678921686 valid_acc: 0.6799601630823815  valid_loss_classification: 5.52481746673584 valid_loss_regression: 1.7491375207901\n",
            "epoch: 13 train loss: 4.410609722137451 valid_acc: 0.6812299648314711  valid_loss_classification: 5.641709327697754 valid_loss_regression: 1.9511420726776123\n",
            "epoch: 14 train loss: 4.343590328658837 valid_acc: 0.6822756839189568  valid_loss_classification: 5.644440174102783 valid_loss_regression: 1.833938479423523\n",
            "epoch: 15 train loss: 4.214746896771417 valid_acc: 0.6802776135196539  valid_loss_classification: 5.40602970123291 valid_loss_regression: 1.7623203992843628\n",
            "epoch: 16 train loss: 4.2168571810791455 valid_acc: 0.6799912856742709  valid_loss_classification: 5.392788887023926 valid_loss_regression: 1.80153226852417\n",
            "epoch: 17 train loss: 4.182513060777084 valid_acc: 0.6785969935576235  valid_loss_classification: 5.282647132873535 valid_loss_regression: 1.6956974267959595\n",
            "epoch: 18 train loss: 4.062675984009452 valid_acc: 0.6798916933802247  valid_loss_classification: 5.301483631134033 valid_loss_regression: 1.702107548713684\n",
            "epoch: 19 train loss: 4.073638110921003 valid_acc: 0.6768790264853257  valid_loss_classification: 5.19232177734375 valid_loss_regression: 1.7550057172775269\n",
            "epoch: 20 train loss: 4.037859875222911 valid_acc: 0.6785036257819551  valid_loss_classification: 5.211328983306885 valid_loss_regression: 1.721746802330017\n",
            "epoch: 21 train loss: 3.9763198935467265 valid_acc: 0.6789953627338084  valid_loss_classification: 5.147522926330566 valid_loss_regression: 1.7224076986312866\n",
            "epoch: 22 train loss: 3.919318327005359 valid_acc: 0.6773271918085338  valid_loss_classification: 5.22928524017334 valid_loss_regression: 1.7392958402633667\n",
            "epoch: 23 train loss: 3.898012472235638 valid_acc: 0.6763125953129376  valid_loss_classification: 5.217833042144775 valid_loss_regression: 1.7485467195510864\n",
            "epoch: 24 train loss: 3.8676666418711343 valid_acc: 0.6791820982851452  valid_loss_classification: 5.015280723571777 valid_loss_regression: 1.697985053062439\n",
            "epoch: 25 train loss: 3.859629827996959 valid_acc: 0.6759702468021537  valid_loss_classification: 5.044895172119141 valid_loss_regression: 1.69166898727417\n",
            "epoch: 26 train loss: 3.8199636659760405 valid_acc: 0.6757648376956833  valid_loss_classification: 5.06388521194458 valid_loss_regression: 1.73619544506073\n",
            "epoch: 27 train loss: 3.7818678738414375 valid_acc: 0.6746879960163082  valid_loss_classification: 5.103414058685303 valid_loss_regression: 1.6762521266937256\n",
            "epoch: 28 train loss: 3.7539417087167934 valid_acc: 0.6744639133547041  valid_loss_classification: 5.158267974853516 valid_loss_regression: 1.754655122756958\n",
            "epoch: 29 train loss: 3.7909847204236016 valid_acc: 0.6739721764028508  valid_loss_classification: 5.040521621704102 valid_loss_regression: 1.6909091472625732\n"
          ]
        }
      ],
      "source": [
        "model=train(\n",
        "          epochs_classifcation_only=30,\n",
        "          epochs_complete_problem =0,\n",
        "          input_size=2,\n",
        "          num_heads=10,\n",
        "          d_model=300,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=1,\n",
        "          num_layers_transformer=0,\n",
        "          encoder_only=False,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=70,\n",
        "          dropout=0.1,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=5e-4,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=True,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "8nqCp8V3FQMx"
      },
      "outputs": [],
      "source": [
        "def evaluate(model,dataloader,criterion_classification,criterion_regression):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    repeat=0\n",
        "    not_repeat=0\n",
        "    correct_not_repeat=0\n",
        "    correct_repeat=0\n",
        "    incorrect_not_repeat_as_repeat=0\n",
        "    incorrect_not_repeat=0\n",
        "    for x, month, day, hour, minute, second, pos_ids, time_targets, target_pos_ids, lengths in dataloader:\n",
        "      batch_size=x.shape[0]\n",
        "      x=x.float().cuda()\n",
        "      pos_ids=pos_ids.cuda()\n",
        "      time_targets = time_targets.cuda()\n",
        "      target_pos_ids = target_pos_ids.cuda()\n",
        "      with autocast(device_type=\"cuda\"):\n",
        "        out=model(x,month,day,hour,minute,second,pos_ids,lengths,reg=True)\n",
        "        x=out[\"next_station\"]\n",
        "        y=out[\"time_regression\"]\n",
        "\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(target_pos_ids, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "        time_targets=torch.nn.utils.rnn.pack_padded_sequence(time_targets, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "        ## compute mask\n",
        "        mask_time_targets = (time_targets.data != -1)\n",
        "        loss_classification=criterion_classification(x.data,target_pos_ids.data)\n",
        "        #print(\"predicted : \",x.data.argmax(dim=1),\"ground truth : \",target_pos_ids.data)\n",
        "        acc+=(x.data.argmax(dim=1)==target_pos_ids.data).sum()\n",
        "        pred=x.data.argmax(dim=1)\n",
        "        pos_ids=torch.nn.utils.rnn.pack_padded_sequence(pos_ids, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "        for i in range(len(target_pos_ids.data)):\n",
        "          if target_pos_ids.data[i]==pos_ids.data[i]:\n",
        "            repeat+=1\n",
        "\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_repeat+=1\n",
        "          else:\n",
        "            not_repeat+=1\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_not_repeat+=1\n",
        "            if target_pos_ids.data[i]!=pred[i]:\n",
        "              incorrect_not_repeat+=1\n",
        "\n",
        "          if pred[i]==pos_ids.data[i] and target_pos_ids.data[i]!=pos_ids.data[i]:\n",
        "            incorrect_not_repeat_as_repeat+=1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        nb_points+=x.data.shape[0]\n",
        "\n",
        "        loss_regression=criterion_regression(y.data,time_targets.data)\n",
        "        loss_regression = (loss_regression * mask_time_targets.data.float()).mean()\n",
        "        loss=loss_classification+loss_regression\n",
        "    print(nb_points,\"repeat: \",repeat,\" not_repeat: \",not_repeat,\" correct_repeat/repeat: \",correct_repeat/repeat,\" correct_not_repeat/not_repeat: \",correct_not_repeat/not_repeat,incorrect_not_repeat_as_repeat/incorrect_not_repeat)\n",
        "    return acc.item()/(nb_points),loss_classification.item(),loss_regression.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cIy9KQFjv8",
        "outputId": "f762e1e6-6b6e-4ee8-eb4c-5bd52fe4ea68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.9290768137039168  correct_not_repeat/not_repeat:  0.1201810588003761 0.8826394929787499\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.703362682824141, 1.4782196283340454, 0.7996112108230591)"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06MhCDWcaF9",
        "outputId": "fbcd7956-2bf4-4691-d1b2-2ed3dded7a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.9280019635058736  correct_not_repeat/not_repeat:  0.12199602020511251 0.8838912133891214\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7030942051535509, 1.4916588068008423, 0.8002792000770569)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonRxU5b96oc",
        "outputId": "981e5fbc-b9dd-4ba5-9a42-4d392a745a85"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8000778631639527  correct_not_repeat/not_repeat:  0.2599330869650784 0.6543552771540007\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6493559586788458, 1.520933985710144, 0.8001168966293335)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz2zFYSsAroq",
        "outputId": "632d2aff-a4ce-494a-d480-9b749a23aaa7"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.844417549680084  correct_not_repeat/not_repeat:  0.20434716056941682 0.7496014950805254\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6658124195329709, 1.6400545835494995, 1.670668125152588)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNlGehwEQtfH",
        "outputId": "9abc90f2-41dc-4d19-81d7-5b8d3deb1053"
      },
      "execution_count": 44,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.25378313416161685  correct_not_repeat/not_repeat:  0.32549036758435196 0.19419049471568436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2737923081147376, 3.320213556289673, 2.4352128505706787)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "Zd9gFldjZZz1",
        "outputId": "a5391ca9-5344-48a0-e218-832de0f0a9ae"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-1128e951507c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#del target_pos_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0moptimizer_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#del pos_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "#del target_pos_ids\n",
        "del model\n",
        "del optimizer_encoder\n",
        "#del pos_ids\n",
        "#del x\n",
        "del loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtscCJTUkD0R",
        "outputId": "4d604b24-43a9-40df-adb3-3a75336e4b6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50673152"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKSIleAQkHd9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "f5D9IoBtGX6R",
        "dDvAwpD4GrJu",
        "S_mzoE-MHqLa",
        "aRGgl2XnIhDQ",
        "ptycyS7FWE4b",
        "nX3QuxcYmRJo",
        "3Namj1ah1Erg",
        "Ul43R9go1HzA"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}