{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# @title device\n",
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "  return device\n",
        "device=get_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZV8IJFGT9T",
        "outputId": "74804888-ad84-4f17-cec2-46a7a92c9b88"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_LSRYqGMO2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qh5nnjRIFe0h",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title code\n",
        "from os import makedirs\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_x(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[0])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def get_y(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[1])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def read_dataframe(name):\n",
        "  if not os.path.exists(name+\".pkl\"):\n",
        "    print(\"reading dataframe: \"+name+\".xlsx\")\n",
        "    df=pd.read_excel(name+\".xlsx\")\n",
        "    df.to_pickle(name+\".pkl\")\n",
        "  else:\n",
        "    print(\"using already read daframe\")\n",
        "\n",
        "def get_vocab(poses,vocab):\n",
        "  for pos in poses:\n",
        "    if pos not in vocab and not any(isinstance(n, float) and math.isnan(n) for n in pos):\n",
        "        vocab[pos]=len(vocab)+1\n",
        "  return vocab\n",
        "\n",
        "def get_fix_time_encoding(df):\n",
        "\n",
        "  df['month_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "  df['month_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "\n",
        "  df['day_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "  df['day_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "\n",
        "  df['hour_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "  df['hour_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "\n",
        "  df['minute_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "  df['minute_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "\n",
        "  df['second_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "  df['second_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "def get_time_data(df):\n",
        "  df['month'] =  df[\"start time\"].dt.month\n",
        "  df['day'] =  df[\"start time\"].dt.day\n",
        "  df['hour'] =  df[\"start time\"].dt.hour\n",
        "  df['minute'] = df[\"start time\"].dt.minute\n",
        "  df['second'] = df[\"start time\"].dt.second\n",
        "  return df\n",
        "\n",
        "\n",
        "def tokenize_pos(pos,vocab):\n",
        "\n",
        "  if math.isnan(pos[0]) and math.isnan(pos[1]):\n",
        "    return len(vocab)\n",
        "  else:\n",
        "    return vocab[pos]\n",
        "\n",
        "def get_coordinates(df,input_position,full_dataset):\n",
        "\n",
        "  if full_dataset:\n",
        "    df['x'] = df['latitude']\n",
        "    df['y'] = df['longitude']\n",
        "  else:\n",
        "    df['x'] = df['location(latitude/lontitude)'].apply(get_x)\n",
        "    df['y'] = df['location(latitude/lontitude)'].apply(get_y)\n",
        "\n",
        "\n",
        "  if input_position:\n",
        "    df['x_normalised']=(df['x']-df['x'].mean())/(df['x'].std())\n",
        "    df['y_normalised']=(df['y']-df['y'].mean())/df['y'].std()\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_joined_coordinates(df):\n",
        "\n",
        "  df['pos']= list(zip(df['x'],df['y']))\n",
        "  poses=df['pos'].unique()\n",
        "\n",
        "  return poses\n",
        "\n",
        "def get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset):\n",
        "  col_to_drop_in_df=['date', 'end time','pos']\n",
        "  col_to_drop_in_dict=['x','y', 'time_to_end', 'time_to_next','start time', 'user id']\n",
        "  col_to_add_to_dict=[]\n",
        "  col_in_input=[]\n",
        "  if not full_dataset:\n",
        "    col_to_drop_in_df+=['location(latitude/lontitude)']\n",
        "  else:\n",
        "    col_to_drop_in_df+=['latitude','longitude']\n",
        "  if fixed_time_encoding:\n",
        "    col_to_drop_in_df+=[]\n",
        "    col_to_drop_in_dict+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "    col_in_input+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "  else:\n",
        "    col_to_add_to_dict+=['month','day','hour','minute','second']\n",
        "  if input_position:\n",
        "    col_to_drop_in_dict += ['x_normalised', 'y_normalised']\n",
        "    col_in_input+=['x_normalised', 'y_normalised']\n",
        "  return col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict\n",
        "\n",
        "def process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections):\n",
        "  #get the time to next connection\n",
        "  df_user[\"time_to_next\"] =  df_user[\"start time\"].diff(-1).dt.total_seconds()\n",
        "  dict_user=df_user.to_dict('list')\n",
        "  #create input\n",
        "  dict_user[\"pos_id\"],dict_user[\"pos_id_target\"]=torch.tensor(dict_user[\"pos_id\"][:-1]),torch.tensor(dict_user[\"pos_id\"][1:])\n",
        "\n",
        "  if col_in_input:\n",
        "    dict_user[\"input\"]=torch.tensor([dict_user[col] for col in col_in_input]).T\n",
        "    dict_user[\"input\"]=dict_user[\"input\"][:-1]\n",
        "\n",
        "  if col_to_add_to_dict:\n",
        "    for col in col_to_add_to_dict:\n",
        "      dict_user[col]=torch.tensor(dict_user[col])\n",
        "      dict_user[col]=dict_user[col][:-1]\n",
        "\n",
        "  dict_user[\"time_target\"]=torch.tensor([dict_user[\"time_to_end\"],dict_user[\"time_to_next\"]]).T\n",
        "  dict_user[\"time_target\"]=dict_user[\"time_target\"][:-1]\n",
        "  for e in col_to_drop_in_dict:\n",
        "    dict_user.pop(e)\n",
        "\n",
        "  if not with_repeated_connections:\n",
        "    dict_user=combine_repeated_connections_in_sequence_user(dict_user)\n",
        "    dict_user=delete_end_of_sequence_repeated_connections(dict_user)\n",
        "  return dict_user\n",
        "\n",
        "def delete_end_of_sequence_repeated_connections(dict_user):\n",
        "  if dict_user['pos_id'][-1]==dict_user[\"pos_id_target\"][-1]:\n",
        "    for key in dict_user:\n",
        "      dict_user[key]=dict_user[key][:-1]\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def combine_repeated_connections_in_sequence_user(dict_user):\n",
        "  index=0\n",
        "  while index < len(dict_user[\"pos_id\"])-1:\n",
        "    if dict_user[\"pos_id\"][index]==dict_user[\"pos_id_target\"][index]:\n",
        "      dict_user[\"pos_id_target\"][index]=dict_user[\"pos_id_target\"][index+1]\n",
        "      dict_user[\"time_target\"][index]=dict_user[\"time_target\"][index+1]\n",
        "      for key in dict_user:\n",
        "        dict_user[key]=torch.cat((dict_user[key][:index+1],dict_user[key][index+2:]))\n",
        "    else:\n",
        "      index+=1\n",
        "\n",
        "\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def normalize_output(list_users):\n",
        "  #get means and stds\n",
        "  time_targets=torch.cat([dict_user[\"time_target\"] for dict_user in list_users],dim=0)\n",
        "  time_targets_mean=time_targets.mean(dim=0)\n",
        "  time_targets_std=time_targets.std(dim=0)\n",
        "  #normalize\n",
        "  for i in range(len(list_users)):\n",
        "    list_users[i][\"time_target\"]=(list_users[i][\"time_target\"]-time_targets_mean)/time_targets_std\n",
        "  return list_users\n",
        "\n",
        "\n",
        "\n",
        "def process_dataframe(name,vocab,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,min_sequence_size,format=\".pkl\"):\n",
        "  df= pd.read_pickle(name+format)\n",
        "  df=df.sort_values('start time')\n",
        "  df=df.drop(['month'],axis=1)\n",
        "\n",
        "  df=get_coordinates(df,input_position,full_dataset)\n",
        "\n",
        "  poses=get_joined_coordinates(df)\n",
        "  vocab=get_vocab(poses,vocab)\n",
        "  df['pos_id'] = df['pos'].apply(lambda pos: tokenize_pos(pos,vocab))\n",
        "\n",
        "  df['time_to_end']=df['end time']-df['start time']\n",
        "  df['time_to_end']=df['time_to_end'].dt.total_seconds()\n",
        "  if fixed_time_encoding:\n",
        "    df=get_fix_time_encoding(df)\n",
        "  else:\n",
        "    df=get_time_data(df)\n",
        "\n",
        "  col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict=get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset)\n",
        "  df=df.drop(col_to_drop_in_df, axis=1)\n",
        "\n",
        "  df_user_group = df.groupby('user id')\n",
        "  list_users=[]\n",
        "  for user, df_user in df_user_group:\n",
        "    if len(df_user)>=min_sequence_size and not df_user['x'].isnull().values.any():\n",
        "        prossessed_user_data=process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections)\n",
        "        if prossessed_user_data[\"pos_id\"].shape[0]>=min_sequence_size-1:\n",
        "          list_users.append(prossessed_user_data)\n",
        "  list_users=normalize_output(list_users)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "def get_raw_data(directory,src_directory,full_dataset):\n",
        "  if  full_dataset:\n",
        "    shutil.copytree(src_directory,directory)#telecomDataset6mont\n",
        "  else:\n",
        "    runcmd('wget http://sguangwang.com/dataset/telecom.zip', verbose = False)\n",
        "    runcmd('unzip /content/telecom.zip')\n",
        "\n",
        "def get_processed_dataset(load_dataset_path):\n",
        "  saved_list_user_path = os.path.join(load_dataset_path,\"list_users\")\n",
        "  saved_vocab_path = os.path.join(load_dataset_path,\"vocab\")\n",
        "  print(\"loading already preprocessed data: \")\n",
        "  print(saved_list_user_path)\n",
        "  print(saved_vocab_path)\n",
        "  list_users=torch.load(saved_list_user_path)\n",
        "  vocab=torch.load(saved_vocab_path)\n",
        "  return list_users,vocab\n",
        "\n",
        "def process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,min_sequence_size):\n",
        "  list_users=[]\n",
        "  vocab={}\n",
        "  if not os.path.exists(directory_raw_data):\n",
        "    print('getting raw data at: '+src_directory_raw_data)\n",
        "    get_raw_data(directory_raw_data,src_directory_raw_data,full_dataset)\n",
        "  for name in os.listdir(directory_raw_data):\n",
        "    if not name.endswith(\".pkl\"):\n",
        "      complete_name=os.path.join(directory_raw_data,\".\".join(name.split(\".\")[:-1]))\n",
        "      print(\"processing dataframe: \"+complete_name)\n",
        "      read_dataframe(complete_name)\n",
        "      new_list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections,min_sequence_size=min_sequence_size)\n",
        "      list_users+=new_list_users\n",
        "  return list_users,vocab\n",
        "\n",
        "def split_long_sequences(list_users,max_sequence_length):\n",
        "  new_list_users=[]\n",
        "  for i in range(len(list_users)):\n",
        "    seq_length=list_users[i][\"input\"].shape[0]\n",
        "    if seq_length>=max_sequence_length:\n",
        "      nb_of_seq=seq_length//max_sequence_length\n",
        "      rest=seq_length%max_sequence_length\n",
        "      list_splitted_seq=nb_of_seq*[{}]\n",
        "      rest_splitted={}\n",
        "      for key in list_users[i]:\n",
        "        for j in range(nb_of_seq):\n",
        "          list_splitted_seq[j][key]=list_users[i][key][max_sequence_length*j:max_sequence_length*(j+1)]\n",
        "        if rest>2:\n",
        "          rest_splitted[key]= list_users[i][key][-rest:]\n",
        "      new_list_users=new_list_users+list_splitted_seq\n",
        "      if len(rest_splitted)>0:\n",
        "        new_list_users+=[rest_splitted]\n",
        "    else:\n",
        "      new_list_users.append(list_users[i])\n",
        "\n",
        "  return new_list_users\n",
        "\n",
        "\n",
        "\n",
        "def save_processed_data(list_users,vocab,path_to_save_dataset):\n",
        "    print(\"creating directory: \"+path_to_save_dataset)\n",
        "    os.makedirs(path_to_save_dataset,exist_ok=True)\n",
        "    print(\"saving processed data at: \")\n",
        "    save_list_user_path = os.path.join(path_to_save_dataset,\"list_users\")\n",
        "    save_vocab_path = os.path.join(path_to_save_dataset,\"vocab\")\n",
        "    print(save_list_user_path)\n",
        "    print(save_vocab_path)\n",
        "    torch.save(list_users,save_list_user_path)\n",
        "    torch.save(vocab,save_vocab_path)\n",
        "\n",
        "def get_processed_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,spliting_long_sequences,with_repeated_connections,max_sequence_length=100,min_sequence_size=1,save=False,path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month\",download=False,load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month\"):\n",
        "  if not download:\n",
        "    list_users,vocab = get_processed_dataset(load_dataset_path)\n",
        "  else:\n",
        "    list_users,vocab=process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,min_sequence_size=min_sequence_size)\n",
        "  if spliting_long_sequences:\n",
        "    print(\"spliting sequences longuer than : \"+str(max_sequence_length)+ \" steps\")\n",
        "    list_users=split_long_sequences(list_users,max_sequence_length)\n",
        "  if save:\n",
        "    save_processed_data(list_users,vocab,path_to_save_dataset)\n",
        "  return list_users,vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRA1WX8UcsV",
        "outputId": "a4c8b589-bac0-4d58-96ae-d576d10ddd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=True,\n",
        "                                    spliting_long_sequences=False,\n",
        "                                    with_repeated_connections=False,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_size=2,\n",
        "                                    save=False,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",\n",
        "                                    download=False,\n",
        "                                    load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sum_len=0\n",
        "len_list=[]\n",
        "for user in list_users:\n",
        "  len_list.append(len(user['pos_id']))\n",
        "  sum_len+=len(user['pos_id'])\n",
        "print(sum_len/len(list_users))\n",
        "len_array=np.array(len_list)\n",
        "print(len_array.mean(),len_array.std(),len_array.max(),len_array.min())\n",
        "plt.hist(len_array,100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3oCMGZR2yP72",
        "outputId": "441ff791-b521-485c-97e0-b58e2f4b3709"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.458622016936104\n",
            "21.458622016936104 25.618203688507787 100 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2448., 1909., 1118., 1149.,  800.,  753.,  626.,  599.,  516.,\n",
              "         471.,  427.,  444.,  381.,  369.,  317.,  337.,  267.,  254.,\n",
              "         239.,  239.,  237.,  234.,  222.,  205.,  206.,  171.,  189.,\n",
              "         178.,  178.,  173.,  147.,  165.,  130.,  128.,  132.,  133.,\n",
              "         134.,  118.,  122.,  109.,  130.,  109.,  118.,  122.,  103.,\n",
              "         116.,   94.,   79.,   77.,   70.,   80.,   74.,   79.,   70.,\n",
              "          82.,   74.,   69.,   61.,   77.,   64.,   69.,   54.,   65.,\n",
              "          72.,   59.,   47.,   41.,   43.,   49.,   33.,   46.,   40.,\n",
              "          49.,   44.,   45.,   34.,   33.,   30.,   31.,   30.,   24.,\n",
              "          32.,   25.,   25.,   21.,   29.,   37.,   25.,   22.,   31.,\n",
              "          19.,   26.,   21.,   21.,   21.,   23.,   17.,   12.,   21.,\n",
              "         696.]),\n",
              " array([  1.  ,   1.99,   2.98,   3.97,   4.96,   5.95,   6.94,   7.93,\n",
              "          8.92,   9.91,  10.9 ,  11.89,  12.88,  13.87,  14.86,  15.85,\n",
              "         16.84,  17.83,  18.82,  19.81,  20.8 ,  21.79,  22.78,  23.77,\n",
              "         24.76,  25.75,  26.74,  27.73,  28.72,  29.71,  30.7 ,  31.69,\n",
              "         32.68,  33.67,  34.66,  35.65,  36.64,  37.63,  38.62,  39.61,\n",
              "         40.6 ,  41.59,  42.58,  43.57,  44.56,  45.55,  46.54,  47.53,\n",
              "         48.52,  49.51,  50.5 ,  51.49,  52.48,  53.47,  54.46,  55.45,\n",
              "         56.44,  57.43,  58.42,  59.41,  60.4 ,  61.39,  62.38,  63.37,\n",
              "         64.36,  65.35,  66.34,  67.33,  68.32,  69.31,  70.3 ,  71.29,\n",
              "         72.28,  73.27,  74.26,  75.25,  76.24,  77.23,  78.22,  79.21,\n",
              "         80.2 ,  81.19,  82.18,  83.17,  84.16,  85.15,  86.14,  87.13,\n",
              "         88.12,  89.11,  90.1 ,  91.09,  92.08,  93.07,  94.06,  95.05,\n",
              "         96.04,  97.03,  98.02,  99.01, 100.  ]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sum_len=0\n",
        "len_list=[]\n",
        "for user in list_users:\n",
        "  len_list.append(len(user['pos_id']))\n",
        "  sum_len+=len(user['pos_id'])\n",
        "print(sum_len/len(list_users))\n",
        "len_array=np.array(len_list)\n",
        "print(len_array.mean(),len_array.std(),len_array.max(),len_array.min())\n",
        "plt.hist(len_array,100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mkzulva2d2ae",
        "outputId": "5f4dc430-5074-4d21-e79f-9cfd3928cab4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.458622016936104\n",
            "21.458622016936104 25.618203688507787 100 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2448., 1909., 1118., 1149.,  800.,  753.,  626.,  599.,  516.,\n",
              "         471.,  427.,  444.,  381.,  369.,  317.,  337.,  267.,  254.,\n",
              "         239.,  239.,  237.,  234.,  222.,  205.,  206.,  171.,  189.,\n",
              "         178.,  178.,  173.,  147.,  165.,  130.,  128.,  132.,  133.,\n",
              "         134.,  118.,  122.,  109.,  130.,  109.,  118.,  122.,  103.,\n",
              "         116.,   94.,   79.,   77.,   70.,   80.,   74.,   79.,   70.,\n",
              "          82.,   74.,   69.,   61.,   77.,   64.,   69.,   54.,   65.,\n",
              "          72.,   59.,   47.,   41.,   43.,   49.,   33.,   46.,   40.,\n",
              "          49.,   44.,   45.,   34.,   33.,   30.,   31.,   30.,   24.,\n",
              "          32.,   25.,   25.,   21.,   29.,   37.,   25.,   22.,   31.,\n",
              "          19.,   26.,   21.,   21.,   21.,   23.,   17.,   12.,   21.,\n",
              "         696.]),\n",
              " array([  1.  ,   1.99,   2.98,   3.97,   4.96,   5.95,   6.94,   7.93,\n",
              "          8.92,   9.91,  10.9 ,  11.89,  12.88,  13.87,  14.86,  15.85,\n",
              "         16.84,  17.83,  18.82,  19.81,  20.8 ,  21.79,  22.78,  23.77,\n",
              "         24.76,  25.75,  26.74,  27.73,  28.72,  29.71,  30.7 ,  31.69,\n",
              "         32.68,  33.67,  34.66,  35.65,  36.64,  37.63,  38.62,  39.61,\n",
              "         40.6 ,  41.59,  42.58,  43.57,  44.56,  45.55,  46.54,  47.53,\n",
              "         48.52,  49.51,  50.5 ,  51.49,  52.48,  53.47,  54.46,  55.45,\n",
              "         56.44,  57.43,  58.42,  59.41,  60.4 ,  61.39,  62.38,  63.37,\n",
              "         64.36,  65.35,  66.34,  67.33,  68.32,  69.31,  70.3 ,  71.29,\n",
              "         72.28,  73.27,  74.26,  75.25,  76.24,  77.23,  78.22,  79.21,\n",
              "         80.2 ,  81.19,  82.18,  83.17,  84.16,  85.15,  86.14,  87.13,\n",
              "         88.12,  89.11,  90.1 ,  91.09,  92.08,  93.07,  94.06,  95.05,\n",
              "         96.04,  97.03,  98.02,  99.01, 100.  ]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "sum_len=0\n",
        "len_list=[]\n",
        "for user in list_users:\n",
        "  len_list.append(len(user['pos_id']))\n",
        "  sum_len+=len(user['pos_id'])\n",
        "print(sum_len/len(list_users))\n",
        "len_array=np.array(len_list)\n",
        "print(len_array.mean(),len_array.std(),len_array.max(),len_array.min())\n",
        "plt.hist(len_array,100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K35diHP9eSr2",
        "outputId": "4b332a01-1c1d-4785-89c8-94c5d5696375"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21.458622016936104\n",
            "21.458622016936104 25.618203688507787 100 1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2448., 1909., 1118., 1149.,  800.,  753.,  626.,  599.,  516.,\n",
              "         471.,  427.,  444.,  381.,  369.,  317.,  337.,  267.,  254.,\n",
              "         239.,  239.,  237.,  234.,  222.,  205.,  206.,  171.,  189.,\n",
              "         178.,  178.,  173.,  147.,  165.,  130.,  128.,  132.,  133.,\n",
              "         134.,  118.,  122.,  109.,  130.,  109.,  118.,  122.,  103.,\n",
              "         116.,   94.,   79.,   77.,   70.,   80.,   74.,   79.,   70.,\n",
              "          82.,   74.,   69.,   61.,   77.,   64.,   69.,   54.,   65.,\n",
              "          72.,   59.,   47.,   41.,   43.,   49.,   33.,   46.,   40.,\n",
              "          49.,   44.,   45.,   34.,   33.,   30.,   31.,   30.,   24.,\n",
              "          32.,   25.,   25.,   21.,   29.,   37.,   25.,   22.,   31.,\n",
              "          19.,   26.,   21.,   21.,   21.,   23.,   17.,   12.,   21.,\n",
              "         696.]),\n",
              " array([  1.  ,   1.99,   2.98,   3.97,   4.96,   5.95,   6.94,   7.93,\n",
              "          8.92,   9.91,  10.9 ,  11.89,  12.88,  13.87,  14.86,  15.85,\n",
              "         16.84,  17.83,  18.82,  19.81,  20.8 ,  21.79,  22.78,  23.77,\n",
              "         24.76,  25.75,  26.74,  27.73,  28.72,  29.71,  30.7 ,  31.69,\n",
              "         32.68,  33.67,  34.66,  35.65,  36.64,  37.63,  38.62,  39.61,\n",
              "         40.6 ,  41.59,  42.58,  43.57,  44.56,  45.55,  46.54,  47.53,\n",
              "         48.52,  49.51,  50.5 ,  51.49,  52.48,  53.47,  54.46,  55.45,\n",
              "         56.44,  57.43,  58.42,  59.41,  60.4 ,  61.39,  62.38,  63.37,\n",
              "         64.36,  65.35,  66.34,  67.33,  68.32,  69.31,  70.3 ,  71.29,\n",
              "         72.28,  73.27,  74.26,  75.25,  76.24,  77.23,  78.22,  79.21,\n",
              "         80.2 ,  81.19,  82.18,  83.17,  84.16,  85.15,  86.14,  87.13,\n",
              "         88.12,  89.11,  90.1 ,  91.09,  92.08,  93.07,  94.06,  95.05,\n",
              "         96.04,  97.03,  98.02,  99.01, 100.  ]),\n",
              " <BarContainer object of 100 artists>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_repeated_end_of_sequence=0\n",
        "len_1=0\n",
        "for user in list_users:\n",
        "  if len(user['pos_id'])==1:\n",
        "    len_1+=1\n",
        "    if user['pos_id'][-1]==user['pos_id_target'][-1]:\n",
        "      nb_repeated_end_of_sequence+=1\n",
        "print(nb_repeated_end_of_sequence/len_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKc-6WsExyck",
        "outputId": "086c565c-e053-4d0d-9d9c-4e4f126bd9b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nb_repeated_end_of_sequence=0\n",
        "len_1=0\n",
        "for user in list_users:\n",
        "  if len(user['pos_id'])==1:\n",
        "    len_1+=1\n",
        "    if user['pos_id'][-1]==user['pos_id_target'][-1]:\n",
        "      nb_repeated_end_of_sequence+=1\n",
        "print(nb_repeated_end_of_sequence/len_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dLZ5FSAdylO",
        "outputId": "154ae112-6bd8-4c5b-d68c-47647a610502"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5D9IoBtGX6R"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvAwpD4GrJu"
      },
      "source": [
        "## Reproducibility seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8p5d4mp9GDah"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import string\n",
        "import random\n",
        "def get_reproducible_seeds(name=\"ProjectLong\",nb_seeds=100):\n",
        "    # Calculate SHA-256 hash\n",
        "    sha256_hash = hashlib.sha256(name.encode()).hexdigest()\n",
        "    # Define character sets\n",
        "    digits = string.digits\n",
        "    # Use the hash to seed the random number generator\n",
        "    hash_as_int = int(sha256_hash, 16)\n",
        "    random.seed(hash_as_int)\n",
        "    # Generate a random list of seed of desired length\n",
        "    reproducibility_seeds = [random.randint(0,10000) for _ in range(nb_seeds)]\n",
        "\n",
        "    return reproducibility_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Mn8U0p9TGJXK"
      },
      "outputs": [],
      "source": [
        "reproducibility_seed=get_reproducible_seeds()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mzoE-MHqLa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8nhxCSLNHsd9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class VariableLengthDatasetWithPosID(Dataset):\n",
        "    def __init__(self, time_series, transform=None):\n",
        "        self.times_series=time_series\n",
        "    def __len__(self):\n",
        "        return len(self.times_series)\n",
        "    def __getitem__(self, idx):\n",
        "        user_dict=self.times_series[idx]\n",
        "        return  user_dict\n",
        "\n",
        "def create_dataset(list_users,split=[0.8,0.1,0.1]):\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "  return dataset_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGgl2XnIhDQ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nke01dG-KJxO"
      },
      "outputs": [],
      "source": [
        "def collate_fn_padd(batch_dict):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "\n",
        "\n",
        "    dict_batch={key: [d[key] for d in batch_dict] for key in batch_dict[0]}\n",
        "    dict_batch[\"lengths\"] = torch.tensor([ user[\"input\"].shape[0] for user in batch_dict ])\n",
        "    if \"input\" in dict_batch:\n",
        "      dict_batch[\"input\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"input\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"month\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"month\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"day\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"day\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"hour\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"hour\"],batch_first=True,padding_value=24)\n",
        "    dict_batch[\"minute\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"minute\"],batch_first=True,padding_value=60)\n",
        "    dict_batch[\"second\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"second\"],batch_first=True,padding_value=60)\n",
        "\n",
        "    dict_batch[\"time_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"time_target\"],batch_first=True,padding_value=-1)\n",
        "    dict_batch[\"pos_id\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id\"],batch_first=True,padding_value=len(vocab))\n",
        "    dict_batch[\"pos_id_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id_target\"],batch_first=True,padding_value=len(vocab))\n",
        "    #print(dict_batch[\"input\"])\n",
        "    return dict_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Yl1E6gY8_P"
      },
      "source": [
        "## Instanciate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GHpriSipY7Kz"
      },
      "outputs": [],
      "source": [
        "dataset_list=create_dataset(list_users)\n",
        "train_dataset=dataset_list[0]\n",
        "valid_dataset=dataset_list[1]\n",
        "test_dataset=dataset_list[2]\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=128,collate_fn=collate_fn_padd,shuffle=True)\n",
        "valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HodJbvKeMe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptycyS7FWE4b"
      },
      "source": [
        "## Transformer Encoder followed by LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oeWr0HDhJfo"
      },
      "source": [
        "### transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l3_bACPajeQx"
      },
      "outputs": [],
      "source": [
        "def get_mask(bath_size,sequence_length,lengths,device):\n",
        "  mask=torch.zeros(bath_size,sequence_length).to(device)\n",
        "  for i, length in enumerate(lengths):\n",
        "    mask[i,length:]=float('-inf')\n",
        "  return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsaggvjghDsq"
      },
      "source": [
        "#### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2yFXZxqeHMwi"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class VanillaPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = (x.transpose(0,1) + self.pe[:x.transpose(0,1).size(0)]).transpose(0,1)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dX-kk1ScG-_n"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.positional_embedding=nn.Embedding(num_embeddings=max_len,embedding_dim= d_model)\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(self.parameters()).device\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size,seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x += self.positional_embedding(torch.arange(0,x.shape[1]).to(self.device))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JzKYvfaWG6cH"
      },
      "outputs": [],
      "source": [
        "def get_PositionalEncoding(d_model: int, dropout: float = 0.1, max_len: int = 2000, learnable=False):\n",
        "  if learnable:\n",
        "    return LearnablePositionalEncoding(d_model, dropout, max_len)\n",
        "  else:\n",
        "    return VanillaPositionalEncoding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-pr0W6xhOkZ"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8SZQnLdN4UxY"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,  dropout=dropout, batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only=False,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,batch_first=batch_first)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers,nhead,dropout,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCkeqmkhRnT"
      },
      "source": [
        "### feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hmqQtP0UhZqg"
      },
      "outputs": [],
      "source": [
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=embedding_dim)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=embedding_dim)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=embedding_dim)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "\n",
        "  def forward(self,dict_batch):\n",
        "    embedding= self.month_embedding(dict_batch['month'])\n",
        "    embedding=+ self.day_embedding(dict_batch['day'])\n",
        "    embedding=+ self.hour_embedding(dict_batch['hour'])\n",
        "    embedding=+ self.minute_embedding(dict_batch['minute'])\n",
        "    embedding=+ self.second_embedding(dict_batch['second'])\n",
        "    return self.dropout(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mCp5Pz6uy-FG"
      },
      "outputs": [],
      "source": [
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bbp1dVXWQs3"
      },
      "source": [
        "#### graph_deepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW174iJrhFC",
        "outputId": "7c078efb-839c-4853-93a9-b034e7f2748c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpysal\n",
            "  Downloading libpysal-4.10-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.12.3)\n",
            "Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from libpysal) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.25.2)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (23.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.11.4)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.10->libpysal) (2.5)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (1.9.5)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->libpysal) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1->libpysal) (3.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (67.7.2)\n",
            "Installing collected packages: libpysal\n",
            "Successfully installed libpysal-4.10\n"
          ]
        }
      ],
      "source": [
        "!pip install libpysal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQvAPDZyWNtg",
        "outputId": "3c4e2d8c-33bc-4162-e70a-d874610eb88e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=93357726eb42dd8b1a060dce2de51863558ae69388ec938890ca78cf9372a6dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sdSTBOc3sKsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, hidden_dim1, hidden_dim2, output_dim,vocab,dropout,device):\n",
        "    super(GCN, self).__init__()\n",
        "    net=get_net(vocab)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "    self.conv1 = GCNConv(2, hidden_dim1)\n",
        "    self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
        "    self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.conv1(self.coordinates, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "    x = self.conv2(x, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv3(x, self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PQfII-W5lETE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozXR4sW0W0Y"
      },
      "source": [
        " #### Combine feature embeddng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "I6mU1qWOjRP3"
      },
      "outputs": [],
      "source": [
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+use_gcn\n",
        "    self.concatenate_features=concatenate_features\n",
        "    self.embedding_dim=d_model\n",
        "    self.keep_input_positions=keep_input_positions\n",
        "    if keep_input_positions:\n",
        "      self.embedding_dim=self.embedding_dim-2\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,nb_of_pos_ids,dropout),TimeStampEmbedding(self.embedding_dim,dropout)]\n",
        "    if use_gcn:\n",
        "      list_feature_embedding.append(GCN(hidden_dim1, hidden_dim2, self.embedding_dim, vocab, dropout,device))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "    if self.keep_input_positions:\n",
        "      embedding=torch.cat((dic_batch[\"input\"],embedding),dim=2)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svMRI0xeji-7"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GSt_zuJRKgBh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(F.relu(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.layer_normalisation:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,d_model,nb_of_pos_ids,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,num_layers_transformer,encoder_only,nhead,learnable_pos_encoding,new_station_binary_classification,use_gcn,vocab,hidden_dim1, hidden_dim2,max_len,dropout,batch_first,concatenate_features,keep_input_positions,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.feature_embedding=Feature_embedding(d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device)\n",
        "\n",
        "    self.num_layers_transformer=num_layers_transformer\n",
        "    if num_layers_transformer>0:\n",
        "      self.pos_encoder = get_PositionalEncoding(d_model, dropout, max_len,learnable_pos_encoding)\n",
        "      self.transformer_model=get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,nhead,dropout,batch_first)\n",
        "\n",
        "    self.num_layers_lstm=num_layers_lstm\n",
        "    if num_layers_lstm>0:\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(d_model,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first) for layer in range(num_layers_lstm)])\n",
        "    self.linear_reg=nn.Linear(d_model,output_regression_size)\n",
        "    self.classifier=nn.Linear(d_model,output_classfication_size)\n",
        "\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.binary_classifier=nn.Linear(d_model,1)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.num_layers_transformer>0:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.num_layers_lstm>0:\n",
        "      if self.num_layers_transformer>0:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.num_layers_lstm>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=F.relu(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(torch.exp(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.new_station_binary_classification:\n",
        "      out[\"new_station\"]=  torch.nn.utils.rnn.PackedSequence( self.binary_classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baselines"
      ],
      "metadata": {
        "id": "zZpbR8rG8kBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class  Baseline_model(nn.Module):\n",
        "  def __init__(self,nb_of_pos_ids):\n",
        "    super().__init__()\n",
        "    self.nb_of_pos_ids=nb_of_pos_ids\n",
        "  def forward(self,dic_batch,reg):\n",
        "    out={}\n",
        "    out[\"next_station\"]=  torch.nn.utils.rnn.pack_padded_sequence(F.one_hot(dic_batch[\"pos_id\"],self.nb_of_pos_ids).float(), lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    return out"
      ],
      "metadata": {
        "id": "jn0xR-ME8tRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Baseline_model(len(vocab)+1)\n",
        "criterion=Total_loss(False)\n",
        "evaluate(model,valid_dataloader,criterion,device,reg=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYH5OMnmELSa",
        "outputId": "fb668202-96cb-41cb-9f16-df7516a96334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classification': 0.0021053161556483374,\n",
              " 'total': 0.0021053161556483374,\n",
              " 'acc': 0.044894637279486165}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28s2GCFETdYS"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_ujoc4c2mQh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "\n",
        "  def forward(self, out, target_pos_ids, index_training_element):\n",
        "    loss_classification=self.criterion(out.data[index_training_element],target_pos_ids.data[index_training_element])\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,out,dict_batch):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Loss_new_station_binary_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion =  nn.BCEWithLogitsLoss()\n",
        "  def forward(self,out,target):\n",
        "    loss_classification=self.criterion(out.data.squeeze(),target.float())\n",
        "    return loss_classification\n",
        "\n",
        "def get_repetition_labels(target_pos_ids,pos_ids):\n",
        "\n",
        "  return (target_pos_ids.data==pos_ids.data).type(torch.LongTensor)\n",
        "\n",
        "def upsampling_strategy(target, epoch, epochs_new_station_only,pourcentage_of_repeat_training_elment):\n",
        "\n",
        "    index_non_repeat =(target==0).nonzero()\n",
        "    coeff=pourcentage_of_repeat_training_elment/(1-pourcentage_of_repeat_training_elment)\n",
        "    index_for_training= index_non_repeat\n",
        "    if epoch>= epochs_new_station_only:\n",
        "      index_repeat = target.nonzero().squeeze()\n",
        "      nb_non_repeat= index_non_repeat.shape[0]\n",
        "      slice_repeat=index_repeat[torch.randperm(index_repeat.shape[0])[:int(coeff*nb_non_repeat)]].squeeze()\n",
        "      index_for_training = torch.cat((index_non_repeat.squeeze(),slice_repeat))\n",
        "    return index_for_training.squeeze()\n",
        "\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self,new_station_binary_classification) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.loss_new_station_binary_classification=Loss_new_station_binary_classification()\n",
        "\n",
        "  def forward(self, out, dict_batch, upsampling,upsampling_strategy, reg=False):\n",
        "    loss={}\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    if self.new_station_binary_classification or upsampling:\n",
        "      pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "      target=get_repetition_labels(target_pos_ids,pos_ids)\n",
        "    else:\n",
        "      pos_ids=None\n",
        "      target=None\n",
        "\n",
        "    if upsampling:\n",
        "      index_training_element=upsampling_strategy(target)\n",
        "    else:\n",
        "      index_training_element=torch.arange(0,target_pos_ids.data.shape[0])\n",
        "\n",
        "    loss[\"classification\"]=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids,index_training_element)\n",
        "    loss[\"total\"]=loss[\"classification\"]\n",
        "    if self.new_station_binary_classification:\n",
        "      loss[\"new_station\"]=self.loss_new_station_binary_classification(out[\"new_station\"],target)\n",
        "      loss[\"total\"]+=loss[\"new_station\"]\n",
        "\n",
        "    if reg:\n",
        "      loss[\"time_regression\"]=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss[\"total\"]+=loss[\"time_regression\"]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "IHCyYC32ToKU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title evaluation\n",
        "from torch import autocast\n",
        "def evaluate(model,dataloader,upsampling,criterion,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        valid_result=criterion(out,dict_batch,upsampling,None,reg=reg)\n",
        "        valid_results=get_sum_valid_results(valid_results,valid_result)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    valid_results=get_mean_valid_results(valid_results,nb_points)\n",
        "    valid_results[\"acc\"]=acc/nb_points\n",
        "\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "yLu25E-eTcbT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title training\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "def train(\n",
        "          epochs_classifcation_only,\n",
        "          epochs_complete_problem,\n",
        "          input_size,\n",
        "          num_heads,\n",
        "          d_model,\n",
        "          nb_of_pos_ids,\n",
        "          num_layers_lstm,\n",
        "          lstm_layer_with_perceptron,\n",
        "          lstm_layer_with_layer_norm,\n",
        "          num_layers_transformer,\n",
        "          encoder_only,\n",
        "          output_regression_size,\n",
        "          output_classfication_size,\n",
        "          nb_batchs,\n",
        "          dropout,\n",
        "          max_len,\n",
        "          weight_decay,\n",
        "          lr,\n",
        "          learnable_pos_encoding,\n",
        "          new_station_binary_classification,\n",
        "          use_gcn,\n",
        "          vocab,hidden_dim1, hidden_dim2,\n",
        "          batch_first,\n",
        "          concatenate_features,\n",
        "          keep_input_positions,\n",
        "          upsampling,\n",
        "          upsampling_strategy,\n",
        "          epochs_new_station_only,\n",
        "          pourcentage_of_repeat_training_elment,\n",
        "          save_best_model,\n",
        "          path_best_model,\n",
        "          batch_size,\n",
        "          device):\n",
        "\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n",
        "                                         nb_of_pos_ids=nb_of_pos_ids,\n",
        "                                         output_regression_size=output_regression_size,\n",
        "                                         output_classfication_size=output_classfication_size,\n",
        "                                         num_layers_lstm=num_layers_lstm,\n",
        "                                         lstm_layer_with_perceptron=lstm_layer_with_perceptron,\n",
        "                                         lstm_layer_with_layer_norm=lstm_layer_with_perceptron,\n",
        "                                         num_layers_transformer=num_layers_transformer,\n",
        "                                         encoder_only=encoder_only,\n",
        "                                         nhead=num_heads,\n",
        "                                         learnable_pos_encoding=learnable_pos_encoding,\n",
        "                                         new_station_binary_classification=new_station_binary_classification,\n",
        "                                         use_gcn=use_gcn,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=hidden_dim1,\n",
        "                                         hidden_dim2=hidden_dim2,\n",
        "                                         max_len=max_len,\n",
        "                                         dropout=dropout,\n",
        "                                         batch_first = batch_first,\n",
        "                                         concatenate_features = concatenate_features,\n",
        "                                         keep_input_positions = keep_input_positions,device=device\n",
        "                                         ).to(device)\n",
        "  if save_best_model:\n",
        "    os.makedirs(path_best_model,exist_ok =True)\n",
        "  optimizer_encoder = optim.Adam( model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  criterion = Total_loss( new_station_binary_classification = new_station_binary_classification)\n",
        "  train_losses, valid_results = {},{}\n",
        "  best_results={}\n",
        "  for epoch in range(epochs):\n",
        "    reg=epoch >= epochs_classifcation_only\n",
        "    epoch_losses={}\n",
        "    model.train()\n",
        "    i=0\n",
        "    for dict_batch in train_dataloader:\n",
        "      optimizer_encoder.zero_grad()\n",
        "      i+=1\n",
        "      if i>=nb_batchs:\n",
        "        break\n",
        "      dict_batch=set_dic_to(dict_batch,device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch, reg)\n",
        "        loss=criterion(out, dict_batch,upsampling,lambda target: upsampling_strategy(target,epoch,epochs_new_station_only,pourcentage_of_repeat_training_elment) ,reg)\n",
        "        loss[\"total\"].backward()\n",
        "        optimizer_encoder.step()\n",
        "      epoch_losses=update_epoch_losses(epoch_losses,loss)\n",
        "      dict_batch.clear()\n",
        "      loss.clear()\n",
        "      out.clear()\n",
        "      del out, loss,dict_batch\n",
        "    epoch_loss=get_epoch_loss(epoch_losses,batch_size)\n",
        "    train_losses=update_train_losses(train_losses,epoch_loss,epoch)\n",
        "    valid_result = evaluate(model,valid_dataloader,upsampling,criterion,device)\n",
        "    best_results = update_best(model,valid_result,best_results,save_best_model,path_best_model)\n",
        "    valid_results = update_valid_results(valid_results,valid_result)\n",
        "    print_results(epoch_loss,valid_result,epoch)\n",
        "\n",
        "  return best_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "\n",
        "def set_dic_to(dict_batch,device):\n",
        "  for key in dict_batch:\n",
        "    if key!=\"lengths\":\n",
        "      dict_batch[key]=dict_batch[key].to(device)\n",
        "  return dict_batch\n",
        "\n",
        "def is_better(valid_result,best_result,key):\n",
        "  match key:\n",
        "    case \"acc\":\n",
        "      return valid_result>best_result\n",
        "    case _:\n",
        "      return valid_result<best_result\n",
        "\n",
        "def update_best(model,valid_result,best_results,save_best_model,path_best_model):\n",
        "  if best_results:\n",
        "    for key in valid_result:\n",
        "      if is_better(valid_result[key],best_results[key],key):\n",
        "        best_results[key]=valid_result[key]\n",
        "        if save_best_model:\n",
        "          save_model(model,path_best_model,key)\n",
        "  else:\n",
        "    for key in valid_result:\n",
        "      best_results[key]=valid_result[key]\n",
        "      if save_best_model:\n",
        "        save_model(model,path_best_model,key)\n",
        "  return best_results\n",
        "\n",
        "def save_model(model,path_best_model,key):\n",
        "  path=os.path.join(path_best_model,key)\n",
        "  torch.save(model.state_dict(), path+\".pth\")\n",
        "\n",
        "def get_sum_valid_results(valid_result,valid_result_batch):\n",
        "  if valid_result:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]+=valid_result_batch[key].item()\n",
        "  else:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]=valid_result_batch[key].item()\n",
        "  return valid_result\n",
        "\n",
        "def get_mean_valid_results(sum_valid_result,nb_element):\n",
        "  for key in sum_valid_result:\n",
        "    sum_valid_result[key]/=nb_element\n",
        "\n",
        "  return sum_valid_result\n",
        "\n",
        "def update_epoch_losses(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key].item())\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key].item()]\n",
        "  return dict_of_list\n",
        "\n",
        "def update_valid_results(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key])\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key]]\n",
        "  return dict_of_list\n",
        "\n",
        "def get_epoch_loss(epoch_losses,batch_size):\n",
        "\n",
        "  epoch_loss={}\n",
        "  for key in epoch_losses:\n",
        "    epoch_loss[key]=np.array(epoch_losses[key]).mean()/batch_size\n",
        "  return epoch_loss\n",
        "\n",
        "def print_results(epoch_loss,valid_result,epoch):\n",
        "\n",
        "  print(\"\\nepoch: \",epoch)\n",
        "  print(\"train :\", end=\"\\t\")\n",
        "  for key in epoch_loss:\n",
        "    print(key,epoch_loss[key], end=\"\\t\")\n",
        "  print(\"\\nvalid :\", end=\"\\t\")\n",
        "  for key in valid_result:\n",
        "    print(key,valid_result[key], end=\"\\t\")\n",
        "\n",
        "def update_train_losses(train_losses,epoch_loss,epoch):\n",
        "\n",
        "  if train_losses:\n",
        "    for key in epoch_loss:\n",
        "      if key in train_losses:\n",
        "        train_losses[key].append(epoch_loss[key])\n",
        "      else:\n",
        "        train_losses[key]=[float('nan')]*(epoch+1)+[epoch_loss[key]]\n",
        "  else:\n",
        "    for key in epoch_loss:\n",
        "      train_losses[key]=[epoch_loss[key]]\n",
        "  return train_losses"
      ],
      "metadata": {
        "id": "8OL2WGr7cGZW",
        "cellView": "form"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4FEU_h1YtX"
      },
      "source": [
        "## Instance of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HL0AZ-YJChyE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "9c10db16-c762-4edd-c052-2d1db99e56cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-63760f90c7cf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Titre par défaut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model=train(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs_complete_problem\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-d40eb1db95f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs_classifcation_only, epochs_complete_problem, input_size, num_heads, d_model, nb_of_pos_ids, num_layers_lstm, lstm_layer_with_perceptron, lstm_layer_with_layer_norm, num_layers_transformer, encoder_only, output_regression_size, output_classfication_size, nb_batchs, dropout, max_len, weight_decay, lr, learnable_pos_encoding, new_station_binary_classification, use_gcn, vocab, hidden_dim1, hidden_dim2, batch_first, concatenate_features, keep_input_positions, upsampling, upsampling_strategy, epochs_new_station_only, pourcentage_of_repeat_training_elment, save_best_model, path_best_model, batch_size, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mdict_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_dic_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupsampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupsampling_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_new_station_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpourcentage_of_repeat_training_elment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-9f52273f7810>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dic_batch, reg)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_lstm\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_transformer\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-f6ba595015a2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dic_batch)\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfeature_emebdding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_feature_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0membedding\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mfeature_emebdding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Titre par défaut\n",
        "model=train(\n",
        "          epochs_classifcation_only=100,\n",
        "          epochs_complete_problem =100,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=1200,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=6,\n",
        "          lstm_layer_with_perceptron=False,\n",
        "          lstm_layer_with_layer_norm=False,\n",
        "          num_layers_transformer=6,\n",
        "          encoder_only=True,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=100,\n",
        "          dropout=0.1,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=3e-4,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=False,\n",
        "          use_gcn=False,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          batch_first= True,\n",
        "          concatenate_features = False,\n",
        "          keep_input_positions = False,\n",
        "          upsampling=False,\n",
        "          upsampling_strategy=upsampling_strategy,\n",
        "          epochs_new_station_only=0,\n",
        "          pourcentage_of_repeat_training_elment=0.1,\n",
        "          save_best_model=True,\n",
        "          path_best_model=\"test_0.5\",\n",
        "          device=device,\n",
        "          batch_size=64\n",
        "          )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hyperparameter tuning"
      ],
      "metadata": {
        "id": "2fL5dCWywLoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn.models import GCN,GAT,GraphSAGE\n",
        "import numpy as np\n",
        "\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "def get_layer(layer_type):\n",
        "  print(layer_type)\n",
        "  match layer_type:\n",
        "    case \"GraphSAGE\":\n",
        "      return GraphSAGE\n",
        "    case \"GCNConv\":\n",
        "      return GCN\n",
        "    case \"GAT\":\n",
        "      return GAT\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self,output_dim,dropout,layer_type,num_layers_gcn,hidden_channels,activation_gcn,norm,net,device):\n",
        "    super(GCN, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "\n",
        "    self.model=get_layer(layer_type)(in_channels=2, out_channels=output_dim, act=activation_gcn, norm=norm, num_layers=num_layers_gcn, hidden_channels=hidden_channels,dropout=dropout)\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.model(self.coordinates,self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    return self.dropout(embedding)"
      ],
      "metadata": {
        "id": "sHxNOtpIczjP"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,config,net,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+config[\"use_gcn\"]\n",
        "    self.concatenate_features=config[\"concatenate_features\"]\n",
        "    self.embedding_dim=config[\"d_model\"]\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,config[\"nb_of_pos_ids\"],config[\"dropout_StationIdEmbedding\"]),TimeStampEmbedding(self.embedding_dim,config[\"dropout_timeStampEmbedding\"])]\n",
        "    if config[\"use_gcn\"]:\n",
        "      list_feature_embedding.append(GCN( self.embedding_dim,config[\"dropout_gcn\"],config[\"layer_type\"],config[\"num_layers_gcn\"],config[\"hidden_channels\"],config[\"activation_gcn\"],config[\"norm\"],net,device))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "\n",
        "    return embedding\n"
      ],
      "metadata": {
        "id": "Ht0do-5IZMvA"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers_transformer,num_heads,dropout_transformers,activation_transformers,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=num_heads, num_encoder_layers=num_layers_transformer, num_decoder_layers=num_layers_transformer, dropout=dropout_transformers,activation=get_activation(activation_transformers), batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,num_heads,dropout_transformers,activation_transformers,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads,batch_first=batch_first,activation=get_activation(activation_transformers),dropout=dropout_transformers)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers_transformer)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers_transformer,num_heads,dropout_transformers,activation_transformers,batch_first=batch_first)"
      ],
      "metadata": {
        "id": "fhN3oIqdSY5H"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model\n",
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model,activation_lstm):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "    self.activation=get_activation(activation_lstm)\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(self.activation(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,activation_lstm,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model,activation_lstm)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.lstm_layer_with_layer_norm:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "class Abs(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.abs(x)\n",
        "\n",
        "class Exp(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.exp(x)\n",
        "\n",
        "class Sig(nn.Module):\n",
        "  def __init__(self,):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def get_positive_function(config):\n",
        "  match config[\"positive_function\"]:\n",
        "    case \"relu\":\n",
        "      return nn.ReLU()\n",
        "    case \"abs\":\n",
        "      return Abs()\n",
        "    case \"exp\":\n",
        "      return Exp()\n",
        "    case \"sig\":\n",
        "      return Sig()\n",
        "\n",
        "\n",
        "def get_activation(activation):\n",
        "  match activation:\n",
        "    case \"ReLU\":\n",
        "      return nn.ReLU()\n",
        "    case \"Tanh\":\n",
        "      return nn.Tanh()\n",
        "    case \"LeakyReLU\":\n",
        "      return nn.LeakyReLU()\n",
        "    case \"SiLU\":\n",
        "      return nn.SiLU()\n",
        "    case \"GELU\":\n",
        "      return nn.GELU()\n",
        "    case \"ELU\":\n",
        "      return nn.ELU()\n",
        "    case \"Mish\":\n",
        "      return nn.Mish()\n",
        "    case \"ReLU6\":\n",
        "      return nn.ReLU6()\n",
        "    case \"PReLU\":\n",
        "      return nn.PReLU()\n",
        "    case \"SELU\":\n",
        "      return nn.SELU()\n",
        "    case \"CELU\":\n",
        "      return nn.CELU()\n",
        "    case \"Hardsigmoid\":\n",
        "      return nn.Hardsigmoid()\n",
        "    case \"Softplus\":\n",
        "      return nn.Softplus()\n",
        "    case \"Hardshrink\":\n",
        "      return nn.Hardshrink()\n",
        "    case \"Sigmoid\":\n",
        "      return nn.Sigmoid()\n",
        "    case \"Hardtanh\":\n",
        "      return nn.Hardtanh()\n",
        "    case \"Tanhshrink\":\n",
        "      return nn.Tanhshrink()\n",
        "    case \"RReLU\":\n",
        "      return nn.RReLU()\n",
        "    case \"Softshrink\":\n",
        "      return nn.Softshrink()\n",
        "    case \"Softsign\":\n",
        "      return nn.Softsign()\n",
        "    case \"LogSigmoid\":\n",
        "      return nn.LogSigmoid()\n",
        "    case \"Softmin\":\n",
        "      return nn.Softmin()\n",
        "    case \"Hardswish\":\n",
        "      return nn.Hardswish()\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,config,net,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=config[\"dropout\"])\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(config[\"d_model\"])\n",
        "    self.feature_embedding=Feature_embedding(config,net,device)\n",
        "    self.activation=get_activation(config[\"activation\"])\n",
        "    if config[\"reg\"]:\n",
        "      self.positive_function=get_positive_function(config)\n",
        "    self.transformers_model=config[\"transformers_model\"]\n",
        "    if self.transformers_model>0:\n",
        "      self.num_layers_transformer=config[\"num_layers_transformer\"]\n",
        "      self.pos_encoder = get_PositionalEncoding(config[\"d_model\"], config[\"dropout_transformers\"], 100,config[\"learnable_pos_encoding\"])\n",
        "      self.transformer_model=get_Transformer_architecture(config[\"d_model\"],config[\"encoder_only\"],config[\"num_layers_transformer\"],config[\"num_heads\"],config[\"dropout_transformers\"],config[\"activation_transformers\"],True,)\n",
        "\n",
        "    self.lstm_model=config[\"lstm_model\"]\n",
        "    if self.lstm_model>0:\n",
        "      self.num_layers_lstm=config[\"num_layers_lstm\"]\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(config[\"d_model\"],2,config[\"nb_of_pos_ids\"],config[\"num_layers_lstm\"],config[\"lstm_layer_with_perceptron\"],config[\"lstm_layer_with_layer_norm\"],config[\"dropout_lstm\"],config[\"activation_lstm\"],True) for layer in range(config[\"num_layers_lstm\"])])\n",
        "    self.linear_reg=nn.Linear(config[\"d_model\"],2)\n",
        "    self.classifier=nn.Linear(config[\"d_model\"],config[\"nb_of_pos_ids\"])\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.transformers_model:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.lstm_model:\n",
        "      if self.transformers_model>0:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.lstm_model>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=self.activation(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(self.positive_function(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oy3XI4E5iCDi"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import autocast\n",
        "def evaluate(model,dataloader,device,reg=False):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    acc=acc/nb_points\n",
        "    return acc"
      ],
      "metadata": {
        "id": "bG3BJIKMGsM-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  def forward(self, out, target_pos_ids):\n",
        "    loss_classification=self.criterion(out.data,target_pos_ids.data)\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,out,dict_batch):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "\n",
        "  def forward(self, out, dict_batch, reg=False):\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    loss_classification=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids)\n",
        "    loss_total=loss_classification\n",
        "    if reg:\n",
        "      loss_time_regression=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss_total+=loss_time_regression\n",
        "    return loss_total"
      ],
      "metadata": {
        "id": "XoDnxzqRWr-5"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "\n",
        "def f_unpack_dict(dct):\n",
        "    res = {}\n",
        "    for (k, v) in dct.items():\n",
        "        if isinstance(v, dict):\n",
        "            res = {**res, **f_unpack_dict(v)}\n",
        "        else:\n",
        "            res[k] = v\n",
        "\n",
        "    return res\n",
        "\n",
        "def get_file_name(name,path=\".\"):\n",
        "  exist=True\n",
        "  idx=0\n",
        "  while exist:\n",
        "    file_path=os.path.join(path,name+\"_\"+str(idx))\n",
        "    exist=os.path.exists(file_path)\n",
        "    idx+=1\n",
        "  return file_path\n",
        "\n",
        "\n",
        "def get_last_file_name(name,path=\".\"):\n",
        "  exist=True\n",
        "  idx=0\n",
        "  file_path=None\n",
        "  while exist:\n",
        "    last_file=file_path\n",
        "    file_path=os.path.join(path,name+\"_\"+str(idx))\n",
        "    exist=os.path.exists(file_path)\n",
        "    idx+=1\n",
        "  return last_file\n",
        "\n",
        "def get_file_name_2(name,path=\".\"):\n",
        "  exist=True\n",
        "  i=1\n",
        "  for file_or_folder in os.listdir(path):\n",
        "    if os.path.isfile(os.path.join(path,file_or_folder)) and file_or_folder.startswith(name):\n",
        "        idx=file_or_folder.split(\"_\")[-2]\n",
        "        if idx.isdigit():\n",
        "          i=max(i,int(idx)+1)\n",
        "  return os.path.join(path,name+\"_\"+str(i))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ibzvugSLizvL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from ray import train, tune\n",
        "from ray.tune.schedulers import ASHAScheduler,AsyncHyperBandScheduler\n",
        "from ray.util.accelerators import NVIDIA_TESLA_V100\n",
        "from hyperopt import hp,Trials\n",
        "import ray\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "\n",
        "def get_model(config,net,device):\n",
        "  return Transformer_encoder_LSTM_decoder(config,net=net,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "def update_best_acc(model,valid_acc,best_acc,nb_epochs_without_improvement):\n",
        "  if valid_acc > best_acc :\n",
        "    best_acc=valid_acc\n",
        "  else:\n",
        "    nb_epochs_without_improvement+=1\n",
        "  return best_acc,nb_epochs_without_improvement\n",
        "\n",
        "\n",
        "def train_(config,model,dataloaders):\n",
        "  print(config)\n",
        "  device=get_device()\n",
        "  epochs= config[\"epochs_classifcation_only\"]\n",
        "  if config[\"reg\"]:\n",
        "    epochs+=config[\"epochs_complete_problem\"]\n",
        "  optimizer_encoder = optim.Adam( model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
        "  criterion = Total_loss()\n",
        "  best_acc=-1\n",
        "  nb_epochs_without_improvement=0\n",
        "  for epoch in range(epochs):\n",
        "    reg=epoch >= config[\"epochs_classifcation_only\"]\n",
        "    epoch_losses=[]\n",
        "    model.train()\n",
        "    i=0\n",
        "    for dict_batch in dataloaders[\"train\"]:\n",
        "      optimizer_encoder.zero_grad()\n",
        "      i+=1\n",
        "      if i>=config[\"nb_batchs\"]:\n",
        "        break\n",
        "      dict_batch=set_dic_to(dict_batch,device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch, reg)\n",
        "        loss=criterion(out,dict_batch, reg)\n",
        "        if loss.isnan():\n",
        "          print(\"loss is undifined\")\n",
        "          return {\"acc\": -1}\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer_encoder.step()\n",
        "\n",
        "      epoch_losses.append(loss.item())\n",
        "\n",
        "      dict_batch.clear()\n",
        "      out.clear()\n",
        "      del out, loss, dict_batch\n",
        "    epoch_loss=np.array(epoch_losses).mean()\n",
        "    valid_acc = evaluate(model,dataloaders[\"valid\"],device) ##TO DO: modify evaluate to only compute\n",
        "    best_acc, nb_epochs_without_improvement = update_best_acc(model,valid_acc,best_acc,nb_epochs_without_improvement)\n",
        "    if config[\"early_stopping\"]< nb_epochs_without_improvement:\n",
        "      return best_acc\n",
        "\n",
        "    print(\"epoch: \", epoch, \"loss : \", epoch_loss, \"acc: \", valid_acc)\n",
        "  return best_acc\n",
        "\n",
        "\n",
        "def get_datasets():\n",
        "  list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=True,\n",
        "                                    spliting_long_sequences=False,\n",
        "                                    with_repeated_connections=False,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_size=2,\n",
        "                                    save=False,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",\n",
        "                                    download=False,\n",
        "                                    load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3\",)\n",
        "  net=get_net(vocab)\n",
        "  reproducibility_seed=get_reproducible_seeds()[0]\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "  return dataset_list,net,len(vocab)+1\n",
        "\n",
        "def get_dataloaders(datasets,batch_size):\n",
        "  train_dataset=datasets[0]\n",
        "  valid_dataset=datasets[1]\n",
        "  train_dataloader=DataLoader(train_dataset,batch_size=batch_size,collate_fn=collate_fn_padd,shuffle=True)\n",
        "  valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "  return {\"train\":train_dataloader,\"valid\":valid_dataloader}\n",
        "\n",
        "def eval_config(config,data=None,net=None):\n",
        "  device=get_device()\n",
        "  config=f_unpack_dict(config)\n",
        "  dataloaders=get_dataloaders(data,config[\"batch_size\"])\n",
        "  if config[\"use_gcn\"]:\n",
        "    model=get_model(config,net,device)\n",
        "  else:\n",
        "    model=get_model(config,None,device)\n",
        "  best_acc = train_(config,model,dataloaders)\n",
        "  return {\"valid_accuracy\": best_acc}\n",
        "\n",
        "def run_xp(xp_name,storage_path,algo,num_samples=10, max_num_epochs=10, gpus_per_trial=1, test=True):\n",
        "  os.environ[\"OMP_NUM_THREADS\"] = '1'\n",
        "  print(os.environ[\"OMP_NUM_THREADS\"])\n",
        "  datasets,net,nb_of_pos_ids=get_datasets()\n",
        "  ray.shutdown()\n",
        "  config_dict= {\n",
        "      \"max_len\":100,\n",
        "      \"nb_of_pos_ids\":nb_of_pos_ids,\n",
        "      \"batch_size\":2**hp.uniformint(\"batch_size\",4,7),\n",
        "      \"nb_batchs\":25*hp.uniformint(\"nb_batchs\",1,8),\n",
        "      \"early_stopping\":hp.uniformint(\"early_stopping\",1,20),\n",
        "      \"epochs_classifcation_only\":hp.uniformint(\"epochs_classifcation_only\",0,100),\n",
        "      \"reg_choice\":hp.choice(\"reg_choice\",\n",
        "                      [\n",
        "                          {\"reg\":True,\"epochs_complete_problem\":hp.uniformint(\"epochs_complete_problem\",0,100)},\n",
        "                          {\"reg\":False},\n",
        "                      ]),\n",
        "      \"lr\":hp.loguniform(\"lr\",-17,0),\n",
        "      \"weight_decay\":hp.loguniform(\"weight_decay\",-17,0),\n",
        "      \"input_size\":2,\n",
        "      \"d_model\":24*hp.uniformint(\"d_model\",1,60),\n",
        "      \"dropout\":hp.uniform(\"dropout\",0,1),\n",
        "      \"dropout_timeStampEmbedding\":hp.uniform(\"dropout_timeStampEmbedding\",0,1),\n",
        "      \"dropout_StationIdEmbedding\":hp.uniform(\"dropout_StationIdEmbedding\",0,1),\n",
        "      \"normalize_features\":hp.choice(\"normalize_features\",[\"before\",\"after\",None]),\n",
        "      \"concatenate_features\":hp.choice(\"concatenate_features\",[True,False]),\n",
        "      \"use_gcn_choice\":hp.choice(\"use_gcn_choice\",\n",
        "                        [\n",
        "                            {\"use_gcn\":True,\n",
        "                             \"layer_type\":hp.choice(\"layer_type\",[\"GCNConv\",\"GraphSAGE\",\"GAT\"]),\n",
        "                             \"num_layers_gcn\":hp.uniformint(\"num_layers\",1,10),\n",
        "                             \"activation_gcn\": hp.choice(\"activation_gcn\",\n",
        "                              ['swish', 'ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']\n",
        "                             ),\n",
        "                             \"norm\": hp.choice(\"norm\",\n",
        "                                               ['BatchNorm', 'GraphNorm', 'LayerNorm', 'PairNorm', 'InstanceNorm']\n",
        "                             ),\n",
        "                             \"dropout_gcn\":hp.uniform(\"dropout_gcn\",0,1),\n",
        "                             \"hidden_channels\":2**hp.uniformint(\"hidden_channels\",6,11)\n",
        "                             },\n",
        "                            {\"use_gcn\":False}\n",
        "                        ]),\n",
        "      \"activation\": hp.choice(\n",
        "                \"activation\",\n",
        "                 ['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "      \"positive_function\":hp.choice(\"positive_function\",[\"relu\",\"exp\",\"abs\",\"sig\"]),\n",
        "      \"transformers_model\":True,\n",
        "      \"num_layers_transformer\":hp.uniformint(\"num_layers_transformer\",1,6),\n",
        "      \"encoder_only\":hp.choice(\"encoder_only\",[True,False]),\n",
        "      \"num_heads\":3*2**hp.uniformint('num_heads', 0, 3),\n",
        "      \"learnable_pos_encoding\": hp.choice(\"learnable_pos_encoding\",[True,False]),\n",
        "      \"activation_transformers\": hp.choice(\"activation_transformers\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "      \"dropout_transformers\":hp.uniform(\"dropout_transformers\",0,1),\n",
        "\n",
        "      \"lstm_model_choice\": hp.choice(\"lstm_model_choice\",\n",
        "                                     [{\"lstm_model\":True,\n",
        "                                       \"num_layers_lstm\":hp.uniformint(\"num_layers_lstm\",1,6),\n",
        "                                       \"lstm_layer_with_perceptron\":\n",
        "                                        hp.choice(\"lstm_layer_with_perceptron\",\n",
        "                                         [{\"lstm_layer_with_perceptron\":True,\n",
        "                                           \"activation_lstm\":hp.choice(\"activation_lstm\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),},\n",
        "                                          {\"lstm_layer_with_perceptron\":False,\n",
        "                                           \"activation_lstm\":None}]),\n",
        "                                       \"lstm_layer_with_layer_norm\":hp.choice(\"lstm_layer_with_layer_norm\",[True,False]),\n",
        "                                       \"dropout_lstm\":hp.uniform(\"dropout_lstm\",0,1),\n",
        "                                       },\n",
        "                                      {\"lstm_model\":False}])}\n",
        "  if algo==None:\n",
        "    algo = HyperOptSearch(space=config_dict, metric=\"valid_accuracy\", mode=\"max\", random_state_seed=get_reproducible_seeds()[0])\n",
        "  trainable_with_gpu = tune.with_resources(eval_config, {\"cpu\": 2, \"gpu\": 1})\n",
        "  tuner = tune.Tuner(\n",
        "        tune.with_parameters(trainable_with_gpu,data=datasets,net=net),\n",
        "        tune_config=tune.TuneConfig(\n",
        "                                search_alg=algo,\n",
        "                                max_concurrent_trials=1,\n",
        "                                num_samples=1 if test else num_samples,\n",
        "                                    ),\n",
        "        run_config=train.RunConfig(\n",
        "            name=xp_name,\n",
        "            storage_path=storage_path,\n",
        "            verbose=0)\n",
        "    )\n",
        "  # To enable GPUs, use this instead:\n",
        "  results = tuner.fit()\n",
        "  return results, algo\n",
        "\n",
        "\n",
        "def save_config_xps_to_drive(xps_name,drive_path,xp_size,xps_number,accuracy_target,max_num_epochs):\n",
        "  dic_config={\n",
        "      \"xps_name\":xps_name,\n",
        "      \"xp_size\":xp_size,\n",
        "      \"xps_number\":xps_number,\n",
        "      \"current_xp\": -1,\n",
        "      \"best_xp\": {\"idx\":-1, \"mean_valid_accuracy\": -1}\n",
        "  }\n",
        "  xps_path=os.path.join(drive_path,xps_name)\n",
        "  xps_configs= os.path.join(xps_path,\"xps_configs\")\n",
        "  os.makedirs(xps_path,exist_ok=True)\n",
        "  torch.save(dic_config,xps_configs)\n",
        "  return xps_path,xps_configs\n",
        "\n",
        "\n",
        "def update_config_dictionnary(xps_configs,best_results,num_xp):\n",
        "\n",
        "  config_dic=torch.load(xps_configs)\n",
        "  config_dic[\"current_xp\"]=num_xp\n",
        "  if config_dic[\"best_xp\"][\"mean_test_accuracy\"]<best_results:\n",
        "    config_dic[\"best_xp\"][\"mean_test_accuracy\"]=best_results\n",
        "    config_dic[\"best_xp\"][\"idx\"]=num_xp\n",
        "  torch.save(config_dic,xps_configs)\n",
        "\n",
        "\n",
        "\n",
        "def update_and_save(xp_name,xps_path,xps_configs,storage_path,results,algo,num_xp,accuracy_target):\n",
        "  best_results=results.get_best_result(metric='mean_test_accuracy',mode='max').metrics['mean_test_accuracy']\n",
        "  accarucy_target_not_reached= best_results< accuracy_target\n",
        "  update_config_dictionnary(xps_configs,best_results,num_xp)\n",
        "  shutil.copytree(os.path.join(storage_path,xp_name),os.path.join(xps_path,xp_name),dirs_exist_ok=True)\n",
        "  if num_xp>=1:\n",
        "    shutil.rmtree(os.path.join(xps_path,\"xp_num_\"+str(num_xp-1)))\n",
        "  shutil.rmtree(os.path.join(storage_path,xp_name))\n",
        "  return accarucy_target_not_reached\n",
        "\n",
        "\n",
        "def run_all_xp(xps_name=\"hyperparameter_tuning_projet_long\", algo=None, xp_size=10, xps_number=10, accuracy_target=0.98, max_num_epochs=30, storage_path='/content/',drive_path=\"/content/drive/MyDrive\"):\n",
        "    accarucy_target_not_reached=True\n",
        "    num_xp=0\n",
        "    xps_path,xps_configs=save_config_xps_to_drive(xps_name,drive_path,xp_size,xps_number,accuracy_target,max_num_epochs)\n",
        "    while num_xp<xps_number and accarucy_target_not_reached:\n",
        "      xp_name= \"xp_num_\"+str(num_xp)\n",
        "      results,algo=run_xp(xp_name,storage_path,algo,num_samples=xp_size, max_num_epochs=max_num_epochs, gpus_per_trial=1, test=False)\n",
        "      accarucy_target_not_reached=update_and_save(xp_name,xps_path,xps_configs,storage_path,results,algo,num_xp,accuracy_target)\n",
        "      num_xp+=1"
      ],
      "metadata": {
        "id": "oNO-AvQjH03u"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp, pyll\n",
        "datasets,net,nb_of_pos_ids=get_datasets()\n",
        "space={\n",
        "      \"max_len\":100,\n",
        "      \"nb_of_pos_ids\":nb_of_pos_ids,\n",
        "      \"batch_size\":2**hp.uniformint(\"batch_size\",4,7),\n",
        "      \"nb_batchs\":25*hp.uniformint(\"nb_batchs\",1,8),\n",
        "      \"early_stopping\":hp.uniformint(\"early_stopping\",1,20),\n",
        "      \"epochs_classifcation_only\":hp.uniformint(\"epochs_classifcation_only\",0,100),\n",
        "      \"reg_choice\":hp.choice(\"reg_choice\",\n",
        "                      [\n",
        "                          {\"reg\":True,\"epochs_complete_problem\":hp.uniformint(\"epochs_complete_problem\",0,100)},\n",
        "                          {\"reg\":False},\n",
        "                      ]),\n",
        "      \"lr\":hp.loguniform(\"lr\",-17,0),\n",
        "      \"weight_decay\":hp.loguniform(\"weight_decay\",-17,0),\n",
        "      \"input_size\":2,\n",
        "      \"d_model\":24*hp.uniformint(\"d_model\",1,60),\n",
        "      \"dropout\":hp.uniform(\"dropout\",0,1),\n",
        "      \"dropout_timeStampEmbedding\":hp.uniform(\"dropout_timeStampEmbedding\",0,1),\n",
        "      \"dropout_StationIdEmbedding\":hp.uniform(\"dropout_StationIdEmbedding\",0,1),\n",
        "      \"normalize_features\":hp.choice(\"normalize_features\",[\"before\",\"after\",None]),\n",
        "      \"concatenate_features\":hp.choice(\"concatenate_features\",[True,False]),\n",
        "      \"use_gcn_choice\":hp.choice(\"use_gcn_choice\",\n",
        "                        [\n",
        "                            {\"use_gcn\":True,\n",
        "                             \"layer_type\":hp.choice(\"layer_type\",[\"GCNConv\",\"GraphSAGE\",\"GAT\"]),\n",
        "                             \"num_layers_gcn\":hp.uniformint(\"num_layers\",1,10),\n",
        "                             \"activation_gcn\": hp.choice(\"activation_gcn\",\n",
        "                              ['swish', 'ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']\n",
        "                             ),\n",
        "                             \"norm\": hp.choice(\"norm\",\n",
        "                                               ['BatchNorm', 'GraphNorm', 'LayerNorm', 'PairNorm', 'InstanceNorm']\n",
        "                             ),\n",
        "                             \"dropout_gcn\":hp.uniform(\"dropout_gcn\",0,1),\n",
        "                             \"hidden_channels\":2**hp.uniformint(\"hidden_channels\",6,11)\n",
        "                             },\n",
        "                            {\"use_gcn\":False}\n",
        "                        ]),\n",
        "      \"activation\": hp.choice(\n",
        "                \"activation\",\n",
        "                 ['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "      \"positive_function\":hp.choice(\"positive_function\",[\"relu\",\"exp\",\"abs\",\"sig\"]),\n",
        "      \"transformers_model\":True,\n",
        "                                       \"num_layers_transformer\":hp.uniformint(\"num_layers_transformer\",1,6),\n",
        "                                       \"encoder_only\":hp.choice(\"encoder_only\",[True,False]),\n",
        "                                       \"num_heads\":3*2**hp.uniformint('num_heads', 0, 3),\n",
        "                                       \"learnable_pos_encoding\": hp.choice(\"learnable_pos_encoding\",[True,False]),\n",
        "                                       \"activation_transformers\": hp.choice(\"activation_transformers\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),\n",
        "                                       \"dropout_transformers\":hp.uniform(\"dropout_transformers\",0,1),\n",
        "      \"lstm_model_choice\": hp.choice(\"lstm_model_choice\",\n",
        "                                     [{\"lstm_model\":True,\n",
        "                                       \"num_layers_lstm\":hp.uniformint(\"num_layers_lstm\",1,6),\n",
        "                                       \"lstm_layer_with_perceptron\":\n",
        "                                        hp.choice(\"lstm_layer_with_perceptron\",\n",
        "                                         [{\"lstm_layer_with_perceptron\":True,\n",
        "                                           \"activation_lstm\":hp.choice(\"activation_lstm\",['ReLU6', 'PReLU', 'SELU', 'ELU', 'Mish', 'CELU', 'ReLU', 'Hardsigmoid', 'Tanh', 'LeakyReLU', 'Softplus', 'Hardshrink','Sigmoid', 'Hardtanh', 'SiLU', 'Tanhshrink', 'RReLU', 'Softshrink', 'Softsign', 'LogSigmoid', 'Softmin', 'GELU', 'Hardswish']),},\n",
        "                                          {\"lstm_layer_with_perceptron\":False,\n",
        "                                           \"activation_lstm\":None}]),\n",
        "                                       \"lstm_layer_with_layer_norm\":hp.choice(\"lstm_layer_with_layer_norm\",[True,False]),\n",
        "                                       \"dropout_lstm\":hp.uniform(\"dropout_lstm\",0,1),\n",
        "                                       },\n",
        "                                      {\"lstm_model\":False}])}\n",
        "config=pyll.stochastic.sample(space)\n",
        "eval_config(config,data=datasets,net=net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6BMsccMiKY1",
        "outputId": "63f574b1-a362-429e-de4c-4cf488cbbc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-1f082812ff3e>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-32-1f082812ff3e>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-32-1f082812ff3e>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-32-1f082812ff3e>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU.\n",
            "GAT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.activation_relu_or_gelu was not True\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.23310593007933533 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'activation': 'GELU', 'activation_transformers': 'RReLU', 'batch_size': 128, 'concatenate_features': False, 'd_model': 744, 'dropout': 0.027467872616931954, 'dropout_StationIdEmbedding': 0.8359129075458374, 'dropout_timeStampEmbedding': 0.5970626755280908, 'dropout_transformers': 0.7636900183203755, 'early_stopping': 11, 'encoder_only': True, 'epochs_classifcation_only': 56, 'input_size': 2, 'learnable_pos_encoding': False, 'lr': 1.159615573363344e-06, 'dropout_lstm': 0.23310593007933533, 'lstm_layer_with_layer_norm': False, 'activation_lstm': None, 'lstm_layer_with_perceptron': False, 'lstm_model': True, 'num_layers_lstm': 2, 'max_len': 100, 'nb_batchs': 100, 'nb_of_pos_ids': 3043, 'normalize_features': 'after', 'num_heads': 6, 'num_layers_transformer': 5, 'positive_function': 'exp', 'epochs_complete_problem': 2, 'reg': True, 'transformers_model': True, 'activation_gcn': 'ELU', 'dropout_gcn': 0.26744942840284525, 'hidden_channels': 256, 'layer_type': 'GAT', 'norm': 'InstanceNorm', 'num_layers_gcn': 9, 'use_gcn': True, 'weight_decay': 0.002847899638598889}\n",
            "CUDA is available. Using GPU.\n",
            "epoch:  0 loss :  8.825319184197319 acc:  0.0003572784315476855\n",
            "epoch:  1 loss :  8.780270913634638 acc:  0.00042426813746287653\n",
            "epoch:  2 loss :  8.73162570144191 acc:  0.000513587745349798\n",
            "epoch:  3 loss :  8.684542800440934 acc:  0.0005805774512649889\n",
            "epoch:  4 loss :  8.648331189396405 acc:  0.0006698970591519103\n",
            "epoch:  5 loss :  8.605268719220403 acc:  0.000781546569010562\n",
            "epoch:  6 loss :  8.58009011817701 acc:  0.0008262063729540227\n",
            "epoch:  7 loss :  8.540807945559724 acc:  0.0008262063729540227\n",
            "epoch:  8 loss :  8.509280484132093 acc:  0.0009378558828126744\n",
            "epoch:  9 loss :  8.475939326816135 acc:  0.0010048455887278656\n",
            "epoch:  10 loss :  8.443052031777121 acc:  0.001027175490699596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_xp('test',\"/content/test\",None,num_samples=1, max_num_epochs=10, gpus_per_trial=1, test=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEF-Ae5AHtQB",
        "outputId": "0a0eb80e-e0c4-4653-9fe3-6955cff423b9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "loading already preprocessed data: \n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/list_users\n",
            "/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements_3/vocab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-0931e4912bd5>:15: FutureWarning: The 'convex hull' option for the 'clip' parameter is deprecated and will be removed in a future release. Use 'convex_hull' instead.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-29-0931e4912bd5>:15: FutureWarning: The 'as_gdf' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-29-0931e4912bd5>:15: FutureWarning: The 'return_input' parameter currently defaults to True but will default to False in a future release. Set it explicitly to avoid this warning.\n",
            "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
            "<ipython-input-29-0931e4912bd5>:16: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/convert.py:278: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  data_dict[key] = torch.as_tensor(value)\n",
            "2024-03-05 21:25:26,390\tINFO worker.py:1724 -- Started a local Ray instance.\n",
            "2024-03-05 21:25:40,391\tINFO tune.py:220 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n",
            "2024-03-05 21:25:40,395\tINFO tune.py:592 -- [output] This will use the new output engine with verbosity 0. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------+\n",
            "| Configuration for experiment     test            |\n",
            "+--------------------------------------------------+\n",
            "| Search algorithm                 SearchGenerator |\n",
            "| Scheduler                        FIFOScheduler   |\n",
            "| Number of trials                 1               |\n",
            "+--------------------------------------------------+\n",
            "\n",
            "View detailed results here: /content/test/test\n",
            "To visualize your results with TensorBoard, run: `tensorboard --logdir /root/ray_results/test`\n",
            "\u001b[36m(eval_config pid=10590)\u001b[0m CUDA is available. Using GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-05 21:26:01,061\tERROR tune_controller.py:1374 -- Trial task failed for trial eval_config_16d28f87\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
            "    result = ray.get(future)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2624, in get\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(TypeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=10590, ip=172.28.0.12, actor_id=f2acafdcbf1bc0b75fd3960001000000, repr=eval_config)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/trainable.py\", line 342, in train\n",
            "    raise skipped from exception_cause(skipped)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/air/_internal/util.py\", line 88, in run\n",
            "    self._ret = self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 115, in <lambda>\n",
            "    training_func=lambda: self._trainable_func(self.config),\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/function_trainable.py\", line 332, in _trainable_func\n",
            "    output = fn()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/ray/tune/trainable/util.py\", line 138, in inner\n",
            "    return trainable(config, **fn_kwargs)\n",
            "  File \"<ipython-input-38-6f5b62be5c13>\", line 95, in eval_config\n",
            "  File \"<ipython-input-38-6f5b62be5c13>\", line 10, in get_model\n",
            "  File \"<ipython-input-32-0cd31519a5a8>\", line 134, in __init__\n",
            "  File \"<ipython-input-30-7c900427c229>\", line 13, in __init__\n",
            "  File \"<ipython-input-29-0931e4912bd5>\", line 48, in __init__\n",
            "TypeError: 'NoneType' object is not callable\n",
            "2024-03-05 21:26:01,083\tWARNING experiment_state.py:323 -- Experiment checkpoint syncing has been triggered multiple times in the last 30.0 seconds. A sync will be triggered whenever a trial has checkpointed more than `num_to_keep` times since last sync or if 300 seconds have passed since last sync. If you have set `num_to_keep` in your `CheckpointConfig`, consider increasing the checkpoint frequency or keeping more checkpoints. You can supress this warning by changing the `TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S` environment variable.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trial eval_config_16d28f87 errored after 0 iterations at 2024-03-05 21:26:01. Total running time: 20s\n",
            "Error file: /root/ray_results/test/eval_config_16d28f87_1_activation=CELU,batch_size=32,concatenate_features=False,d_model=1200,dropout=0.2593,dropout_StationIdEmbed_2024-03-05_21-25-40/error.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-05 21:26:01,355\tERROR tune.py:1038 -- Trials did not complete: [eval_config_16d28f87]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(ResultGrid<[\n",
              "   Result(\n",
              "     error='RayTaskError(TypeError)',\n",
              "     metrics={},\n",
              "     path='/content/test/test/eval_config_16d28f87_1_activation=CELU,batch_size=32,concatenate_features=False,d_model=1200,dropout=0.2593,dropout_StationIdEmbed_2024-03-05_21-25-40',\n",
              "     filesystem='local',\n",
              "     checkpoint=None\n",
              "   )\n",
              " ]>,\n",
              " <ray.tune.search.hyperopt.hyperopt_search.HyperOptSearch at 0x7e8e7f668a90>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"ray[tune]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkCi3L-UKKtv",
        "outputId": "81ad7b03-5a76-4e81-e179-be2b8f87034e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray[tune]\n",
            "  Downloading ray-2.9.3-cp310-cp310-manylinux2014_x86_64.whl (64.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.13.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (23.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.31.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.5.3)\n",
            "Collecting tensorboardX>=1.9 (from ray[tune])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2023.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow>=6.0.1->ray[tune]) (1.25.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.18.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n",
            "Installing collected packages: tensorboardX, ray\n",
            "Successfully installed ray-2.9.3 tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## check performance on new station"
      ],
      "metadata": {
        "id": "QXU5DJC8p169"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_repeat(model,dataloader,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    repeat=0\n",
        "    not_repeat=0\n",
        "    correct_not_repeat=0\n",
        "    correct_repeat=0\n",
        "    incorrect_not_repeat_as_repeat=0\n",
        "    incorrect_not_repeat=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "        pred=out[\"next_station\"].data.argmax(dim=1)\n",
        "        pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        for i in range(len(target_pos_ids.data)):\n",
        "          if target_pos_ids.data[i]==pos_ids.data[i]:\n",
        "            repeat+=1\n",
        "\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_repeat+=1\n",
        "          else:\n",
        "            not_repeat+=1\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_not_repeat+=1\n",
        "            if target_pos_ids.data[i]!=pred[i]:\n",
        "              incorrect_not_repeat+=1\n",
        "\n",
        "          if pred[i]==pos_ids.data[i] and target_pos_ids.data[i]!=pos_ids.data[i]:\n",
        "            incorrect_not_repeat_as_repeat+=1\n",
        "    print(nb_points,\"repeat: \",repeat,\" not_repeat: \",not_repeat,\" correct_repeat/repeat: \",correct_repeat/repeat,\" correct_not_repeat/not_repeat: \",correct_not_repeat/not_repeat,incorrect_not_repeat_as_repeat/incorrect_not_repeat)\n",
        "    return valid_results"
      ],
      "metadata": {
        "id": "HClPVCKb59Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=768,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=0,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqswVm-Flq",
        "outputId": "4f8a2d24-1d1d-4d5e-da84-30957736cbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6509191238701378  correct_not_repeat/not_repeat:  0.2762021385930769 0.4746223564954683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=12,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufqL-c1e27Vm",
        "outputId": "5e436410-45a5-4c71-da92-f5b40a030112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.572683570872406  correct_not_repeat/not_repeat:  0.24545712973693992 0.38929461542920074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=10,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqLMoo9hpA82",
        "outputId": "cc34ad4e-47d5-4efd-d99e-5c06837a607e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5703307491790515  correct_not_repeat/not_repeat:  0.22293411471430757 0.40792435839711844\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYc14HpTkU2z",
        "outputId": "36544e2a-7d0b-4787-df90-4160a18680cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.44894038389925184  correct_not_repeat/not_repeat:  0.29502962979160746 0.2873538261112317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "tI1PRax29Fqw",
        "outputId": "d9d5ea9f-4378-49ec-be25-2fe2ce5e37f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d5fdb97ca0b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-17df1714a1da>\u001b[0m in \u001b[0;36mevaluate_repeat\u001b[0;34m(model, dataloader, device, reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpos_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrepeat\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucdMSE-0T3-s",
        "outputId": "4f766291-2702-431e-d059-241a1a1ccfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7462507193879279  correct_not_repeat/not_repeat:  0.23080623646979073 0.5669490561746645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEt-Bw9FgvDq",
        "outputId": "70b7fed7-4cc6-495a-de11-57721bf5d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4839957344527574  correct_not_repeat/not_repeat:  0.35011261507511315 0.2974764468371467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaphByRZOIo",
        "outputId": "13cfb863-5bb6-43a5-f104-7ce207f93c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6290835844138258  correct_not_repeat/not_repeat:  0.26852681988148086 0.44345460524349045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGE1rXtIIEpE",
        "outputId": "775055f6-3dae-4158-fdc4-078b4adba7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5781001387995531  correct_not_repeat/not_repeat:  0.3046073779274453 0.4137291280148423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=3,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "QPBs0PUrKUra",
        "outputId": "196fe7cd-25e3-479d-8c4b-b6a83ccd3977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.linear1.weight\", \"transformer_model.transformer.decoder.layers.5.linear1.bias\", \"transformer_model.transformer.decoder.layers.5.linear2.weight\", \"transformer_model.transformer.decoder.layers.5.linear2.bias\", \"transformer_model.transformer.decoder.layers.5.norm1.weight\", \"transformer_model.transformer.decoder.layers.5.norm1.bias\", \"transformer_model.transformer.decoder.layers.5.norm2.weight\", \"transformer_model.transformer.decoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.norm3.weight\", \"transformer_model.transformer.decoder.layers.5.norm3.bias\", \"transformer_lstm__list.2.layer_normalisation.weight\", \"transformer_lstm__list.2.layer_normalisation.bias\", \"transformer_lstm__list.2.lstm.weight_ih_l0\", \"transformer_lstm__list.2.lstm.weight_hh_l0\", \"transformer_lstm__list.2.lstm.bias_ih_l0\", \"transformer_lstm__list.2.lstm.bias_hh_l0\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.bias\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-10bf83ce1050>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                          ).to(device)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cIy9KQFjv8",
        "outputId": "3ea5d475-a56f-4118-d568-2c020806ed64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8564101696062832  correct_not_repeat/not_repeat:  0.09422492401215805 0.8021341316208778\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.643730131126935, 2.2387092113494873, 4.385555267333984)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06MhCDWcaF9",
        "outputId": "4a84b9d7-b594-41b9-bca7-acacf62010b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6269508107925116  correct_not_repeat/not_repeat:  0.24020904856661782 0.4275024463247568\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5190344566683142, 3.329756021499634, 2.0473709106445312)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonRxU5b96oc",
        "outputId": "baba1be5-861e-44f1-95fb-808d9cd6b5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7019025694844104  correct_not_repeat/not_repeat:  0.2555815529946863 0.5174044590664747\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5773612306040138, 2.5278093814849854, 2.7385761737823486)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz2zFYSsAroq",
        "outputId": "10b15668-6b3b-44d9-eb4f-c28a02cbd09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7179745421307424  correct_not_repeat/not_repeat:  0.2436203013273272 0.5562012142237641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5856108172094187, 2.1441447734832764, 1.2037302255630493)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlGehwEQtfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378de12f-1ebf-44f6-b4b6-e57d19a2bb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8789227800534886  correct_not_repeat/not_repeat:  0.11254947409853272 0.8136211314803864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6650741059388481, 1.82258939743042, 2.1341636180877686)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejz7LOwNTZ-k",
        "outputId": "5d16ae15-48ea-4ff8-a2e7-718c1be73c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8187565591252243  correct_not_repeat/not_repeat:  0.1462683956178522 0.7372573126376722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6311055788439596, 2.027265787124634, 2.6414260864257812)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9yHv6HjJ-N",
        "outputId": "ba6f7b9e-b88f-407c-d22b-1e6913f0e56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.3789143166661024  correct_not_repeat/not_repeat:  0.3284642802475345 0.28316509280364704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3648367472709855, 2.700303077697754, 3.602348566055298)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLWBTILrc94",
        "outputId": "c04e1913-1724-4df4-e563-1801d24ca813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4562527506009005  correct_not_repeat/not_repeat:  0.30379829874702063 0.3240153275959545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4137118868488654, 2.806699752807617, 3.5753602981567383)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## clean cache"
      ],
      "metadata": {
        "id": "SCR7Ctsbp_nT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtscCJTUkD0R",
        "outputId": "b7dbe03d-21dc-456a-eeba-5bb3a1b676c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5605595648"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_allocated()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IV_LSRYqGMO2",
        "dDvAwpD4GrJu",
        "bcCkeqmkhRnT",
        "3Bbp1dVXWQs3",
        "svMRI0xeji-7",
        "zZpbR8rG8kBn",
        "QXU5DJC8p169"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}