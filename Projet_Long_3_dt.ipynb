{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briag1/ProjetLong/blob/main/Projet_Long_3_dt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# @title device\n",
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "  return device\n",
        "device=get_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZV8IJFGT9T",
        "outputId": "9b710a2a-ef00-47de-ec45-cfba99fefe6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available. Using GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_LSRYqGMO2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qh5nnjRIFe0h"
      },
      "outputs": [],
      "source": [
        "# @title code\n",
        "from os import makedirs\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_x(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[0])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def get_y(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[1])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def read_dataframe(name):\n",
        "  if not os.path.exists(name+\".pkl\"):\n",
        "    print(\"reading dataframe: \"+name+\".xlsx\")\n",
        "    df=pd.read_excel(name+\".xlsx\")\n",
        "    df.to_pickle(name+\".pkl\")\n",
        "  else:\n",
        "    print(\"using already read daframe\")\n",
        "\n",
        "def get_vocab(poses,vocab):\n",
        "  for pos in poses:\n",
        "    if pos not in vocab and not any(isinstance(n, float) and math.isnan(n) for n in pos):\n",
        "        vocab[pos]=len(vocab)+1\n",
        "  return vocab\n",
        "\n",
        "def get_fix_time_encoding(df):\n",
        "\n",
        "  df['month_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "  df['month_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "\n",
        "  df['day_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "  df['day_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "\n",
        "  df['hour_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "  df['hour_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "\n",
        "  df['minute_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "  df['minute_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "\n",
        "  df['second_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "  df['second_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "def get_time_data(df):\n",
        "  df['month'] =  df[\"start time\"].dt.month\n",
        "  df['day'] =  df[\"start time\"].dt.day\n",
        "  df['hour'] =  df[\"start time\"].dt.hour\n",
        "  df['minute'] = df[\"start time\"].dt.minute\n",
        "  df['second'] = df[\"start time\"].dt.second\n",
        "  return df\n",
        "\n",
        "\n",
        "def tokenize_pos(pos,vocab):\n",
        "\n",
        "  if math.isnan(pos[0]) and math.isnan(pos[1]):\n",
        "    return len(vocab)\n",
        "  else:\n",
        "    return vocab[pos]\n",
        "\n",
        "def get_coordinates(df,input_position,full_dataset):\n",
        "\n",
        "  if full_dataset:\n",
        "    df['x'] = df['latitude']\n",
        "    df['y'] = df['longitude']\n",
        "  else:\n",
        "    df['x'] = df['location(latitude/lontitude)'].apply(get_x)\n",
        "    df['y'] = df['location(latitude/lontitude)'].apply(get_y)\n",
        "\n",
        "\n",
        "  if input_position:\n",
        "    df['x_normalised']=(df['x']-df['x'].mean())/(df['x'].std())\n",
        "    df['y_normalised']=(df['y']-df['y'].mean())/df['y'].std()\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_joined_coordinates(df):\n",
        "\n",
        "  df['pos']= list(zip(df['x'],df['y']))\n",
        "  poses=df['pos'].unique()\n",
        "\n",
        "  return poses\n",
        "\n",
        "def get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset):\n",
        "  col_to_drop_in_df=['date', 'end time','pos']\n",
        "  col_to_drop_in_dict=['x','y', 'time_to_end', 'time_to_next','start time', 'user id']\n",
        "  col_to_add_to_dict=[]\n",
        "  col_in_input=[]\n",
        "  if not full_dataset:\n",
        "    col_to_drop_in_df+=['location(latitude/lontitude)']\n",
        "  else:\n",
        "    col_to_drop_in_df+=['latitude','longitude']\n",
        "  if fixed_time_encoding:\n",
        "    col_to_drop_in_df+=[]\n",
        "    col_to_drop_in_dict+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "    col_in_input+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "  else:\n",
        "    col_to_add_to_dict+=['month','day','hour','minute','second']\n",
        "  if input_position:\n",
        "    col_to_drop_in_dict += ['x_normalised', 'y_normalised']\n",
        "    col_in_input+=['x_normalised', 'y_normalised']\n",
        "  return col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict\n",
        "\n",
        "def process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections):\n",
        "  #get the time to next connection\n",
        "  df_user[\"time_to_next\"] =  df_user[\"start time\"].diff(-1).dt.total_seconds()\n",
        "  dict_user=df_user.to_dict('list')\n",
        "  #create input\n",
        "  dict_user[\"pos_id\"],dict_user[\"pos_id_target\"]=torch.tensor(dict_user[\"pos_id\"][:-1]),torch.tensor(dict_user[\"pos_id\"][1:])\n",
        "\n",
        "  if col_in_input:\n",
        "    dict_user[\"input\"]=torch.tensor([dict_user[col] for col in col_in_input]).T\n",
        "    dict_user[\"input\"]=dict_user[\"input\"][:-1]\n",
        "\n",
        "  if col_to_add_to_dict:\n",
        "    for col in col_to_add_to_dict:\n",
        "      dict_user[col]=torch.tensor(dict_user[col])\n",
        "      dict_user[col]=dict_user[col][:-1]\n",
        "\n",
        "  dict_user[\"time_target\"]=torch.tensor([dict_user[\"time_to_end\"],dict_user[\"time_to_next\"]]).T\n",
        "  dict_user[\"time_target\"]=dict_user[\"time_target\"][:-1]\n",
        "  for e in col_to_drop_in_dict:\n",
        "    dict_user.pop(e)\n",
        "\n",
        "  if not with_repeated_connections:\n",
        "    dict_user=combine_repeated_connections_in_sequence_user(dict_user)\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine_repeated_connections_in_sequence_user(dict_user):\n",
        "  index=0\n",
        "  for key in dict_user:\n",
        "    if dict_user[key].shape[0]!=dict_user[\"pos_id\"].shape[0]:\n",
        "      print(key)\n",
        "  #print(dict_user[\"pos_id\"],dict_user[\"pos_id_target\"])\n",
        "  while index < len(dict_user[\"pos_id\"])-1:\n",
        "    #print(dict_user[\"pos_id\"][index],dict_user[\"pos_id_target\"][index])\n",
        "    if dict_user[\"pos_id\"][index]==dict_user[\"pos_id_target\"][index]:\n",
        "      #print(\"equal\")\n",
        "      dict_user[\"pos_id_target\"][index]=dict_user[\"pos_id_target\"][index+1]\n",
        "      dict_user[\"time_target\"][index]=dict_user[\"time_target\"][index+1]\n",
        "      for key in dict_user:\n",
        "\n",
        "        dict_user[key]=torch.cat((dict_user[key][:index+1],dict_user[key][index+2:]))\n",
        "      #print()\n",
        "    else:\n",
        "      #print(\"different\")\n",
        "      index+=1\n",
        "\n",
        "\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def normalize_output(list_users):\n",
        "  #get means and stds\n",
        "  time_targets=torch.cat([dict_user[\"time_target\"] for dict_user in list_users],dim=0)\n",
        "  time_targets_mean=time_targets.mean(dim=0)\n",
        "  time_targets_std=time_targets.std(dim=0)\n",
        "  #normalize\n",
        "  for i in range(len(list_users)):\n",
        "    list_users[i][\"time_target\"]=(list_users[i][\"time_target\"]-time_targets_mean)/time_targets_std\n",
        "  return list_users\n",
        "\n",
        "\n",
        "\n",
        "def process_dataframe(name,vocab,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,format=\".pkl\"):\n",
        "  df= pd.read_pickle(name+format)\n",
        "  df=df.sort_values('start time')\n",
        "  df=df.drop(['month'],axis=1)\n",
        "\n",
        "  df=get_coordinates(df,input_position,full_dataset)\n",
        "\n",
        "  poses=get_joined_coordinates(df)\n",
        "  vocab=get_vocab(poses,vocab)\n",
        "  df['pos_id'] = df['pos'].apply(lambda pos: tokenize_pos(pos,vocab))\n",
        "\n",
        "  df['time_to_end']=df['end time']-df['start time']\n",
        "  df['time_to_end']=df['time_to_end'].dt.total_seconds()\n",
        "  if fixed_time_encoding:\n",
        "    df=get_fix_time_encoding(df)\n",
        "  else:\n",
        "    df=get_time_data(df)\n",
        "\n",
        "  col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict=get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset)\n",
        "  df=df.drop(col_to_drop_in_df, axis=1)\n",
        "\n",
        "  df_user_group = df.groupby('user id')\n",
        "  list_users=[]\n",
        "  for user, df_user in df_user_group:\n",
        "    if len(df_user)>=2 and not df_user['x'].isnull().values.any():\n",
        "        list_users.append(process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections))\n",
        "  list_users=normalize_output(list_users)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "def get_raw_data(directory,src_directory,full_dataset):\n",
        "  if  full_dataset:\n",
        "    shutil.copytree(src_directory,directory)#telecomDataset6mont\n",
        "  else:\n",
        "    runcmd('wget http://sguangwang.com/dataset/telecom.zip', verbose = False)\n",
        "    runcmd('unzip /content/telecom.zip')\n",
        "\n",
        "def get_processed_dataset(load_dataset_path):\n",
        "  saved_list_user_path = os.path.join(load_dataset_path,\"list_users\")\n",
        "  saved_vocab_path = os.path.join(load_dataset_path,\"vocab\")\n",
        "  print(\"loading already preprocessed data: \")\n",
        "  print(saved_list_user_path)\n",
        "  print(saved_vocab_path)\n",
        "  list_users=torch.load(saved_list_user_path)\n",
        "  vocab=torch.load(saved_vocab_path)\n",
        "  return list_users,vocab\n",
        "\n",
        "def process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections):\n",
        "  list_users=[]\n",
        "  vocab={}\n",
        "  if not os.path.exists(directory_raw_data):\n",
        "    print('getting raw data at: '+src_directory_raw_data)\n",
        "    get_raw_data(directory_raw_data,src_directory_raw_data,full_dataset)\n",
        "  if full_dataset:\n",
        "    for name in os.listdir(directory_raw_data):\n",
        "      if not name.endswith(\".pkl\"):\n",
        "        complete_name=os.path.join(directory_raw_data,\".\".join(name.split(\".\")[:-1]))\n",
        "        print(\"processing dataframe: \"+complete_name)\n",
        "        read_dataframe(complete_name)\n",
        "        new_list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "        list_users+=new_list_users\n",
        "  else:\n",
        "    complete_name = \"/content/dataset-telecom/data_6.1~6.30_\"\n",
        "    read_dataframe(complete_name)\n",
        "    list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def split_long_sequences(list_users,max_sequence_length):\n",
        "  new_list_users=[]\n",
        "  for i in range(len(list_users)):\n",
        "    seq_length=list_users[i][\"input\"].shape[0]\n",
        "    if seq_length>=max_sequence_length:\n",
        "      nb_of_seq=seq_length//max_sequence_length\n",
        "      rest=seq_length%max_sequence_length\n",
        "      list_splitted_seq=nb_of_seq*[{}]\n",
        "      rest_splitted={}\n",
        "      for key in list_users[i]:\n",
        "        for j in range(nb_of_seq):\n",
        "          list_splitted_seq[j][key]=list_users[i][key][max_sequence_length*j:max_sequence_length*(j+1)]\n",
        "        if rest>2:\n",
        "          rest_splitted[key]= list_users[i][key][-rest:]\n",
        "      new_list_users=new_list_users+list_splitted_seq\n",
        "      if len(rest_splitted)>0:\n",
        "        new_list_users+=[rest_splitted]\n",
        "    else:\n",
        "      new_list_users.append(list_users[i])\n",
        "\n",
        "  return new_list_users\n",
        "\n",
        "\n",
        "\n",
        "def save_processed_data(list_users,vocab,path_to_save_dataset):\n",
        "    print(\"creating directory: \"+path_to_save_dataset)\n",
        "    os.makedirs(path_to_save_dataset,exist_ok=True)\n",
        "    print(\"saving processed data at: \")\n",
        "    save_list_user_path = os.path.join(path_to_save_dataset,\"list_users\")\n",
        "    save_vocab_path = os.path.join(path_to_save_dataset,\"vocab\")\n",
        "    print(save_list_user_path)\n",
        "    print(save_vocab_path)\n",
        "    torch.save(list_users,save_list_user_path)\n",
        "    torch.save(vocab,save_vocab_path)\n",
        "\n",
        "def get_processed_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,spliting_long_sequences,with_repeated_connections,max_sequence_length=100,min_sequence_length=3,save=False,path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month\",download=False,load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month\"):\n",
        "  if not download:\n",
        "    list_users,vocab = get_processed_dataset(load_dataset_path)\n",
        "  else:\n",
        "    list_users,vocab=process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections)\n",
        "  if spliting_long_sequences:\n",
        "    print(\"spliting sequences longuer than : \"+str(max_sequence_length)+ \" steps\")\n",
        "    list_users=split_long_sequences(list_users,max_sequence_length)\n",
        "  if save:\n",
        "    save_processed_data(list_users,vocab,path_to_save_dataset)\n",
        "  return list_users,vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRA1WX8UcsV",
        "outputId": "e4981d40-c6a4-4563-dcea-3eaf98268b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting raw data at: drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\n",
            "reading dataframe: /content/dataset-telecom/data_6.1~6.30_.xlsx\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=False,\n",
        "                                    spliting_long_sequences=False,\n",
        "                                    with_repeated_connections=False,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_length=3,\n",
        "                                    save=False,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",\n",
        "                                    download=True,\n",
        "                                    load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5D9IoBtGX6R"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvAwpD4GrJu"
      },
      "source": [
        "## Reproducibility seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p5d4mp9GDah"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import string\n",
        "import random\n",
        "def get_reproducible_seeds(name=\"ProjectLong\",nb_seeds=100):\n",
        "    # Calculate SHA-256 hash\n",
        "    sha256_hash = hashlib.sha256(name.encode()).hexdigest()\n",
        "    # Define character sets\n",
        "    digits = string.digits\n",
        "    # Use the hash to seed the random number generator\n",
        "    hash_as_int = int(sha256_hash, 16)\n",
        "    random.seed(hash_as_int)\n",
        "    # Generate a random list of seed of desired length\n",
        "    reproducibility_seeds = [random.randint(0,10000) for _ in range(nb_seeds)]\n",
        "\n",
        "    return reproducibility_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn8U0p9TGJXK"
      },
      "outputs": [],
      "source": [
        "reproducibility_seed=get_reproducible_seeds()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mzoE-MHqLa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nhxCSLNHsd9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class VariableLengthDatasetWithPosID(Dataset):\n",
        "    def __init__(self, time_series, transform=None):\n",
        "        self.times_series=time_series\n",
        "    def __len__(self):\n",
        "        return len(self.times_series)\n",
        "    def __getitem__(self, idx):\n",
        "        user_dict=self.times_series[idx]\n",
        "        return  user_dict\n",
        "\n",
        "def create_dataset(list_users,split=[0.8,0.1,0.1]):\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "  return dataset_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGgl2XnIhDQ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nke01dG-KJxO"
      },
      "outputs": [],
      "source": [
        "def collate_fn_padd(batch_dict):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "\n",
        "\n",
        "    dict_batch={key: [d[key] for d in batch_dict] for key in batch_dict[0]}\n",
        "    dict_batch[\"lengths\"] = torch.tensor([ user[\"input\"].shape[0] for user in batch_dict ])\n",
        "    if \"input\" in dict_batch:\n",
        "      dict_batch[\"input\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"input\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"month\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"month\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"day\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"day\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"hour\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"hour\"],batch_first=True,padding_value=24)\n",
        "    dict_batch[\"minute\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"minute\"],batch_first=True,padding_value=60)\n",
        "    dict_batch[\"second\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"second\"],batch_first=True,padding_value=60)\n",
        "\n",
        "    dict_batch[\"time_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"time_target\"],batch_first=True,padding_value=-1)\n",
        "    dict_batch[\"pos_id\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id\"],batch_first=True,padding_value=len(vocab))\n",
        "    dict_batch[\"pos_id_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id_target\"],batch_first=True,padding_value=len(vocab))\n",
        "    #print(dict_batch[\"input\"])\n",
        "    return dict_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Yl1E6gY8_P"
      },
      "source": [
        "## Instanciate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHpriSipY7Kz"
      },
      "outputs": [],
      "source": [
        "dataset_list=create_dataset(list_users)\n",
        "train_dataset=dataset_list[0]\n",
        "valid_dataset=dataset_list[1]\n",
        "test_dataset=dataset_list[2]\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=64,collate_fn=collate_fn_padd,shuffle=True)\n",
        "valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HodJbvKeMe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptycyS7FWE4b"
      },
      "source": [
        "## Transformer Encoder followed by LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oeWr0HDhJfo"
      },
      "source": [
        "### transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_bACPajeQx"
      },
      "outputs": [],
      "source": [
        "def get_mask(bath_size,sequence_length,lengths,device):\n",
        "  mask=torch.zeros(bath_size,sequence_length).to(device)\n",
        "  for i, length in enumerate(lengths):\n",
        "    mask[i,length:]=1\n",
        "  return mask.bool()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsaggvjghDsq"
      },
      "source": [
        "#### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yFXZxqeHMwi"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class VanillaPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = (x.transpose(0,1) + self.pe[:x.transpose(0,1).size(0)]).transpose(0,1)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX-kk1ScG-_n"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.positional_embedding=nn.Embedding(num_embeddings=max_len,embedding_dim= d_model)\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(self.parameters()).device\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size,seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x += self.positional_embedding(torch.arange(0,x.shape[1]).to(self.device))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzKYvfaWG6cH"
      },
      "outputs": [],
      "source": [
        "def get_PositionalEncoding(d_model: int, dropout: float = 0.1, max_len: int = 2000, learnable=False):\n",
        "  if learnable:\n",
        "    return LearnablePositionalEncoding(d_model, dropout, max_len)\n",
        "  else:\n",
        "    return VanillaPositionalEncoding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-pr0W6xhOkZ"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZQnLdN4UxY"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,  dropout=dropout, batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only=False,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,batch_first=batch_first)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers,nhead,dropout,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCkeqmkhRnT"
      },
      "source": [
        "### feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmqQtP0UhZqg"
      },
      "outputs": [],
      "source": [
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=embedding_dim)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=embedding_dim)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=embedding_dim)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "\n",
        "  def forward(self,dict_batch):\n",
        "    embedding= self.month_embedding(dict_batch['month'])\n",
        "    embedding=+ self.day_embedding(dict_batch['day'])\n",
        "    embedding=+ self.hour_embedding(dict_batch['hour'])\n",
        "    embedding=+ self.minute_embedding(dict_batch['minute'])\n",
        "    embedding=+ self.second_embedding(dict_batch['second'])\n",
        "    return self.dropout(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCp5Pz6uy-FG"
      },
      "outputs": [],
      "source": [
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bbp1dVXWQs3"
      },
      "source": [
        "#### graph_deepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW174iJrhFC",
        "outputId": "6ad9be4e-f4cd-40d0-a6fb-0cd7c1b561d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpysal\n",
            "  Downloading libpysal-4.9.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.12.3)\n",
            "Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from libpysal) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.25.2)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (23.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.11.4)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.10->libpysal) (2.5)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (1.9.5)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (67.7.2)\n",
            "Installing collected packages: libpysal\n",
            "Successfully installed libpysal-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install libpysal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQvAPDZyWNtg",
        "outputId": "c0fe4dde-5968-49ea-c87f-398c269f8a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=3049d4ae756209f0ed6d2a1856d71ae9d30d88b08233d9124d9ba4a0cbc02455\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdSTBOc3sKsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, hidden_dim1, hidden_dim2, output_dim,vocab,dropout,device):\n",
        "    super(GCN, self).__init__()\n",
        "    net=get_net(vocab)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "    self.conv1 = GCNConv(2, hidden_dim1)\n",
        "    self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
        "    self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.conv1(self.coordinates, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "    x = self.conv2(x, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv3(x, self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozXR4sW0W0Y"
      },
      "source": [
        " #### Combine feature embeddng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6mU1qWOjRP3"
      },
      "outputs": [],
      "source": [
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+use_gcn\n",
        "    self.concatenate_features=concatenate_features\n",
        "    self.embedding_dim=d_model\n",
        "    self.keep_input_positions=keep_input_positions\n",
        "    if keep_input_positions:\n",
        "      self.embedding_dim=self.embedding_dim-2\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,nb_of_pos_ids,dropout),TimeStampEmbedding(self.embedding_dim,dropout)]\n",
        "    if use_gcn:\n",
        "      list_feature_embedding.append(GCN(hidden_dim1, hidden_dim2, self.embedding_dim, vocab, dropout,device))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "    if self.keep_input_positions:\n",
        "      embedding=torch.cat((dic_batch[\"input\"],embedding),dim=2)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svMRI0xeji-7"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSt_zuJRKgBh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(F.relu(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.layer_normalisation:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,d_model,nb_of_pos_ids,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,num_layers_transformer,encoder_only,nhead,learnable_pos_encoding,new_station_binary_classification,use_gcn,vocab,hidden_dim1, hidden_dim2,max_len,dropout,batch_first,concatenate_features,keep_input_positions,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.feature_embedding=Feature_embedding(d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device)\n",
        "\n",
        "    self.num_layers_transformer=num_layers_transformer\n",
        "    if num_layers_transformer>0:\n",
        "      self.pos_encoder = get_PositionalEncoding(d_model, dropout, max_len,learnable_pos_encoding)\n",
        "      self.transformer_model=get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,nhead,dropout,batch_first)\n",
        "\n",
        "    self.num_layers_lstm=num_layers_lstm\n",
        "    if num_layers_lstm>0:\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(d_model,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first) for layer in range(num_layers_lstm)])\n",
        "    self.linear_reg=nn.Linear(d_model,output_regression_size)\n",
        "    self.classifier=nn.Linear(d_model,output_classfication_size)\n",
        "\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.binary_classifier=nn.Linear(d_model,1)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.num_layers_transformer>0:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.num_layers_lstm>0:\n",
        "      if self.num_layers_transformer>0:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.num_layers_lstm>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=F.relu(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(torch.exp(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.new_station_binary_classification:\n",
        "      out[\"new_station\"]=  torch.nn.utils.rnn.PackedSequence( self.binary_classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baselines"
      ],
      "metadata": {
        "id": "zZpbR8rG8kBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class  Baseline_model(nn.Module):\n",
        "  def __init__(self,nb_of_pos_ids):\n",
        "    super().__init__()\n",
        "    self.nb_of_pos_ids=nb_of_pos_ids\n",
        "  def forward(self,dic_batch,reg):\n",
        "    out={}\n",
        "    out[\"next_station\"]=  torch.nn.utils.rnn.pack_padded_sequence(F.one_hot(dic_batch[\"pos_id\"],self.nb_of_pos_ids).float(), lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    return out"
      ],
      "metadata": {
        "id": "jn0xR-ME8tRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Baseline_model(len(vocab)+1)\n",
        "criterion=Total_loss(False)\n",
        "evaluate(model,valid_dataloader,criterion,device,reg=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYH5OMnmELSa",
        "outputId": "fb668202-96cb-41cb-9f16-df7516a96334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classification': 0.0021053161556483374,\n",
              " 'total': 0.0021053161556483374,\n",
              " 'acc': 0.044894637279486165}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28s2GCFETdYS"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujoc4c2mQh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "\n",
        "  def forward(self, out, target_pos_ids, index_training_element):\n",
        "    loss_classification=self.criterion(out.data[index_training_element],target_pos_ids.data[index_training_element])\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,out,dict_batch):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Loss_new_station_binary_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion =  nn.BCEWithLogitsLoss()\n",
        "  def forward(self,out,target):\n",
        "    loss_classification=self.criterion(out.data.squeeze(),target.float())\n",
        "    return loss_classification\n",
        "\n",
        "def get_repetition_labels(target_pos_ids,pos_ids):\n",
        "\n",
        "  return (target_pos_ids.data==pos_ids.data).type(torch.LongTensor)\n",
        "\n",
        "def upsampling_strategy(target, epoch, epochs_new_station_only,pourcentage_of_repeat_training_elment):\n",
        "\n",
        "    index_non_repeat =(target==0).nonzero()\n",
        "    coeff=pourcentage_of_repeat_training_elment/(1-pourcentage_of_repeat_training_elment)\n",
        "    index_for_training= index_non_repeat\n",
        "    if epoch>= epochs_new_station_only:\n",
        "      index_repeat = target.nonzero().squeeze()\n",
        "      nb_non_repeat= index_non_repeat.shape[0]\n",
        "      slice_repeat=index_repeat[torch.randperm(index_repeat.shape[0])[:int(coeff*nb_non_repeat)]].squeeze()\n",
        "      index_for_training = torch.cat((index_non_repeat.squeeze(),slice_repeat))\n",
        "    return index_for_training.squeeze()\n",
        "\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self,new_station_binary_classification) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.loss_new_station_binary_classification=Loss_new_station_binary_classification()\n",
        "\n",
        "  def forward(self, out, dict_batch, upsampling,upsampling_strategy, reg=False):\n",
        "    loss={}\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    if self.new_station_binary_classification or upsampling:\n",
        "      pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "      target=get_repetition_labels(target_pos_ids,pos_ids)\n",
        "    else:\n",
        "      pos_ids=None\n",
        "      target=None\n",
        "\n",
        "    if upsampling:\n",
        "      index_training_element=upsampling_strategy(target)\n",
        "    else:\n",
        "      index_training_element=torch.arange(0,target_pos_ids.data.shape[0])\n",
        "\n",
        "    loss[\"classification\"]=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids,index_training_element)\n",
        "    loss[\"total\"]=loss[\"classification\"]\n",
        "    if self.new_station_binary_classification:\n",
        "      loss[\"new_station\"]=self.loss_new_station_binary_classification(out[\"new_station\"],target)\n",
        "      loss[\"total\"]+=loss[\"new_station\"]\n",
        "\n",
        "    if reg:\n",
        "      loss[\"time_regression\"]=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss[\"total\"]+=loss[\"time_regression\"]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHCyYC32ToKU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title evaluation\n",
        "from torch import autocast\n",
        "def evaluate(model,dataloader,upsampling,criterion,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        valid_result=criterion(out,dict_batch,upsampling,None,reg=reg)\n",
        "        valid_results=get_sum_valid_results(valid_results,valid_result)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    valid_results=get_mean_valid_results(valid_results,nb_points)\n",
        "    valid_results[\"acc\"]=acc/nb_points\n",
        "\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLu25E-eTcbT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title training\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "def train(\n",
        "          epochs_classifcation_only,\n",
        "          epochs_complete_problem,\n",
        "          input_size,\n",
        "          num_heads,\n",
        "          d_model,\n",
        "          nb_of_pos_ids,\n",
        "          num_layers_lstm,\n",
        "          lstm_layer_with_perceptron,\n",
        "          lstm_layer_with_layer_norm,\n",
        "          num_layers_transformer,\n",
        "          encoder_only,\n",
        "          output_regression_size,\n",
        "          output_classfication_size,\n",
        "          nb_batchs,\n",
        "          dropout,\n",
        "          max_len,\n",
        "          weight_decay,\n",
        "          lr,\n",
        "          learnable_pos_encoding,\n",
        "          new_station_binary_classification,\n",
        "          use_gcn,\n",
        "          vocab,hidden_dim1, hidden_dim2,\n",
        "          batch_first,\n",
        "          concatenate_features,\n",
        "          keep_input_positions,\n",
        "          upsampling,\n",
        "          upsampling_strategy,\n",
        "          epochs_new_station_only,\n",
        "          pourcentage_of_repeat_training_elment,\n",
        "          save_best_model,\n",
        "          path_best_model,\n",
        "          batch_size,\n",
        "          device):\n",
        "\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n",
        "                                         nb_of_pos_ids=nb_of_pos_ids,\n",
        "                                         output_regression_size=output_regression_size,\n",
        "                                         output_classfication_size=output_classfication_size,\n",
        "                                         num_layers_lstm=num_layers_lstm,\n",
        "                                         lstm_layer_with_perceptron=lstm_layer_with_perceptron,\n",
        "                                         lstm_layer_with_layer_norm=lstm_layer_with_perceptron,\n",
        "                                         num_layers_transformer=num_layers_transformer,\n",
        "                                         encoder_only=encoder_only,\n",
        "                                         nhead=num_heads,\n",
        "                                         learnable_pos_encoding=learnable_pos_encoding,\n",
        "                                         new_station_binary_classification=new_station_binary_classification,\n",
        "                                         use_gcn=use_gcn,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=hidden_dim1,\n",
        "                                         hidden_dim2=hidden_dim2,\n",
        "                                         max_len=max_len,\n",
        "                                         dropout=dropout,\n",
        "                                         batch_first = batch_first,\n",
        "                                         concatenate_features = concatenate_features,\n",
        "                                         keep_input_positions = keep_input_positions,device=device\n",
        "                                         ).to(device)\n",
        "  if save_best_model:\n",
        "    os.makedirs(path_best_model,exist_ok =True)\n",
        "  optimizer_encoder = optim.Adam( model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  criterion = Total_loss( new_station_binary_classification = new_station_binary_classification)\n",
        "  train_losses, valid_results = {},{}\n",
        "  best_results={}\n",
        "  for epoch in range(epochs):\n",
        "    reg=epoch >= epochs_classifcation_only\n",
        "    epoch_losses={}\n",
        "    model.train()\n",
        "    i=0\n",
        "    for dict_batch in train_dataloader:\n",
        "      optimizer_encoder.zero_grad()\n",
        "      i+=1\n",
        "      if i>=nb_batchs:\n",
        "        break\n",
        "      dict_batch=set_dic_to(dict_batch,device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch, reg)\n",
        "        loss=criterion(out, dict_batch,upsampling,lambda target: upsampling_strategy(target,epoch,epochs_new_station_only,pourcentage_of_repeat_training_elment) ,reg)\n",
        "        loss[\"total\"].backward()\n",
        "        optimizer_encoder.step()\n",
        "      epoch_losses=update_epoch_losses(epoch_losses,loss)\n",
        "      dict_batch.clear()\n",
        "      loss.clear()\n",
        "      out.clear()\n",
        "      del out, loss,dict_batch\n",
        "    epoch_loss=get_epoch_loss(epoch_losses,batch_size)\n",
        "    train_losses=update_train_losses(train_losses,epoch_loss,epoch)\n",
        "    valid_result = evaluate(model,valid_dataloader,upsampling,criterion,device)\n",
        "    best_results = update_best(model,valid_result,best_results,save_best_model,path_best_model)\n",
        "    valid_results = update_valid_results(valid_results,valid_result)\n",
        "    print_results(epoch_loss,valid_result,epoch)\n",
        "\n",
        "  return best_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "\n",
        "def set_dic_to(dict_batch,device):\n",
        "  for key in dict_batch:\n",
        "    if key!=\"lengths\":\n",
        "      dict_batch[key]=dict_batch[key].to(device)\n",
        "  return dict_batch\n",
        "\n",
        "def is_better(valid_result,best_result,key):\n",
        "  match key:\n",
        "    case \"acc\":\n",
        "      return valid_result>best_result\n",
        "    case _:\n",
        "      return valid_result<best_result\n",
        "\n",
        "def update_best(model,valid_result,best_results,save_best_model,path_best_model):\n",
        "  if best_results:\n",
        "    for key in valid_result:\n",
        "      if is_better(valid_result[key],best_results[key],key):\n",
        "        best_results[key]=valid_result[key]\n",
        "        if save_best_model:\n",
        "          save_model(model,path_best_model,key)\n",
        "  else:\n",
        "    for key in valid_result:\n",
        "      best_results[key]=valid_result[key]\n",
        "      if save_best_model:\n",
        "        save_model(model,path_best_model,key)\n",
        "  return best_results\n",
        "\n",
        "def save_model(model,path_best_model,key):\n",
        "  path=os.path.join(path_best_model,key)\n",
        "  torch.save(model.state_dict(), path+\".pth\")\n",
        "\n",
        "def get_sum_valid_results(valid_result,valid_result_batch):\n",
        "  if valid_result:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]+=valid_result_batch[key].item()\n",
        "  else:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]=valid_result_batch[key].item()\n",
        "  return valid_result\n",
        "\n",
        "def get_mean_valid_results(sum_valid_result,nb_element):\n",
        "  for key in sum_valid_result:\n",
        "    sum_valid_result[key]/=nb_element\n",
        "\n",
        "  return sum_valid_result\n",
        "\n",
        "def update_epoch_losses(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key].item())\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key].item()]\n",
        "  return dict_of_list\n",
        "\n",
        "def update_valid_results(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key])\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key]]\n",
        "  return dict_of_list\n",
        "\n",
        "def get_epoch_loss(epoch_losses,batch_size):\n",
        "\n",
        "  epoch_loss={}\n",
        "  for key in epoch_losses:\n",
        "    epoch_loss[key]=np.array(epoch_losses[key]).mean()/batch_size\n",
        "  return epoch_loss\n",
        "\n",
        "def print_results(epoch_loss,valid_result,epoch):\n",
        "\n",
        "  print(\"\\nepoch: \",epoch)\n",
        "  print(\"train :\", end=\"\\t\")\n",
        "  for key in epoch_loss:\n",
        "    print(key,epoch_loss[key], end=\"\\t\")\n",
        "  print(\"\\nvalid :\", end=\"\\t\")\n",
        "  for key in valid_result:\n",
        "    print(key,valid_result[key], end=\"\\t\")\n",
        "\n",
        "def update_train_losses(train_losses,epoch_loss,epoch):\n",
        "\n",
        "  if train_losses:\n",
        "    for key in epoch_loss:\n",
        "      if key in train_losses:\n",
        "        train_losses[key].append(epoch_loss[key])\n",
        "      else:\n",
        "        train_losses[key]=[float('nan')]*(epoch+1)+[epoch_loss[key]]\n",
        "  else:\n",
        "    for key in epoch_loss:\n",
        "      train_losses[key]=[epoch_loss[key]]\n",
        "  return train_losses"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8OL2WGr7cGZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4FEU_h1YtX"
      },
      "source": [
        "## Instance of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "HL0AZ-YJChyE",
        "outputId": "ccfdf139-20a5-452c-fe6c-6e36be5a95fd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-eb8eac0884b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Titre par défaut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model=train(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs_complete_problem\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-d40eb1db95f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs_classifcation_only, epochs_complete_problem, input_size, num_heads, d_model, nb_of_pos_ids, num_layers_lstm, lstm_layer_with_perceptron, lstm_layer_with_layer_norm, num_layers_transformer, encoder_only, output_regression_size, output_classfication_size, nb_batchs, dropout, max_len, weight_decay, lr, learnable_pos_encoding, new_station_binary_classification, use_gcn, vocab, hidden_dim1, hidden_dim2, batch_first, concatenate_features, keep_input_positions, upsampling, upsampling_strategy, epochs_new_station_only, pourcentage_of_repeat_training_elment, save_best_model, path_best_model, batch_size, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mdict_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_dic_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupsampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupsampling_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_new_station_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpourcentage_of_repeat_training_elment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2dbafaf65fc1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dic_batch, reg)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmask_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_lstm\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_transformer\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-a8f6ed0bd9ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       return self.transformer(x,\n\u001b[0m\u001b[1;32m      7\u001b[0m                        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                        \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    204\u001b[0m         memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask,\n\u001b[1;32m    205\u001b[0m                               is_causal=src_is_causal)\n\u001b[0;32m--> 206\u001b[0;31m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[0m\u001b[1;32m    207\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                               \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    461\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    863\u001b[0m     def _mha_block(self, x: Tensor, mem: Tensor,\n\u001b[1;32m    864\u001b[0m                    attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 865\u001b[0;31m         x = self.multihead_attn(x, mem, mem,\n\u001b[0m\u001b[1;32m    866\u001b[0m                                 \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5438\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5440\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5441\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# @title Titre par défaut\n",
        "model=train(\n",
        "          epochs_classifcation_only=100,\n",
        "          epochs_complete_problem =100,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=1200,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=2,\n",
        "          lstm_layer_with_perceptron=False,\n",
        "          lstm_layer_with_layer_norm=False,\n",
        "          num_layers_transformer=6,\n",
        "          encoder_only=False,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=100,\n",
        "          dropout=0.1,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=1e-3,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=False,\n",
        "          use_gcn=False,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          batch_first= True,\n",
        "          concatenate_features = True,\n",
        "          keep_input_positions = False,\n",
        "          upsampling=False,\n",
        "          upsampling_strategy=upsampling_strategy,\n",
        "          epochs_new_station_only=0,\n",
        "          pourcentage_of_repeat_training_elment=0.1,\n",
        "          save_best_model=True,\n",
        "          path_best_model=\"test_0.5\",\n",
        "          device=device,\n",
        "          batch_size=64\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_repeat(model,dataloader,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    repeat=0\n",
        "    not_repeat=0\n",
        "    correct_not_repeat=0\n",
        "    correct_repeat=0\n",
        "    incorrect_not_repeat_as_repeat=0\n",
        "    incorrect_not_repeat=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "        pred=out[\"next_station\"].data.argmax(dim=1)\n",
        "        pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        for i in range(len(target_pos_ids.data)):\n",
        "          if target_pos_ids.data[i]==pos_ids.data[i]:\n",
        "            repeat+=1\n",
        "\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_repeat+=1\n",
        "          else:\n",
        "            not_repeat+=1\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_not_repeat+=1\n",
        "            if target_pos_ids.data[i]!=pred[i]:\n",
        "              incorrect_not_repeat+=1\n",
        "\n",
        "          if pred[i]==pos_ids.data[i] and target_pos_ids.data[i]!=pos_ids.data[i]:\n",
        "            incorrect_not_repeat_as_repeat+=1\n",
        "    print(nb_points,\"repeat: \",repeat,\" not_repeat: \",not_repeat,\" correct_repeat/repeat: \",correct_repeat/repeat,\" correct_not_repeat/not_repeat: \",correct_not_repeat/not_repeat,incorrect_not_repeat_as_repeat/incorrect_not_repeat)\n",
        "    return valid_results"
      ],
      "metadata": {
        "id": "HClPVCKb59Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=768,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=0,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqswVm-Flq",
        "outputId": "4f8a2d24-1d1d-4d5e-da84-30957736cbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6509191238701378  correct_not_repeat/not_repeat:  0.2762021385930769 0.4746223564954683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=12,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufqL-c1e27Vm",
        "outputId": "5e436410-45a5-4c71-da92-f5b40a030112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.572683570872406  correct_not_repeat/not_repeat:  0.24545712973693992 0.38929461542920074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=10,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqLMoo9hpA82",
        "outputId": "cc34ad4e-47d5-4efd-d99e-5c06837a607e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5703307491790515  correct_not_repeat/not_repeat:  0.22293411471430757 0.40792435839711844\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYc14HpTkU2z",
        "outputId": "36544e2a-7d0b-4787-df90-4160a18680cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.44894038389925184  correct_not_repeat/not_repeat:  0.29502962979160746 0.2873538261112317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "tI1PRax29Fqw",
        "outputId": "d9d5ea9f-4378-49ec-be25-2fe2ce5e37f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d5fdb97ca0b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-17df1714a1da>\u001b[0m in \u001b[0;36mevaluate_repeat\u001b[0;34m(model, dataloader, device, reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpos_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrepeat\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucdMSE-0T3-s",
        "outputId": "4f766291-2702-431e-d059-241a1a1ccfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7462507193879279  correct_not_repeat/not_repeat:  0.23080623646979073 0.5669490561746645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEt-Bw9FgvDq",
        "outputId": "70b7fed7-4cc6-495a-de11-57721bf5d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4839957344527574  correct_not_repeat/not_repeat:  0.35011261507511315 0.2974764468371467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaphByRZOIo",
        "outputId": "13cfb863-5bb6-43a5-f104-7ce207f93c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6290835844138258  correct_not_repeat/not_repeat:  0.26852681988148086 0.44345460524349045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGE1rXtIIEpE",
        "outputId": "775055f6-3dae-4158-fdc4-078b4adba7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5781001387995531  correct_not_repeat/not_repeat:  0.3046073779274453 0.4137291280148423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=3,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "QPBs0PUrKUra",
        "outputId": "196fe7cd-25e3-479d-8c4b-b6a83ccd3977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.linear1.weight\", \"transformer_model.transformer.decoder.layers.5.linear1.bias\", \"transformer_model.transformer.decoder.layers.5.linear2.weight\", \"transformer_model.transformer.decoder.layers.5.linear2.bias\", \"transformer_model.transformer.decoder.layers.5.norm1.weight\", \"transformer_model.transformer.decoder.layers.5.norm1.bias\", \"transformer_model.transformer.decoder.layers.5.norm2.weight\", \"transformer_model.transformer.decoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.norm3.weight\", \"transformer_model.transformer.decoder.layers.5.norm3.bias\", \"transformer_lstm__list.2.layer_normalisation.weight\", \"transformer_lstm__list.2.layer_normalisation.bias\", \"transformer_lstm__list.2.lstm.weight_ih_l0\", \"transformer_lstm__list.2.lstm.weight_hh_l0\", \"transformer_lstm__list.2.lstm.bias_ih_l0\", \"transformer_lstm__list.2.lstm.bias_hh_l0\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.bias\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-10bf83ce1050>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                          ).to(device)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cIy9KQFjv8",
        "outputId": "3ea5d475-a56f-4118-d568-2c020806ed64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8564101696062832  correct_not_repeat/not_repeat:  0.09422492401215805 0.8021341316208778\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.643730131126935, 2.2387092113494873, 4.385555267333984)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06MhCDWcaF9",
        "outputId": "4a84b9d7-b594-41b9-bca7-acacf62010b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6269508107925116  correct_not_repeat/not_repeat:  0.24020904856661782 0.4275024463247568\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5190344566683142, 3.329756021499634, 2.0473709106445312)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonRxU5b96oc",
        "outputId": "baba1be5-861e-44f1-95fb-808d9cd6b5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7019025694844104  correct_not_repeat/not_repeat:  0.2555815529946863 0.5174044590664747\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5773612306040138, 2.5278093814849854, 2.7385761737823486)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz2zFYSsAroq",
        "outputId": "10b15668-6b3b-44d9-eb4f-c28a02cbd09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7179745421307424  correct_not_repeat/not_repeat:  0.2436203013273272 0.5562012142237641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5856108172094187, 2.1441447734832764, 1.2037302255630493)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlGehwEQtfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378de12f-1ebf-44f6-b4b6-e57d19a2bb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8789227800534886  correct_not_repeat/not_repeat:  0.11254947409853272 0.8136211314803864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6650741059388481, 1.82258939743042, 2.1341636180877686)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejz7LOwNTZ-k",
        "outputId": "5d16ae15-48ea-4ff8-a2e7-718c1be73c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8187565591252243  correct_not_repeat/not_repeat:  0.1462683956178522 0.7372573126376722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6311055788439596, 2.027265787124634, 2.6414260864257812)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9yHv6HjJ-N",
        "outputId": "ba6f7b9e-b88f-407c-d22b-1e6913f0e56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.3789143166661024  correct_not_repeat/not_repeat:  0.3284642802475345 0.28316509280364704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3648367472709855, 2.700303077697754, 3.602348566055298)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLWBTILrc94",
        "outputId": "c04e1913-1724-4df4-e563-1801d24ca813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4562527506009005  correct_not_repeat/not_repeat:  0.30379829874702063 0.3240153275959545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4137118868488654, 2.806699752807617, 3.5753602981567383)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtscCJTUkD0R",
        "outputId": "d2e3c16a-065e-4503-fa5c-4f991dea2ff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7411692032"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if None:\n",
        "  print(1)"
      ],
      "metadata": {
        "id": "42I-z2ls4-Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree classifier"
      ],
      "metadata": {
        "id": "wIjYebkGJ8i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_without_pos_id=[]\n",
        "pos_id_targets=[]\n",
        "pos_ids=[]\n",
        "for user in list_users:\n",
        "  inputs_without_pos_id.append(user[\"input\"])\n",
        "  pos_id_targets.append(user[\"pos_id_target\"])\n",
        "  pos_ids.append(user[\"pos_id\"])\n",
        "print(inputs_without_pos_id[0].shape,pos_id_targets[0].shape,pos_ids[0].shape)"
      ],
      "metadata": {
        "id": "5qdHh6Ts5Bn1",
        "outputId": "7332f67f-a405-46df-b508-00396774b226",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 2]) torch.Size([11]) torch.Size([11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_pos_id=[]\n",
        "for inp,pos in zip(inputs_without_pos_id,pos_ids):\n",
        "  inputs_with_pos_id.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))\n",
        "\n",
        "  # add user id to the inputs\n",
        "inputs_with_pos_id_user_id=[]\n",
        "for idx,inp in enumerate(inputs_with_pos_id):\n",
        "  inputs_with_pos_id_user_id.append(torch.cat([inp, torch.ones(inp.size(0),1)*idx],dim=1))"
      ],
      "metadata": {
        "id": "Hqe9GmrNKHpd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_pos_id = inputs_with_pos_id_user_id"
      ],
      "metadata": {
        "id": "_Iov5qQCKOEf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into training and testing sets\n",
        "#Get the inputs and targets\n",
        "inputs = torch.cat(inputs_with_pos_id,dim=0)\n",
        "pos_id_targets = torch.cat(pos_id_targets,dim=0)\n",
        "print(len(inputs), len(pos_id_targets) )"
      ],
      "metadata": {
        "id": "u17C-RluKcID",
        "outputId": "50739cbf-2a06-4fb3-f09e-c0dee3424582",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54745 54745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "ExA8oZy4KjUW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#Do cross validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, pos_id_targets, test_size=0.3, random_state=1)\n",
        "\n",
        "#got best decision tree classifier\n",
        "# 'criterion': ['gini', 'entropy'],\n",
        "parameters = { 'criterion': ['gini', 'entropy'], 'max_depth': [ i for i in range(100,201,30)], 'min_samples_split': [ i for i in range(100,301,50)], 'splitter':['best', 'random']}\n",
        "best_score = -1\n",
        "# for criterion in parameters['criterion']:\n",
        "for criterion in parameters['criterion']:\n",
        "  for max_depth in parameters['max_depth']:\n",
        "    for min_samples_split in parameters['min_samples_split']:\n",
        "      for splitter in parameters['splitter']:\n",
        "        clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, splitter=splitter, random_state=12)\n",
        "        clf.fit(X_train, y_train)\n",
        "        score = clf.score(X_test, y_test)\n",
        "        if score > best_score:\n",
        "          best_score = score\n",
        "          best_parameters = {'criterion': criterion, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'splitter':splitter}"
      ],
      "metadata": {
        "id": "gLpUTDQXKkg4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "6OCspM3UK62N",
        "outputId": "dc31e565-1702-48c1-ec86-659e604fedcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score:  0.2532269849001461\n",
            "{'criterion': 'gini', 'max_depth': 100, 'min_samples_split': 100, 'splitter': 'best'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross score:  [0.09654273 0.09264092 0.08990084 0.0940762  0.09381524]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary with the number of data for each class\n",
        "class_count={}\n",
        "print(vocab)\n",
        "for pos_id in pos_ids:\n",
        "  for id in pos_id:\n",
        "    if id.item() in class_count:\n",
        "      class_count[id.item()]+=1\n",
        "    else:\n",
        "      class_count[id.item()]=1\n",
        "print(class_count)\n",
        "print(len(class_count))\n",
        "print(max(class_count.keys()))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#See the tendency of the number of connections for each class\n",
        "print(\"min number of connections for each class\"\n",
        "      ,min(class_count.values())\n",
        "      ,min(class_count,key=class_count.get))\n",
        "print(\"max number of connections for each class\"\n",
        "      ,max(class_count.values())\n",
        "      ,max(class_count,key=class_count.get))\n",
        "\n",
        "plt.plot(list(class_count.values()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2UOJ4O63xDu",
        "outputId": "3f5b1046-7e58-44a9-c9f7-d557fa6b5586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(30.935314, 121.733322): 1, (31.144812, 121.119606): 2, (31.381763, 121.301878): 3, (31.056648, 121.404326): 4, (31.100034, 121.663489): 5, (31.040378, 121.255563): 6, (31.231508, 121.577691): 7, (31.036429, 121.324198): 8, (31.33775, 121.306733): 9, (31.298668, 121.541944): 10, (31.187254, 121.536259): 11, (30.889377, 121.176142): 12, (29.526266, 119.910488): 13, (31.327319, 121.462047): 14, (31.113744, 121.341878): 15, (31.250826, 121.578689): 16, (31.338059, 121.289933): 17, (31.349748, 121.506246): 18, (31.246059, 121.362706): 19, (31.414397, 121.481621): 20, (31.078791, 121.422082): 21, (31.129955, 121.336848): 22, (31.210033, 121.405714): 23, (31.355465, 121.412867): 24, (31.254534, 121.34772): 25, (31.202912, 121.712449): 26, (31.319806, 121.526248): 27, (31.282271, 121.524511): 28, (31.215246, 121.604329): 29, (31.025227, 121.44251): 30, (30.808567, 121.29074): 31, (34.638922, 119.467017): 32, (31.094143, 121.488864): 33, (31.242157, 121.568785): 34, (31.319501, 121.584673): 35, (31.247519, 121.322265): 36, (31.300084, 121.504034): 37, (31.318646, 121.53899): 38, (31.021462, 121.321255): 39, (31.255582, 121.363523): 40, (31.285741, 121.626244): 41, (31.05815, 120.970188): 42, (41.677262, 125.960124): 43, (31.263678, 121.468663): 44, (31.273451, 121.681542): 45, (31.301602, 121.487289): 46, (31.116021, 121.287621): 47, (31.205839, 121.462046): 48, (31.140384, 121.620758): 49, (31.387987, 121.495216): 50, (31.082701, 121.448694): 51, (31.114131, 121.394475): 52, (31.134254, 121.280533): 53, (31.152157, 121.514858): 54, (31.420016, 121.44113): 55, (30.716187, 121.349498): 56, (31.124664, 121.323455): 57, (30.866289, 121.103027): 58, (30.914669, 121.471638): 59, (31.215664, 121.46829): 60, (31.239025, 121.551622): 61, (31.174661, 121.52822): 62, (31.031459, 121.300841): 63, (31.066368, 121.2082): 64, (31.158843, 121.127268): 65, (30.898595, 121.030128): 66, (31.242627, 121.730079): 67, (31.230933, 121.347295): 68, (31.253601, 121.4782): 69, (31.117844, 121.402074): 70, (31.232877, 121.48753): 71, (30.920156, 121.488943): 72, (31.184888, 121.510295): 73, (31.111396, 121.379353): 74, (31.16691, 121.115166): 75, (31.19143, 121.411411): 76, (31.219114, 121.423499): 77, (31.117005, 121.271194): 78, (31.171101, 121.507145): 79, (30.731903, 121.334173): 80, (31.307268, 121.521916): 81, (31.370336, 121.265507): 82, (31.03333, 121.427385): 83, (31.159704, 121.457647): 84, (31.444014, 121.25661): 85, (31.183857, 121.637321): 86, (31.236856, 121.40554): 87, (31.078561, 121.564629): 88, (30.943662, 121.155665): 89, (31.264146, 121.532591): 90, (31.109649, 121.402441): 91, (31.31394, 121.382812): 92, (30.97935, 121.815249): 93, (31.380093, 121.517779): 94, (31.123394, 121.278015): 95, (31.146522, 121.352501): 96, (31.17939, 121.754087): 97, (30.74139, 121.380141): 98, (31.142218, 121.413767): 99, (30.948712, 121.365593): 100, (31.158515, 121.1384): 101, (31.28711, 121.226904): 102, (31.062583, 121.44986): 103, (31.057696, 121.207516): 104, (31.221591, 121.351862): 105, (31.269868, 121.533249): 106, (31.150797, 121.728702): 107, (31.242948, 121.608734): 108, (31.131416, 121.429704): 109, (31.114038, 121.415096): 110, (31.228014, 121.477667): 111, (31.32712, 121.254377): 112, (31.236104, 121.375177): 113, (31.217074, 121.510123): 114, (30.899117, 121.175731): 115, (31.042021, 121.204506): 116, (31.109666, 121.052906): 117, (31.103853, 121.810034): 118, (31.456615, 121.221022): 119, (31.36124, 121.451839): 120, (31.321416, 121.215): 121, (31.27975, 121.497791): 122, (31.011886, 121.234971): 123, (31.273767, 121.222048): 124, (30.746286, 121.344028): 125, (31.254912, 121.371838): 126, (29.263844, 115.023159): 127, (30.751838, 121.360056): 128, (31.300119, 121.532041): 129, (30.716044, 121.291037): 130, (31.188696, 121.181232): 131, (31.265321, 121.503333): 132, (31.100465, 121.401931): 133, (31.042149, 121.539262): 134, (31.272151, 121.375814): 135, (31.038201, 121.216468): 136, (31.353347, 121.441038): 137, (31.241274, 121.36272): 138, (31.116103, 121.43571): 139, (31.357773, 121.508693): 140, (31.17255, 121.363745): 141, (31.382497, 121.548578): 142, (31.246106, 121.443663): 143, (36.406412, 102.003965): 144, (31.200783, 121.446011): 145, (31.180753, 121.749174): 146, (31.00153, 121.235235): 147, (30.892415, 121.023553): 148, (31.155403, 121.57642): 149, (31.342336, 121.605688): 150, (31.177646, 121.555355): 151, (31.013227, 121.552205): 152, (31.221052, 121.503967): 153, (31.050708, 121.217094): 154, (31.150662, 121.434931): 155, (31.269517, 121.498812): 156, (31.305522, 121.483135): 157, (30.919334, 121.475563): 158, (31.162868, 121.448422): 159, (31.292661, 121.305822): 160, (31.30092, 121.16892): 161, (31.250271, 121.406561): 162, (31.337091, 121.543621): 163, (31.339114, 121.321261): 164, (31.038291, 121.194794): 165, (31.370654, 121.459272): 166, (31.360292, 121.583955): 167, (30.915722, 121.763195): 168, (31.203978, 121.423912): 169, (31.235929, 121.449652): 170, (31.108567, 121.766873): 171, (31.162282, 121.426688): 172, (31.127217, 121.335543): 173, (31.312192, 121.456456): 174, (30.991472, 121.430726): 175, (31.260959, 121.562594): 176, (31.209182, 121.752433): 177, (31.098558, 121.377147): 178, (31.240601, 121.442307): 179, (30.824298, 121.363989): 180, (31.112344, 121.480257): 181, (31.182198, 121.390214): 182, (31.019691, 121.249497): 183, (31.24822, 121.296297): 184, (31.112548, 121.538798): 185, (31.242841, 121.391309): 186, (31.254166, 121.224476): 187, (31.152177, 121.172752): 188, (31.154923, 121.36045): 189, (31.301132, 121.177821): 190, (31.166408, 121.569203): 191, (31.247155, 121.23112): 192, (31.146311, 121.399951): 193, (31.054594, 121.339133): 194, (31.273019, 121.307617): 195, (31.355885, 121.375057): 196, (31.119557, 121.036331): 197, (31.268642, 121.332479): 198, (31.357217, 121.439635): 199, (22.522803, 114.218796): 200, (31.069261, 121.375484): 201, (31.279012, 121.514374): 202, (31.203552, 121.371544): 203, (31.19206, 121.438501): 204, (31.120864, 121.389141): 205, (31.280262, 121.364633): 206, (31.266666, 121.598298): 207, (31.343106, 121.455682): 208, (31.198093, 121.126168): 209, (31.27795, 121.538718): 210, (31.232095, 121.20583): 211, (31.207125, 121.299924): 212, (30.739797, 121.32739): 213, (31.10888, 121.390919): 214, (31.155848, 121.383402): 215, (30.936752, 121.470783): 216, (31.030308, 121.650922): 217, (31.254308, 121.459288): 218, (31.321039, 121.453913): 219, (31.232639, 121.705254): 220, (31.147783, 121.366475): 221, (31.378285, 121.436137): 222, (31.178359, 121.279284): 223, (31.209025, 121.616514): 224, (31.175781, 121.643301): 225, (31.160079, 121.37079): 226, (31.308964, 121.403345): 227, (30.718302, 121.314644): 228, (31.190389, 121.457678): 229, (31.252404, 121.435035): 230, (31.202075, 121.187355): 231, (31.251584, 121.4917): 232, (31.235454, 121.172523): 233, (31.205759, 121.26316): 234, (30.939177, 121.223182): 235, (31.325382, 121.553285): 236, (31.176998, 121.423586): 237, (31.069946, 121.719641): 238, (31.254981, 121.623923): 239, (31.031203, 121.65009): 240, (31.115176, 121.36579): 241, (31.2077, 121.669382): 242, (31.314038, 121.509762): 243, (31.148848, 121.314851): 244, (31.260465, 121.461228): 245, (31.211485, 121.403238): 246, (31.274169, 121.221302): 247, (31.262756, 121.625447): 248, (31.230109, 121.434393): 249, (31.286022, 121.497436): 250, (31.163705, 121.503747): 251, (30.910136, 121.843044): 252, (30.960682, 121.447242): 253, (31.263651, 121.573401): 254, (31.042056, 121.720903): 255, (30.919796, 121.459621): 256, (31.202395, 121.6278): 257, (31.045101, 121.223676): 258, (31.227795, 121.254172): 259, (31.326648, 121.304442): 260, (31.099642, 121.449578): 261, (31.368461, 121.372259): 262, (30.891482, 121.739107): 263, (31.051898, 121.769904): 264, (31.262048, 121.458128): 265, (31.159756, 121.525433): 266, (30.88883, 121.130607): 267, (31.402828, 121.48779): 268, (31.270626, 121.5908): 269, (31.276718, 121.371696): 270, (31.355737, 121.403678): 271, (31.337218, 121.465181): 272, (31.215428, 121.660427): 273, (31.015724, 121.40907): 274, (31.324464, 121.387532): 275, (31.273045, 121.546148): 276, (31.134786, 121.444969): 277, (31.175318, 121.181299): 278, (30.934254, 121.279662): 279, (31.143965, 121.573343): 280, (31.201259, 121.40143): 281, (30.967774, 121.447499): 282, (31.252105, 121.56398): 283, (31.201251, 121.561484): 284, (31.030356, 121.535809): 285, (31.260814, 121.490337): 286, (31.423448, 121.433415): 287, (30.988441, 121.87486): 288, (31.086136, 121.507054): 289, (31.211591, 121.499553): 290, (31.268894, 121.692432): 291, (30.900013, 121.049649): 292, (31.247688, 121.528106): 293, (31.176978, 121.499522): 294, (31.303976, 121.474504): 295, (31.294508, 121.537685): 296, (31.018176, 121.418004): 297, (31.228755, 121.523626): 298, (30.723654, 121.330778): 299, (30.970389, 121.451558): 300, (31.359311, 121.310645): 301, (31.260835, 121.366953): 302, (31.234496, 121.521516): 303, (31.137509, 121.40768): 304, (31.211598, 121.439406): 305, (31.104981, 121.360938): 306, (31.188821, 121.387044): 307, (31.143774, 121.519955): 308, (31.168853, 121.394657): 309, (31.346682, 121.49352): 310, (34.689694, 112.407126): 311, (31.053362, 121.431405): 312, (30.999098, 121.447826): 313, (31.311329, 121.495756): 314, (31.255942, 121.38141): 315, (31.246345, 121.45371): 316, (31.096325, 121.341209): 317, (31.382812, 121.242351): 318, (31.19706, 121.095538): 319, (26.139329, 103.078562): 320, (31.210891, 121.3779): 321, (31.125695, 121.46401): 322, (30.889117, 121.319998): 323, (31.242837, 121.455083): 324, (31.142941, 121.443942): 325, (31.163459, 121.574025): 326, (31.250332, 121.594504): 327, (31.233973, 121.41641): 328, (30.847557, 121.226776): 329, (31.26484, 121.498029): 330, (31.273183, 121.488391): 331, (31.216359, 121.397227): 332, (31.297527, 121.376): 333, (31.286632, 121.552094): 334, (31.141245, 121.500503): 335, (31.192915, 121.682688): 336, (31.121932, 121.33481): 337, (31.069676, 121.773361): 338, (31.06623, 121.321933): 339, (30.983162, 121.054057): 340, (31.206194, 121.456569): 341, (30.887807, 121.346419): 342, (31.204999, 121.471193): 343, (31.131231, 121.398758): 344, (30.995864, 121.568444): 345, (31.218201, 121.487151): 346, (31.090076, 121.406999): 347, (31.007327, 121.431151): 348, (31.182483, 121.5762): 349, (31.111607, 121.324796): 350, (31.2355, 121.417315): 351, (31.276178, 121.553996): 352, (31.184319, 121.230946): 353, (31.267266, 121.385292): 354, (31.261563, 121.376836): 355, (31.214683, 121.551574): 356, (31.318661, 121.194317): 357, (31.238593, 121.450062): 358, (31.019246, 121.250749): 359, (31.000658, 121.659325): 360, (31.176379, 121.397706): 361, (31.13107, 121.298444): 362, (31.214561, 121.298904): 363, (31.2676, 121.541803): 364, (31.273242, 121.499598): 365, (30.79706, 121.431713): 366, (31.231687, 121.32576): 367, (30.932475, 121.692651): 368, (31.233761, 121.670586): 369, (31.26604, 121.399004): 370, (31.140979, 121.586245): 371, (31.284148, 121.481157): 372, (31.194194, 121.456589): 373, (30.739639, 121.373695): 374, (31.288777, 121.390078): 375, (31.405231, 121.506638): 376, (31.31724, 121.237444): 377, (31.056596, 121.721736): 378, (31.348418, 121.427796): 379, (31.133956, 121.526289): 380, (31.224385, 121.143509): 381, (31.456258, 121.413525): 382, (30.900531, 121.399787): 383, (30.986462, 121.409757): 384, (31.092058, 121.38753): 385, (31.04139, 121.741651): 386, (31.018376, 121.40507): 387, (31.095884, 121.350551): 388, (31.105352, 121.438535): 389, (31.230287, 121.557116): 390, (31.149677, 121.729053): 391, (31.246527, 121.534982): 392, (31.395915, 121.363062): 393, (31.27431, 121.448589): 394, (31.102445, 121.413911): 395, (31.174053, 121.282683): 396, (31.189335, 121.781953): 397, (31.287814, 121.355886): 398, (31.242514, 121.493696): 399, (31.225266, 121.444323): 400, (31.370388, 121.187891): 401, (31.214209, 121.659493): 402, (31.109806, 121.242241): 403, (31.119806, 121.071074): 404, (31.206547, 121.306923): 405, (31.190168, 121.467092): 406, (31.306089, 121.474725): 407, (31.260555, 121.410883): 408, (31.173856, 121.350522): 409, (31.243013, 121.416207): 410, (30.877822, 121.548349): 411, (31.184979, 121.31046): 412, (31.337262, 121.473928): 413, (31.33167, 121.440751): 414, (30.73407, 121.350673): 415, (31.395438, 121.472743): 416, (30.98383, 121.235077): 417, (31.181381, 121.688934): 418, (31.073061, 120.970637): 419, (31.378631, 121.277965): 420, (31.319077, 121.331853): 421, (30.90387, 121.433071): 422, (31.326138, 121.48973): 423, (31.255905, 121.430231): 424, (31.157536, 121.203337): 425, (31.238561, 121.455041): 426, (31.120602, 121.581473): 427, (31.365408, 121.245027): 428, (31.206089, 121.105074): 429, (31.108876, 121.345256): 430, (31.262746, 121.517701): 431, (31.229334, 121.368291): 432, (31.260165, 121.541462): 433, (31.209905, 121.37472): 434, (31.198952, 121.70244): 435, (31.189701, 121.543143): 436, (31.344291, 121.487858): 437, (31.265, 121.48958): 438, (31.208108, 121.41483): 439, (31.280339, 121.402957): 440, (31.015959, 121.227509): 441, (31.197909, 121.426542): 442, (30.938138, 121.327668): 443, (31.19232, 121.142889): 444, (31.009423, 121.245898): 445, (31.163432, 121.354451): 446, (35.379598, 116.072359): 447, (31.207205, 121.482691): 448, (31.019982, 121.244295): 449, (31.086916, 121.387535): 450, (31.233201, 121.469528): 451, (31.311942, 121.54204): 452, (31.217372, 121.460724): 453, (30.908583, 121.656169): 454, (31.204614, 121.432408): 455, (31.057069, 121.599172): 456, (31.164195, 121.330502): 457, (31.200882, 121.446511): 458, (31.269929, 121.453419): 459, (31.182415, 121.381272): 460, (31.276317, 121.524794): 461, (31.12408, 121.716759): 462, (31.277512, 121.287969): 463, (31.25007, 121.546845): 464, (31.080824, 121.390896): 465, (31.196266, 121.537132): 466, (31.13756, 121.339005): 467, (31.263762, 121.481215): 468, (31.356238, 121.568226): 469, (31.266679, 121.539067): 470, (31.167947, 121.670231): 471, (30.87706, 121.856429): 472, (31.075231, 121.436269): 473, (31.252264, 121.38269): 474, (30.844397, 121.528845): 475, (31.186465, 121.429887): 476, (30.998611, 121.216212): 477, (31.270362, 121.612773): 478, (31.396351, 121.44222): 479, (30.874287, 121.67603): 480, (31.344675, 121.443963): 481, (31.211107, 121.59082): 482, (31.455174, 121.413981): 483, (31.241739, 121.537288): 484, (30.941447, 121.293964): 485, (31.371694, 121.443832): 486, (31.25393, 121.455087): 487, (31.19107, 121.554321): 488, (31.237872, 121.470259): 489, (31.125467, 121.581832): 490, (30.715526, 121.351073): 491, (31.259633, 121.594477): 492, (31.058092, 121.235964): 493, (30.903508, 121.331287): 494, (31.214059, 121.37764): 495, (31.025763, 121.541983): 496, (31.24448, 121.37062): 497, (30.93817, 121.865652): 498, (31.352538, 121.490272): 499, (31.208346, 121.544892): 500, (31.195139, 121.385973): 501, (31.285698, 121.612262): 502, (31.265326, 121.394456): 503, (31.246097, 121.39928): 504, (31.143297, 121.132883): 505, (31.251239, 121.428578): 506, (31.244102, 121.437744): 507, (31.155384, 121.771585): 508, (30.867598, 121.198612): 509, (31.17023, 121.337413): 510, (31.274536, 121.097082): 511, (31.382771, 121.291756): 512, (31.154875, 121.131226): 513, (31.296405, 121.392803): 514, (31.116903, 121.595022): 515, (30.983141, 121.540945): 516, (31.282514, 121.343407): 517, (31.163201, 121.11139): 518, (31.058407, 121.330724): 519, (31.294393, 121.497298): 520, (31.150483, 121.491401): 521, (30.860668, 121.470193): 522, (31.190301, 121.566862): 523, (31.158559, 121.319801): 524, (31.29992, 121.669771): 525, (31.201279, 121.709485): 526, (31.073134, 121.317814): 527, (31.179835, 121.429502): 528, (31.268663, 121.428101): 529, (31.134548, 121.39095): 530, (30.900529, 121.246804): 531, (31.148773, 121.387158): 532, (31.151539, 121.41109): 533, (31.152182, 121.564661): 534, (31.132785, 121.413457): 535, (31.272162, 121.459351): 536, (31.452354, 121.33861): 537, (31.213015, 121.547755): 538, (31.312291, 121.46631): 539, (31.03732, 121.18963): 540, (31.220649, 121.35994): 541, (31.118723, 121.248449): 542, (31.236728, 121.42407): 543, (31.223004, 121.465956): 544, (31.222233, 121.623547): 545, (31.042415, 121.482986): 546, (31.315825, 121.532151): 547, (31.220434, 121.408625): 548, (31.16346, 121.405127): 549, (31.036623, 121.377512): 550, (31.258255, 121.471554): 551, (31.045516, 121.216817): 552, (31.29092, 121.359453): 553, (31.236862, 121.355595): 554, (31.196135, 121.558528): 555, (31.26338, 121.433161): 556, (31.142865, 121.330843): 557, (31.159304, 121.358718): 558, (31.10114, 121.181649): 559, (31.323091, 121.364299): 560, (31.223665, 121.458791): 561, (31.248587, 121.277434): 562, (31.211574, 121.366182): 563, (31.276982, 121.460649): 564, (31.324905, 121.29416): 565, (30.87366, 121.082631): 566, (31.327488, 121.437837): 567, (31.327049, 121.443238): 568, (31.072213, 121.658885): 569, (31.020065, 121.23311): 570, (31.015452, 121.154814): 571, (31.384186, 121.484871): 572, (31.197573, 121.454035): 573, (31.048845, 121.231719): 574, (38.052584, 114.484137): 575, (31.311763, 121.21253): 576, (31.044065, 121.537499): 577, (30.705762, 121.33512): 578, (30.982046, 121.207872): 579, (31.17605, 121.431225): 580, (31.111821, 121.385462): 581, (31.488248, 121.349833): 582, (31.099755, 121.009542): 583, (31.366638, 121.43543): 584, (31.387389, 121.431922): 585, (31.213659, 121.504661): 586, (31.145387, 121.61623): 587, (31.044667, 121.46191): 588, (31.230395, 121.46599): 589, (31.320121, 121.459018): 590, (31.190185, 121.443475): 591, (31.155297, 121.573504): 592, (31.130862, 121.091425): 593, (31.254111, 121.408424): 594, (31.403259, 121.447841): 595, (31.209644, 121.56447): 596, (30.935837, 121.561311): 597, (31.277766, 121.433047): 598, (31.216371, 121.601926): 599, (31.158202, 121.389411): 600, (31.05799, 121.626154): 601, (30.996346, 121.297687): 602, (31.215381, 121.439262): 603, (31.155344, 121.2025): 604, (31.094914, 121.43682): 605, (31.395973, 121.258897): 606, (31.367578, 121.31858): 607, (31.210843, 121.491168): 608, (31.124501, 121.32791): 609, (30.999472, 121.378674): 610, (30.922611, 121.473581): 611, (31.232002, 121.359292): 612, (31.168938, 121.54333): 613, (31.177169, 121.445517): 614, (31.045313, 121.844791): 615, (31.175621, 121.317835): 616, (31.23593, 121.220028): 617, (31.13623, 121.77848): 618, (31.235177, 121.451385): 619, (31.228847, 121.338653): 620, (31.262448, 121.504717): 621, (31.217476, 121.494788): 622, (31.234004, 121.276012): 623, (31.311631, 121.354326): 624, (31.217968, 121.41467): 625, (31.241014, 121.48529): 626, (31.224226, 121.43839): 627, (31.224718, 121.344451): 628, (31.159569, 121.145966): 629, (31.023902, 121.350511): 630, (31.220809, 121.305363): 631, (31.294194, 121.316828): 632, (31.222119, 121.454078): 633, (31.265705, 121.420959): 634, (31.430211, 121.28239): 635, (31.180466, 121.441251): 636, (31.281813, 121.354285): 637, (31.121363, 121.369095): 638, (31.245624, 121.522839): 639, (31.125764, 121.562057): 640, (31.195131, 121.418788): 641, (30.829105, 121.180127): 642, (31.3397, 121.298473): 643, (30.799332, 121.268825): 644, (31.16807, 121.488657): 645, (31.047975, 121.785062): 646, (30.832601, 121.445485): 647, (31.039301, 121.593594): 648, (31.374842, 121.264224): 649, (31.275929, 121.473484): 650, (31.181485, 121.520026): 651, (30.895699, 121.478289): 652, (31.252974, 121.502684): 653, (31.074013, 121.407487): 654, (30.960478, 121.057529): 655, (31.102, 121.462298): 656, (31.204343, 121.407018): 657, (31.402222, 121.493387): 658, (31.153746, 121.504388): 659, (31.137143, 121.545715): 660, (30.832683, 121.481868): 661, (31.01294, 121.637001): 662, (31.191674, 121.699103): 663, (31.11674, 121.586334): 664, (31.248146, 121.441241): 665, (30.945561, 121.150834): 666, (31.318695, 121.410448): 667, (31.235453, 121.653548): 668, (31.32899, 121.323419): 669, (30.926464, 121.682732): 670, (31.24641, 121.548541): 671, (31.034322, 121.601024): 672, (31.145439, 121.417433): 673, (31.226966, 121.53174): 674, (31.182371, 121.526439): 675, (31.219133, 121.524846): 676, (31.459469, 121.412299): 677, (30.954604, 121.333773): 678, (31.331242, 121.225906): 679, (30.884387, 121.408648): 680, (31.203031, 121.443586): 681, (31.128227, 121.45441): 682, (31.30647, 121.492925): 683, (31.214767, 121.444427): 684, (31.292717, 121.482235): 685, (31.239726, 121.139516): 686, (31.294378, 121.567937): 687, (31.121152, 121.141999): 688, (31.186628, 121.372116): 689, (31.213701, 121.41802): 690, (31.063923, 121.353076): 691, (31.227933, 121.45361): 692, (30.797557, 121.356113): 693, (31.280867, 121.656939): 694, (31.294155, 121.418063): 695, (31.423181, 121.387917): 696, (31.098701, 121.582178): 697, (31.300693, 121.496182): 698, (31.214115, 121.406884): 699, (31.22121, 121.411568): 700, (31.250674, 121.47282): 701, (31.232857, 121.475333): 702, (31.22135, 121.391319): 703, (31.17665, 121.132941): 704, (30.950067, 121.449083): 705, (31.077825, 121.33981): 706, (46.777465, 131.812182): 707, (31.2317, 121.546506): 708, (31.082614, 121.523874): 709, (31.127837, 121.380654): 710, (31.414938, 121.486809): 711, (31.194792, 121.466884): 712, (31.318974, 121.412838): 713, (31.24143, 121.475172): 714, (31.274535, 121.48034): 715, (31.122623, 121.433813): 716, (31.318369, 121.407014): 717, (30.918205, 121.509992): 718, (31.13264, 121.32443): 719, (31.299261, 121.578516): 720, (31.276268, 121.683198): 721, (31.292008, 121.36941): 722, (30.730296, 121.342643): 723, (31.01613, 121.203009): 724, (31.19382, 121.49524): 725, (31.342417, 121.40922): 726, (31.23301, 121.442524): 727, (31.28023, 121.543164): 728, (31.32719, 121.53013): 729, (31.373346, 121.135761): 730, (31.037985, 121.752781): 731, (31.140855, 121.360032): 732, (30.898527, 121.22367): 733, (31.012212, 121.413472): 734, (31.224452, 121.511537): 735, (31.274564, 121.51979): 736, (31.205813, 121.413544): 737, (31.202883, 121.476296): 738, (31.18851, 121.403115): 739, (31.177858, 121.346956): 740, (30.719537, 121.355948): 741, (31.160199, 121.557134): 742, (31.25563, 121.508513): 743, (31.132311, 121.362691): 744, (31.171972, 121.140685): 745, (30.866245, 121.343018): 746, (31.217516, 121.452872): 747, (31.146551, 121.235394): 748, (31.298752, 121.491919): 749, (31.204179, 121.538588): 750, (31.191847, 121.381117): 751, (30.909885, 121.195459): 752, (31.281567, 121.557955): 753, (30.814573, 121.352883): 754, (31.012926, 121.423939): 755, (31.260429, 121.426787): 756, (31.371366, 121.236234): 757, (31.256151, 121.557845): 758, (31.122978, 121.704554): 759, (31.035283, 121.246093): 760, (31.201303, 121.434409): 761, (31.183323, 121.447822): 762, (31.281363, 121.459935): 763, (31.397749, 121.267217): 764, (31.285861, 121.507007): 765, (31.274417, 121.257132): 766, (31.219239, 121.418564): 767, (31.045068, 121.232113): 768, (31.120822, 121.29001): 769, (31.198048, 121.604048): 770, (31.301877, 121.353206): 771, (31.255293, 121.49343): 772, (31.236935, 121.106513): 773, (31.225092, 121.38464): 774, (31.051247, 121.480621): 775, (31.225207, 121.452948): 776, (31.303478, 121.55082): 777, (30.8406, 121.251681): 778, (31.156874, 121.348458): 779, (31.245814, 121.411472): 780, (30.918282, 121.639066): 781, (31.072138, 121.661831): 782, (31.312494, 121.157796): 783, (31.282237, 121.42091): 784, (31.225831, 121.412695): 785, (31.239319, 121.265377): 786, (30.920347, 121.057024): 787, (31.278892, 121.710661): 788, (31.085146, 121.589124): 789, (31.127879, 121.452728): 790, (31.185228, 121.1697): 791, (31.401391, 121.463758): 792, (31.312825, 121.491112): 793, (30.984047, 121.726879): 794, (31.00953, 121.052076): 795, (31.207679, 121.489691): 796, (30.816155, 121.298392): 797, (31.192975, 121.449755): 798, (31.126339, 121.573213): 799, (31.113159, 121.06421): 800, (31.240587, 121.402322): 801, (31.317294, 121.460019): 802, (31.324375, 121.433451): 803, (31.19332, 121.430385): 804, (31.220416, 121.529149): 805, (31.216526, 121.326846): 806, (31.122449, 121.582926): 807, (31.195907, 121.410659): 808, (31.194567, 121.453929): 809, (31.266743, 121.481644): 810, (30.975655, 121.228194): 811, (31.268263, 121.394134): 812, (31.049556, 121.600672): 813, (31.208384, 121.466365): 814, (31.335151, 121.590721): 815, (31.241036, 121.374063): 816, (30.7834, 121.299618): 817, (31.242991, 121.515376): 818, (31.299851, 121.163211): 819, (31.01217, 121.265575): 820, (30.831909, 121.397355): 821, (31.304918, 121.497837): 822, (31.215081, 121.481286): 823, (31.043159, 121.420296): 824, (31.307198, 121.511929): 825, (31.159667, 121.515041): 826, (31.038911, 121.226883): 827, (31.161027, 121.722478): 828, (31.088099, 121.430111): 829, (31.300545, 121.170082): 830, (31.030806, 121.396875): 831, (31.273276, 121.1303): 832, (31.276349, 121.39526): 833, (31.230201, 121.308739): 834, (31.087306, 121.275545): 835, (31.240478, 121.526884): 836, (31.284042, 121.583676): 837, (31.017385, 121.715431): 838, (31.24708, 121.419525): 839, (31.197656, 121.463533): 840, (31.213728, 121.386102): 841, (31.388641, 121.444174): 842, (31.039193, 121.604274): 843, (31.242365, 121.503571): 844, (31.15228, 121.118001): 845, (31.144212, 121.50825): 846, (31.193409, 121.393885): 847, (31.185812, 121.457923): 848, (31.258677, 121.392956): 849, (31.016895, 121.42971): 850, (31.198836, 121.215066): 851, (31.241362, 121.395822): 852, (31.023331, 121.315006): 853, (31.260742, 121.399188): 854, (31.337749, 121.579688): 855, (31.149689, 121.531301): 856, (30.948533, 121.465504): 857, (31.257553, 121.311711): 858, (31.00948, 121.4998): 859, (31.26748, 121.57859): 860, (31.211617, 121.486703): 861, (31.244559, 121.724977): 862, (31.245663, 121.449725): 863, (30.947965, 121.640593): 864, (31.10366, 121.420217): 865, (31.23423, 121.119889): 866, (30.983966, 121.737727): 867, (31.354948, 121.496863): 868, (31.358748, 121.52806): 869, (31.201647, 121.420098): 870, (30.843004, 121.438792): 871, (31.25901, 121.65797): 872, (30.823953, 121.528367): 873, (31.172757, 121.425965): 874, (31.019533, 121.388705): 875, (31.052986, 121.506423): 876, (31.301307, 121.33124): 877, (31.295347, 121.515155): 878, (31.234545, 121.743402): 879, (31.324066, 121.536866): 880, (30.910085, 121.504947): 881, (31.248482, 121.53958): 882, (31.059755, 121.389723): 883, (31.318389, 121.653955): 884, (31.157241, 121.418919): 885, (30.92668, 121.466784): 886, (31.218812, 121.401309): 887, (31.288102, 121.122551): 888, (31.334449, 121.536876): 889, (31.221001, 121.731159): 890, (31.266813, 121.512658): 891, (31.298177, 121.473198): 892, (31.045834, 121.408322): 893, (30.865062, 121.662776): 894, (30.795602, 121.152307): 895, (31.294598, 121.206157): 896, (31.096053, 121.518157): 897, (30.977517, 121.669761): 898, (30.858416, 121.035215): 899, (30.93561, 121.517165): 900, (31.057816, 121.413943): 901, (31.327484, 121.406697): 902, (31.425786, 121.422506): 903, (31.250734, 121.487731): 904, (31.264477, 121.44249): 905, (31.38336, 121.420672): 906, (31.318281, 121.447698): 907, (31.032565, 121.409407): 908, (31.312069, 121.172955): 909, (31.313954, 121.499987): 910, (30.924981, 121.710847): 911, (31.139657, 121.298915): 912, (31.157677, 121.424531): 913, (30.904757, 121.467941): 914, (31.219088, 121.428934): 915, (31.149628, 121.43924): 916, (31.297281, 121.505124): 917, (31.315501, 121.535127): 918, (31.25821, 121.479271): 919, (31.069757, 121.399847): 920, (31.360948, 121.595293): 921, (31.351256, 121.519123): 922, (31.295335, 121.555781): 923, (30.872205, 121.530081): 924, (31.390234, 121.254572): 925, (31.277576, 121.460905): 926, (31.316557, 121.516135): 927, (31.120714, 121.42693): 928, (31.235072, 121.518805): 929, (31.255173, 121.403569): 930, (30.831429, 121.242309): 931, (31.30835, 121.657013): 932, (31.333408, 121.552846): 933, (31.253394, 121.389864): 934, (31.139721, 121.673912): 935, (31.261924, 121.568031): 936, (30.929919, 121.206039): 937, (31.070388, 121.532962): 938, (31.221319, 121.166272): 939, (31.383625, 121.268069): 940, (31.189723, 121.356104): 941, (31.460409, 121.401649): 942, (31.167279, 121.113649): 943, (31.242808, 121.420927): 944, (31.054324, 121.793035): 945, (31.081475, 121.523285): 946, (31.138371, 121.416975): 947, (31.180763, 121.476049): 948, (31.267654, 121.473299): 949, (31.260903, 121.618966): 950, (31.088801, 121.536584): 951, (31.313065, 121.417971): 952, (31.185065, 121.503655): 953, (31.219945, 121.285654): 954, (30.766975, 121.362906): 955, (30.934396, 121.369499): 956, (31.39034, 121.272165): 957, (30.741272, 121.3348): 958, (31.43737, 121.432451): 959, (31.283201, 121.665402): 960, (31.046524, 121.749155): 961, (31.370457, 121.470415): 962, (30.908378, 121.634574): 963, (31.255025, 121.199554): 964, (31.064763, 121.396913): 965, (31.005717, 121.202014): 966, (30.894639, 121.21016): 967, (30.893975, 121.016922): 968, (31.364407, 121.561457): 969, (30.997412, 121.247239): 970, (31.168922, 121.412651): 971, (30.834815, 121.197836): 972, (31.251358, 121.485526): 973, (31.174203, 121.431404): 974, (31.253808, 121.483756): 975, (31.262941, 121.602924): 976, (31.236381, 121.484201): 977, (31.239655, 121.478097): 978, (31.199721, 121.325753): 979, (31.468523, 121.425585): 980, (30.978506, 121.447689): 981, (31.176489, 121.188906): 982, (31.271472, 121.494721): 983, (30.989121, 121.127308): 984, (30.796696, 121.200272): 985, (31.150398, 121.537521): 986, (31.361645, 121.475443): 987, (31.166007, 121.457766): 988, (31.1744, 121.285083): 989, (30.855412, 121.569454): 990, (31.260816, 121.36983): 991, (31.136777, 121.546762): 992, (30.915122, 121.560642): 993, (31.301749, 121.457406): 994, (31.302, 121.670498): 995, (31.287632, 121.458734): 996, (31.317987, 120.619907): 997, (31.120753, 121.381579): 998, (31.039158, 121.239632): 999, (31.236177, 121.493181): 1000, (31.163393, 121.381735): 1001, (31.453478, 121.256429): 1002, (31.202118, 121.395128): 1003, (31.176807, 121.251063): 1004, (31.035151, 121.544702): 1005, (31.294885, 121.244609): 1006, (30.929412, 121.460403): 1007, (31.106759, 121.398956): 1008, (30.915686, 121.462098): 1009, (31.350616, 121.589116): 1010, (31.427978, 121.433301): 1011, (31.109271, 121.617421): 1012, (30.966714, 121.771375): 1013, (31.22714, 121.391029): 1014, (31.28386, 121.530324): 1015, (31.178278, 121.606216): 1016, (30.774911, 121.259927): 1017, (31.364503, 121.391608): 1018, (31.293561, 121.362372): 1019, (31.040309, 121.308498): 1020, (31.036987, 121.386554): 1021, (31.243446, 121.675041): 1022, (31.165339, 121.427732): 1023, (31.171648, 121.110276): 1024, (31.116535, 121.678055): 1025, (31.230748, 121.485488): 1026, (31.290434, 121.425041): 1027, (31.054947, 120.995325): 1028, (31.07195, 121.150279): 1029, (31.373432, 121.490334): 1030, (31.456227, 121.359857): 1031, (31.046945, 121.764826): 1032, (30.763589, 121.264689): 1033, (31.011339, 121.296295): 1034, (31.441487, 121.179013): 1035, (31.434069, 121.448741): 1036, (31.233674, 121.498294): 1037, (34.556383, 118.79231): 1038, (31.323113, 121.616609): 1039, (31.277484, 121.382842): 1040, (31.360977, 121.444195): 1041, (30.985081, 121.691229): 1042, (31.240412, 121.147662): 1043, (31.235461, 121.498056): 1044, (31.04822, 121.231928): 1045, (31.251883, 121.418603): 1046, (31.145641, 121.449009): 1047, (31.148334, 121.420448): 1048, (31.317417, 121.48383): 1049, (31.190828, 121.420662): 1050, (31.291192, 121.65263): 1051, (31.053198, 120.907525): 1052, (39.612987, 118.20604): 1053, (31.036404, 121.272345): 1054, (31.244292, 121.495265): 1055, (31.305433, 121.435633): 1056, (31.08906, 121.722984): 1057, (31.241762, 121.50497): 1058, (31.234259, 121.52572): 1059, (31.033659, 121.577846): 1060, (31.28627, 121.599397): 1061, (31.256559, 121.43557): 1062, (31.195709, 121.424093): 1063, (31.358014, 121.37617): 1064, (31.421831, 121.349166): 1065, (30.891157, 121.186557): 1066, (31.219455, 121.098597): 1067, (31.236138, 121.463199): 1068, (30.993883, 121.129758): 1069, (31.270302, 121.398224): 1070, (31.242078, 121.331128): 1071, (31.459342, 121.319364): 1072, (30.733903, 121.341904): 1073, (30.912905, 121.801126): 1074, (31.281917, 121.435906): 1075, (31.163339, 121.469291): 1076, (31.203473, 121.445984): 1077, (31.099085, 121.386207): 1078, (31.058968, 121.787072): 1079, (31.426865, 121.351198): 1080, (31.301954, 121.54744): 1081, (31.232729, 121.566935): 1082, (30.969483, 121.53972): 1083, (31.231068, 121.4021): 1084, (31.200224, 121.316463): 1085, (31.270279, 121.531241): 1086, (31.002291, 121.873286): 1087, (31.237022, 121.546184): 1088, (31.121127, 121.243829): 1089, (30.771941, 121.373524): 1090, (31.132833, 121.17281): 1091, (31.22255, 121.440445): 1092, (31.116791, 121.166516): 1093, (31.249162, 121.487899): 1094, (31.225318, 121.437806): 1095, (31.087507, 121.398717): 1096, (30.85108, 121.855994): 1097, (31.256071, 121.462096): 1098, (31.004198, 121.067198): 1099, (31.270369, 121.443633): 1100, (31.370992, 121.177459): 1101, (31.180175, 121.422303): 1102, (30.798179, 121.426469): 1103, (31.051162, 121.311884): 1104, (31.209913, 121.425292): 1105, (31.178033, 121.487803): 1106, (31.242893, 121.465382): 1107, (31.213582, 121.38377): 1108, (31.002479, 121.390889): 1109, (31.17762, 121.511042): 1110, (31.126777, 121.367557): 1111, (31.237944, 121.693732): 1112, (31.3065, 121.429743): 1113, (30.945306, 121.729327): 1114, (31.021822, 121.590566): 1115, (30.747024, 121.287991): 1116, (31.316181, 121.405014): 1117, (31.235247, 121.383101): 1118, (31.243338, 121.253922): 1119, (31.228047, 121.425893): 1120, (30.978588, 121.581677): 1121, (31.105808, 121.038742): 1122, (31.312672, 121.593011): 1123, (31.308902, 121.502016): 1124, (31.147903, 121.403379): 1125, (31.201965, 121.452457): 1126, (31.345443, 121.596458): 1127, (31.320829, 121.389518): 1128, (31.283959, 121.416644): 1129, (31.267478, 121.5468): 1130, (31.221876, 121.687381): 1131, (31.068209, 121.373076): 1132, (30.875057, 121.324697): 1133, (31.204349, 121.693531): 1134, (31.289489, 121.341656): 1135, (31.070388, 121.520258): 1136, (31.041762, 120.928733): 1137, (31.20901, 121.478403): 1138, (31.307215, 121.486962): 1139, (31.29099, 121.697503): 1140, (31.396687, 121.378844): 1141, (30.852446, 121.262447): 1142, (31.08111, 121.263537): 1143, (31.3691, 121.415317): 1144, (31.22258, 121.428599): 1145, (31.255256, 121.500499): 1146, (30.900763, 121.246509): 1147, (31.048503, 121.141872): 1148, (30.933365, 121.873737): 1149, (31.244859, 121.489305): 1150, (31.288233, 121.304952): 1151, (31.281665, 121.42621): 1152, (31.331055, 121.52962): 1153, (30.72456, 121.344043): 1154, (31.285059, 121.406807): 1155, (31.217237, 121.432824): 1156, (31.176813, 121.439114): 1157, (31.337637, 121.446597): 1158, (31.232407, 121.652986): 1159, (31.296656, 121.483976): 1160, (31.159536, 121.1442): 1161, (31.323598, 121.638883): 1162, (31.239454, 121.46057): 1163, (31.291143, 121.472012): 1164, (31.197573, 121.37641): 1165, (31.413754, 121.202263): 1166, (31.376717, 121.539395): 1167, (32.902077, 109.42358): 1168, (31.233516, 121.491431): 1169, (31.078365, 121.404647): 1170, (31.110093, 121.371253): 1171, (31.050344, 121.770082): 1172, (31.168241, 121.437559): 1173, (31.161635, 121.286602): 1174, (31.065206, 121.803371): 1175, (31.473557, 121.331298): 1176, (31.228927, 121.486896): 1177, (30.967591, 121.656531): 1178, (31.150193, 121.45419): 1179, (31.192275, 121.462826): 1180, (31.247009, 121.47942): 1181, (30.802409, 121.205114): 1182, (31.130891, 121.472384): 1183, (31.171686, 121.441221): 1184, (31.273982, 121.512661): 1185, (31.223114, 121.522857): 1186, (31.170097, 121.608906): 1187, (31.221777, 121.558508): 1188, (31.294609, 121.526886): 1189, (31.160355, 121.454785): 1190, (31.278008, 121.492049): 1191, (31.080002, 121.798389): 1192, (31.373175, 121.438741): 1193, (31.431346, 121.222574): 1194, (31.253346, 121.448039): 1195, (31.396658, 121.496972): 1196, (31.141982, 121.459872): 1197, (31.243709, 121.197504): 1198, (31.068626, 121.108326): 1199, (31.222168, 121.378951): 1200, (31.334186, 121.50238): 1201, (31.19868, 121.485666): 1202, (31.173163, 121.356571): 1203, (31.157022, 121.498865): 1204, (31.289793, 121.455017): 1205, (30.844717, 121.528288): 1206, (30.736093, 121.355957): 1207, (31.126179, 121.378046): 1208, (31.280857, 121.169253): 1209, (31.333315, 121.188402): 1210, (30.955208, 121.732358): 1211, (31.197114, 121.442853): 1212, (31.191818, 121.538713): 1213, (30.866409, 121.188512): 1214, (31.233629, 121.532011): 1215, (31.144653, 121.478451): 1216, (31.227167, 121.633295): 1217, (31.28915, 121.229167): 1218, (31.304036, 121.448053): 1219, (31.234867, 121.46325): 1220, (31.290887, 121.435064): 1221, (30.957633, 121.343012): 1222, (31.168296, 121.588625): 1223, (31.059304, 121.53566): 1224, (31.250339, 121.440196): 1225, (31.11794, 121.457471): 1226, (31.235682, 121.487831): 1227, (31.410741, 121.442349): 1228, (31.218716, 121.568667): 1229, (31.013082, 121.391811): 1230, (31.222609, 121.378494): 1231, (31.264433, 121.541398): 1232, (31.232518, 121.465177): 1233, (31.23271, 121.455345): 1234, (31.309342, 121.580807): 1235, (30.873326, 121.284335): 1236, (31.252495, 121.390194): 1237, (31.109998, 121.164842): 1238, (31.304576, 121.522177): 1239, (31.271819, 121.546358): 1240, (31.184883, 121.430689): 1241, (31.26338, 121.517162): 1242, (31.437104, 121.31285): 1243, (31.037068, 121.686977): 1244, (31.216752, 121.476401): 1245, (31.284953, 121.489071): 1246, (31.119287, 121.204986): 1247, (31.22622, 121.506261): 1248, (31.240869, 121.481603): 1249, (31.224731, 121.43464): 1250, (31.221346, 121.201454): 1251, (30.92643, 121.782514): 1252, (30.948124, 121.381501): 1253, (31.300338, 121.526827): 1254, (31.235847, 121.536101): 1255, (31.127539, 121.389284): 1256, (31.021245, 121.226791): 1257, (31.355364, 121.537096): 1258, (31.209872, 121.409627): 1259, (31.417324, 121.491388): 1260, (31.245393, 121.705496): 1261, (30.956624, 121.334663): 1262, (31.309645, 121.4911): 1263, (30.86683, 121.83302): 1264, (31.335141, 121.413142): 1265, (31.23543, 121.447652): 1266, (31.152749, 121.182589): 1267, (30.879349, 121.094749): 1268, (31.302288, 121.510464): 1269, (31.026816, 121.426094): 1270, (31.280389, 121.467635): 1271, (31.398623, 121.409041): 1272, (31.310614, 121.476617): 1273, (31.17247, 121.404203): 1274, (31.254124, 121.605142): 1275, (31.143835, 120.935676): 1276, (31.288583, 121.428686): 1277, (30.905367, 121.832974): 1278, (31.221201, 121.497347): 1279, (31.223175, 121.184646): 1280, (31.208589, 121.524447): 1281, (30.749894, 121.270796): 1282, (31.203987, 121.520591): 1283, (30.975236, 121.195313): 1284, (31.305269, 121.397452): 1285, (31.327299, 121.452936): 1286, (31.252241, 121.57094): 1287, (31.098359, 121.073778): 1288, (31.228627, 121.480756): 1289, (31.071955, 121.234457): 1290, (31.08093, 121.159291): 1291, (31.155342, 121.8105): 1292, (31.33454, 121.46611): 1293, (31.213849, 121.43028): 1294, (31.259319, 121.494159): 1295, (31.286501, 121.363758): 1296, (31.379455, 121.372876): 1297, (31.162395, 121.353836): 1298, (31.46933, 121.246736): 1299, (31.145654, 121.375598): 1300, (31.169796, 121.52847): 1301, (31.106089, 121.039819): 1302, (31.256426, 121.61001): 1303, (30.9152, 121.664095): 1304, (30.960195, 121.398632): 1305, (30.910239, 121.419143): 1306, (30.990942, 121.12541): 1307, (31.42225, 121.355698): 1308, (31.406426, 121.476433): 1309, (31.432457, 121.359456): 1310, (31.177067, 121.370844): 1311, (30.897085, 121.617832): 1312, (31.240874, 121.518086): 1313, (30.965531, 121.208487): 1314, (31.278808, 121.419722): 1315, (30.874563, 121.067902): 1316, (31.202262, 121.564812): 1317, (31.058805, 121.593285): 1318, (31.267263, 121.086225): 1319, (31.319422, 121.469372): 1320, (31.228726, 121.482748): 1321, (31.195321, 121.445546): 1322, (31.300493, 121.323029): 1323, (31.113798, 121.382691): 1324, (31.307142, 121.362041): 1325, (30.930023, 121.011969): 1326, (30.99055, 121.126163): 1327, (31.285944, 121.44442): 1328, (30.894923, 121.319564): 1329, (31.023064, 121.534267): 1330, (31.236405, 121.432864): 1331, (31.240561, 121.459431): 1332, (31.195613, 121.40425): 1333, (31.227762, 121.495598): 1334, (31.099126, 121.813976): 1335, (31.217682, 121.499709): 1336, (31.212718, 121.447853): 1337, (31.155349, 121.375527): 1338, (31.217512, 121.533844): 1339, (30.830149, 121.35652): 1340, (31.292462, 121.271212): 1341, (31.27328, 121.47376): 1342, (30.853798, 121.344272): 1343, (31.3038, 121.450721): 1344, (30.846647, 121.431405): 1345, (30.718108, 121.283169): 1346, (31.219437, 121.374523): 1347, (31.382369, 121.258579): 1348, (31.212113, 121.444193): 1349, (30.827125, 121.329382): 1350, (31.022923, 121.806437): 1351, (30.956973, 121.097333): 1352, (31.236009, 121.478941): 1353, (30.94602, 121.106186): 1354, (31.415226, 121.349387): 1355, (31.284675, 121.516006): 1356, (31.166058, 121.403691): 1357, (31.030657, 121.236586): 1358, (31.387989, 121.396749): 1359, (31.259763, 121.089338): 1360, (31.209182, 121.535827): 1361, (39.084494, 117.236165): 1362, (31.081917, 121.062248): 1363, (31.307911, 121.518921): 1364, (30.810888, 121.234763): 1365, (35.24116, 113.276351): 1366, (31.33876, 121.437747): 1367, (31.232893, 121.380242): 1368, (30.977355, 121.74808): 1369, (31.242867, 121.490229): 1370, (30.893492, 121.315973): 1371, (31.221325, 121.448881): 1372, (31.202103, 121.445912): 1373, (31.259267, 121.369254): 1374, (31.217323, 121.753758): 1375, (31.183859, 121.415119): 1376, (31.184347, 121.441392): 1377, (31.159523, 121.435774): 1378, (31.25423, 121.461072): 1379, (31.157362, 121.221594): 1380, (31.168519, 121.804669): 1381, (31.065995, 121.314532): 1382, (31.073706, 121.680729): 1383, (31.251614, 121.464007): 1384, (30.91796, 121.543057): 1385, (31.012108, 121.074488): 1386, (30.889117, 121.317123): 1387, (30.801265, 121.395878): 1388, (31.344797, 121.500999): 1389, (31.230277, 121.448573): 1390, (31.219533, 121.427561): 1391, (31.180994, 121.377204): 1392, (31.492475, 121.328439): 1393, (31.08532, 121.237002): 1394, (31.134917, 121.334281): 1395, (31.191702, 121.449143): 1396, (31.209479, 121.702069): 1397, (30.80226, 121.311879): 1398, (31.278028, 121.583992): 1399, (31.304081, 121.326517): 1400, (31.318258, 121.167266): 1401, (31.231663, 121.428471): 1402, (31.213061, 121.483541): 1403, (26.215115, 109.744661): 1404, (31.283733, 121.149832): 1405, (31.260888, 121.340352): 1406, (31.406017, 121.240101): 1407, (31.256232, 121.498254): 1408, (31.151421, 121.279419): 1409, (31.264503, 121.715846): 1410, (31.412982, 121.496282): 1411, (31.162111, 121.566673): 1412, (30.92405, 121.648972): 1413, (31.209904, 121.234203): 1414, (31.266413, 121.351003): 1415, (31.2716, 121.465223): 1416, (31.060659, 121.399957): 1417, (31.328545, 121.538592): 1418, (31.010467, 121.237408): 1419, (31.299799, 121.530915): 1420, (31.24478, 121.505751): 1421, (31.143754, 121.434357): 1422, (31.157151, 121.431115): 1423, (31.294057, 121.171316): 1424, (31.087224, 121.466989): 1425, (31.346801, 121.577398): 1426, (31.283289, 121.510576): 1427, (31.108792, 121.061126): 1428, (31.007639, 121.416247): 1429, (31.162463, 121.364262): 1430, (31.200249, 121.443811): 1431, (31.224154, 121.43807): 1432, (31.051731, 121.423756): 1433, (31.412391, 121.491247): 1434, (31.30048, 121.469206): 1435, (31.181785, 121.529159): 1436, (31.21852, 121.44019): 1437, (31.047441, 121.470836): 1438, (31.224117, 121.4732): 1439, (31.222272, 121.438997): 1440, (31.405792, 121.489703): 1441, (31.242584, 121.449606): 1442, (31.171916, 121.718029): 1443, (30.946702, 121.624426): 1444, (31.251894, 121.464026): 1445, (24.284812, 102.999068): 1446, (31.253206, 121.459958): 1447, (31.178962, 121.388264): 1448, (31.240806, 121.481992): 1449, (31.225275, 121.47164): 1450, (31.237635, 121.476519): 1451, (31.353624, 121.155902): 1452, (31.168125, 121.503534): 1453, (31.259739, 121.488421): 1454, (31.30685, 121.519579): 1455, (30.790922, 121.348126): 1456, (30.889897, 121.320413): 1457, (31.221662, 121.539905): 1458, (31.242152, 121.609858): 1459, (31.190483, 121.434806): 1460, (31.020849, 121.759757): 1461, (31.152045, 121.420017): 1462, (31.254042, 121.464758): 1463, (31.483764, 121.274813): 1464, (31.044697, 121.22462): 1465, (31.276567, 121.483483): 1466, (30.846072, 121.32313): 1467, (31.310089, 121.640057): 1468, (31.273024, 121.42572): 1469, (28.738742, 120.640606): 1470, (31.288667, 121.527375): 1471, (31.299749, 121.170321): 1472, (30.953524, 121.483682): 1473, (31.240013, 121.487148): 1474, (34.798608, 116.090395): 1475, (31.200719, 121.519009): 1476, (31.126748, 120.934108): 1477, (31.210582, 121.529652): 1478, (31.251161, 121.489743): 1479, (31.139255, 121.409759): 1480, (31.227666, 121.431017): 1481, (31.224551, 121.547983): 1482, (30.908341, 121.870538): 1483, (31.200432, 121.447381): 1484, (30.880562, 121.486561): 1485, (31.113501, 121.393037): 1486, (30.948258, 121.324472): 1487, (31.198799, 121.383701): 1488, (31.230363, 121.5056): 1489, (31.240686, 121.480699): 1490, (31.415595, 121.188844): 1491, (31.060038, 121.195223): 1492, (30.838466, 121.228977): 1493, (30.874727, 121.250912): 1494, (31.010124, 121.538644): 1495, (31.211634, 121.520845): 1496, (31.164838, 121.410811): 1497, (31.229071, 121.452449): 1498, (31.214958, 121.527054): 1499, (31.018662, 121.420209): 1500, (31.207348, 121.70185): 1501, (31.269521, 121.153127): 1502, (31.258903, 121.441965): 1503, (31.304737, 121.522227): 1504, (31.210579, 121.384648): 1505, (31.359545, 121.18156): 1506, (31.074814, 121.713455): 1507, (31.218022, 121.504024): 1508, (31.238742, 121.530031): 1509, (31.318438, 121.497035): 1510, (31.231468, 121.537215): 1511, (31.042348, 121.28911): 1512, (31.244568, 121.441817): 1513, (31.279322, 121.466136): 1514, (31.223446, 121.432052): 1515, (31.228852, 121.541643): 1516, (31.303224, 121.308726): 1517, (31.299874, 121.298501): 1518, (31.207247, 121.572933): 1519, (31.265054, 121.456413): 1520, (31.201064, 121.438699): 1521, (31.213903, 121.410198): 1522, (31.199522, 121.44697): 1523, (31.294834, 121.431554): 1524, (31.241577, 121.485099): 1525, (31.251986, 121.456773): 1526, (31.174287, 121.435995): 1527, (31.330908, 121.45516): 1528, (31.228017, 121.444777): 1529, (31.263495, 121.496653): 1530, (31.260217, 121.525816): 1531, (31.400853, 121.483671): 1532, (31.046753, 121.062205): 1533, (31.19124, 121.507961): 1534, (31.25037, 121.488549): 1535, (31.218516, 121.463689): 1536, (31.234499, 121.463079): 1537, (31.30446, 121.426575): 1538, (31.185189, 121.445449): 1539, (31.229057, 121.452254): 1540, (31.161066, 121.354642): 1541, (31.339206, 121.453188): 1542, (31.293519, 121.22306): 1543, (31.233644, 121.525549): 1544, (31.37273, 121.546117): 1545, (31.16872, 121.768142): 1546, (31.232853, 121.486408): 1547, (31.302355, 121.515812): 1548, (31.302746, 121.376994): 1549, (31.231585, 121.517218): 1550, (30.773575, 121.14093): 1551, (31.215347, 121.134685): 1552, (31.236828, 121.514196): 1553, (31.224539, 121.550163): 1554, (31.305114, 121.41898): 1555, (31.16584, 121.530742): 1556, (31.180581, 121.527653): 1557, (30.895162, 121.094602): 1558, (31.206958, 121.591146): 1559, (31.231369, 121.403139): 1560, (31.242261, 121.507912): 1561, (31.123621, 121.266561): 1562, (31.27369, 121.537119): 1563, (31.210887, 121.315496): 1564, (31.024911, 121.255017): 1565, (31.251619, 121.486701): 1566, (30.962059, 121.244644): 1567, (31.277147, 121.514954): 1568, (31.238815, 121.559352): 1569, (31.32087, 121.393055): 1570, (30.811433, 121.309343): 1571, (31.430745, 121.187414): 1572, (30.974035, 121.554233): 1573, (31.0887, 121.513069): 1574, (31.039411, 121.819385): 1575, (31.188514, 121.49841): 1576, (31.214193, 121.376945): 1577, (31.243974, 121.555758): 1578, (30.891109, 121.903749): 1579, (31.276315, 121.564096): 1580, (31.240837, 121.409164): 1581, (31.039155, 121.241352): 1582, (30.796356, 121.199264): 1583, (31.214342, 121.376038): 1584, (31.365913, 121.177965): 1585, (31.25372, 121.437778): 1586, (31.224373, 121.364172): 1587, (31.275331, 121.286529): 1588, (31.201237, 121.669336): 1589, (30.923134, 121.481921): 1590, (31.246946, 121.513919): 1591, (31.243738, 121.486223): 1592, (31.289509, 121.447288): 1593, (31.297689, 121.447357): 1594, (30.919514, 121.45936): 1595, (31.224126, 121.547664): 1596, (31.280348, 121.592755): 1597, (31.391131, 121.251789): 1598, (31.180897, 121.463904): 1599, (31.188512, 121.437591): 1600, (30.994999, 121.264867): 1601, (31.397648, 121.28551): 1602, (31.293762, 121.442609): 1603, (31.055129, 121.770244): 1604, (31.200811, 121.444401): 1605, (31.246387, 121.458214): 1606, (31.012788, 121.228073): 1607, (30.736314, 121.354852): 1608, (31.246573, 121.509144): 1609, (31.25079, 121.48933): 1610, (31.21245, 121.637072): 1611, (31.201966, 121.545756): 1612, (31.356868, 121.555161): 1613, (31.166046, 121.642137): 1614, (31.221705, 121.642288): 1615, (31.024417, 121.471297): 1616, (31.23936, 121.488752): 1617, (30.741693, 121.367052): 1618, (31.236991, 121.441554): 1619, (31.291242, 121.489761): 1620, (31.122643, 121.734179): 1621, (31.041182, 121.739041): 1622, (31.032045, 121.783775): 1623, (31.235154, 121.533056): 1624, (30.919873, 121.464262): 1625, (31.146285, 121.518893): 1626, (31.247569, 121.41575): 1627, (31.137582, 121.083916): 1628, (31.224552, 121.482535): 1629, (31.313444, 121.309439): 1630, (31.209598, 121.437679): 1631, (31.235043, 121.523389): 1632, (31.188068, 121.699562): 1633, (31.227165, 121.480195): 1634, (31.252877, 121.148181): 1635, (31.223981, 121.478274): 1636, (31.238801, 121.386263): 1637, (30.734301, 121.25722): 1638, (31.261132, 121.093167): 1639, (31.266232, 121.402951): 1640, (31.175983, 121.381611): 1641, (31.272774, 121.436412): 1642, (31.18026, 121.406339): 1643, (31.162714, 121.772242): 1644, (31.00816, 121.274391): 1645, (31.284086, 121.212427): 1646, (31.057607, 121.030715): 1647, (31.067235, 121.762108): 1648, (30.889225, 121.177092): 1649, (31.311405, 121.553882): 1650, (31.252485, 121.549613): 1651, (31.225653, 121.480878): 1652, (30.993467, 121.234309): 1653, (31.243463, 121.4907): 1654, (30.89659, 121.281933): 1655, (31.240412, 121.45723): 1656, (30.912581, 121.465142): 1657, (30.800042, 121.417303): 1658, (31.237435, 121.455917): 1659, (31.253657, 121.443859): 1660, (30.860242, 121.79388): 1661, (46.247857, 128.762232): 1662, (31.289282, 121.378597): 1663, (31.154675, 121.81374): 1664, (31.178088, 121.413515): 1665, (31.273966, 121.429374): 1666, (31.341417, 121.271089): 1667, (31.284292, 121.273506): 1668, (31.219656, 121.49234): 1669, (31.323457, 121.57815): 1670, (31.299073, 121.552202): 1671, (31.258982, 121.494994): 1672, (31.4163, 121.496785): 1673, (31.244555, 121.5099): 1674, (31.344366, 121.51915): 1675, (31.409644, 121.303936): 1676, (31.410089, 121.244982): 1677, (31.234796, 121.533464): 1678, (31.292699, 121.478195): 1679, (31.209704, 121.368486): 1680, (31.278161, 121.486144): 1681, (31.207139, 121.449578): 1682, (31.058917, 121.770251): 1683, (31.429482, 121.184088): 1684, (31.492988, 121.292044): 1685, (30.850119, 121.499982): 1686, (31.084634, 120.989297): 1687, (31.132602, 121.095863): 1688, (30.814394, 121.18284): 1689, (31.262032, 121.117499): 1690, (31.367392, 121.276877): 1691, (31.256595, 121.493953): 1692, (31.214719, 121.624972): 1693, (31.010331, 121.381275): 1694, (31.291773, 121.203129): 1695, (31.045964, 121.420076): 1696, (31.258656, 121.472538): 1697, (30.97533, 121.271449): 1698, (30.9269, 121.467678): 1699, (30.963494, 121.160188): 1700, (31.308192, 121.667576): 1701, (31.223134, 121.43817): 1702, (31.101299, 120.918138): 1703, (31.295004, 121.516367): 1704, (31.288044, 121.356098): 1705, (30.88378, 121.573239): 1706, (31.385543, 121.469415): 1707, (31.210579, 121.453849): 1708, (31.326418, 121.501541): 1709, (31.245242, 121.479617): 1710, (31.253405, 121.091261): 1711, (31.469563, 121.384552): 1712, (31.193515, 121.531662): 1713, (31.241223, 121.45891): 1714, (31.045842, 121.756555): 1715, (30.7653, 121.292527): 1716, (31.278478, 121.420747): 1717, (30.903041, 121.922883): 1718, (31.301967, 121.515226): 1719, (31.194928, 121.366998): 1720, (31.272659, 121.601779): 1721, (30.90618, 121.178287): 1722, (30.976182, 121.750557): 1723, (31.174217, 121.289644): 1724, (31.326619, 121.493433): 1725, (31.19183, 121.59097): 1726, (31.174304, 121.436093): 1727, (31.134484, 121.42764): 1728, (31.244229, 121.475231): 1729, (31.303373, 121.4429): 1730, (31.317584, 121.626266): 1731, (31.407391, 121.490586): 1732, (31.301057, 121.388942): 1733, (31.223107, 121.454329): 1734, (31.283403, 121.541263): 1735, (30.898697, 121.172576): 1736, (31.467705, 121.201637): 1737, (31.198372, 121.355749): 1738, (31.054506, 121.460002): 1739, (31.154909, 121.436989): 1740, (31.307614, 121.526541): 1741, (31.182189, 121.394088): 1742, (30.953548, 121.636227): 1743, (31.486771, 121.360405): 1744, (31.280452, 121.480426): 1745, (31.148785, 121.115114): 1746, (31.15702, 121.106352): 1747, (31.276616, 121.593917): 1748, (31.233452, 121.480931): 1749, (31.24256, 121.490511): 1750, (30.824719, 121.471519): 1751, (31.240453, 121.511773): 1752, (31.214641, 121.525998): 1753, (30.89577, 121.697282): 1754, (30.904648, 121.173227): 1755, (31.052639, 121.761696): 1756, (31.285391, 121.497527): 1757, (31.4193, 121.285094): 1758, (31.166365, 121.419792): 1759, (31.420803, 121.280523): 1760, (31.217162, 121.411803): 1761, (31.159771, 121.58896): 1762, (31.035335, 121.117893): 1763, (31.294215, 121.527852): 1764, (30.882016, 121.532008): 1765, (30.933573, 121.55651): 1766, (31.380359, 121.304458): 1767, (31.410197, 121.500761): 1768, (30.932543, 121.716591): 1769, (31.312117, 121.474859): 1770, (31.48259, 121.348661): 1771, (31.249918, 121.47846): 1772, (31.248195, 121.490929): 1773, (31.060535, 121.274528): 1774, (31.307339, 121.534119): 1775, (30.918284, 120.995861): 1776, (31.181629, 121.510713): 1777, (31.229094, 121.480266): 1778, (31.119755, 121.577952): 1779, (31.304538, 121.526838): 1780, (30.956183, 121.908274): 1781, (31.21899, 121.492002): 1782, (31.20172, 121.743118): 1783, (31.006332, 121.579916): 1784, (25.222206, 117.086322): 1785, (31.18078, 121.423677): 1786, (31.265918, 121.428644): 1787, (31.314684, 121.522729): 1788, (31.211641, 121.474534): 1789, (31.038964, 121.2261): 1790, (31.257756, 121.601633): 1791, (31.19458, 121.436178): 1792, (31.441726, 121.246593): 1793, (31.233151, 121.530498): 1794, (31.298418, 121.539277): 1795, (31.332113, 121.451373): 1796, (31.233984, 121.524823): 1797, (31.378141, 121.503801): 1798, (31.436148, 121.224416): 1799, (31.139656, 121.378972): 1800, (31.235631, 121.567852): 1801, (31.012479, 121.240994): 1802, (31.018041, 121.055413): 1803, (31.165389, 121.119468): 1804, (31.327657, 121.544364): 1805, (31.317512, 121.404518): 1806, (30.990721, 121.803969): 1807, (30.758612, 121.320541): 1808, (31.179594, 121.55951): 1809, (31.264874, 121.459338): 1810, (31.292519, 121.549839): 1811, (31.436901, 121.294601): 1812, (31.169488, 121.349667): 1813, (31.247955, 121.495452): 1814, (31.184426, 121.15811): 1815, (31.193811, 121.373792): 1816, (31.306306, 121.28988): 1817, (31.217261, 121.391532): 1818, (31.039725, 121.225427): 1819, (30.935029, 121.186594): 1820, (30.77648, 121.358227): 1821, (31.258551, 121.584794): 1822, (31.257123, 121.503291): 1823, (31.192397, 121.384407): 1824, (30.901978, 121.452316): 1825, (31.010877, 121.240031): 1826, (31.216472, 121.413336): 1827, (31.174748, 121.39247): 1828, (31.297887, 121.62077): 1829, (31.234688, 121.502656): 1830, (31.275801, 121.484797): 1831, (31.395694, 121.391302): 1832, (30.837343, 121.275733): 1833, (30.957104, 121.146044): 1834, (31.369006, 121.25552): 1835, (31.243266, 121.443875): 1836, (31.156928, 121.121902): 1837, (30.893084, 121.252519): 1838, (31.124812, 121.508263): 1839, (31.340851, 121.63262): 1840, (31.450256, 121.254671): 1841, (31.209424, 121.530284): 1842, (31.267737, 121.529422): 1843, (30.802978, 121.397313): 1844, (31.219647, 121.446669): 1845, (31.248715, 121.481228): 1846, (31.32755, 121.15343): 1847, (31.267826, 121.521146): 1848, (31.261655, 121.474716): 1849, (31.038725, 121.464203): 1850, (31.234225, 121.525481): 1851, (31.217249, 121.504369): 1852, (31.348473, 121.384381): 1853, (31.402193, 121.367083): 1854, (31.274336, 121.39328): 1855, (30.844718, 121.256228): 1856, (31.266646, 121.609157): 1857, (31.186057, 121.44872): 1858, (31.085713, 120.966216): 1859, (31.091117, 121.549904): 1860, (31.19979, 121.389906): 1861, (30.903899, 121.906187): 1862, (31.154462, 121.123826): 1863, (30.838141, 121.189035): 1864, (31.265902, 121.537589): 1865, (30.993285, 121.36769): 1866, (31.21138, 121.728982): 1867, (31.212027, 121.491516): 1868, (31.156454, 121.815297): 1869, (30.898128, 121.536836): 1870, (30.876032, 121.466173): 1871, (31.262331, 121.563212): 1872, (31.038898, 121.23984): 1873, (30.897457, 121.36324): 1874, (31.129482, 121.192266): 1875, (31.320837, 121.542217): 1876, (31.039851, 121.603151): 1877, (31.139017, 121.545465): 1878, (30.869886, 121.901843): 1879, (31.397562, 121.15864): 1880, (31.107362, 121.020176): 1881, (31.21309, 121.531691): 1882, (31.358732, 121.508333): 1883, (31.052253, 121.347624): 1884, (30.781368, 121.418953): 1885, (31.281994, 121.631556): 1886, (31.341175, 121.618019): 1887, (31.41351, 121.367585): 1888, (31.310235, 121.437455): 1889, (31.09599, 121.238111): 1890, (30.931335, 121.472783): 1891, (30.949238, 121.021134): 1892, (31.070433, 121.467199): 1893, (30.992078, 121.31207): 1894, (31.054051, 121.741771): 1895, (31.186698, 121.44855): 1896, (31.267542, 121.685192): 1897, (31.023241, 121.226517): 1898, (31.186176, 121.53911): 1899, (31.280091, 121.519684): 1900, (31.42718, 121.33925): 1901, (31.200179, 121.551597): 1902, (30.935874, 121.692581): 1903, (30.873376, 121.040678): 1904, (31.008378, 121.431928): 1905, (31.370243, 121.576881): 1906, (30.975316, 121.349759): 1907, (31.189366, 121.443482): 1908, (31.217315, 121.635295): 1909, (31.307814, 121.504168): 1910, (30.903117, 121.146985): 1911, (31.285798, 121.546427): 1912, (30.889354, 121.488288): 1913, (31.180526, 121.349428): 1914, (31.198268, 121.383605): 1915, (31.211846, 121.475732): 1916, (31.180824, 121.525363): 1917, (31.20694, 121.459659): 1918, (31.200248, 121.474414): 1919, (31.172778, 121.53453): 1920, (31.266257, 121.506521): 1921, (31.238943, 121.498554): 1922, (31.259098, 121.373134): 1923, (31.198673, 121.479377): 1924, (31.182725, 121.41492): 1925, (31.217729, 121.472441): 1926, (31.288572, 121.534233): 1927, (31.231119, 121.484436): 1928, (31.340297, 121.26652): 1929, (31.236369, 121.476308): 1930, (30.991693, 121.639711): 1931, (31.254643, 121.569349): 1932, (31.18713, 121.451332): 1933, (31.501733, 121.334549): 1934, (30.781759, 121.168333): 1935, (31.196975, 121.440602): 1936, (31.251746, 121.488779): 1937, (31.054254, 121.087156): 1938, (31.200293, 121.400145): 1939, (30.86245, 121.124035): 1940, (30.855815, 121.311882): 1941, (31.302901, 121.553485): 1942, (31.279258, 121.474153): 1943, (31.2798, 121.459715): 1944, (31.291293, 121.559335): 1945, (31.242512, 121.481432): 1946, (31.172866, 121.434994): 1947, (31.292718, 121.410291): 1948, (31.248429, 121.463698): 1949, (31.252347, 121.459317): 1950, (31.268106, 121.49354): 1951, (31.204601, 121.484087): 1952, (31.29962, 121.536202): 1953, (31.327041, 121.661258): 1954, (31.256767, 121.438713): 1955, (31.1634, 121.396418): 1956, (31.150303, 121.430423): 1957, (30.897459, 121.151158): 1958, (31.25142, 121.449186): 1959, (31.444231, 121.225828): 1960, (31.267528, 121.490264): 1961, (31.111061, 121.395692): 1962, (31.223297, 121.430839): 1963, (30.978325, 121.70767): 1964, (31.234221, 121.462296): 1965, (31.112806, 121.462647): 1966, (31.420118, 121.349715): 1967, (30.828106, 121.187081): 1968, (31.19907, 121.517372): 1969, (31.290758, 121.514018): 1970, (31.182122, 121.377455): 1971, (31.206201, 121.085991): 1972, (31.062883, 121.744224): 1973, (31.188332, 121.405187): 1974, (30.918525, 121.74387): 1975, (31.220133, 121.471187): 1976, (31.196602, 121.461665): 1977, (31.248325, 121.158122): 1978, (31.256807, 121.518545): 1979, (31.299376, 121.670896): 1980, (31.211337, 121.413801): 1981, (31.223117, 121.504741): 1982, (31.111358, 121.020928): 1983, (31.241188, 121.483341): 1984, (31.092304, 120.912151): 1985, (30.956486, 121.40597): 1986, (31.203019, 121.443614): 1987, (31.24583, 121.392643): 1988, (31.26905, 121.627732): 1989, (31.243292, 121.518833): 1990, (31.223411, 121.428559): 1991, (31.233617, 121.523688): 1992, (31.356305, 121.293716): 1993, (31.251005, 121.418683): 1994, (31.292709, 121.497635): 1995, (30.738649, 121.382934): 1996, (31.371973, 121.278978): 1997, (31.325552, 121.473746): 1998, (31.286066, 121.474323): 1999, (31.229191, 121.453189): 2000, (30.807278, 121.304136): 2001, (31.219947, 121.467027): 2002, (31.211492, 121.571765): 2003, (31.260723, 121.490166): 2004, (31.195442, 121.754406): 2005, (31.175943, 121.424363): 2006, (30.841016, 121.482504): 2007, (30.940176, 121.086747): 2008, (30.963144, 121.39936): 2009, (31.247628, 121.510061): 2010, (31.1171, 121.391692): 2011, (30.921604, 121.471336): 2012, (31.243738, 121.475784): 2013, (31.392798, 121.246646): 2014, (30.852916, 121.364997): 2015, (31.296619, 121.497055): 2016, (31.404161, 121.47109): 2017, (31.237961, 121.477253): 2018, (31.30361, 121.492602): 2019, (31.242355, 121.490982): 2020, (31.233645, 121.53018): 2021, (31.116086, 121.488253): 2022, (31.38417, 121.344688): 2023, (30.841553, 121.161299): 2024, (31.220424, 121.383546): 2025, (31.268884, 121.44402): 2026, (31.045319, 121.846169): 2027, (31.259841, 121.618221): 2028, (31.150287, 121.471783): 2029, (31.289072, 121.481426): 2030, (31.25867, 121.428896): 2031, (31.260365, 121.454262): 2032, (31.290502, 121.434311): 2033, (31.234105, 121.34044): 2034, (31.332279, 121.286543): 2035, (31.220334, 121.367655): 2036, (31.339843, 121.232133): 2037, (30.85104, 121.524116): 2038, (31.262045, 121.446891): 2039, (31.236742, 121.385193): 2040, (30.902581, 121.167864): 2041, (31.206573, 121.701043): 2042, (31.278711, 121.514214): 2043, (31.252686, 121.491508): 2044, (31.11447, 121.460831): 2045, (31.142588, 121.756503): 2046, (31.243532, 121.330166): 2047, (31.024958, 120.918706): 2048, (30.442177, 120.618727): 2049, (30.815977, 121.436091): 2050, (41.835279, 123.498927): 2051, (31.286253, 121.531109): 2052, (31.195623, 121.398393): 2053, (30.721392, 121.345766): 2054, (31.155148, 121.597536): 2055, (30.872829, 121.424028): 2056, (31.314113, 121.110731): 2057, (31.183064, 121.407773): 2058, (31.242039, 121.687875): 2059, (31.247026, 121.448423): 2060, (31.31764, 121.496784): 2061, (31.246573, 121.495702): 2062, (30.793053, 121.176224): 2063, (31.259114, 121.376147): 2064, (31.272769, 121.57809): 2065, (31.217073, 121.524494): 2066, (31.298712, 121.617334): 2067, (31.225269, 121.491889): 2068, (31.204036, 121.437644): 2069, (31.260715, 121.622366): 2070, (31.267571, 121.5032): 2071, (31.250978, 121.44639): 2072, (31.241054, 121.449606): 2073, (31.206668, 121.449678): 2074, (31.260709, 121.087935): 2075, (31.268862, 121.434292): 2076, (31.348925, 121.178176): 2077, (31.384742, 121.277311): 2078, (30.993594, 121.348984): 2079, (30.94469, 121.299291): 2080, (31.23909, 121.474138): 2081, (31.128344, 121.629697): 2082, (31.497375, 121.324955): 2083, (31.242082, 121.522779): 2084, (31.50595, 121.282812): 2085, (31.279562, 121.467095): 2086, (30.779826, 121.376806): 2087, (31.222754, 121.459663): 2088, (31.415037, 121.260851): 2089, (31.291587, 121.511561): 2090, (31.242967, 121.51048): 2091, (31.312979, 121.545236): 2092, (31.204763, 121.526768): 2093, (31.239615, 121.489026): 2094, (30.929761, 121.587992): 2095, (31.332214, 121.21858): 2096, (31.239474, 121.495166): 2097, (31.225477, 121.448153): 2098, (31.268538, 121.49713): 2099, (31.246371, 121.513215): 2100, (31.216686, 121.493011): 2101, (31.193686, 121.423417): 2102, (31.030606, 121.225475): 2103, (31.262004, 121.350318): 2104, (31.244701, 121.514543): 2105, (31.285418, 121.47179): 2106, (31.250986, 121.460075): 2107, (31.234709, 121.436318): 2108, (31.201321, 121.404211): 2109, (31.236743, 121.36086): 2110, (31.038799, 121.216584): 2111, (31.216659, 121.391301): 2112, (31.226998, 121.556375): 2113, (31.193643, 121.402511): 2114, (31.270785, 121.465575): 2115, (31.110606, 121.517194): 2116, (31.248657, 121.537875): 2117, (31.188145, 121.432275): 2118, (31.02877, 121.456878): 2119, (31.249226, 121.448568): 2120, (31.234382, 121.531106): 2121, (31.22278, 121.450373): 2122, (31.292867, 121.42296): 2123, (31.150587, 121.499019): 2124, (31.362367, 121.461011): 2125, (31.227544, 121.537532): 2126, (31.241242, 121.517216): 2127, (31.20367, 121.543991): 2128, (31.168501, 121.402505): 2129, (31.203981, 121.483421): 2130, (31.29711, 121.519384): 2131, (31.188877, 121.43412): 2132, (31.258321, 121.496097): 2133, (31.194107, 121.399831): 2134, (31.190103, 121.485774): 2135, (31.216092, 121.508679): 2136, (31.016211, 121.412788): 2137, (31.184951, 121.409317): 2138, (31.185044, 121.414303): 2139, (31.19927, 121.6228): 2140, (30.750457, 121.376839): 2141, (31.255248, 121.416487): 2142, (31.198113, 121.590353): 2143, (31.200428, 121.437655): 2144, (31.08705, 121.503604): 2145, (31.10346, 121.419988): 2146, (31.243213, 121.504331): 2147, (31.238774, 121.496149): 2148, (31.310425, 121.439368): 2149, (31.264391, 121.519923): 2150, (31.245488, 121.513767): 2151, (31.302086, 121.510034): 2152, (31.244931, 121.50832): 2153, (31.238392, 121.428006): 2154, (31.233051, 121.455285): 2155, (31.225831, 121.36681): 2156, (31.271455, 121.513192): 2157, (31.462643, 121.331794): 2158, (31.112899, 121.367429): 2159, (31.121996, 121.393805): 2160, (31.273189, 121.472745): 2161, (31.284385, 121.511361): 2162, (31.364338, 121.251014): 2163, (31.178673, 121.521509): 2164, (31.218016, 121.568018): 2165, (31.24018, 121.506063): 2166, (31.195877, 121.597043): 2167, (31.239763, 121.540092): 2168, (31.25863, 121.490496): 2169, (31.24062, 121.428563): 2170, (31.245005, 121.517227): 2171, (31.106732, 121.290566): 2172, (31.250767, 121.458177): 2173, (31.111986, 121.38521): 2174, (31.216528, 121.439617): 2175, (31.229645, 121.481533): 2176, (31.213561, 121.634923): 2177, (30.990222, 121.082742): 2178, (30.918528, 121.113765): 2179, (31.247971, 121.513421): 2180, (31.18458, 121.420921): 2181, (31.236173, 121.466734): 2182, (31.23537, 121.522706): 2183, (31.2106, 121.411728): 2184, (31.21535, 121.46606): 2185, (31.241765, 121.496726): 2186, (31.236197, 121.485283): 2187, (31.389487, 121.259676): 2188, (31.237926, 121.46594): 2189, (31.143681, 121.353698): 2190, (31.197512, 121.470198): 2191, (31.188258, 121.301297): 2192, (31.10221, 121.277743): 2193, (31.087108, 121.371206): 2194, (31.195887, 121.591747): 2195, (31.301584, 121.490912): 2196, (31.098201, 121.323149): 2197, (31.214741, 121.427168): 2198, (31.211757, 121.46438): 2199, (31.266483, 121.545252): 2200, (31.207251, 121.407122): 2201, (31.185126, 121.451633): 2202, (31.463063, 121.345403): 2203, (31.4189, 121.295725): 2204, (31.27726, 121.460289): 2205, (31.169703, 121.412163): 2206, (31.225675, 121.364645): 2207, (31.25879, 121.627202): 2208, (31.143351, 121.352765): 2209, (31.256584, 121.4705): 2210, (31.343386, 121.208271): 2211, (31.250428, 121.460956): 2212, (31.230981, 121.542285): 2213, (31.20414, 121.406657): 2214, (31.212328, 121.407081): 2215, (31.283777, 121.379482): 2216, (31.245183, 121.484965): 2217, (31.475425, 121.356951): 2218, (31.222328, 121.461659): 2219, (31.236576, 121.465894): 2220, (31.234168, 121.473173): 2221, (31.170967, 121.40072): 2222, (30.770905, 121.407376): 2223, (31.198953, 121.516066): 2224, (31.09201, 121.395696): 2225, (31.103957, 121.456645): 2226, (31.236339, 121.52576): 2227, (31.314136, 121.549429): 2228, (31.230406, 121.482326): 2229, (31.22132, 121.438917): 2230, (31.447176, 121.385936): 2231, (31.290912, 121.498587): 2232, (31.277632, 121.380997): 2233, (31.278753, 121.505741): 2234, (31.299613, 121.442959): 2235, (30.975347, 121.032558): 2236, (31.232268, 121.541982): 2237, (31.365012, 121.504701): 2238, (31.391154, 121.246483): 2239, (31.270809, 121.543999): 2240, (31.307358, 121.524928): 2241, (31.235341, 121.477273): 2242, (31.326513, 121.490258): 2243, (31.189816, 121.430881): 2244, (31.140348, 121.210964): 2245, (31.192457, 121.443575): 2246, (31.210657, 121.474053): 2247, (31.209934, 121.604472): 2248, (31.265413, 121.564718): 2249, (31.238839, 121.438593): 2250, (31.280003, 121.361419): 2251, (31.284348, 121.564303): 2252, (31.229151, 121.484838): 2253, (31.235908, 121.57067): 2254, (31.248866, 121.478438): 2255, (31.270571, 121.476897): 2256, (31.227254, 121.402789): 2257, (31.231328, 121.487479): 2258, (31.217037, 121.43649): 2259, (31.229203, 121.449776): 2260, (31.256537, 121.625952): 2261, (31.207094, 121.713617): 2262, (31.2754, 121.485024): 2263, (31.30379, 121.455146): 2264, (31.240865, 121.426827): 2265, (31.379546, 121.260455): 2266, (31.333313, 121.333805): 2267, (31.240405, 121.569186): 2268, (31.206761, 121.399923): 2269, (31.23787, 121.543207): 2270, (31.236531, 121.491325): 2271, (31.281993, 121.464732): 2272, (31.156811, 121.236841): 2273, (31.317624, 121.405225): 2274, (31.193771, 121.546521): 2275, (30.901446, 121.904248): 2276, (31.214969, 121.586763): 2277, (31.234737, 121.513936): 2278, (30.930847, 121.490112): 2279, (31.07252, 121.404373): 2280, (31.305124, 121.454071): 2281, (31.265604, 121.567367): 2282, (31.036007, 121.46539): 2283, (31.224855, 121.480154): 2284, (30.967758, 121.08302): 2285, (31.240661, 121.512436): 2286, (31.243917, 121.496829): 2287, (31.223017, 121.465143): 2288, (30.979419, 121.022464): 2289, (31.262993, 121.08814): 2290, (31.18158, 121.295188): 2291, (31.225663, 121.452017): 2292, (30.940039, 121.548407): 2293, (31.258441, 121.461361): 2294, (31.217655, 121.534591): 2295, (30.891706, 121.171358): 2296, (30.763912, 121.333802): 2297, (31.177291, 121.405153): 2298, (31.152563, 121.460663): 2299, (31.199535, 121.456715): 2300, (31.195642, 121.446626): 2301, (31.399445, 121.464161): 2302, (31.161442, 121.507559): 2303, (31.270493, 121.353356): 2304, (31.211534, 121.510558): 2305, (31.299183, 121.628904): 2306, (31.136936, 121.325876): 2307, (31.267888, 121.46753): 2308, (31.197152, 121.444999): 2309, (31.296101, 121.505474): 2310, (35.522852, 102.0076): 2311, (31.169078, 121.433389): 2312, (31.233659, 121.565806): 2313, (30.97167, 121.185692): 2314, (31.177439, 121.108654): 2315, (31.235501, 121.402602): 2316, (31.299291, 121.454034): 2317, (31.169494, 121.302234): 2318, (31.301105, 121.430139): 2319, (31.15562, 121.130864): 2320, (31.22656, 121.490417): 2321, (31.219856, 121.568149): 2322, (31.283793, 121.518888): 2323, (31.236431, 121.460781): 2324, (31.113675, 121.383752): 2325, (31.236926, 121.454604): 2326, (30.981236, 121.488197): 2327, (31.054777, 121.166179): 2328, (31.237789, 121.461058): 2329, (31.240979, 121.677316): 2330, (31.220745, 121.438858): 2331, (31.23301, 121.556388): 2332, (31.17255, 121.405133): 2333, (31.263879, 121.49436): 2334, (30.987088, 121.588932): 2335, (31.1823, 121.394288): 2336, (31.235419, 121.486195): 2337, (31.217217, 121.558644): 2338, (30.747936, 121.348822): 2339, (31.234835, 121.527767): 2340, (31.237696, 121.382534): 2341, (31.269535, 121.4975): 2342, (31.25841, 121.392912): 2343, (31.228282, 121.556676): 2344, (31.264405, 121.489289): 2345, (31.22978, 121.540987): 2346, (31.238517, 121.429012): 2347, (31.292582, 121.508354): 2348, (31.206458, 121.562768): 2349, (31.23095, 121.537305): 2350, (31.138666, 121.398483): 2351, (31.246038, 121.531441): 2352, (31.187712, 121.432106): 2353, (31.232527, 121.530359): 2354, (31.182032, 121.388195): 2355, (31.193646, 121.449596): 2356, (31.202004, 121.591317): 2357, (31.208153, 121.41083): 2358, (31.025516, 121.30169): 2359, (31.239743, 121.512257): 2360, (31.235554, 121.479641): 2361, (31.419608, 121.431596): 2362, (31.266108, 121.341404): 2363, (31.19622, 121.468181): 2364, (31.233112, 121.442667): 2365, (31.17625, 121.418829): 2366, (31.21759, 121.40829): 2367, (31.180043, 121.609028): 2368, (31.182633, 121.439015): 2369, (31.248135, 121.539233): 2370, (31.150865, 121.336324): 2371, (31.234787, 121.498324): 2372, (31.218928, 121.402968): 2373, (31.233983, 121.455055): 2374, (31.177836, 121.408499): 2375, (31.348039, 121.147744): 2376, (31.386162, 121.255053): 2377, (31.215028, 121.438872): 2378, (31.256803, 121.522183): 2379, (31.241234, 121.460559): 2380, (31.258629, 121.491056): 2381, (30.806653, 121.257345): 2382, (31.228956, 121.481687): 2383, (31.258745, 121.370293): 2384, (31.23282, 121.468049): 2385, (31.228634, 121.536498): 2386, (31.209857, 121.644884): 2387, (31.264302, 121.428277): 2388, (31.210053, 121.464569): 2389, (31.296226, 121.494005): 2390, (31.328439, 121.384246): 2391, (31.179351, 121.31286): 2392, (31.187859, 121.444801): 2393, (31.235982, 121.466191): 2394, (31.37219, 121.448243): 2395, (31.213332, 121.619545): 2396, (31.23423, 121.436418): 2397, (31.237667, 121.434224): 2398, (31.293848, 121.528021): 2399, (31.039424, 121.225364): 2400, (31.244479, 121.429372): 2401, (31.237221, 121.467334): 2402, (31.200668, 121.480775): 2403, (31.230519, 121.520449): 2404, (31.281772, 121.484273): 2405, (31.228956, 121.535003): 2406, (31.203194, 121.46865): 2407, (31.216146, 121.472933): 2408, (31.192935, 121.39968): 2409, (31.270144, 121.354637): 2410, (31.199096, 121.401958): 2411, (31.234888, 121.447606): 2412, (31.228906, 121.451142): 2413, (30.972355, 121.610007): 2414, (31.214689, 121.426655): 2415, (31.21176, 121.45331): 2416, (47.35092, 130.301233): 2417, (31.249796, 121.582523): 2418, (31.359987, 121.417748): 2419, (31.06954, 120.928583): 2420, (31.260348, 121.542138): 2421, (30.881092, 121.162611): 2422, (31.246782, 121.528695): 2423, (30.941552, 121.1676): 2424, (31.154497, 121.501351): 2425, (31.043246, 120.924071): 2426, (31.239726, 121.510008): 2427, (31.156954, 121.146623): 2428, (31.252588, 121.463263): 2429, (31.315817, 121.359172): 2430, (30.899462, 121.164967): 2431, (31.211781, 121.405437): 2432, (31.012103, 121.414993): 2433, (31.186202, 121.54793): 2434, (31.193331, 121.599017): 2435, (31.249356, 121.410275): 2436, (31.161896, 121.135143): 2437, (31.251094, 121.416046): 2438, (31.249067, 121.543826): 2439, (31.256654, 121.599709): 2440, (31.170526, 121.34996): 2441, (31.209454, 121.524245): 2442, (31.232395, 121.564673): 2443, (31.149062, 121.392526): 2444, (31.21882, 121.436387): 2445, (31.236897, 121.363985): 2446, (31.1917, 121.440205): 2447, (31.294781, 121.494788): 2448, (31.231316, 121.54242): 2449, (30.83237, 121.439654): 2450, (31.200779, 121.533917): 2451, (31.236842, 121.433815): 2452, (31.275472, 121.436799): 2453, (31.23672, 121.488128): 2454, (31.260173, 121.348277): 2455, (31.193608, 121.448072): 2456, (31.221688, 121.462858): 2457, (31.242363, 121.480168): 2458, (31.238453, 121.377045): 2459, (31.209344, 121.414036): 2460, (31.306637, 121.523563): 2461, (31.17148, 121.411456): 2462, (31.290316, 121.457814): 2463, (31.186411, 121.402496): 2464, (31.265994, 121.519353): 2465, (31.276741, 121.514196): 2466, (31.278997, 121.510278): 2467, (31.308523, 121.423052): 2468, (31.240977, 121.494087): 2469, (31.188183, 121.440258): 2470, (31.233708, 121.454373): 2471, (31.243061, 121.517168): 2472, (31.248836, 121.463246): 2473, (31.177098, 121.525845): 2474, (31.201009, 121.610969): 2475, (31.063518, 121.215539): 2476, (31.212034, 121.476918): 2477, (31.191496, 121.41093): 2478, (31.227864, 121.638217): 2479, (31.295365, 121.16604): 2480, (31.242706, 121.49688): 2481, (31.123026, 120.926117): 2482, (31.218858, 121.465451): 2483, (31.303333, 121.455058): 2484, (30.879413, 121.704723): 2485, (31.198912, 121.444187): 2486, (31.052413, 121.218957): 2487, (31.192182, 121.466139): 2488, (31.154144, 121.118793): 2489, (31.236055, 121.434759): 2490, (31.256113, 121.510151): 2491, (31.26153, 121.582783): 2492, (31.111905, 121.05229): 2493, (31.197367, 121.479142): 2494, (31.233107, 121.417598): 2495, (31.129363, 121.425144): 2496, (31.267858, 121.495135): 2497, (31.410254, 121.496122): 2498, (31.203742, 121.461251): 2499, (31.338453, 121.449399): 2500, (31.216462, 121.677078): 2501, (31.061709, 121.771457): 2502, (31.173828, 121.781108): 2503, (31.228542, 121.538488): 2504, (31.210354, 121.595268): 2505, (31.275255, 121.45991): 2506, (31.258454, 121.440744): 2507, (31.272257, 121.513471): 2508, (31.233059, 121.458379): 2509, (31.241674, 121.485882): 2510, (31.226676, 121.555069): 2511, (31.211339, 121.406552): 2512, (31.201121, 121.445016): 2513, (31.295646, 121.498045): 2514, (31.156365, 121.41831): 2515, (31.234916, 121.447878): 2516, (30.866941, 121.595653): 2517, (31.261499, 121.457989): 2518, (31.238385, 121.521449): 2519, (31.246512, 121.514772): 2520, (31.213397, 121.413655): 2521, (31.241084, 121.467265): 2522, (31.289799, 121.455117): 2523, (31.146112, 121.504322): 2524, (31.200865, 121.448273): 2525, (31.230671, 121.552848): 2526, (31.249854, 121.497793): 2527, (31.350476, 121.519951): 2528, (31.205115, 121.492127): 2529, (31.211021, 121.569582): 2530, (30.910821, 121.465194): 2531, (31.404925, 121.473328): 2532, (30.864276, 121.112687): 2533, (31.276778, 121.542361): 2534, (31.115773, 120.907942): 2535, (31.25495, 121.739898): 2536, (31.242864, 121.426322): 2537, (31.375832, 121.389413): 2538, (31.244865, 121.549508): 2539, (31.299734, 121.531717): 2540, (30.719644, 121.345988): 2541, (31.246935, 121.49406): 2542, (31.194719, 121.448676): 2543, (31.246271, 121.425156): 2544, (31.17216, 121.347748): 2545, (31.258483, 121.368346): 2546, (31.334437, 121.640763): 2547, (31.234093, 121.475011): 2548, (31.257724, 121.595342): 2549, (30.939279, 121.074396): 2550, (31.262311, 121.449241): 2551, (31.316634, 121.657988): 2552, (31.200875, 121.481499): 2553, (31.234528, 121.528657): 2554, (31.19565, 121.439802): 2555, (31.259353, 121.427018): 2556, (30.940804, 121.073937): 2557, (31.145979, 121.376079): 2558, (31.2114, 121.476689): 2559, (31.233185, 121.481944): 2560, (31.233011, 121.523809): 2561, (31.239149, 121.488022): 2562, (31.239862, 121.490491): 2563, (31.239492, 121.486003): 2564, (31.21351, 121.3728): 2565, (31.235309, 121.529189): 2566, (31.25316, 121.459116): 2567, (31.184752, 121.387556): 2568, (31.213212, 121.470589): 2569, (31.223367, 121.46595): 2570, (31.267673, 121.346341): 2571, (31.264645, 121.446818): 2572, (31.103599, 121.416145): 2573, (29.151779, 120.985563): 2574, (31.230405, 121.534439): 2575, (31.19601, 121.444227): 2576, (31.297053, 121.52596): 2577, (31.311026, 121.456026): 2578, (31.203621, 121.393816): 2579, (31.242132, 121.488632): 2580, (31.139021, 121.323325): 2581, (31.014893, 121.047477): 2582, (31.39714, 121.507729): 2583, (31.246176, 121.494923): 2584, (31.23478, 121.51668): 2585, (31.32589, 121.477565): 2586, (31.351912, 121.58074): 2587, (31.197034, 121.391103): 2588, (31.241675, 121.441049): 2589, (31.237596, 121.488614): 2590, (31.24672, 121.38645): 2591, (31.246871, 121.409978): 2592, (31.199128, 121.540588): 2593, (31.215311, 121.523574): 2594, (31.24188, 121.487961): 2595, (31.224841, 121.440899): 2596, (31.226708, 121.481109): 2597, (31.203225, 121.399229): 2598, (31.263574, 121.586365): 2599, (31.247886, 121.391323): 2600, (31.279298, 121.485756): 2601, (30.735389, 121.358248): 2602, (31.258361, 121.594427): 2603, (31.192395, 121.748164): 2604, (31.223306, 121.378382): 2605, (31.239804, 121.545118): 2606, (31.189897, 121.354305): 2607, (31.301024, 121.509195): 2608, (31.224391, 121.489143): 2609, (31.212572, 121.537172): 2610, (31.160324, 121.508559): 2611, (31.215893, 121.558154): 2612, (31.238616, 121.477514): 2613, (31.226286, 121.600124): 2614, (30.679943, 104.067923): 2615, (31.194364, 121.400164): 2616, (31.253467, 121.447489): 2617, (31.242324, 121.221984): 2618, (31.255872, 121.493382): 2619, (31.012822, 121.051558): 2620, (31.276883, 121.507533): 2621, (31.212793, 121.494061): 2622, (31.189, 121.431937): 2623, (31.249972, 121.422237): 2624, (31.251211, 121.491022): 2625, (31.034225, 121.254609): 2626, (31.240285, 121.546323): 2627, (31.231815, 121.464062): 2628, (31.203422, 121.526246): 2629, (31.191178, 121.46648): 2630, (31.216269, 121.435171): 2631, (31.27495, 121.506381): 2632, (31.233679, 121.494765): 2633, (31.240588, 121.379145): 2634, (31.229886, 121.536598): 2635, (30.95485, 121.328077): 2636, (31.087196, 121.505435): 2637, (31.247703, 121.536476): 2638, (31.152571, 121.412981): 2639, (31.203911, 121.590509): 2640, (31.082557, 121.430065): 2641, (31.224487, 121.452917): 2642, (31.23976, 121.482648): 2643, (31.180728, 121.386832): 2644, (31.210101, 121.414922): 2645, (31.208403, 121.441631): 2646, (31.187492, 121.452395): 2647, (31.135275, 121.765517): 2648, (30.867838, 121.737782): 2649, (31.243189, 121.515482): 2650, (31.187384, 121.470686): 2651, (31.218005, 121.430759): 2652, (30.831962, 121.534676): 2653, (30.730789, 121.343497): 2654, (31.33881, 121.604229): 2655, (31.27688, 121.420858): 2656, (31.18674, 121.380533): 2657, (31.251313, 121.488307): 2658, (31.190941, 121.420437): 2659, (31.271086, 121.549327): 2660, (31.030352, 120.990008): 2661, (31.229148, 121.495962): 2662, (31.19914, 121.438003): 2663, (31.241369, 121.458067): 2664, (31.229998, 121.538807): 2665, (31.230429, 121.468099): 2666, (31.376534, 121.563363): 2667, (30.735272, 121.380359): 2668, (31.287524, 121.51338): 2669, (31.211093, 121.409091): 2670, (31.239094, 121.519821): 2671, (31.259135, 121.616618): 2672, (31.233431, 121.471698): 2673, (31.210632, 121.477007): 2674, (31.23923, 121.483027): 2675, (31.095422, 121.393617): 2676, (31.287554, 121.539): 2677, (31.242208, 121.509707): 2678, (31.245215, 121.601813): 2679, (31.331954, 121.451325): 2680, (31.231035, 121.555602): 2681, (30.929541, 121.616421): 2682, (31.260724, 121.469924): 2683, (31.226563, 121.483486): 2684, (31.184327, 121.381012): 2685, (31.085245, 121.533086): 2686, (31.038935, 121.465427): 2687, (31.232548, 121.478971): 2688, (30.735262, 121.351726): 2689, (31.278145, 121.452159): 2690, (31.264003, 121.489834): 2691, (31.232971, 121.47882): 2692, (31.191797, 121.442896): 2693, (31.274419, 121.475879): 2694, (31.183039, 121.434356): 2695, (31.151008, 121.324166): 2696, (31.245569, 121.5156): 2697, (31.177248, 121.51249): 2698, (31.123763, 120.901878): 2699, (31.242744, 121.425797): 2700, (31.187829, 121.452956): 2701, (31.21953, 121.455395): 2702, (31.167548, 121.425927): 2703, (31.237955, 121.493402): 2704, (31.027984, 121.267724): 2705, (31.260191, 121.617048): 2706, (31.23341, 121.483804): 2707, (31.169908, 121.418183): 2708, (31.203252, 121.441646): 2709, (31.2146, 121.522908): 2710, (31.236337, 121.490883): 2711, (31.40369, 121.255993): 2712, (31.234238, 121.521932): 2713, (31.106411, 121.450195): 2714, (28.812629, 115.952954): 2715, (31.26999, 121.472205): 2716, (31.23811, 121.493447): 2717, (30.995671, 121.095344): 2718, (31.003024, 121.254796): 2719, (31.2419, 121.506952): 2720, (31.168688, 121.487892): 2721, (31.216531, 121.541197): 2722, (31.19053, 121.445372): 2723, (31.198273, 121.445101): 2724, (31.224758, 121.446392): 2725, (31.279618, 121.439189): 2726, (31.250629, 121.598677): 2727, (31.22903, 121.3909): 2728, (31.201959, 121.61095): 2729, (31.231339, 121.543694): 2730, (30.976672, 121.459345): 2731, (31.201817, 121.452116): 2732, (31.110847, 121.509512): 2733, (31.410799, 121.48934): 2734, (31.250145, 121.450312): 2735, (31.22716, 121.386851): 2736, (31.269951, 121.336766): 2737, (31.22626, 121.551311): 2738, (31.315735, 121.534779): 2739, (30.829073, 121.470254): 2740, (31.231401, 121.497622): 2741, (31.234885, 121.529933): 2742, (31.223721, 121.482534): 2743, (31.244249, 121.675329): 2744, (31.309, 121.525098): 2745, (31.213659, 121.455113): 2746, (31.250814, 121.449225): 2747, (31.204322, 121.455438): 2748, (31.294157, 121.432501): 2749, (31.571904, 121.158978): 2750, (31.174223, 121.436719): 2751, (31.170804, 121.278846): 2752, (31.216682, 121.468605): 2753, (31.223648, 121.467523): 2754, (31.245938, 121.422548): 2755, (31.343354, 121.59331): 2756, (30.880213, 121.907656): 2757, (31.198142, 121.446688): 2758, (31.350554, 121.391849): 2759, (31.289528, 121.412608): 2760, (31.243869, 121.440506): 2761, (31.395845, 121.450008): 2762, (31.246433, 121.443184): 2763, (31.231749, 121.486203): 2764, (31.224006, 121.423572): 2765, (31.286523, 121.538876): 2766, (31.239836, 121.497502): 2767, (31.20907, 121.436851): 2768, (31.254745, 121.556081): 2769}\n",
            "{528: 41, 874: 32, 237: 25, 580: 61, 1102: 45, 180: 20, 764: 20, 1638: 29, 558: 137, 652: 96, 158: 73, 446: 63, 1325: 45, 59: 24, 1595: 18, 1009: 21, 886: 16, 285: 32, 1310: 2, 713: 6, 211: 37, 82: 10, 1585: 44, 757: 23, 428: 48, 1194: 52, 1407: 71, 1101: 56, 906: 13, 222: 73, 3: 12, 377: 50, 161: 17, 1549: 18, 260: 200, 2096: 7, 1602: 30, 1572: 12, 120: 16, 2267: 33, 100: 2, 471: 26, 169: 50, 1023: 3, 172: 24, 870: 8, 641: 10, 442: 21, 761: 47, 737: 12, 1241: 12, 2368: 4, 802: 84, 151: 44, 86: 111, 257: 75, 2387: 1, 725: 202, 114: 64, 778: 24, 798: 35, 762: 25, 591: 23, 1105: 5, 1294: 10, 2688: 5, 978: 41, 2242: 2, 305: 9, 1933: 6, 765: 25, 1139: 64, 531: 43, 1658: 19, 654: 38, 389: 65, 385: 183, 865: 133, 139: 115, 70: 87, 2573: 2, 224: 39, 555: 29, 284: 28, 596: 23, 1217: 28, 545: 30, 273: 84, 770: 14, 1134: 53, 320: 156, 668: 30, 807: 11, 242: 60, 264: 166, 225: 40, 917: 45, 749: 48, 698: 68, 520: 82, 1160: 46, 157: 100, 822: 42, 46: 52, 1620: 8, 444: 58, 704: 112, 1422: 58, 214: 102, 673: 98, 77: 48, 1404: 13, 109: 128, 947: 99, 275: 25, 842: 100, 166: 72, 486: 102, 1707: 41, 458: 45, 1030: 33, 173: 129, 1395: 16, 1263: 12, 1273: 26, 407: 42, 800: 36, 1430: 45, 115: 50, 1649: 17, 523: 46, 229: 23, 1396: 2, 848: 5, 1180: 6, 1599: 4, 1126: 45, 712: 5, 977: 23, 614: 11, 948: 10, 453: 36, 747: 8, 1390: 13, 1858: 1, 1581: 6, 1046: 21, 801: 16, 1195: 14, 1737: 8, 780: 27, 2438: 2, 219: 102, 208: 110, 272: 87, 1129: 21, 81: 93, 1528: 21, 907: 55, 586: 51, 1770: 7, 372: 50, 414: 168, 1634: 15, 1158: 24, 1664: 7, 178: 104, 1309: 71, 711: 49, 1411: 109, 376: 51, 1532: 14, 20: 53, 1706: 6, 2485: 2, 491: 104, 2541: 7, 313: 117, 1539: 1, 958: 76, 581: 143, 998: 39, 968: 1, 1270: 40, 99: 21, 306: 72, 74: 48, 221: 54, 231: 82, 1280: 21, 234: 113, 429: 12, 851: 103, 363: 110, 631: 79, 954: 68, 233: 2, 1414: 55, 1022: 41, 1159: 17, 957: 24, 2078: 19, 1011: 25, 2059: 36, 1112: 46, 1397: 69, 1261: 74, 388: 80, 317: 35, 519: 51, 706: 40, 745: 48, 339: 104, 1499: 31, 1756: 22, 1589: 29, 1042: 26, 336: 70, 418: 29, 2501: 1, 2003: 1, 1229: 7, 218: 62, 2036: 8, 1553: 4, 2648: 1, 618: 11, 663: 70, 450: 99, 979: 45, 1088: 35, 441: 64, 123: 100, 477: 43, 595: 42, 903: 45, 382: 31, 980: 16, 1712: 4, 483: 32, 942: 12, 1967: 3, 1065: 14, 1308: 12, 1685: 6, 1144: 31, 379: 170, 24: 57, 71: 19, 1490: 5, 1249: 16, 1286: 26, 611: 28, 721: 13, 1625: 18, 1605: 28, 479: 12, 1031: 13, 537: 6, 2231: 8, 2203: 8, 290: 171, 36: 109, 195: 252, 198: 156, 463: 99, 1415: 1, 858: 161, 769: 108, 15: 53, 468: 24, 680: 4, 14: 75, 390: 131, 1629: 10, 207: 56, 73: 125, 976: 32, 750: 28, 1475: 9, 1778: 4, 1636: 14, 65: 22, 773: 36, 928: 87, 110: 71, 1055: 25, 399: 28, 304: 107, 1037: 27, 844: 28, 1486: 51, 1378: 20, 703: 15, 699: 6, 1981: 14, 246: 40, 690: 48, 2157: 2, 41: 20, 627: 33, 1437: 49, 370: 19, 439: 11, 52: 76, 1048: 5, 1423: 3, 1962: 3, 2011: 4, 793: 89, 638: 46, 1480: 39, 205: 96, 241: 33, 616: 19, 133: 72, 496: 37, 325: 28, 535: 70, 344: 109, 885: 83, 650: 52, 926: 6, 564: 34, 1277: 12, 1342: 49, 1231: 2, 394: 25, 1587: 11, 1593: 18, 1825: 27, 1590: 5, 1699: 9, 1871: 80, 1306: 52, 193: 137, 1184: 21, 1125: 46, 532: 72, 1336: 35, 922: 7, 37: 47, 1049: 55, 331: 28, 983: 11, 715: 17, 1246: 50, 27: 30, 1709: 29, 810: 19, 1000: 11, 1675: 6, 927: 42, 243: 66, 226: 178, 1145: 61, 1156: 15, 1353: 27, 159: 8, 1515: 2, 156: 51, 365: 47, 132: 42, 609: 113, 719: 65, 22: 73, 467: 182, 606: 30, 925: 70, 1135: 19, 121: 15, 1506: 60, 679: 32, 415: 105, 2283: 13, 1462: 3, 1154: 104, 127: 30, 1331: 23, 825: 42, 878: 44, 1269: 6, 2090: 1, 1548: 16, 727: 14, 311: 16, 1619: 15, 1068: 21, 2032: 2, 949: 17, 919: 20, 1446: 38, 1230: 16, 348: 114, 387: 53, 405: 44, 21: 92, 91: 56, 1369: 21, 1252: 15, 171: 57, 1973: 8, 794: 32, 867: 8, 961: 8, 1032: 19, 1013: 9, 56: 123, 80: 140, 1885: 10, 1844: 4, 1103: 42, 741: 78, 2223: 8, 723: 27, 2297: 4, 98: 65, 125: 70, 557: 53, 350: 17, 1379: 11, 1442: 2, 658: 56, 358: 23, 249: 17, 1250: 26, 1402: 6, 476: 14, 7: 44, 1413: 44, 877: 54, 28: 46, 202: 69, 1356: 27, 1900: 2, 364: 6, 461: 23, 1185: 23, 1357: 14, 590: 24, 1421: 3, 1058: 38, 1603: 18, 396: 115, 527: 70, 337: 103, 572: 24, 281: 24, 50: 33, 707: 44, 521: 29, 1216: 11, 701: 31, 1445: 38, 1463: 39, 245: 67, 44: 35, 505: 10, 2429: 5, 1107: 22, 551: 45, 639: 109, 2352: 4, 440: 20, 230: 19, 60: 59, 1202: 7, 714: 36, 1949: 6, 1006: 23, 1040: 17, 2002: 17, 1924: 2, 1311: 19, 648: 8, 1017: 46, 1033: 43, 577: 5, 1224: 28, 13: 32, 775: 7, 51: 36, 876: 23, 946: 51, 134: 12, 549: 20, 971: 22, 254: 25, 182: 5, 457: 57, 530: 36, 913: 45, 1496: 15, 1008: 40, 1441: 28, 753: 30, 2252: 2, 837: 25, 261: 107, 395: 100, 1305: 1, 300: 11, 1728: 14, 605: 210, 1455: 7, 1364: 7, 1239: 38, 836: 102, 1059: 47, 1734: 21, 1432: 5, 548: 10, 1120: 7, 1561: 16, 176: 47, 936: 35, 882: 55, 758: 17, 1255: 15, 625: 29, 341: 30, 48: 20, 1942: 3, 777: 1, 592: 6, 49: 50, 371: 13, 170: 91, 1686: 22, 1206: 25, 475: 17, 2007: 13, 873: 5, 1743: 86, 672: 26, 1723: 3, 748: 16, 2273: 1, 2245: 3, 425: 111, 989: 32, 403: 38, 189: 93, 1298: 7, 1021: 81, 550: 48, 883: 58, 920: 89, 4: 41, 2637: 5, 993: 96, 1312: 56, 83: 78, 588: 74, 734: 89, 755: 59, 1429: 17, 2433: 1, 312: 65, 1433: 41, 850: 98, 1791: 16, 456: 12, 669: 66, 421: 108, 565: 70, 9: 69, 771: 43, 634: 35, 398: 74, 1019: 48, 2377: 1, 401: 120, 164: 36, 92: 89, 17: 126, 1630: 18, 1400: 57, 632: 33, 589: 33, 923: 42, 474: 41, 432: 50, 186: 26, 497: 61, 816: 6, 1200: 12, 934: 73, 113: 15, 2175: 1, 367: 37, 433: 27, 702: 27, 149: 116, 1383: 28, 1057: 53, 1996: 7, 938: 61, 1115: 15, 1784: 8, 152: 19, 283: 115, 651: 88, 32: 50, 1283: 76, 466: 27, 1516: 21, 298: 22, 626: 31, 1592: 5, 1617: 17, 2769: 1, 252: 89, 498: 3, 6: 95, 64: 71, 820: 57, 760: 40, 445: 24, 1985: 1, 2253: 1, 1170: 85, 640: 78, 813: 17, 427: 59, 613: 35, 62: 17, 1436: 6, 742: 25, 1368: 48, 289: 14, 490: 16, 1152: 1, 1018: 11, 1359: 12, 1997: 18, 1993: 7, 1242: 13, 1531: 18, 1086: 36, 122: 53, 633: 10, 544: 45, 1295: 15, 413: 67, 1245: 54, 1450: 9, 61: 68, 392: 53, 1293: 14, 324: 9, 1077: 8, 1484: 40, 451: 45, 1633: 5, 1438: 33, 829: 118, 1132: 51, 965: 46, 347: 46, 1739: 20, 889: 43, 720: 22, 16: 25, 1313: 40, 1990: 2, 763: 23, 996: 15, 1: 60, 1211: 60, 963: 117, 1769: 15, 1114: 24, 1964: 18, 25: 69, 138: 65, 19: 68, 1679: 10, 554: 4, 454: 161, 781: 81, 1883: 4, 1304: 40, 2295: 6, 1537: 12, 1234: 25, 1458: 62, 1361: 24, 891: 29, 1110: 48, 1643: 27, 1497: 10, 1274: 34, 1508: 9, 2006: 15, 2101: 5, 579: 64, 1665: 24, 493: 23, 361: 49, 570: 39, 827: 13, 1647: 7, 1024: 4, 513: 8, 2620: 1, 1405: 23, 404: 6, 845: 22, 1837: 2, 872: 105, 326: 131, 843: 57, 662: 29, 478: 24, 1330: 7, 892: 18, 568: 99, 2390: 22, 562: 105, 623: 89, 834: 35, 806: 10, 184: 70, 1119: 34, 766: 71, 192: 57, 786: 55, 974: 11, 1362: 4, 967: 35, 509: 16, 267: 7, 841: 12, 563: 9, 659: 63, 635: 27, 85: 44, 1758: 10, 1812: 10, 1793: 16, 1760: 4, 1999: 13, 1667: 21, 346: 11, 69: 6, 649: 27, 1078: 44, 183: 77, 187: 84, 964: 27, 124: 63, 277: 19, 716: 167, 322: 19, 682: 80, 1966: 29, 309: 30, 181: 12, 103: 16, 1166: 9, 119: 25, 352: 53, 1130: 8, 1399: 7, 2282: 6, 276: 56, 410: 14, 2471: 3, 1266: 35, 751: 36, 179: 17, 1467: 5, 1941: 11, 746: 46, 1340: 29, 1371: 8, 1655: 15, 1133: 52, 1350: 24, 78: 127, 209: 57, 908: 72, 710: 35, 2074: 4, 375: 82, 1285: 44, 1323: 81, 57: 89, 861: 8, 1626: 2, 826: 50, 251: 97, 1869: 10, 1137: 54, 732: 109, 1096: 50, 656: 22, 315: 48, 973: 14, 1479: 1, 2714: 5, 302: 18, 2546: 3, 1071: 11, 1193: 1, 987: 22, 962: 10, 1613: 14, 516: 64, 1495: 28, 256: 56, 345: 26, 696: 59, 1035: 37, 524: 47, 859: 51, 1616: 53, 282: 132, 881: 22, 901: 20, 1501: 18, 288: 2, 1718: 2, 369: 22, 526: 65, 68: 51, 1939: 6, 612: 22, 1705: 3, 637: 75, 351: 16, 1157: 15, 1676: 29, 893: 93, 95: 134, 1111: 10, 2305: 3, 1786: 7, 2407: 2, 53: 39, 244: 13, 593: 39, 1562: 8, 2393: 4, 1171: 8, 1977: 6, 1236: 38, 1608: 14, 1697: 27, 1736: 16, 1151: 33, 1341: 21, 1817: 15, 1518: 26, 318: 93, 1646: 29, 247: 75, 2211: 28, 1210: 40, 2077: 6, 1391: 5, 201: 70, 1183: 36, 567: 66, 1003: 6, 1412: 5, 2029: 4, 803: 30, 402: 16, 1615: 6, 815: 24, 1426: 36, 1127: 48, 150: 29, 1010: 36, 167: 6, 1670: 17, 855: 3, 933: 46, 1650: 19, 2756: 1, 969: 23, 818: 24, 1457: 37, 1329: 26, 875: 61, 297: 54, 831: 22, 238: 83, 217: 22, 435: 27, 89: 8, 666: 31, 1834: 19, 1284: 36, 1406: 10, 2363: 1, 88: 18, 1327: 11, 473: 32, 541: 13, 47: 106, 500: 35, 356: 45, 538: 19, 2307: 2, 517: 47, 607: 93, 717: 92, 952: 7, 1889: 43, 1056: 5, 1201: 22, 869: 14, 729: 73, 38: 38, 759: 55, 31: 16, 797: 18, 506: 22, 1896: 1, 424: 23, 1465: 2, 409: 11, 1143: 43, 835: 44, 87: 30, 162: 52, 1627: 5, 1640: 9, 852: 38, 147: 8, 154: 24, 574: 90, 2056: 35, 522: 78, 72: 26, 1766: 3, 1282: 8, 299: 96, 1861: 3, 569: 42, 601: 28, 1060: 21, 1025: 2, 731: 11, 1461: 4, 1172: 16, 1223: 18, 492: 42, 34: 34, 540: 24, 362: 64, 1409: 24, 542: 37, 1751: 26, 1367: 95, 1730: 15, 481: 147, 1542: 63, 507: 8, 1233: 23, 2187: 1, 944: 9, 400: 19, 692: 57, 700: 14, 785: 11, 740: 17, 1014: 11, 2559: 2, 144: 38, 534: 20, 1485: 38, 924: 93, 1192: 31, 945: 17, 744: 47, 1226: 14, 743: 8, 1979: 6, 1227: 11, 1146: 5, 1082: 23, 887: 10, 1818: 2, 332: 37, 148: 8, 2654: 2, 1522: 7, 657: 6, 1403: 20, 1789: 11, 814: 14, 1012: 12, 617: 62, 430: 48, 1394: 13, 236: 10, 58: 10, 1316: 5, 1892: 9, 1722: 22, 12: 25, 1443: 5, 26: 16, 508: 41, 1682: 12, 684: 13, 1618: 49, 128: 42, 374: 21, 160: 82, 1372: 10, 2122: 5, 863: 4, 1880: 25, 1684: 6, 112: 4, 896: 8, 1773: 1, 1147: 37, 1113: 15, 902: 34, 174: 19, 1538: 16, 1251: 17, 101: 41, 1746: 10, 1004: 86, 1178: 20, 314: 87, 310: 75, 423: 57, 436: 25, 1321: 19, 18: 32, 438: 18, 271: 29, 792: 13, 196: 16, 163: 24, 416: 9, 2091: 1, 206: 57, 556: 17, 594: 24, 200: 29, 54: 30, 1188: 54, 1898: 2, 1419: 4, 966: 14, 274: 63, 1866: 26, 194: 111, 1694: 67, 2079: 14, 1588: 10, 1131: 10, 1290: 10, 738: 24, 2076: 1, 1642: 6, 910: 41, 2094: 1, 735: 5, 1622: 2, 1489: 2, 1810: 1, 1424: 21, 459: 16, 1919: 19, 1248: 14, 1520: 26, 145: 42, 2133: 1, 718: 50, 1767: 30, 301: 65, 1691: 7, 890: 24, 1867: 7, 1887: 7, 1272: 7, 512: 54, 1820: 11, 752: 12, 1358: 7, 1601: 15, 1354: 1, 235: 21, 1352: 2, 384: 6, 677: 36, 393: 62, 1141: 33, 959: 30, 1854: 33, 1355: 14, 1888: 21, 582: 6, 265: 25, 2218: 20, 1771: 19, 999: 8, 472: 19, 1093: 23, 1983: 6, 645: 12, 1047: 13, 294: 23, 266: 10, 278: 101, 212: 31, 912: 31, 45: 88, 11: 39, 1554: 4, 805: 6, 1905: 1, 1472: 20, 1089: 10, 373: 31, 1320: 6, 539: 19, 2205: 3, 578: 70, 1493: 32, 2001: 10, 1571: 10, 213: 40, 1808: 5, 817: 11, 821: 27, 342: 33, 1657: 20, 494: 54, 2015: 29, 2339: 2, 323: 32, 598: 14, 693: 7, 1929: 5, 23: 8, 155: 14, 2300: 6, 840: 13, 916: 6, 2288: 2, 518: 42, 2: 31, 688: 12, 1628: 18, 1747: 6, 1360: 1, 117: 5, 1199: 1, 559: 14, 55: 17, 286: 14, 683: 35, 191: 94, 2427: 1, 1678: 17, 2147: 4, 2550: 3, 1635: 10, 1986: 18, 2335: 13, 1253: 1, 2009: 16, 1209: 27, 511: 6, 1639: 3, 1319: 7, 279: 20, 2641: 5, 1366: 21, 986: 10, 2098: 3, 1109: 18, 1104: 27, 259: 61, 135: 23, 587: 21, 1510: 23, 1725: 16, 1466: 5, 1198: 17, 788: 11, 1140: 43, 63: 47, 1663: 35, 333: 50, 560: 44, 1668: 7, 1492: 32, 116: 41, 165: 12, 1763: 5, 130: 6, 860: 33, 1469: 1, 1315: 4, 784: 15, 1524: 18, 1075: 20, 250: 43, 1666: 6, 1213: 14, 1586: 3, 2408: 7, 1094: 10, 988: 5, 1931: 7, 1783: 10, 240: 54, 253: 19, 981: 111, 1687: 2, 1262: 37, 510: 63, 1123: 15, 1954: 5, 465: 78, 223: 26, 2185: 3, 1673: 33, 354: 9, 1221: 16, 1027: 8, 726: 44, 137: 7, 129: 31, 628: 20, 514: 43, 296: 12, 1215: 12, 529: 12, 111: 21, 1726: 9, 1281: 13, 502: 41, 681: 4, 420: 49, 929: 9, 1680: 13, 2188: 10, 2204: 3, 1901: 2, 1754: 18, 670: 10, 695: 4, 644: 31, 604: 60, 96: 137, 1653: 3, 1238: 55, 287: 51, 1444: 9, 597: 11, 2095: 1, 1121: 6, 1385: 9, 447: 58, 10: 85, 1081: 13, 1811: 22, 426: 2, 408: 20, 1671: 16, 1451: 7, 1525: 2, 561: 44, 462: 53, 1729: 11, 1416: 24, 1926: 6, 1007: 14, 2596: 1, 1205: 34, 460: 12, 619: 9, 689: 5, 1708: 1, 914: 41, 1264: 6, 1661: 55, 1002: 7, 1299: 8, 1237: 11, 503: 12, 321: 9, 1347: 20, 767: 8, 1714: 1, 603: 13, 1918: 7, 489: 16, 1498: 10, 2073: 5, 1142: 3, 1470: 13, 1755: 3, 1494: 4, 756: 7, 239: 55, 2040: 3, 1118: 2, 722: 24, 1856: 11, 2382: 4, 536: 22, 1344: 5, 2039: 7, 1700: 1, 1659: 6, 904: 13, 232: 8, 1535: 6, 1564: 4, 849: 39, 854: 7, 790: 12, 355: 42, 29: 22, 1275: 31, 146: 15, 1375: 17, 177: 5, 1303: 43, 708: 11, 1701: 13, 655: 36, 694: 75, 291: 55, 1410: 7, 1517: 14, 733: 27, 215: 58, 921: 7, 1546: 9, 386: 4, 799: 8, 1644: 5, 2327: 4, 5: 8, 838: 18, 1039: 20, 955: 18, 1529: 12, 1677: 43, 126: 29, 897: 4, 856: 11, 796: 14, 76: 45, 30: 83, 2325: 3, 1052: 5, 175: 143, 1029: 26, 104: 17, 782: 24, 204: 2, 1398: 3, 1365: 5, 1583: 6, 67: 48, 220: 20, 2081: 1, 2536: 13, 862: 39, 258: 67, 984: 22, 1069: 47, 1148: 15, 552: 33, 1785: 8, 1872: 2, 768: 7, 642: 2, 1116: 35, 1090: 30, 2130: 4, 1913: 32, 1333: 32, 501: 69, 2219: 14, 107: 47, 1045: 28, 1247: 33, 1302: 37, 1349: 37, 2552: 4, 1291: 29, 1257: 3, 960: 73, 819: 1, 783: 5, 2197: 1, 1925: 1, 1256: 12, 1972: 6, 335: 17, 674: 97, 546: 13, 1850: 30, 1324: 7, 504: 35, 866: 2, 1452: 4, 935: 13, 411: 31, 1765: 39, 1870: 7, 1346: 5, 280: 24, 533: 61, 2410: 2, 1874: 30, 956: 18, 1085: 15, 1099: 21, 340: 5, 795: 1, 2500: 2, 1806: 1, 1175: 16, 2012: 2, 1838: 12, 2378: 1, 900: 49, 661: 17, 270: 4, 2068: 1, 664: 11, 2089: 3, 349: 31, 1187: 9, 1155: 10, 584: 32, 199: 20, 2736: 1, 353: 112, 188: 18, 2141: 5, 2054: 4, 2251: 13, 585: 32, 2395: 1, 1832: 10, 2484: 2, 772: 6, 2049: 8, 2166: 1, 1505: 1, 328: 31, 1186: 8, 499: 71, 437: 42, 1067: 17, 982: 4, 1847: 5, 868: 106, 1196: 60, 2048: 4, 1136: 23, 1957: 3, 2274: 1, 269: 16, 1912: 8, 1015: 54, 1735: 35, 884: 7, 1503: 5, 378: 7, 1702: 1, 1481: 3, 1189: 17, 1292: 11, 1153: 15, 905: 8, 1565: 6, 136: 7, 482: 17, 1596: 7, 1511: 6, 140: 34, 1464: 3, 620: 14, 643: 15, 615: 19, 515: 23, 452: 13, 1471: 23, 1258: 9, 880: 43, 1001: 70, 1338: 11, 1300: 27, 1884: 22, 1521: 13, 1645: 43, 1212: 28, 168: 41, 1975: 5, 610: 27, 602: 24, 1072: 14, 1478: 3, 1016: 6, 2183: 12, 1597: 1, 1339: 9, 2037: 2, 576: 8, 39: 21, 1041: 24, 94: 7, 1363: 18, 909: 11, 1388: 18, 1393: 4, 2461: 1, 1328: 8, 2734: 1, 636: 56, 2376: 4, 75: 6, 1243: 12, 2527: 1, 1418: 2, 2092: 3, 2691: 2, 90: 40, 210: 53, 1563: 66, 728: 31, 106: 3, 1865: 2, 368: 29, 839: 16, 624: 61, 975: 1, 262: 2, 1149: 10, 2146: 3, 1733: 31, 1594: 44, 2496: 3, 888: 4, 832: 20, 939: 4, 488: 22, 553: 33, 697: 30, 1779: 3, 2022: 2, 1218: 16, 327: 21, 1757: 2, 779: 10, 316: 8, 1606: 10, 1334: 3, 1169: 8, 1550: 7, 1287: 5, 2189: 1, 2220: 5, 1163: 12, 754: 23, 1821: 16, 105: 5, 1431: 20, 102: 3, 575: 17, 455: 20, 1100: 7, 1530: 3, 2660: 1, 812: 4, 1600: 2, 804: 7, 573: 18, 1559: 6, 1401: 8, 830: 15, 228: 28, 1288: 3, 1689: 15, 1968: 9, 1182: 7, 1073: 3, 203: 5, 1681: 2, 1875: 22, 1662: 8, 1054: 6, 1092: 27, 1191: 4, 898: 11, 864: 30, 1833: 3, 1064: 3, 1799: 22, 248: 36, 1674: 3, 1740: 4, 1380: 27, 449: 10, 2388: 3, 1150: 9, 268: 6, 2020: 2, 1168: 10, 990: 5, 2517: 2, 2293: 3, 329: 45, 422: 23, 383: 4, 1318: 3, 685: 12, 2030: 1, 2236: 7, 2027: 11, 833: 8, 1575: 14, 547: 14, 1482: 2, 2127: 27, 334: 12, 1044: 60, 366: 20, 871: 24, 2050: 4, 647: 13, 1748: 2, 1220: 3, 1932: 3, 2119: 17, 2137: 24, 216: 4, 709: 2, 84: 17, 1063: 4, 2114: 9, 847: 21, 307: 77, 1742: 43, 808: 14, 1908: 4, 1534: 7, 2730: 1, 1976: 8, 2753: 1, 292: 2, 2213: 1, 2449: 1, 2254: 3, 2401: 1, 1652: 1, 2014: 12, 487: 8, 2173: 10, 1098: 20, 970: 16, 2342: 4, 1948: 8, 1573: 22, 255: 6, 1720: 1, 1190: 2, 724: 17, 1846: 5, 2086: 4, 1578: 1, 2754: 1, 360: 27, 2414: 7, 2509: 2, 2453: 3, 2633: 1, 1951: 5, 2512: 1, 600: 47, 2444: 4, 1084: 45, 1987: 1, 543: 11, 263: 5, 131: 40, 2063: 2, 1509: 6, 622: 19, 1823: 4, 2181: 1, 972: 9, 571: 9, 2038: 9, 1864: 5, 2422: 4, 1268: 4, 1911: 4, 2533: 3, 1698: 14, 2024: 3, 1488: 7, 1824: 34, 1641: 1, 1969: 1, 789: 9, 940: 11, 1381: 6, 143: 25, 293: 24, 79: 78, 1556: 38, 2583: 7, 1732: 6, 1091: 7, 943: 17, 1428: 9, 2489: 1, 1545: 14, 1648: 17, 1083: 15, 621: 21, 330: 6, 1621: 23, 1176: 5, 1768: 36, 1796: 2, 1992: 2, 675: 10, 2477: 1, 1301: 30, 2434: 4, 1998: 2, 646: 23, 295: 42, 1244: 18, 2280: 1, 359: 42, 1207: 6, 2535: 2, 1507: 13, 1079: 22, 823: 21, 338: 25, 118: 12, 1279: 1, 2301: 4, 1204: 3, 1557: 3, 853: 9, 2344: 1, 380: 3, 1036: 10, 1434: 2, 434: 2, 1197: 33, 190: 3, 774: 6, 1228: 3, 1828: 13, 1917: 11, 1713: 8, 2374: 7, 1106: 5, 2135: 3, 2488: 3, 2435: 5, 1460: 1, 1138: 6, 1420: 3, 1903: 7, 1020: 25, 2057: 2, 2129: 1, 824: 6, 2217: 1, 2071: 4, 1759: 10, 1117: 7, 1173: 10, 1937: 1, 412: 6, 791: 26, 2225: 9, 2476: 1, 1271: 3, 66: 2, 1982: 2, 1314: 33, 2314: 4, 630: 42, 2372: 7, 153: 3, 1408: 7, 1576: 7, 671: 32, 1902: 4, 2313: 3, 42: 27, 419: 56, 1028: 4, 1859: 12, 1895: 1, 227: 31, 417: 20, 953: 6, 1693: 8, 1519: 4, 2018: 3, 141: 5, 1208: 11, 1798: 9, 2238: 1, 1772: 1, 1855: 7, 1574: 8, 608: 36, 1897: 5, 1792: 5, 899: 4, 448: 25, 1566: 1, 1278: 18, 1448: 6, 2016: 2, 776: 3, 2499: 2, 525: 37, 1980: 6, 995: 7, 1710: 3, 1326: 1, 480: 40, 1845: 3, 2258: 2, 43: 1, 2131: 2, 185: 46, 1296: 16, 1940: 2, 566: 5, 1540: 4, 1074: 23, 1097: 16, 931: 35, 1934: 2, 1080: 5, 730: 4, 2668: 1, 1370: 6, 1179: 15, 676: 7, 2160: 8, 1752: 3, 2472: 2, 2601: 2, 2061: 1, 2562: 2, 997: 1, 660: 8, 2450: 4, 828: 31, 391: 36, 1345: 2, 1533: 1, 1502: 2, 994: 29, 1831: 1, 1745: 13, 1807: 10, 1351: 15, 1623: 5, 93: 5, 1122: 10, 895: 7, 1607: 9, 2075: 2, 932: 17, 2354: 1, 2580: 2, 319: 24, 381: 10, 1552: 22, 2371: 5, 2381: 1, 1804: 2, 1604: 39, 142: 14, 1167: 2, 33: 3, 1066: 7, 1958: 12, 2031: 2, 2725: 1, 930: 20, 1567: 16, 937: 21, 1800: 1, 2595: 1, 2289: 11, 1222: 8, 678: 3, 2112: 2, 2158: 3, 1960: 2, 1841: 1, 2750: 1, 2356: 5, 2456: 1, 1297: 14, 1526: 1, 1656: 3, 2478: 19, 2621: 1, 1440: 10, 1164: 17, 2548: 2, 1816: 3, 1165: 5, 846: 2, 108: 5, 2557: 3, 1776: 1, 787: 4, 1043: 2, 686: 22, 1062: 2, 2190: 21, 2209: 6, 8: 3, 1034: 8, 2359: 13, 1835: 19, 1456: 29, 1921: 9, 1439: 3, 1513: 1, 1624: 3, 2340: 1, 1427: 1, 2346: 4, 1551: 6, 1609: 2, 1881: 10, 1061: 17, 495: 11, 705: 15, 691: 19, 2285: 2, 1343: 41, 2448: 1, 857: 1, 1128: 11, 2673: 1, 2692: 1, 1879: 5, 1387: 1, 2178: 3, 2599: 1, 1914: 1, 1527: 3, 1727: 1, 811: 22, 485: 8, 443: 2, 1894: 4, 1978: 4, 2350: 1, 2066: 1, 197: 35, 1203: 2, 1491: 3, 2309: 7, 2758: 1, 2097: 8, 2717: 1, 1076: 5, 406: 2, 1809: 1, 464: 32, 1254: 5, 2694: 1, 2380: 1, 2689: 1, 397: 3, 1953: 1, 1307: 14, 2576: 2, 303: 1, 2162: 2, 665: 14, 2013: 2, 1660: 1, 1944: 2, 911: 9, 1174: 3, 1459: 3, 2270: 2, 1232: 5, 1805: 2, 2276: 1, 879: 20, 1731: 2, 1569: 17, 1651: 2, 1373: 2, 2000: 1, 2044: 2, 1909: 15, 1890: 1, 1848: 1, 2064: 1, 2343: 1, 1715: 1, 1504: 1, 1267: 12, 2538: 1, 2391: 1, 1570: 11, 1614: 1, 1181: 4, 1780: 5, 1580: 3, 1794: 3, 1873: 4, 2228: 2, 2128: 1, 1240: 1, 951: 4, 2328: 1, 1989: 1, 1579: 3, 2104: 8, 2047: 1, 2111: 5, 35: 2, 1514: 1, 1541: 3, 1598: 2, 1289: 14, 97: 2, 1386: 1, 2266: 2, 1468: 2, 1026: 1, 1547: 3, 2058: 5, 629: 1, 484: 2, 343: 7, 2364: 1, 2224: 4, 2604: 1, 2284: 2, 1337: 4, 1435: 1, 2087: 3, 1842: 1, 1750: 2, 2161: 1, 2208: 2, 2159: 6, 2676: 1, 2568: 1, 1882: 9, 2600: 1, 2142: 3, 2025: 1, 2269: 1, 991: 8, 2113: 7, 2622: 4, 1523: 1, 2035: 1, 1852: 1, 918: 18, 2560: 1, 1830: 1, 2539: 6, 739: 1, 1214: 8, 1695: 1, 894: 73, 2053: 1, 1950: 1, 1558: 20, 1276: 3, 2628: 9, 2246: 1, 2767: 1, 469: 2, 2419: 3, 2682: 1, 2430: 3, 1920: 1, 2534: 1, 2431: 3, 1265: 7, 2351: 2, 1317: 1, 736: 14, 1782: 4, 2070: 4, 1787: 1, 1764: 2, 2677: 1, 2326: 3, 2051: 6, 2619: 1, 1005: 1, 2136: 1, 2532: 9, 1744: 1, 1868: 2, 1984: 1, 1259: 1, 2413: 11, 992: 2, 2103: 3, 1857: 1, 687: 3, 1417: 10, 2623: 1, 2613: 2, 1703: 1, 1688: 1, 1795: 1, 2460: 2, 2510: 1, 1749: 1, 2740: 1, 2260: 5, 2591: 1, 2469: 1, 653: 2, 2639: 1, 2421: 2, 2415: 1, 2713: 1, 1235: 2, 2194: 2, 1927: 1, 2264: 1, 1970: 1, 2085: 1, 667: 1, 1654: 1, 2588: 1, 2582: 7, 1819: 1, 950: 34, 2005: 1, 2763: 3, 2262: 1, 2464: 1, 1839: 3, 1916: 1, 2716: 1, 1815: 18, 2177: 2, 2226: 1, 1425: 1, 599: 1, 2505: 2, 1335: 19, 1893: 1, 1051: 8, 2631: 1, 2603: 1, 2360: 3, 1384: 1, 1683: 1, 1738: 1}\n",
            "2183\n",
            "2769\n",
            "min number of connections for each class 1 2387\n",
            "max number of connections for each class 252 195\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.colors as colors\n",
        "print(list_users[0])\n",
        "\n",
        "# create a color map\n",
        "cmap = plt.cm.jet\n",
        "norm = colors.Normalize(vmin=0, vmax=10)\n",
        "\n",
        "# plot the itinerary of 10 users in the dataset\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(10):\n",
        "    user = list_users[i]\n",
        "    x = user['input'][:,0].numpy()\n",
        "    y = user['input'][:,1].numpy()\n",
        "    color = cmap(norm(i))\n",
        "    ax.plot(x, y, color=color)\n",
        "ax.set_xlabel('x')n\n",
        "ax.set_title('Itinerary of 10 users in the dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5cRrQ5xg8D0H",
        "outputId": "a9fd4694-f7a4-4c58-de95-972c051c0f73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pos_id': tensor([ 528,  874,  237,  528,  580,  528,  237,  528, 1102,  237,  874]), 'month': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), 'day': tensor([ 1,  1,  1,  1,  2,  4,  5,  6, 13, 14, 14]), 'hour': tensor([ 2,  3, 10, 11, 20, 23,  3, 14,  9, 15, 18]), 'minute': tensor([37,  1, 21, 18, 23,  1, 33,  1, 39, 54, 51]), 'second': tensor([ 7,  2, 56, 13,  1, 40, 15, 16, 58, 39, 32]), 'pos_id_target': tensor([ 874,  237,  528,  580,  528,  237,  528, 1102,  237,  874, 1157]), 'input': tensor([[-0.0264,  0.0821],\n",
            "        [-0.0368,  0.0794],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0320,  0.0835],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0260,  0.0766],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0368,  0.0794]]), 'time_target': tensor([[-0.2760,  0.2672],\n",
            "        [-0.6802, -0.0909],\n",
            "        [ 0.0460,  0.2394],\n",
            "        [-0.2387, -1.4165],\n",
            "        [-0.6762, -2.3213],\n",
            "        [-0.4034,  0.0545],\n",
            "        [-0.6181, -0.9830],\n",
            "        [-0.6331, -8.1427],\n",
            "        [-0.5198, -1.2704],\n",
            "        [-0.6887,  0.1368],\n",
            "        [-0.3630, -0.8243]])}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "def plot_itinerary(list_users):\n",
        "    fig, ax = plt.subplots()\n",
        "    for i in range(10):\n",
        "        user = random.choice(list_users)\n",
        "        # color = cm.nipy_spectral(random.choice(range(0, 10)))\n",
        "        norm = colors.Normalize(vmin=0, vmax=10)\n",
        "        for j in range(len(user['pos_id'])):\n",
        "          ax.plot(user['input'][j,0], user['input'][j,1], 'o', color=color)\n",
        "    plt.show()\n",
        "\n",
        "plot_itinerary(list_users)"
      ],
      "metadata": {
        "id": "4VTCPvjr9d50",
        "outputId": "d5fd4998-3f2e-43c8-bba1-074dc7eb7bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Approach"
      ],
      "metadata": {
        "id": "la6oh-00LxD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "oEoCaV7OLBmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(inputs, pos_id_targets, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "id": "zINuSgqbMD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode the target\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "#go to GPU\n",
        "# X_train = X_train.to('cuda')\n",
        "# X_test = X_test.to('cuda')\n",
        "# y_train_encoded = torch.tensor(y_train_encoded).to('cuda')\n",
        "# y_test_encoded = torch.tensor(y_test_encoded).to('cuda')\n",
        "#Search for the best parameters\n",
        "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
        "# 'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
        "# 'n_estimators': [100, 200, 300, 400, 500],\n",
        "# 'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
        "# 'min_child_weight': [1, 2, 3, 4, 5]\n",
        "# parameters = {'max_depth': [30, 40, 50, 60, 70, 80, 90, 100], 'learning_rate': [0.05, 0.1, 0.15, 0.2], 'n_estimators': [50, 70, 100, 130, 170], 'gamma': [0, 0.1, 0.2, 0.3, 0.4], 'min_child_weight': [5, 10, 20, 30, 40]}\n",
        "parameters = {'max_depth': [30, 40], 'learning_rate': [0.1, 0.2], 'n_estimators': [50, 70, 100], 'gamma': [0.2, 0.3, 0.4], 'min_child_weight': [10]}\n",
        "best_score = -1\n",
        "\n",
        "for max_depth in parameters['max_depth']:\n",
        "    for learning_rate in parameters['learning_rate']:\n",
        "        for n_estimators in parameters['n_estimators']:\n",
        "            for gamma in parameters['gamma']:\n",
        "                for min_child_weight in parameters['min_child_weight']:\n",
        "                    model = xgb.XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, gamma=gamma, min_child_weight=min_child_weight, gpu_id=0) #, gpu_id=0\n",
        "                    model.fit(X_train, y_train_encoded)\n",
        "                    score = model.score(X_test, y_test_encoded)\n",
        "                    print(score)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_parameters = {'max_depth': max_depth, 'learning_rate': learning_rate, 'n_estimators': n_estimators, 'gamma': gamma, 'min_child_weight': min_child_weight}\n",
        "\n",
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "# cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=5)\n",
        "print(\"cross score: \",cv_scores)\n",
        "\n",
        "#Predict the next position\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "id": "R-25EdbFMF0i",
        "outputId": "ab4e9d23-4432-4055-ba2c-cfc890984a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.02289332683877253\n",
            "0.022710667316122746\n",
            "0.02228446176327326\n",
            "0.023380418899171942\n",
            "0.02368485143692158\n",
            "0.023928397467121287\n",
            "0.023563078421821725\n",
            "0.023563078421821725\n",
            "0.02429371651242085\n",
            "0.02307598636142231\n",
            "0.024171943497320995\n",
            "0.024171943497320995\n",
            "0.022771553823672675\n",
            "0.024354603019970774\n",
            "0.023928397467121287\n",
            "0.023015099853872383\n",
            "0.02429371651242085\n",
            "0.023928397467121287\n",
            "0.02289332683877253\n",
            "0.022710667316122746\n",
            "0.02228446176327326\n",
            "0.023380418899171942\n",
            "0.02368485143692158\n",
            "0.023928397467121287\n",
            "0.023563078421821725\n",
            "0.023563078421821725\n",
            "0.02429371651242085\n",
            "0.02307598636142231\n",
            "0.024171943497320995\n",
            "0.024171943497320995\n",
            "0.022771553823672675\n",
            "0.024354603019970774\n",
            "0.023928397467121287\n",
            "0.023015099853872383\n",
            "0.02429371651242085\n",
            "0.023928397467121287\n",
            "best score:  0.024354603019970774\n",
            "{'max_depth': 30, 'learning_rate': 0.2, 'n_estimators': 70, 'gamma': 0.3, 'min_child_weight': 10}\n",
            "cross score:  [0.11441618 0.10281837 0.10503653 0.09551148 0.10138309]\n",
            "Accuracy: 0.023928397467121287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t24nQ6zkMSyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "f5D9IoBtGX6R",
        "dDvAwpD4GrJu",
        "S_mzoE-MHqLa",
        "ptycyS7FWE4b",
        "3Bbp1dVXWQs3",
        "zZpbR8rG8kBn"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}