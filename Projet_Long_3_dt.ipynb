{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briag1/ProjetLong/blob/main/Projet_Long_3_dt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# @title device\n",
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "  return device\n",
        "device=get_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZV8IJFGT9T",
        "outputId": "601014cf-39eb-4f7c-9c82-8828b6ab80e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Using CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_LSRYqGMO2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qh5nnjRIFe0h"
      },
      "outputs": [],
      "source": [
        "# @title code\n",
        "from os import makedirs\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_x(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[0])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def get_y(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[1])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def read_dataframe(name):\n",
        "  if not os.path.exists(name+\".pkl\"):\n",
        "    print(\"reading dataframe: \"+name+\".xlsx\")\n",
        "    df=pd.read_excel(name+\".xlsx\")\n",
        "    df.to_pickle(name+\".pkl\")\n",
        "  else:\n",
        "    print(\"using already read daframe\")\n",
        "\n",
        "def get_vocab(poses,vocab):\n",
        "  for pos in poses:\n",
        "    if pos not in vocab and not any(isinstance(n, float) and math.isnan(n) for n in pos):\n",
        "        vocab[pos]=len(vocab)+1\n",
        "  return vocab\n",
        "\n",
        "def get_fix_time_encoding(df):\n",
        "\n",
        "  df['month_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "  df['month_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "\n",
        "  df['day_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "  df['day_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "\n",
        "  df['hour_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "  df['hour_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "\n",
        "  df['minute_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "  df['minute_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "\n",
        "  df['second_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "  df['second_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "def get_time_data(df):\n",
        "  df['month'] =  df[\"start time\"].dt.month\n",
        "  df['day'] =  df[\"start time\"].dt.day\n",
        "  df['hour'] =  df[\"start time\"].dt.hour\n",
        "  df['minute'] = df[\"start time\"].dt.minute\n",
        "  df['second'] = df[\"start time\"].dt.second\n",
        "  return df\n",
        "\n",
        "\n",
        "def tokenize_pos(pos,vocab):\n",
        "\n",
        "  if math.isnan(pos[0]) and math.isnan(pos[1]):\n",
        "    return len(vocab)\n",
        "  else:\n",
        "    return vocab[pos]\n",
        "\n",
        "def get_coordinates(df,input_position,full_dataset):\n",
        "\n",
        "  if full_dataset:\n",
        "    df['x'] = df['latitude']\n",
        "    df['y'] = df['longitude']\n",
        "  else:\n",
        "    df['x'] = df['location(latitude/lontitude)'].apply(get_x)\n",
        "    df['y'] = df['location(latitude/lontitude)'].apply(get_y)\n",
        "\n",
        "\n",
        "  if input_position:\n",
        "    df['x_normalised']=(df['x']-df['x'].mean())/(df['x'].std())\n",
        "    df['y_normalised']=(df['y']-df['y'].mean())/df['y'].std()\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_joined_coordinates(df):\n",
        "\n",
        "  df['pos']= list(zip(df['x'],df['y']))\n",
        "  poses=df['pos'].unique()\n",
        "\n",
        "  return poses\n",
        "\n",
        "def get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset):\n",
        "  col_to_drop_in_df=['date', 'end time','pos']\n",
        "  col_to_drop_in_dict=['x','y', 'time_to_end', 'time_to_next','start time', 'user id']\n",
        "  col_to_add_to_dict=[]\n",
        "  col_in_input=[]\n",
        "  if not full_dataset:\n",
        "    col_to_drop_in_df+=['location(latitude/lontitude)']\n",
        "  else:\n",
        "    col_to_drop_in_df+=['latitude','longitude']\n",
        "  if fixed_time_encoding:\n",
        "    col_to_drop_in_df+=[]\n",
        "    col_to_drop_in_dict+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "    col_in_input+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "  else:\n",
        "    col_to_add_to_dict+=['month','day','hour','minute','second']\n",
        "  if input_position:\n",
        "    col_to_drop_in_dict += ['x_normalised', 'y_normalised']\n",
        "    col_in_input+=['x_normalised', 'y_normalised']\n",
        "  return col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict\n",
        "\n",
        "def process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections):\n",
        "  #get the time to next connection\n",
        "  df_user[\"time_to_next\"] =  df_user[\"start time\"].diff(-1).dt.total_seconds()\n",
        "  dict_user=df_user.to_dict('list')\n",
        "  #create input\n",
        "  dict_user[\"pos_id\"],dict_user[\"pos_id_target\"]=torch.tensor(dict_user[\"pos_id\"][:-1]),torch.tensor(dict_user[\"pos_id\"][1:])\n",
        "\n",
        "  if col_in_input:\n",
        "    dict_user[\"input\"]=torch.tensor([dict_user[col] for col in col_in_input]).T\n",
        "    dict_user[\"input\"]=dict_user[\"input\"][:-1]\n",
        "\n",
        "  if col_to_add_to_dict:\n",
        "    for col in col_to_add_to_dict:\n",
        "      dict_user[col]=torch.tensor(dict_user[col])\n",
        "      dict_user[col]=dict_user[col][:-1]\n",
        "\n",
        "  dict_user[\"time_target\"]=torch.tensor([dict_user[\"time_to_end\"],dict_user[\"time_to_next\"]]).T\n",
        "  dict_user[\"time_target\"]=dict_user[\"time_target\"][:-1]\n",
        "  for e in col_to_drop_in_dict:\n",
        "    dict_user.pop(e)\n",
        "\n",
        "  if not with_repeated_connections:\n",
        "    dict_user=combine_repeated_connections_in_sequence_user(dict_user)\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine_repeated_connections_in_sequence_user(dict_user):\n",
        "  index=0\n",
        "  for key in dict_user:\n",
        "    if dict_user[key].shape[0]!=dict_user[\"pos_id\"].shape[0]:\n",
        "      print(key)\n",
        "  #print(dict_user[\"pos_id\"],dict_user[\"pos_id_target\"])\n",
        "  while index < len(dict_user[\"pos_id\"])-1:\n",
        "    #print(dict_user[\"pos_id\"][index],dict_user[\"pos_id_target\"][index])\n",
        "    if dict_user[\"pos_id\"][index]==dict_user[\"pos_id_target\"][index]:\n",
        "      #print(\"equal\")\n",
        "      dict_user[\"pos_id_target\"][index]=dict_user[\"pos_id_target\"][index+1]\n",
        "      dict_user[\"time_target\"][index]=dict_user[\"time_target\"][index+1]\n",
        "      for key in dict_user:\n",
        "\n",
        "        dict_user[key]=torch.cat((dict_user[key][:index+1],dict_user[key][index+2:]))\n",
        "      #print()\n",
        "    else:\n",
        "      #print(\"different\")\n",
        "      index+=1\n",
        "\n",
        "\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def normalize_output(list_users):\n",
        "  #get means and stds\n",
        "  time_targets=torch.cat([dict_user[\"time_target\"] for dict_user in list_users],dim=0)\n",
        "  time_targets_mean=time_targets.mean(dim=0)\n",
        "  time_targets_std=time_targets.std(dim=0)\n",
        "  #normalize\n",
        "  for i in range(len(list_users)):\n",
        "    list_users[i][\"time_target\"]=(list_users[i][\"time_target\"]-time_targets_mean)/time_targets_std\n",
        "  return list_users\n",
        "\n",
        "\n",
        "\n",
        "def process_dataframe(name,vocab,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,format=\".pkl\"):\n",
        "  df= pd.read_pickle(name+format)\n",
        "  df=df.sort_values('start time')\n",
        "  df=df.drop(['month'],axis=1)\n",
        "\n",
        "  df=get_coordinates(df,input_position,full_dataset)\n",
        "\n",
        "  poses=get_joined_coordinates(df)\n",
        "  vocab=get_vocab(poses,vocab)\n",
        "  df['pos_id'] = df['pos'].apply(lambda pos: tokenize_pos(pos,vocab))\n",
        "\n",
        "  df['time_to_end']=df['end time']-df['start time']\n",
        "  df['time_to_end']=df['time_to_end'].dt.total_seconds()\n",
        "  if fixed_time_encoding:\n",
        "    df=get_fix_time_encoding(df)\n",
        "  else:\n",
        "    df=get_time_data(df)\n",
        "\n",
        "  col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict=get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset)\n",
        "  df=df.drop(col_to_drop_in_df, axis=1)\n",
        "\n",
        "  df_user_group = df.groupby('user id')\n",
        "  list_users=[]\n",
        "  for user, df_user in df_user_group:\n",
        "    if len(df_user)>=2 and not df_user['x'].isnull().values.any():\n",
        "        list_users.append(process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections))\n",
        "  list_users=normalize_output(list_users)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "def get_raw_data(directory,src_directory,full_dataset):\n",
        "  if  full_dataset:\n",
        "    shutil.copytree(src_directory,directory)#telecomDataset6mont\n",
        "  else:\n",
        "    runcmd('wget http://sguangwang.com/dataset/telecom.zip', verbose = False)\n",
        "    runcmd('unzip /content/telecom.zip')\n",
        "\n",
        "def get_processed_dataset(load_dataset_path):\n",
        "  saved_list_user_path = os.path.join(load_dataset_path,\"list_users\")\n",
        "  saved_vocab_path = os.path.join(load_dataset_path,\"vocab\")\n",
        "  print(\"loading already preprocessed data: \")\n",
        "  print(saved_list_user_path)\n",
        "  print(saved_vocab_path)\n",
        "  list_users=torch.load(saved_list_user_path)\n",
        "  vocab=torch.load(saved_vocab_path)\n",
        "  return list_users,vocab\n",
        "\n",
        "def process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections):\n",
        "  list_users=[]\n",
        "  vocab={}\n",
        "  if not os.path.exists(directory_raw_data):\n",
        "    print('getting raw data at: '+src_directory_raw_data)\n",
        "    get_raw_data(directory_raw_data,src_directory_raw_data,full_dataset)\n",
        "  if full_dataset:\n",
        "    for name in os.listdir(directory_raw_data):\n",
        "      if not name.endswith(\".pkl\"):\n",
        "        complete_name=os.path.join(directory_raw_data,\".\".join(name.split(\".\")[:-1]))\n",
        "        print(\"processing dataframe: \"+complete_name)\n",
        "        read_dataframe(complete_name)\n",
        "        new_list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "        list_users+=new_list_users\n",
        "  else:\n",
        "    complete_name = \"/content/dataset-telecom/data_6.1~6.30_\"\n",
        "    read_dataframe(complete_name)\n",
        "    list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def split_long_sequences(list_users,max_sequence_length):\n",
        "  new_list_users=[]\n",
        "  for i in range(len(list_users)):\n",
        "    seq_length=list_users[i][\"input\"].shape[0]\n",
        "    if seq_length>=max_sequence_length:\n",
        "      nb_of_seq=seq_length//max_sequence_length\n",
        "      rest=seq_length%max_sequence_length\n",
        "      list_splitted_seq=nb_of_seq*[{}]\n",
        "      rest_splitted={}\n",
        "      for key in list_users[i]:\n",
        "        for j in range(nb_of_seq):\n",
        "          list_splitted_seq[j][key]=list_users[i][key][max_sequence_length*j:max_sequence_length*(j+1)]\n",
        "        if rest>2:\n",
        "          rest_splitted[key]= list_users[i][key][-rest:]\n",
        "      new_list_users=new_list_users+list_splitted_seq\n",
        "      if len(rest_splitted)>0:\n",
        "        new_list_users+=[rest_splitted]\n",
        "    else:\n",
        "      new_list_users.append(list_users[i])\n",
        "\n",
        "  return new_list_users\n",
        "\n",
        "\n",
        "\n",
        "def save_processed_data(list_users,vocab,path_to_save_dataset):\n",
        "    print(\"creating directory: \"+path_to_save_dataset)\n",
        "    os.makedirs(path_to_save_dataset,exist_ok=True)\n",
        "    print(\"saving processed data at: \")\n",
        "    save_list_user_path = os.path.join(path_to_save_dataset,\"list_users\")\n",
        "    save_vocab_path = os.path.join(path_to_save_dataset,\"vocab\")\n",
        "    print(save_list_user_path)\n",
        "    print(save_vocab_path)\n",
        "    torch.save(list_users,save_list_user_path)\n",
        "    torch.save(vocab,save_vocab_path)\n",
        "\n",
        "def get_processed_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,spliting_long_sequences,with_repeated_connections,max_sequence_length=100,min_sequence_length=3,save=False,path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month\",download=False,load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month\"):\n",
        "  if not download:\n",
        "    list_users,vocab = get_processed_dataset(load_dataset_path)\n",
        "  else:\n",
        "    list_users,vocab=process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections)\n",
        "  if spliting_long_sequences:\n",
        "    print(\"spliting sequences longuer than : \"+str(max_sequence_length)+ \" steps\")\n",
        "    list_users=split_long_sequences(list_users,max_sequence_length)\n",
        "  if save:\n",
        "    save_processed_data(list_users,vocab,path_to_save_dataset)\n",
        "  return list_users,vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "sSRA1WX8UcsV"
      },
      "outputs": [],
      "source": [
        "# list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "#                                     directory_raw_data='/content/dataset-telecom-6month',\n",
        "#                                     fixed_time_encoding=False,\n",
        "#                                     input_position=True,\n",
        "#                                     full_dataset=False,\n",
        "#                                     spliting_long_sequences=False,\n",
        "#                                     with_repeated_connections=False,\n",
        "#                                     max_sequence_length=100,\n",
        "#                                     min_sequence_length=3,\n",
        "#                                     save=False,\n",
        "#                                     path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",\n",
        "#                                     download=True,\n",
        "#                                     load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip /content/archive.zip\n",
        "\n"
      ],
      "metadata": {
        "id": "swOpL-WiECVR",
        "outputId": "54857900-0c3e-40ca-a20c-b698849e120c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "replace data_10.110.15.xlsx? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "name_data = \"/content/data_8.18.15\"\n",
        "complete_name = name_data\n",
        "read_dataframe(name_data)\n"
      ],
      "metadata": {
        "id": "MTyCLcFRWPwG",
        "outputId": "de08ea19-7107-4643-ddf5-47d30137f8a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading dataframe: /content/data_8.18.15.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory_raw_data='/content/dataset-telecom-6month'\n",
        "fixed_time_encoding=False\n",
        "input_position=True\n",
        "full_dataset=True\n",
        "spliting_long_sequences=False\n",
        "with_repeated_connections=False\n",
        "max_sequence_length=100\n",
        "min_sequence_length=3\n",
        "save=False\n",
        "download=True\n",
        "load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\"\n",
        "path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\"\n",
        "list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)"
      ],
      "metadata": {
        "id": "BWDWzCzXYvpG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlhN5gNSdtTa",
        "outputId": "3cdf8169-18ba-4ace-9e01-09aa6baab194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pos_id': tensor([21]),\n",
              " 'month': tensor([8]),\n",
              " 'day': tensor([5]),\n",
              " 'hour': tensor([9]),\n",
              " 'minute': tensor([42]),\n",
              " 'second': tensor([28]),\n",
              " 'pos_id_target': tensor([21]),\n",
              " 'input': tensor([[0.0904, 0.1386]]),\n",
              " 'time_target': tensor([[-0.3533,  0.2975]])}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_data_less_than_nb_min(inputs, nb_min):\n",
        "    return [inp for inp in inputs if inp.shape[0]>=nb_min]\n",
        "\n",
        "\n",
        "def inputs_for_classification(list_users, pos_id=True, user_id=True, date_time=False, time_to_end=False, time_to_end_and_target=False, delete_data_less_nb_min=True, nb_min=5):\n",
        "    inputs=[]\n",
        "    pos_id_targets=[]\n",
        "    pos_ids=[]\n",
        "    time_targets=[]\n",
        "    if date_time:\n",
        "        month = []\n",
        "        day = []\n",
        "        hour = []\n",
        "        minute = []\n",
        "        second = []\n",
        "\n",
        "        for user in list_users:\n",
        "            inputs.append(user[\"input\"])\n",
        "            pos_id_targets.append(user[\"pos_id_target\"])\n",
        "            time_targets.append(user[\"time_target\"])\n",
        "            pos_ids.append(user[\"pos_id\"])\n",
        "            month.append(user[\"month\"])\n",
        "            day.append(user[\"day\"])\n",
        "            hour.append(user[\"hour\"])\n",
        "            minute.append(user[\"minute\"])\n",
        "            second.append(user[\"second\"])\n",
        "        # print(pos_ids)\n",
        "        # print(month)\n",
        "    else:\n",
        "        for user in list_users:\n",
        "          inputs.append(user[\"input\"])\n",
        "          pos_id_targets.append(user[\"pos_id_target\"])\n",
        "          time_targets.append(user[\"time_target\"])\n",
        "          pos_ids.append(user[\"pos_id\"])\n",
        "    # print(inputs[0].shape,pos_id_targets[0].shape,pos_ids[0].shape)\n",
        "\n",
        "    if delete_data_less_nb_min:\n",
        "        inputs=delete_data_less_than_nb_min(inputs,nb_min)\n",
        "        pos_id_targets=delete_data_less_than_nb_min(pos_id_targets,nb_min)\n",
        "        pos_ids=delete_data_less_than_nb_min(pos_ids,nb_min)\n",
        "        time_targets=delete_data_less_than_nb_min(time_targets,nb_min)\n",
        "        if date_time:\n",
        "            month=delete_data_less_than_nb_min(month,nb_min)\n",
        "            day=delete_data_less_than_nb_min(day,nb_min)\n",
        "            hour=delete_data_less_than_nb_min(hour,nb_min)\n",
        "            minute=delete_data_less_than_nb_min(minute,nb_min)\n",
        "            second=delete_data_less_than_nb_min(second,nb_min)\n",
        "\n",
        "\n",
        "    if pos_id:\n",
        "        inputs_with_pos_id=[]\n",
        "        for inp,pos in zip(inputs,pos_ids):\n",
        "            inputs_with_pos_id.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))\n",
        "        inputs=inputs_with_pos_id\n",
        "    if date_time:\n",
        "        inputs_with_pos_id_and_date_time=[]\n",
        "        for i in range(len(inputs)):\n",
        "            inputs_with_pos_id_and_date_time.append(torch.cat([inputs[i], month[i].unsqueeze(1), day[i].unsqueeze(1), hour[i].unsqueeze(1), minute[i].unsqueeze(1), second[i].unsqueeze(1)],dim=1))\n",
        "        inputs=inputs_with_pos_id_and_date_time\n",
        "    if time_to_end:\n",
        "        time_to_end =  []\n",
        "        for i in range(len(time_targets)):\n",
        "            tmp = []\n",
        "            for j in range(len(time_targets[i])):\n",
        "                tmp.append(time_targets[i][j][0])\n",
        "            time_to_end.append(tmp)\n",
        "        time_to_end = [torch.tensor(time_to_end[i]) for i in range(len(time_to_end))]\n",
        "\n",
        "        inputs_with_pos_id_and_time_to_end=[]\n",
        "        for inp,pos in zip(inputs,time_to_end):\n",
        "            inputs_with_pos_id_and_time_to_end.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))\n",
        "        inputs=inputs_with_pos_id_and_time_to_end\n",
        "    if time_to_end_and_target:\n",
        "        inputs_with_pos_id_and_time_target=[]\n",
        "        for inp,tim in zip(inputs,time_targets):\n",
        "            inputs_with_pos_id_and_time_target.append(torch.cat((inp, tim), dim=1))\n",
        "        inputs=inputs_with_pos_id_and_time_target\n",
        "    if user_id:\n",
        "        inputs_with_pos_id_user_id=[]\n",
        "        for idx,inp in enumerate(inputs):\n",
        "            inputs_with_pos_id_user_id.append(torch.cat([torch.tensor([idx]*inp.shape[0]).unsqueeze(1),inp],dim=1))\n",
        "        inputs=inputs_with_pos_id_user_id\n",
        "    return inputs,pos_id_targets\n",
        "\n",
        "def inputs_outputs_for_classification(data_entry, targets):\n",
        "  inputs = torch.cat(all_inputs,dim=0)\n",
        "  outputs = torch.cat(pos_id_targets,dim=0)\n",
        "  print(\"len of the dataset\")\n",
        "  print(len(inputs), len(outputs) )\n",
        "  return inputs, outputs\n",
        "\n",
        "def trouver_cle_par_valeur(dictionnaire, valeur_recherchee):\n",
        "    for cle, valeur in dictionnaire.items():\n",
        "        if valeur == valeur_recherchee:\n",
        "            return cle\n",
        "    return None\n",
        "\n",
        "def get_x_y_target_from_outputs(outputs, vocab):\n",
        "    print(outputs[1].item())\n",
        "    x_y = [trouver_cle_par_valeur(vocab, i.item()) for i in outputs]\n",
        "    return [torch.tensor(x_y[i]) for i in range(len(outputs))]\n",
        "\n",
        "def inputs_group_by_user(list_users):\n",
        "  inputs_for_group,pos_id_targets=inputs_for_classification(list_users, pos_id=True, user_id=True, time_to_end=False, time_to_end_and_target=False, delete_data_less_nb_min=True, nb_min=5)\n",
        "\n",
        "  #for each user_id, concatenate by columns the inputs\n",
        "  inputs_group_by_user_id=[]\n",
        "\n",
        "  for i in range(len(inputs_for_group)):\n",
        "    inp = torch.stack(tuple(inputs_for_group[i][j] for j in range(len(inputs_for_group[i]))), dim=1)\n",
        "    inputs_group_by_user_id.append(inp)\n",
        "\n",
        "  inputs_group = [inputs_group_by_user_id[i].numpy() for i in range(len(inputs_group_by_user_id))]\n",
        "  inputs_group = [inputs_group[i].tolist() for i in range(len(inputs_group))]\n",
        "\n",
        "  return inputs_group\n",
        "\n",
        "def group_inputs_by_number_connections_and_users_id(inputs_group, pas_group=5):\n",
        "  nb_connections=[]\n",
        "  for user in inputs_group:\n",
        "    nb_connections.append(len(user[0]))\n",
        "  #group users by number of connections\n",
        "  group = [i for i in range(0, max(nb_connections)+pas_group, pas_group)]\n",
        "  group_users = []\n",
        "  group_users_id = []\n",
        "  for i in range(len(group)-1):\n",
        "    tmp = []\n",
        "    tmp_id = []\n",
        "    for j in range(len(nb_connections)):\n",
        "      if nb_connections[j] >= group[i] and nb_connections[j] < group[i+1]:\n",
        "        tmp.append(nb_connections[j])\n",
        "        tmp_id.append(j)\n",
        "    group_users.append(tmp)\n",
        "    group_users_id.append(tmp_id)\n",
        "  return group_users_id, group_users\n"
      ],
      "metadata": {
        "id": "L4WiObfV-oQL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_users[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVgwKJjuRdB4",
        "outputId": "72e5dccb-2a8d-4905-9625-e58595c47449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pos_id': tensor([ 528,  874,  237,  528,  580,  528,  237,  528, 1102,  237,  874]), 'month': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), 'day': tensor([ 1,  1,  1,  1,  2,  4,  5,  6, 13, 14, 14]), 'hour': tensor([ 2,  3, 10, 11, 20, 23,  3, 14,  9, 15, 18]), 'minute': tensor([37,  1, 21, 18, 23,  1, 33,  1, 39, 54, 51]), 'second': tensor([ 7,  2, 56, 13,  1, 40, 15, 16, 58, 39, 32]), 'pos_id_target': tensor([ 874,  237,  528,  580,  528,  237,  528, 1102,  237,  874, 1157]), 'input': tensor([[-0.0264,  0.0821],\n",
            "        [-0.0368,  0.0794],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0320,  0.0835],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0260,  0.0766],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0368,  0.0794]]), 'time_target': tensor([[-0.2760,  0.2672],\n",
            "        [-0.6802, -0.0909],\n",
            "        [ 0.0460,  0.2394],\n",
            "        [-0.2387, -1.4165],\n",
            "        [-0.6762, -2.3213],\n",
            "        [-0.4034,  0.0545],\n",
            "        [-0.6181, -0.9830],\n",
            "        [-0.6331, -8.1427],\n",
            "        [-0.5198, -1.2704],\n",
            "        [-0.6887,  0.1368],\n",
            "        [-0.3630, -0.8243]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "xQPyWItnFg1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def plot_nb_users_by_group_of_nuber_of_connections(list_users):\n",
        "  inputs_group = inputs_group_by_user(list_users)\n",
        "  group_users_id, group_users = group_inputs_by_number_connections_and_users_id(inputs_group)\n",
        "  #plot the number of users in each group\n",
        "  plt.bar([i for i in range(len(group_users))], [len(group_users[i]) for i in range(len(group_users))])\n",
        "  plt.xlabel(\"number of connections\")\n",
        "  plt.ylabel(\"number of users\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_itinerary_users_by_pos_id(list_users, tab_user_id_plot = [444, 445, 446, 447], number_group_to_plot = 6 ):\n",
        "  inputs_group = inputs_group_by_user(list_users)\n",
        "  group_users_id, group_users = group_inputs_by_number_connections_and_users_id(inputs_group)\n",
        "  number_user_plot = len(tab_user_id_plot)\n",
        "\n",
        "  group_of_10 = [[list_users[i] for i in group_users_id[j]] for j in range(len(group_users_id))]\n",
        "\n",
        "  cmap = plt.cm.jet\n",
        "  norm = colors.Normalize(vmin=0, vmax=number_user_plot)\n",
        "  group_3 = [list_users[i] for i in tab_user_id_plot]\n",
        "\n",
        "  # plot the itinerary of 10 users in the dataset\n",
        "  fig, ax = plt.subplots()\n",
        "  for i in range(number_user_plot):\n",
        "      user = group_3[i]\n",
        "      # user = np.random.choice(group_of_10[number_group_to_plot])\n",
        "      pos_id = user[\"pos_id\"]\n",
        "      color = cmap(norm(i))\n",
        "      ax.plot([i for i in range(len(pos_id))], pos_id,  'x', color=color)\n",
        "  ax.set_xlabel('evolution')\n",
        "  ax.set_ylabel('pos_id')\n",
        "  ax.set_title('Itinerary of ' + str(number_user_plot)+ ' users in the dataset')\n",
        "  plt.show()\n",
        "\n",
        "def plot_itinerary_users_by_x_y(list_users, tab_user_id_plot = [444, 445, 446, 447], number_group_to_plot = 6 ):\n",
        "    inputs_group = inputs_group_by_user(list_users)\n",
        "    group_users_id, group_users = group_inputs_by_number_connections_and_users_id(inputs_group)\n",
        "    print(group_users_id)\n",
        "    number_user_plot = len(tab_user_id_plot)\n",
        "\n",
        "    cmap = plt.cm.jet\n",
        "    norm = colors.Normalize(vmin=0, vmax=number_user_plot)\n",
        "    group_3 = [list_users[i] for i in tab_user_id_plot]\n",
        "\n",
        "    # plot the itinerary of 10 users in the dataset\n",
        "    fig, ax = plt.subplots()\n",
        "    for i in range(number_user_plot):\n",
        "        user = group_3[i]\n",
        "        # user = np.random.choice(group_of_10[number_group_to_plot])\n",
        "        x = user['input'][:,0].numpy()\n",
        "        y = user['input'][:,1].numpy()\n",
        "        color = cmap(norm(i))\n",
        "        for i, txt in enumerate(range(1, len(x) + 1)):\n",
        "          plt.annotate(txt, (x[i], y[i]), textcoords=\"offset points\", xytext=(0,0), ha='center', color=color)\n",
        "\n",
        "        ax.plot(x, y,  'x', color=color)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title('Itinerary of ' + str(number_user_plot)+ ' users in the dataset')\n",
        "    plt.show()\n",
        "\n",
        "def plot_pos_id_with_time_to_end_for_user(list_users, user_id):\n",
        "  inputs_with_pos_id_user_id, targets = inputs_for_classification(list_users, time_to_end=True)\n",
        "  tab_user_id = []\n",
        "  for t in range(len(inputs_with_pos_id_user_id)):\n",
        "      if inputs_with_pos_id_user_id[t][0][0].item() == user_id:\n",
        "          tab_user_id = inputs_with_pos_id_user_id[t]\n",
        "  tab_user_id = tab_user_id.numpy()\n",
        "  plt.bar([i for i in range(0, 2*len(tab_user_id), 2)],tab_user_id[:,3], width=tab_user_id[:,4])\n",
        "  plt.xlabel(\"number of connections with its time to end\")\n",
        "  plt.ylabel(\"pos_id\")\n",
        "  plt.title(\"pos_id with time_to_end for user \"+str(user_id))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "s5skrqXLIBHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "# plot_nb_users_by_group_of_nuber_of_connections(list_users)\n",
        "\n",
        "plot_itinerary_users_by_pos_id(list_users, tab_user_id_plot=[1184, 1198, 1206, 1210])\n",
        "plot_itinerary_users_by_x_y(list_users, tab_user_id_plot=[1184, 1198, 1206, 1210])\n",
        "\n",
        "# plot_pos_id_with_time_to_end_for_user(list_users, 123)\n",
        "# plot_pos_id_with_time_to_end_for_user(list_users,124)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "LR9dxkt9AVNC",
        "outputId": "8cabe121-6dd1-4610-91dc-dcc89071c9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 2]) torch.Size([11]) torch.Size([11])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 2]) torch.Size([11]) torch.Size([11])\n",
            "[[], [10, 15, 30, 38, 40, 44, 46, 47, 49, 50, 55, 58, 65, 69, 70, 72, 74, 76, 81, 86, 99, 109, 111, 118, 120, 126, 128, 131, 135, 145, 148, 155, 158, 160, 162, 163, 179, 189, 192, 199, 200, 202, 205, 208, 210, 215, 216, 219, 223, 226, 227, 230, 234, 240, 244, 251, 252, 256, 259, 262, 265, 270, 278, 279, 284, 289, 294, 296, 304, 306, 308, 311, 314, 315, 317, 342, 351, 352, 354, 356, 357, 358, 363, 366, 367, 369, 387, 391, 400, 402, 403, 404, 406, 407, 413, 419, 421, 424, 426, 437, 440, 448, 460, 461, 486, 487, 499, 512, 526, 537, 538, 541, 548, 550, 555, 556, 558, 559, 565, 568, 570, 576, 578, 586, 588, 590, 598, 603, 611, 618, 619, 628, 631, 637, 638, 649, 652, 657, 662, 665, 675, 681, 693, 697, 700, 704, 708, 710, 712, 725, 727, 729, 730, 738, 739, 746, 750, 753, 754, 758, 761, 771, 778, 779, 786, 791, 792, 795, 798, 800, 803, 804, 805, 806, 809, 811, 817, 819, 828, 834, 840, 842, 844, 852, 853, 858, 863, 873, 874, 875, 877, 880, 888, 892, 899, 902, 905, 913, 919, 921, 925, 927, 928, 931, 936, 941, 946, 950, 953, 954, 955, 956, 963, 966, 968, 976, 978, 988, 993, 998, 1009, 1012, 1013, 1017, 1019, 1021, 1025, 1034, 1036, 1037, 1039, 1040, 1042, 1049, 1061, 1062, 1063, 1065, 1066, 1067, 1068, 1076, 1081, 1087, 1094, 1098, 1102, 1108, 1117, 1118, 1123, 1132, 1134, 1141, 1146, 1147, 1157, 1159, 1161, 1170, 1175, 1183, 1184, 1198, 1206, 1210, 1221, 1222, 1226, 1232, 1243, 1248, 1249, 1252, 1253, 1255, 1258, 1264, 1272, 1273, 1281, 1299, 1302, 1310, 1313, 1318, 1321, 1323, 1329, 1330, 1331, 1332, 1333, 1336, 1339, 1343, 1344, 1345, 1361, 1362, 1371, 1373, 1375, 1376, 1382, 1384, 1388, 1397, 1401, 1403, 1409, 1414, 1418, 1423, 1425, 1427, 1439, 1443, 1444, 1450, 1452, 1453, 1465, 1466, 1467, 1472, 1473, 1478, 1480, 1484, 1492, 1497, 1502, 1505, 1525, 1528, 1529, 1538, 1552, 1553], [0, 1, 5, 8, 12, 14, 27, 29, 31, 36, 68, 77, 90, 97, 103, 112, 116, 123, 127, 129, 137, 147, 149, 166, 167, 204, 207, 212, 214, 218, 220, 222, 247, 255, 268, 269, 273, 282, 290, 293, 313, 326, 328, 332, 335, 343, 345, 347, 349, 350, 353, 392, 405, 408, 427, 428, 438, 443, 449, 452, 457, 469, 477, 478, 480, 481, 483, 488, 489, 490, 491, 492, 493, 504, 505, 521, 533, 534, 539, 543, 545, 552, 561, 573, 575, 587, 589, 591, 592, 597, 602, 609, 612, 614, 615, 617, 620, 624, 627, 634, 641, 651, 653, 654, 664, 670, 671, 676, 677, 699, 722, 723, 731, 734, 735, 736, 740, 743, 748, 751, 765, 766, 774, 781, 789, 796, 799, 826, 827, 835, 843, 845, 850, 862, 867, 872, 894, 898, 903, 915, 917, 918, 933, 967, 973, 984, 985, 992, 999, 1000, 1002, 1004, 1005, 1007, 1008, 1014, 1015, 1016, 1022, 1026, 1027, 1044, 1046, 1052, 1064, 1072, 1073, 1086, 1088, 1091, 1106, 1112, 1120, 1131, 1135, 1138, 1140, 1156, 1165, 1167, 1194, 1196, 1199, 1202, 1212, 1230, 1231, 1242, 1244, 1245, 1250, 1262, 1263, 1277, 1278, 1300, 1314, 1315, 1317, 1340, 1349, 1351, 1354, 1357, 1364, 1367, 1378, 1385, 1387, 1392, 1398, 1407, 1408, 1411, 1429, 1431, 1438, 1454, 1457, 1461, 1481, 1486, 1501, 1503, 1504, 1509, 1516, 1518, 1543, 1547, 1551, 1555], [4, 43, 51, 54, 79, 91, 93, 95, 96, 100, 113, 115, 141, 142, 150, 153, 169, 182, 194, 206, 211, 225, 235, 236, 242, 249, 250, 258, 266, 271, 276, 286, 292, 301, 316, 319, 327, 329, 331, 338, 348, 355, 368, 374, 376, 378, 382, 389, 393, 401, 409, 453, 465, 466, 495, 503, 506, 509, 525, 529, 530, 547, 551, 584, 596, 601, 604, 610, 613, 626, 643, 674, 678, 685, 689, 696, 702, 703, 711, 715, 741, 768, 772, 783, 810, 832, 838, 857, 860, 866, 868, 869, 878, 879, 882, 890, 907, 911, 937, 951, 958, 986, 990, 991, 995, 1003, 1006, 1054, 1055, 1058, 1070, 1078, 1090, 1105, 1107, 1116, 1125, 1143, 1155, 1179, 1186, 1228, 1235, 1241, 1246, 1265, 1271, 1280, 1282, 1288, 1290, 1304, 1326, 1328, 1337, 1342, 1348, 1352, 1353, 1358, 1386, 1412, 1420, 1422, 1483, 1487, 1493, 1512, 1532], [13, 16, 23, 35, 37, 45, 52, 53, 59, 60, 82, 98, 108, 136, 143, 151, 164, 180, 181, 185, 238, 243, 253, 254, 277, 280, 281, 309, 322, 330, 339, 377, 379, 381, 395, 397, 429, 430, 445, 456, 473, 475, 485, 498, 544, 567, 599, 605, 639, 640, 656, 658, 663, 672, 673, 705, 773, 787, 797, 815, 820, 824, 831, 841, 870, 871, 883, 889, 896, 908, 962, 971, 975, 979, 994, 1010, 1018, 1031, 1035, 1047, 1048, 1059, 1082, 1093, 1115, 1144, 1150, 1160, 1163, 1187, 1197, 1205, 1211, 1219, 1220, 1225, 1251, 1267, 1283, 1316, 1334, 1360, 1370, 1372, 1374, 1390, 1404, 1419, 1432, 1433, 1458, 1460, 1462, 1464, 1474, 1489, 1490, 1494, 1500, 1510, 1519, 1520, 1533, 1535, 1554], [33, 56, 73, 78, 84, 89, 101, 140, 144, 152, 175, 184, 186, 190, 198, 229, 263, 264, 272, 303, 334, 359, 384, 390, 410, 412, 417, 434, 444, 450, 500, 553, 564, 582, 621, 633, 646, 650, 669, 687, 688, 692, 713, 714, 720, 724, 733, 745, 749, 755, 759, 777, 790, 851, 859, 861, 864, 886, 923, 929, 972, 1020, 1023, 1028, 1050, 1100, 1103, 1110, 1137, 1164, 1171, 1174, 1176, 1192, 1203, 1208, 1214, 1234, 1286, 1293, 1303, 1320, 1338, 1377, 1399, 1434, 1447, 1522, 1537, 1548, 1550], [64, 83, 134, 154, 171, 231, 233, 245, 287, 297, 324, 333, 340, 344, 371, 386, 396, 398, 422, 431, 458, 467, 479, 484, 496, 508, 522, 554, 560, 593, 625, 632, 644, 648, 684, 695, 717, 719, 728, 742, 769, 833, 855, 856, 885, 939, 942, 977, 1033, 1045, 1074, 1075, 1096, 1114, 1121, 1126, 1181, 1182, 1188, 1189, 1207, 1227, 1257, 1287, 1289, 1308, 1309, 1312, 1322, 1359, 1380, 1383, 1391, 1410, 1428, 1436, 1446, 1451, 1463, 1479, 1485, 1511, 1541, 1549, 1556], [24, 32, 41, 104, 124, 125, 177, 195, 209, 232, 298, 310, 383, 420, 441, 463, 482, 516, 532, 536, 557, 566, 594, 607, 690, 691, 718, 726, 737, 752, 775, 814, 830, 848, 897, 904, 912, 922, 932, 944, 949, 959, 982, 1051, 1092, 1101, 1104, 1154, 1173, 1200, 1209, 1218, 1236, 1237, 1259, 1276, 1284, 1285, 1292, 1294, 1296, 1297, 1365, 1381, 1389, 1406, 1435, 1440, 1445, 1448, 1468, 1471, 1476, 1477, 1491, 1499, 1513, 1531, 1540, 1542, 1545], [6, 9, 19, 20, 25, 42, 61, 75, 94, 106, 107, 117, 173, 178, 183, 188, 302, 323, 360, 364, 433, 464, 474, 476, 524, 562, 600, 655, 661, 698, 747, 793, 802, 839, 865, 881, 906, 926, 957, 960, 965, 1041, 1085, 1133, 1213, 1217, 1223, 1256, 1306, 1325, 1394, 1400, 1415, 1421, 1424, 1426, 1488, 1507, 1527, 1539, 1544], [66, 92, 119, 121, 161, 224, 299, 305, 375, 447, 470, 502, 523, 563, 571, 583, 636, 660, 668, 763, 812, 813, 829, 916, 934, 981, 1038, 1097, 1109, 1111, 1113, 1122, 1127, 1149, 1168, 1229, 1238, 1301, 1369, 1430, 1437, 1456, 1469, 1482, 1523], [11, 28, 85, 193, 201, 213, 248, 442, 514, 540, 623, 645, 683, 707, 716, 721, 757, 823, 893, 914, 938, 945, 964, 974, 1053, 1056, 1089, 1139, 1142, 1145, 1172, 1177, 1191, 1195, 1201, 1216, 1307, 1449, 1470, 1475], [3, 18, 21, 34, 67, 88, 146, 217, 257, 312, 336, 346, 372, 416, 418, 432, 439, 497, 511, 531, 569, 580, 666, 694, 706, 762, 891, 924, 947, 969, 1024, 1030, 1060, 1152, 1178, 1215, 1266, 1268, 1327, 1346, 1396, 1495, 1496, 1534, 1536], [2, 48, 170, 191, 288, 321, 415, 425, 472, 630, 635, 647, 732, 744, 770, 788, 822, 849, 854, 876, 895, 920, 989, 1001, 1029, 1071, 1084, 1095, 1124, 1130, 1136, 1151, 1169, 1180, 1185, 1291, 1319, 1379, 1515], [71, 157, 197, 237, 275, 370, 414, 423, 435, 446, 455, 507, 577, 579, 682, 756, 764, 821, 837, 961, 997, 1128, 1162, 1193, 1239, 1260, 1270, 1298, 1324, 1350, 1355, 1441], [7, 130, 168, 261, 295, 494, 515, 572, 595, 608, 679, 686, 900, 930, 987, 1057, 1099, 1204, 1233, 1240, 1275, 1395, 1402, 1455, 1524], [239, 318, 337, 380, 399, 528, 574, 606, 780, 782, 784, 794, 807, 808, 847, 901, 909, 1011, 1119, 1158, 1224, 1247, 1254, 1274, 1341, 1363, 1368], [17, 57, 203, 246, 260, 291, 365, 388, 454, 471, 767, 887, 948, 970, 1043, 1077, 1148, 1335, 1356, 1366, 1506, 1521], [174, 196, 285, 362, 394, 462, 581, 667, 709, 818, 836, 952, 1279, 1413, 1416, 1417, 1459], [165, 518, 519, 520, 549, 616, 659, 680, 996, 1069, 1083, 1153, 1269, 1347, 1442], [110, 385, 436, 585, 642, 1393, 1517], [114, 133, 501, 801, 910, 940, 1295, 1305], [172, 283, 341, 468, 517, 1166], [102, 274, 361, 459, 510, 622, 884, 1498], [221, 325, 411, 546, 776, 785, 846, 943, 1032, 1557], [105, 122, 156, 159, 187, 1514], [26, 39, 139, 241, 307, 513, 980, 1261], [320, 1311, 1405, 1526], [80, 132], [62, 63, 1129], [983, 1530], [228, 267, 527, 542, 1508], [176, 300], [22, 451], [373, 935], [1190], [], [138, 1080], [], [825, 1079], [87, 535, 760], [], [701], [], [], [1546], [816], [], [], [], [629]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5D9IoBtGX6R"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvAwpD4GrJu"
      },
      "source": [
        "## Reproducibility seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p5d4mp9GDah"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import string\n",
        "import random\n",
        "def get_reproducible_seeds(name=\"ProjectLong\",nb_seeds=100):\n",
        "    # Calculate SHA-256 hash\n",
        "    sha256_hash = hashlib.sha256(name.encode()).hexdigest()\n",
        "    # Define character sets\n",
        "    digits = string.digits\n",
        "    # Use the hash to seed the random number generator\n",
        "    hash_as_int = int(sha256_hash, 16)\n",
        "    random.seed(hash_as_int)\n",
        "    # Generate a random list of seed of desired length\n",
        "    reproducibility_seeds = [random.randint(0,10000) for _ in range(nb_seeds)]\n",
        "\n",
        "    return reproducibility_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn8U0p9TGJXK"
      },
      "outputs": [],
      "source": [
        "reproducibility_seed=get_reproducible_seeds()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mzoE-MHqLa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nhxCSLNHsd9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class VariableLengthDatasetWithPosID(Dataset):\n",
        "    def __init__(self, time_series, transform=None):\n",
        "        self.times_series=time_series\n",
        "    def __len__(self):\n",
        "        return len(self.times_series)\n",
        "    def __getitem__(self, idx):\n",
        "        user_dict=self.times_series[idx]\n",
        "        return  user_dict\n",
        "\n",
        "def create_dataset(list_users,split=[0.8,0.1,0.1]):\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "  return dataset_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGgl2XnIhDQ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nke01dG-KJxO"
      },
      "outputs": [],
      "source": [
        "def collate_fn_padd(batch_dict):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "\n",
        "\n",
        "    dict_batch={key: [d[key] for d in batch_dict] for key in batch_dict[0]}\n",
        "    dict_batch[\"lengths\"] = torch.tensor([ user[\"input\"].shape[0] for user in batch_dict ])\n",
        "    if \"input\" in dict_batch:\n",
        "      dict_batch[\"input\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"input\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"month\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"month\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"day\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"day\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"hour\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"hour\"],batch_first=True,padding_value=24)\n",
        "    dict_batch[\"minute\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"minute\"],batch_first=True,padding_value=60)\n",
        "    dict_batch[\"second\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"second\"],batch_first=True,padding_value=60)\n",
        "\n",
        "    dict_batch[\"time_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"time_target\"],batch_first=True,padding_value=-1)\n",
        "    dict_batch[\"pos_id\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id\"],batch_first=True,padding_value=len(vocab))\n",
        "    dict_batch[\"pos_id_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id_target\"],batch_first=True,padding_value=len(vocab))\n",
        "    #print(dict_batch[\"input\"])\n",
        "    return dict_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Yl1E6gY8_P"
      },
      "source": [
        "## Instanciate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHpriSipY7Kz"
      },
      "outputs": [],
      "source": [
        "dataset_list=create_dataset(list_users)\n",
        "train_dataset=dataset_list[0]\n",
        "valid_dataset=dataset_list[1]\n",
        "test_dataset=dataset_list[2]\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=64,collate_fn=collate_fn_padd,shuffle=True)\n",
        "valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HodJbvKeMe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptycyS7FWE4b"
      },
      "source": [
        "## Transformer Encoder followed by LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oeWr0HDhJfo"
      },
      "source": [
        "### transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_bACPajeQx"
      },
      "outputs": [],
      "source": [
        "def get_mask(bath_size,sequence_length,lengths,device):\n",
        "  mask=torch.zeros(bath_size,sequence_length).to(device)\n",
        "  for i, length in enumerate(lengths):\n",
        "    mask[i,length:]=1\n",
        "  return mask.bool()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsaggvjghDsq"
      },
      "source": [
        "#### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yFXZxqeHMwi"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class VanillaPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = (x.transpose(0,1) + self.pe[:x.transpose(0,1).size(0)]).transpose(0,1)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX-kk1ScG-_n"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.positional_embedding=nn.Embedding(num_embeddings=max_len,embedding_dim= d_model)\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(self.parameters()).device\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size,seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x += self.positional_embedding(torch.arange(0,x.shape[1]).to(self.device))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzKYvfaWG6cH"
      },
      "outputs": [],
      "source": [
        "def get_PositionalEncoding(d_model: int, dropout: float = 0.1, max_len: int = 2000, learnable=False):\n",
        "  if learnable:\n",
        "    return LearnablePositionalEncoding(d_model, dropout, max_len)\n",
        "  else:\n",
        "    return VanillaPositionalEncoding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-pr0W6xhOkZ"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZQnLdN4UxY"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,  dropout=dropout, batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only=False,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,batch_first=batch_first)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers,nhead,dropout,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCkeqmkhRnT"
      },
      "source": [
        "### feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmqQtP0UhZqg"
      },
      "outputs": [],
      "source": [
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=embedding_dim)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=embedding_dim)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=embedding_dim)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "\n",
        "  def forward(self,dict_batch):\n",
        "    embedding= self.month_embedding(dict_batch['month'])\n",
        "    embedding=+ self.day_embedding(dict_batch['day'])\n",
        "    embedding=+ self.hour_embedding(dict_batch['hour'])\n",
        "    embedding=+ self.minute_embedding(dict_batch['minute'])\n",
        "    embedding=+ self.second_embedding(dict_batch['second'])\n",
        "    return self.dropout(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCp5Pz6uy-FG"
      },
      "outputs": [],
      "source": [
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bbp1dVXWQs3"
      },
      "source": [
        "#### graph_deepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW174iJrhFC",
        "outputId": "6ad9be4e-f4cd-40d0-a6fb-0cd7c1b561d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpysal\n",
            "  Downloading libpysal-4.9.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.12.3)\n",
            "Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from libpysal) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.25.2)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (23.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.11.4)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.10->libpysal) (2.5)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (1.9.5)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (67.7.2)\n",
            "Installing collected packages: libpysal\n",
            "Successfully installed libpysal-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install libpysal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQvAPDZyWNtg",
        "outputId": "c0fe4dde-5968-49ea-c87f-398c269f8a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=3049d4ae756209f0ed6d2a1856d71ae9d30d88b08233d9124d9ba4a0cbc02455\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdSTBOc3sKsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, hidden_dim1, hidden_dim2, output_dim,vocab,dropout,device):\n",
        "    super(GCN, self).__init__()\n",
        "    net=get_net(vocab)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "    self.conv1 = GCNConv(2, hidden_dim1)\n",
        "    self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
        "    self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.conv1(self.coordinates, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "    x = self.conv2(x, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv3(x, self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozXR4sW0W0Y"
      },
      "source": [
        " #### Combine feature embeddng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6mU1qWOjRP3"
      },
      "outputs": [],
      "source": [
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+use_gcn\n",
        "    self.concatenate_features=concatenate_features\n",
        "    self.embedding_dim=d_model\n",
        "    self.keep_input_positions=keep_input_positions\n",
        "    if keep_input_positions:\n",
        "      self.embedding_dim=self.embedding_dim-2\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,nb_of_pos_ids,dropout),TimeStampEmbedding(self.embedding_dim,dropout)]\n",
        "    if use_gcn:\n",
        "      list_feature_embedding.append(GCN(hidden_dim1, hidden_dim2, self.embedding_dim, vocab, dropout,device))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "    if self.keep_input_positions:\n",
        "      embedding=torch.cat((dic_batch[\"input\"],embedding),dim=2)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svMRI0xeji-7"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSt_zuJRKgBh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(F.relu(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.layer_normalisation:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,d_model,nb_of_pos_ids,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,num_layers_transformer,encoder_only,nhead,learnable_pos_encoding,new_station_binary_classification,use_gcn,vocab,hidden_dim1, hidden_dim2,max_len,dropout,batch_first,concatenate_features,keep_input_positions,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.feature_embedding=Feature_embedding(d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device)\n",
        "\n",
        "    self.num_layers_transformer=num_layers_transformer\n",
        "    if num_layers_transformer>0:\n",
        "      self.pos_encoder = get_PositionalEncoding(d_model, dropout, max_len,learnable_pos_encoding)\n",
        "      self.transformer_model=get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,nhead,dropout,batch_first)\n",
        "\n",
        "    self.num_layers_lstm=num_layers_lstm\n",
        "    if num_layers_lstm>0:\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(d_model,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first) for layer in range(num_layers_lstm)])\n",
        "    self.linear_reg=nn.Linear(d_model,output_regression_size)\n",
        "    self.classifier=nn.Linear(d_model,output_classfication_size)\n",
        "\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.binary_classifier=nn.Linear(d_model,1)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.num_layers_transformer>0:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.num_layers_lstm>0:\n",
        "      if self.num_layers_transformer>0:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.num_layers_lstm>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=F.relu(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(torch.exp(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.new_station_binary_classification:\n",
        "      out[\"new_station\"]=  torch.nn.utils.rnn.PackedSequence( self.binary_classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baselines"
      ],
      "metadata": {
        "id": "zZpbR8rG8kBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class  Baseline_model(nn.Module):\n",
        "  def __init__(self,nb_of_pos_ids):\n",
        "    super().__init__()\n",
        "    self.nb_of_pos_ids=nb_of_pos_ids\n",
        "  def forward(self,dic_batch,reg):\n",
        "    out={}\n",
        "    out[\"next_station\"]=  torch.nn.utils.rnn.pack_padded_sequence(F.one_hot(dic_batch[\"pos_id\"],self.nb_of_pos_ids).float(), lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    return out"
      ],
      "metadata": {
        "id": "jn0xR-ME8tRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Baseline_model(len(vocab)+1)\n",
        "criterion=Total_loss(False)\n",
        "evaluate(model,valid_dataloader,criterion,device,reg=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYH5OMnmELSa",
        "outputId": "fb668202-96cb-41cb-9f16-df7516a96334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classification': 0.0021053161556483374,\n",
              " 'total': 0.0021053161556483374,\n",
              " 'acc': 0.044894637279486165}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28s2GCFETdYS"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujoc4c2mQh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "\n",
        "  def forward(self, out, target_pos_ids, index_training_element):\n",
        "    loss_classification=self.criterion(out.data[index_training_element],target_pos_ids.data[index_training_element])\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,out,dict_batch):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Loss_new_station_binary_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion =  nn.BCEWithLogitsLoss()\n",
        "  def forward(self,out,target):\n",
        "    loss_classification=self.criterion(out.data.squeeze(),target.float())\n",
        "    return loss_classification\n",
        "\n",
        "def get_repetition_labels(target_pos_ids,pos_ids):\n",
        "\n",
        "  return (target_pos_ids.data==pos_ids.data).type(torch.LongTensor)\n",
        "\n",
        "def upsampling_strategy(target, epoch, epochs_new_station_only,pourcentage_of_repeat_training_elment):\n",
        "\n",
        "    index_non_repeat =(target==0).nonzero()\n",
        "    coeff=pourcentage_of_repeat_training_elment/(1-pourcentage_of_repeat_training_elment)\n",
        "    index_for_training= index_non_repeat\n",
        "    if epoch>= epochs_new_station_only:\n",
        "      index_repeat = target.nonzero().squeeze()\n",
        "      nb_non_repeat= index_non_repeat.shape[0]\n",
        "      slice_repeat=index_repeat[torch.randperm(index_repeat.shape[0])[:int(coeff*nb_non_repeat)]].squeeze()\n",
        "      index_for_training = torch.cat((index_non_repeat.squeeze(),slice_repeat))\n",
        "    return index_for_training.squeeze()\n",
        "\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self,new_station_binary_classification) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.loss_new_station_binary_classification=Loss_new_station_binary_classification()\n",
        "\n",
        "  def forward(self, out, dict_batch, upsampling,upsampling_strategy, reg=False):\n",
        "    loss={}\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    if self.new_station_binary_classification or upsampling:\n",
        "      pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "      target=get_repetition_labels(target_pos_ids,pos_ids)\n",
        "    else:\n",
        "      pos_ids=None\n",
        "      target=None\n",
        "\n",
        "    if upsampling:\n",
        "      index_training_element=upsampling_strategy(target)\n",
        "    else:\n",
        "      index_training_element=torch.arange(0,target_pos_ids.data.shape[0])\n",
        "\n",
        "    loss[\"classification\"]=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids,index_training_element)\n",
        "    loss[\"total\"]=loss[\"classification\"]\n",
        "    if self.new_station_binary_classification:\n",
        "      loss[\"new_station\"]=self.loss_new_station_binary_classification(out[\"new_station\"],target)\n",
        "      loss[\"total\"]+=loss[\"new_station\"]\n",
        "\n",
        "    if reg:\n",
        "      loss[\"time_regression\"]=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss[\"total\"]+=loss[\"time_regression\"]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHCyYC32ToKU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title evaluation\n",
        "from torch import autocast\n",
        "def evaluate(model,dataloader,upsampling,criterion,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        valid_result=criterion(out,dict_batch,upsampling,None,reg=reg)\n",
        "        valid_results=get_sum_valid_results(valid_results,valid_result)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    valid_results=get_mean_valid_results(valid_results,nb_points)\n",
        "    valid_results[\"acc\"]=acc/nb_points\n",
        "\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLu25E-eTcbT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title training\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "def train(\n",
        "          epochs_classifcation_only,\n",
        "          epochs_complete_problem,\n",
        "          input_size,\n",
        "          num_heads,\n",
        "          d_model,\n",
        "          nb_of_pos_ids,\n",
        "          num_layers_lstm,\n",
        "          lstm_layer_with_perceptron,\n",
        "          lstm_layer_with_layer_norm,\n",
        "          num_layers_transformer,\n",
        "          encoder_only,\n",
        "          output_regression_size,\n",
        "          output_classfication_size,\n",
        "          nb_batchs,\n",
        "          dropout,\n",
        "          max_len,\n",
        "          weight_decay,\n",
        "          lr,\n",
        "          learnable_pos_encoding,\n",
        "          new_station_binary_classification,\n",
        "          use_gcn,\n",
        "          vocab,hidden_dim1, hidden_dim2,\n",
        "          batch_first,\n",
        "          concatenate_features,\n",
        "          keep_input_positions,\n",
        "          upsampling,\n",
        "          upsampling_strategy,\n",
        "          epochs_new_station_only,\n",
        "          pourcentage_of_repeat_training_elment,\n",
        "          save_best_model,\n",
        "          path_best_model,\n",
        "          batch_size,\n",
        "          device):\n",
        "\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n",
        "                                         nb_of_pos_ids=nb_of_pos_ids,\n",
        "                                         output_regression_size=output_regression_size,\n",
        "                                         output_classfication_size=output_classfication_size,\n",
        "                                         num_layers_lstm=num_layers_lstm,\n",
        "                                         lstm_layer_with_perceptron=lstm_layer_with_perceptron,\n",
        "                                         lstm_layer_with_layer_norm=lstm_layer_with_perceptron,\n",
        "                                         num_layers_transformer=num_layers_transformer,\n",
        "                                         encoder_only=encoder_only,\n",
        "                                         nhead=num_heads,\n",
        "                                         learnable_pos_encoding=learnable_pos_encoding,\n",
        "                                         new_station_binary_classification=new_station_binary_classification,\n",
        "                                         use_gcn=use_gcn,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=hidden_dim1,\n",
        "                                         hidden_dim2=hidden_dim2,\n",
        "                                         max_len=max_len,\n",
        "                                         dropout=dropout,\n",
        "                                         batch_first = batch_first,\n",
        "                                         concatenate_features = concatenate_features,\n",
        "                                         keep_input_positions = keep_input_positions,device=device\n",
        "                                         ).to(device)\n",
        "  if save_best_model:\n",
        "    os.makedirs(path_best_model,exist_ok =True)\n",
        "  optimizer_encoder = optim.Adam( model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  criterion = Total_loss( new_station_binary_classification = new_station_binary_classification)\n",
        "  train_losses, valid_results = {},{}\n",
        "  best_results={}\n",
        "  for epoch in range(epochs):\n",
        "    reg=epoch >= epochs_classifcation_only\n",
        "    epoch_losses={}\n",
        "    model.train()\n",
        "    i=0\n",
        "    for dict_batch in train_dataloader:\n",
        "      optimizer_encoder.zero_grad()\n",
        "      i+=1\n",
        "      if i>=nb_batchs:\n",
        "        break\n",
        "      dict_batch=set_dic_to(dict_batch,device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch, reg)\n",
        "        loss=criterion(out, dict_batch,upsampling,lambda target: upsampling_strategy(target,epoch,epochs_new_station_only,pourcentage_of_repeat_training_elment) ,reg)\n",
        "        loss[\"total\"].backward()\n",
        "        optimizer_encoder.step()\n",
        "      epoch_losses=update_epoch_losses(epoch_losses,loss)\n",
        "      dict_batch.clear()\n",
        "      loss.clear()\n",
        "      out.clear()\n",
        "      del out, loss,dict_batch\n",
        "    epoch_loss=get_epoch_loss(epoch_losses,batch_size)\n",
        "    train_losses=update_train_losses(train_losses,epoch_loss,epoch)\n",
        "    valid_result = evaluate(model,valid_dataloader,upsampling,criterion,device)\n",
        "    best_results = update_best(model,valid_result,best_results,save_best_model,path_best_model)\n",
        "    valid_results = update_valid_results(valid_results,valid_result)\n",
        "    print_results(epoch_loss,valid_result,epoch)\n",
        "\n",
        "  return best_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "\n",
        "def set_dic_to(dict_batch,device):\n",
        "  for key in dict_batch:\n",
        "    if key!=\"lengths\":\n",
        "      dict_batch[key]=dict_batch[key].to(device)\n",
        "  return dict_batch\n",
        "\n",
        "def is_better(valid_result,best_result,key):\n",
        "  match key:\n",
        "    case \"acc\":\n",
        "      return valid_result>best_result\n",
        "    case _:\n",
        "      return valid_result<best_result\n",
        "\n",
        "def update_best(model,valid_result,best_results,save_best_model,path_best_model):\n",
        "  if best_results:\n",
        "    for key in valid_result:\n",
        "      if is_better(valid_result[key],best_results[key],key):\n",
        "        best_results[key]=valid_result[key]\n",
        "        if save_best_model:\n",
        "          save_model(model,path_best_model,key)\n",
        "  else:\n",
        "    for key in valid_result:\n",
        "      best_results[key]=valid_result[key]\n",
        "      if save_best_model:\n",
        "        save_model(model,path_best_model,key)\n",
        "  return best_results\n",
        "\n",
        "def save_model(model,path_best_model,key):\n",
        "  path=os.path.join(path_best_model,key)\n",
        "  torch.save(model.state_dict(), path+\".pth\")\n",
        "\n",
        "def get_sum_valid_results(valid_result,valid_result_batch):\n",
        "  if valid_result:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]+=valid_result_batch[key].item()\n",
        "  else:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]=valid_result_batch[key].item()\n",
        "  return valid_result\n",
        "\n",
        "def get_mean_valid_results(sum_valid_result,nb_element):\n",
        "  for key in sum_valid_result:\n",
        "    sum_valid_result[key]/=nb_element\n",
        "\n",
        "  return sum_valid_result\n",
        "\n",
        "def update_epoch_losses(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key].item())\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key].item()]\n",
        "  return dict_of_list\n",
        "\n",
        "def update_valid_results(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key])\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key]]\n",
        "  return dict_of_list\n",
        "\n",
        "def get_epoch_loss(epoch_losses,batch_size):\n",
        "\n",
        "  epoch_loss={}\n",
        "  for key in epoch_losses:\n",
        "    epoch_loss[key]=np.array(epoch_losses[key]).mean()/batch_size\n",
        "  return epoch_loss\n",
        "\n",
        "def print_results(epoch_loss,valid_result,epoch):\n",
        "\n",
        "  print(\"\\nepoch: \",epoch)\n",
        "  print(\"train :\", end=\"\\t\")\n",
        "  for key in epoch_loss:\n",
        "    print(key,epoch_loss[key], end=\"\\t\")\n",
        "  print(\"\\nvalid :\", end=\"\\t\")\n",
        "  for key in valid_result:\n",
        "    print(key,valid_result[key], end=\"\\t\")\n",
        "\n",
        "def update_train_losses(train_losses,epoch_loss,epoch):\n",
        "\n",
        "  if train_losses:\n",
        "    for key in epoch_loss:\n",
        "      if key in train_losses:\n",
        "        train_losses[key].append(epoch_loss[key])\n",
        "      else:\n",
        "        train_losses[key]=[float('nan')]*(epoch+1)+[epoch_loss[key]]\n",
        "  else:\n",
        "    for key in epoch_loss:\n",
        "      train_losses[key]=[epoch_loss[key]]\n",
        "  return train_losses"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8OL2WGr7cGZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4FEU_h1YtX"
      },
      "source": [
        "## Instance of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "HL0AZ-YJChyE",
        "outputId": "ccfdf139-20a5-452c-fe6c-6e36be5a95fd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-eb8eac0884b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Titre par défaut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model=train(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs_complete_problem\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-d40eb1db95f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs_classifcation_only, epochs_complete_problem, input_size, num_heads, d_model, nb_of_pos_ids, num_layers_lstm, lstm_layer_with_perceptron, lstm_layer_with_layer_norm, num_layers_transformer, encoder_only, output_regression_size, output_classfication_size, nb_batchs, dropout, max_len, weight_decay, lr, learnable_pos_encoding, new_station_binary_classification, use_gcn, vocab, hidden_dim1, hidden_dim2, batch_first, concatenate_features, keep_input_positions, upsampling, upsampling_strategy, epochs_new_station_only, pourcentage_of_repeat_training_elment, save_best_model, path_best_model, batch_size, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mdict_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_dic_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupsampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupsampling_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_new_station_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpourcentage_of_repeat_training_elment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2dbafaf65fc1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dic_batch, reg)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmask_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_lstm\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_transformer\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-a8f6ed0bd9ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       return self.transformer(x,\n\u001b[0m\u001b[1;32m      7\u001b[0m                        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                        \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    204\u001b[0m         memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask,\n\u001b[1;32m    205\u001b[0m                               is_causal=src_is_causal)\n\u001b[0;32m--> 206\u001b[0;31m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[0m\u001b[1;32m    207\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                               \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    461\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    863\u001b[0m     def _mha_block(self, x: Tensor, mem: Tensor,\n\u001b[1;32m    864\u001b[0m                    attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 865\u001b[0;31m         x = self.multihead_attn(x, mem, mem,\n\u001b[0m\u001b[1;32m    866\u001b[0m                                 \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5438\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5440\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5441\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# @title Titre par défaut\n",
        "model=train(\n",
        "          epochs_classifcation_only=100,\n",
        "          epochs_complete_problem =100,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=1200,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=2,\n",
        "          lstm_layer_with_perceptron=False,\n",
        "          lstm_layer_with_layer_norm=False,\n",
        "          num_layers_transformer=6,\n",
        "          encoder_only=False,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=100,\n",
        "          dropout=0.1,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=1e-3,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=False,\n",
        "          use_gcn=False,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          batch_first= True,\n",
        "          concatenate_features = True,\n",
        "          keep_input_positions = False,\n",
        "          upsampling=False,\n",
        "          upsampling_strategy=upsampling_strategy,\n",
        "          epochs_new_station_only=0,\n",
        "          pourcentage_of_repeat_training_elment=0.1,\n",
        "          save_best_model=True,\n",
        "          path_best_model=\"test_0.5\",\n",
        "          device=device,\n",
        "          batch_size=64\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_repeat(model,dataloader,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    repeat=0\n",
        "    not_repeat=0\n",
        "    correct_not_repeat=0\n",
        "    correct_repeat=0\n",
        "    incorrect_not_repeat_as_repeat=0\n",
        "    incorrect_not_repeat=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "        pred=out[\"next_station\"].data.argmax(dim=1)\n",
        "        pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        for i in range(len(target_pos_ids.data)):\n",
        "          if target_pos_ids.data[i]==pos_ids.data[i]:\n",
        "            repeat+=1\n",
        "\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_repeat+=1\n",
        "          else:\n",
        "            not_repeat+=1\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_not_repeat+=1\n",
        "            if target_pos_ids.data[i]!=pred[i]:\n",
        "              incorrect_not_repeat+=1\n",
        "\n",
        "          if pred[i]==pos_ids.data[i] and target_pos_ids.data[i]!=pos_ids.data[i]:\n",
        "            incorrect_not_repeat_as_repeat+=1\n",
        "    print(nb_points,\"repeat: \",repeat,\" not_repeat: \",not_repeat,\" correct_repeat/repeat: \",correct_repeat/repeat,\" correct_not_repeat/not_repeat: \",correct_not_repeat/not_repeat,incorrect_not_repeat_as_repeat/incorrect_not_repeat)\n",
        "    return valid_results"
      ],
      "metadata": {
        "id": "HClPVCKb59Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=768,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=0,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqswVm-Flq",
        "outputId": "4f8a2d24-1d1d-4d5e-da84-30957736cbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6509191238701378  correct_not_repeat/not_repeat:  0.2762021385930769 0.4746223564954683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=12,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufqL-c1e27Vm",
        "outputId": "5e436410-45a5-4c71-da92-f5b40a030112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.572683570872406  correct_not_repeat/not_repeat:  0.24545712973693992 0.38929461542920074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=10,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqLMoo9hpA82",
        "outputId": "cc34ad4e-47d5-4efd-d99e-5c06837a607e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5703307491790515  correct_not_repeat/not_repeat:  0.22293411471430757 0.40792435839711844\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYc14HpTkU2z",
        "outputId": "36544e2a-7d0b-4787-df90-4160a18680cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.44894038389925184  correct_not_repeat/not_repeat:  0.29502962979160746 0.2873538261112317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "tI1PRax29Fqw",
        "outputId": "d9d5ea9f-4378-49ec-be25-2fe2ce5e37f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d5fdb97ca0b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-17df1714a1da>\u001b[0m in \u001b[0;36mevaluate_repeat\u001b[0;34m(model, dataloader, device, reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpos_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrepeat\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucdMSE-0T3-s",
        "outputId": "4f766291-2702-431e-d059-241a1a1ccfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7462507193879279  correct_not_repeat/not_repeat:  0.23080623646979073 0.5669490561746645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEt-Bw9FgvDq",
        "outputId": "70b7fed7-4cc6-495a-de11-57721bf5d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4839957344527574  correct_not_repeat/not_repeat:  0.35011261507511315 0.2974764468371467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaphByRZOIo",
        "outputId": "13cfb863-5bb6-43a5-f104-7ce207f93c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6290835844138258  correct_not_repeat/not_repeat:  0.26852681988148086 0.44345460524349045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGE1rXtIIEpE",
        "outputId": "775055f6-3dae-4158-fdc4-078b4adba7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5781001387995531  correct_not_repeat/not_repeat:  0.3046073779274453 0.4137291280148423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=3,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "QPBs0PUrKUra",
        "outputId": "196fe7cd-25e3-479d-8c4b-b6a83ccd3977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.linear1.weight\", \"transformer_model.transformer.decoder.layers.5.linear1.bias\", \"transformer_model.transformer.decoder.layers.5.linear2.weight\", \"transformer_model.transformer.decoder.layers.5.linear2.bias\", \"transformer_model.transformer.decoder.layers.5.norm1.weight\", \"transformer_model.transformer.decoder.layers.5.norm1.bias\", \"transformer_model.transformer.decoder.layers.5.norm2.weight\", \"transformer_model.transformer.decoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.norm3.weight\", \"transformer_model.transformer.decoder.layers.5.norm3.bias\", \"transformer_lstm__list.2.layer_normalisation.weight\", \"transformer_lstm__list.2.layer_normalisation.bias\", \"transformer_lstm__list.2.lstm.weight_ih_l0\", \"transformer_lstm__list.2.lstm.weight_hh_l0\", \"transformer_lstm__list.2.lstm.bias_ih_l0\", \"transformer_lstm__list.2.lstm.bias_hh_l0\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.bias\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-10bf83ce1050>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                          ).to(device)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cIy9KQFjv8",
        "outputId": "3ea5d475-a56f-4118-d568-2c020806ed64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8564101696062832  correct_not_repeat/not_repeat:  0.09422492401215805 0.8021341316208778\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.643730131126935, 2.2387092113494873, 4.385555267333984)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06MhCDWcaF9",
        "outputId": "4a84b9d7-b594-41b9-bca7-acacf62010b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6269508107925116  correct_not_repeat/not_repeat:  0.24020904856661782 0.4275024463247568\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5190344566683142, 3.329756021499634, 2.0473709106445312)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonRxU5b96oc",
        "outputId": "baba1be5-861e-44f1-95fb-808d9cd6b5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7019025694844104  correct_not_repeat/not_repeat:  0.2555815529946863 0.5174044590664747\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5773612306040138, 2.5278093814849854, 2.7385761737823486)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz2zFYSsAroq",
        "outputId": "10b15668-6b3b-44d9-eb4f-c28a02cbd09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7179745421307424  correct_not_repeat/not_repeat:  0.2436203013273272 0.5562012142237641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5856108172094187, 2.1441447734832764, 1.2037302255630493)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlGehwEQtfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378de12f-1ebf-44f6-b4b6-e57d19a2bb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8789227800534886  correct_not_repeat/not_repeat:  0.11254947409853272 0.8136211314803864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6650741059388481, 1.82258939743042, 2.1341636180877686)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejz7LOwNTZ-k",
        "outputId": "5d16ae15-48ea-4ff8-a2e7-718c1be73c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8187565591252243  correct_not_repeat/not_repeat:  0.1462683956178522 0.7372573126376722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6311055788439596, 2.027265787124634, 2.6414260864257812)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9yHv6HjJ-N",
        "outputId": "ba6f7b9e-b88f-407c-d22b-1e6913f0e56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.3789143166661024  correct_not_repeat/not_repeat:  0.3284642802475345 0.28316509280364704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3648367472709855, 2.700303077697754, 3.602348566055298)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLWBTILrc94",
        "outputId": "c04e1913-1724-4df4-e563-1801d24ca813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4562527506009005  correct_not_repeat/not_repeat:  0.30379829874702063 0.3240153275959545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4137118868488654, 2.806699752807617, 3.5753602981567383)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtscCJTUkD0R",
        "outputId": "d2e3c16a-065e-4503-fa5c-4f991dea2ff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7411692032"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if None:\n",
        "  print(1)"
      ],
      "metadata": {
        "id": "42I-z2ls4-Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree classifier"
      ],
      "metadata": {
        "id": "wIjYebkGJ8i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list_users"
      ],
      "metadata": {
        "id": "iec_zdeKwrBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_inputs,pos_id_targets=inputs_for_classification(list_users, pos_id=True, user_id=True, date_time=True, time_to_end=True, time_to_end_and_target=False, delete_data_less_nb_min=True, nb_min=5)"
      ],
      "metadata": {
        "id": "MAQ_RqjZz4Mo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs = inputs_outputs_for_classification(all_inputs,pos_id_targets)\n",
        "# outputs = get_x_y_target_from_outputs(outputs, vocab)\n",
        "print(inputs[0])\n",
        "outputs[0]"
      ],
      "metadata": {
        "id": "PjrSYgChroTj",
        "outputId": "6e1d0269-8eb2-4434-ea11-f0efefe00a62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of the dataset\n",
            "37495 37495\n",
            "tensor([ 0.0000e+00, -3.0781e-02,  6.8448e-04,  1.4890e+03,  8.0000e+00,\n",
            "         3.0000e+00,  1.9000e+01,  2.2000e+01,  5.0000e+01,  6.8017e-02])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1517)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def get_correlation(inputs):\n",
        "    df = pd.DataFrame(inputs)\n",
        "    corr = df.corr()\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
        "    return corr\n",
        "\n",
        "corr = get_correlation(inputs_with_pos_id_and_time_to_end)"
      ],
      "metadata": {
        "id": "kQqvIv24gzeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "ExA8oZy4KjUW"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do cross validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.15, random_state=1)"
      ],
      "metadata": {
        "id": "DCa_zL0Oe3hI"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#got best decision tree classifier\n",
        "# 'criterion': ['gini', 'entropy'],\n",
        "parameters = { 'criterion': ['gini', 'entropy'], 'max_depth': [ i for i in range(5,50,10)], 'min_samples_split': [ i for i in range(5,40,10)], 'splitter':['best', 'random']}\n",
        "best_score = -1\n",
        "# for criterion in parameters['criterion']:\n",
        "for criterion in parameters['criterion']:\n",
        "  for max_depth in parameters['max_depth']:\n",
        "    for min_samples_split in parameters['min_samples_split']:\n",
        "      for splitter in parameters['splitter']:\n",
        "        clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, splitter=splitter, random_state=12)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        score = f1_score(y_test, y_pred, average='weighted')\n",
        "        print(score)\n",
        "        if score > best_score:\n",
        "          best_score = score\n",
        "          best_parameters = {'criterion': criterion, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'splitter':splitter}"
      ],
      "metadata": {
        "id": "gLpUTDQXKkg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86fd5017-63ed-401c-a7af-84d656bbed9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.012888181181372048\n",
            "0.0057894411189085065\n",
            "0.012795809276115974\n",
            "0.0057894411189085065\n",
            "0.012795809276115974\n",
            "0.0057894411189085065\n",
            "0.012795809276115974\n",
            "0.004799557472474284\n",
            "0.28010130216594264\n",
            "0.2909129173994079\n",
            "0.29206027223922343\n",
            "0.3041865918982461\n",
            "0.289565755612811\n",
            "0.2603824136850747\n",
            "0.2808280084961187\n",
            "0.265540320395725\n",
            "0.2963746767090376\n",
            "0.3430114602967176\n",
            "0.3228333618323077\n",
            "0.3586057912282696\n",
            "0.32229223567756354\n",
            "0.3331944395303213\n",
            "0.30915159792539926\n",
            "0.3060297631712142\n",
            "0.29782592461670554\n",
            "0.3441284275936861\n",
            "0.3226821994921924\n",
            "0.35731509457843585\n",
            "0.3216476405022953\n",
            "0.3292679040950715\n",
            "0.308774695725674\n",
            "0.3060297631712142\n",
            "0.29873257453762464\n",
            "0.3441284275936861\n",
            "0.3226462278372646\n",
            "0.35731509457843585\n",
            "0.3216476405022953\n",
            "0.3292679040950715\n",
            "0.308774695725674\n",
            "0.3060297631712142\n",
            "0.009955124043873683\n",
            "0.005113396298172373\n",
            "0.009955124043873683\n",
            "0.005113396298172373\n",
            "0.009955124043873683\n",
            "0.005113396298172373\n",
            "0.009955124043873683\n",
            "0.005113396298172373\n",
            "0.31044853224246877\n",
            "0.29995179069073213\n",
            "0.3337235648315884\n",
            "0.3176905465250616\n",
            "0.3212522461369917\n",
            "0.2779671039930858\n",
            "0.30204074233932604\n",
            "0.26694837361487195\n",
            "0.3072546527519418\n",
            "0.31902951671976787\n",
            "0.3324254767768645\n",
            "0.31640219854710955\n",
            "0.32130921649065675\n",
            "0.31843932007126335\n",
            "0.3018711793948153\n",
            "0.26514198227075325\n",
            "0.3072546527519418\n",
            "0.32221875335736594\n",
            "0.3324254767768645\n",
            "0.31640219854710955\n",
            "0.32130921649065675\n",
            "0.31843932007126335\n",
            "0.3018711793948153\n",
            "0.26514198227075325\n",
            "0.3072546527519418\n",
            "0.32221875335736594\n",
            "0.3324254767768645\n",
            "0.31640219854710955\n",
            "0.32130921649065675\n",
            "0.31843932007126335\n",
            "0.3018711793948153\n",
            "0.26514198227075325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "6OCspM3UK62N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c301192f-f042-4ab3-875b-a4d19fe3181d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score:  0.3591406742764203\n",
            "{'criterion': 'gini', 'max_depth': 35, 'min_samples_split': 15, 'splitter': 'random'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross score:  [0.27902161 0.26918597 0.27203313 0.25174735 0.24851152]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = { 'n_estimators': [ i for i in range(50,150,30)], 'max_depth': [ i for i in range(10,30,10)], 'min_samples_split': [ i for i in range(20,40,10)]}\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf = GridSearchCV(clf, parameters, cv=5)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_index_)\n"
      ],
      "metadata": {
        "id": "wVnwYXsOe_3L",
        "outputId": "b3b36769-3aa9-415d-c7b7-c0ecaa48df27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Créer un modèle d'arbre de décision\n",
        "model = RandomForestClassifier(n_estimators=50, min_samples_leaf = 5, max_features = 0.7, random_state=42)\n",
        "# model = DecisionTreeClassifier(criterion='entropy')\n",
        "# model = SVC(random_state=42)\n",
        "# Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Exactitude :\", accuracy)\n",
        "print(model.get_params())\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "6lIeX2pDQx9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3931cb5c-a781-490d-823c-181ddaf2f6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exactitude : 0.38708389877863814\n",
            "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 0.7, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 5, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross score:  [0.38885194 0.38783928 0.39169771 0.3954231  0.39449175]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install catboost\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Créer un modèle d'arbre de décision\n",
        "# model = RandomForestClassifier(n_estimators=50, min_samples_leaf = 5, max_features = 0.7, random_state=42)\n",
        "model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=10)\n",
        "# model = SVC(random_state=42)\n",
        "# Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Exactitude :\", accuracy)\n",
        "print(model.get_params())\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a_EcGZkqiFK",
        "outputId": "ff150a08-225f-4a57-8c1c-907cd628a34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "inputs = [inp.tolist() for inp in inputs]\n",
        "outputs = [inp.tolist() for inp in outputs]\n",
        "#Do cross validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.25, random_state=1)\n",
        "# Créer un modèle d'arbre de décision\n",
        "# model = KNeighborsClassifier(n_neighbors=5, metric='l2')\n",
        "# model = DecisionTreeClassifier(criterion='entropy')\n",
        "model = CatBoostClassifier(iterations=50, learning_rate=0.1, loss_function='MultiClass')\n",
        "# Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Exactitude :\", accuracy)\n",
        "print(model.get_params())\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUXy-DJKpyPx",
        "outputId": "c9670685-cd2a-47e3-b00e-a782fc7b5e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 7.2019131\ttotal: 3m 33s\tremaining: 2h 54m 33s\n",
            "1:\tlearn: 6.8811740\ttotal: 7m 2s\tremaining: 2h 48m 54s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOmBJAknRSHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary with the number of data for each class\n",
        "class_count={}\n",
        "print(vocab)\n",
        "for pos_id in pos_ids:\n",
        "  for id in pos_id:\n",
        "    if id.item() in class_count:\n",
        "      class_count[id.item()]+=1\n",
        "    else:\n",
        "      class_count[id.item()]=1\n",
        "print(class_count)\n",
        "print(len(class_count))\n",
        "print(max(class_count.keys()))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#See the tendency of the number of connections for each class\n",
        "print(\"min number of connections for each class\"\n",
        "      ,min(class_count.values())\n",
        "      ,min(class_count,key=class_count.get))\n",
        "print(\"max number of connections for each class\"\n",
        "      ,max(class_count.values())\n",
        "      ,max(class_count,key=class_count.get))\n",
        "\n",
        "plt.plot(list(class_count.values()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2UOJ4O63xDu",
        "outputId": "3f5b1046-7e58-44a9-c9f7-d557fa6b5586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(30.935314, 121.733322): 1, (31.144812, 121.119606): 2, (31.381763, 121.301878): 3, (31.056648, 121.404326): 4, (31.100034, 121.663489): 5, (31.040378, 121.255563): 6, (31.231508, 121.577691): 7, (31.036429, 121.324198): 8, (31.33775, 121.306733): 9, (31.298668, 121.541944): 10, (31.187254, 121.536259): 11, (30.889377, 121.176142): 12, (29.526266, 119.910488): 13, (31.327319, 121.462047): 14, (31.113744, 121.341878): 15, (31.250826, 121.578689): 16, (31.338059, 121.289933): 17, (31.349748, 121.506246): 18, (31.246059, 121.362706): 19, (31.414397, 121.481621): 20, (31.078791, 121.422082): 21, (31.129955, 121.336848): 22, (31.210033, 121.405714): 23, (31.355465, 121.412867): 24, (31.254534, 121.34772): 25, (31.202912, 121.712449): 26, (31.319806, 121.526248): 27, (31.282271, 121.524511): 28, (31.215246, 121.604329): 29, (31.025227, 121.44251): 30, (30.808567, 121.29074): 31, (34.638922, 119.467017): 32, (31.094143, 121.488864): 33, (31.242157, 121.568785): 34, (31.319501, 121.584673): 35, (31.247519, 121.322265): 36, (31.300084, 121.504034): 37, (31.318646, 121.53899): 38, (31.021462, 121.321255): 39, (31.255582, 121.363523): 40, (31.285741, 121.626244): 41, (31.05815, 120.970188): 42, (41.677262, 125.960124): 43, (31.263678, 121.468663): 44, (31.273451, 121.681542): 45, (31.301602, 121.487289): 46, (31.116021, 121.287621): 47, (31.205839, 121.462046): 48, (31.140384, 121.620758): 49, (31.387987, 121.495216): 50, (31.082701, 121.448694): 51, (31.114131, 121.394475): 52, (31.134254, 121.280533): 53, (31.152157, 121.514858): 54, (31.420016, 121.44113): 55, (30.716187, 121.349498): 56, (31.124664, 121.323455): 57, (30.866289, 121.103027): 58, (30.914669, 121.471638): 59, (31.215664, 121.46829): 60, (31.239025, 121.551622): 61, (31.174661, 121.52822): 62, (31.031459, 121.300841): 63, (31.066368, 121.2082): 64, (31.158843, 121.127268): 65, (30.898595, 121.030128): 66, (31.242627, 121.730079): 67, (31.230933, 121.347295): 68, (31.253601, 121.4782): 69, (31.117844, 121.402074): 70, (31.232877, 121.48753): 71, (30.920156, 121.488943): 72, (31.184888, 121.510295): 73, (31.111396, 121.379353): 74, (31.16691, 121.115166): 75, (31.19143, 121.411411): 76, (31.219114, 121.423499): 77, (31.117005, 121.271194): 78, (31.171101, 121.507145): 79, (30.731903, 121.334173): 80, (31.307268, 121.521916): 81, (31.370336, 121.265507): 82, (31.03333, 121.427385): 83, (31.159704, 121.457647): 84, (31.444014, 121.25661): 85, (31.183857, 121.637321): 86, (31.236856, 121.40554): 87, (31.078561, 121.564629): 88, (30.943662, 121.155665): 89, (31.264146, 121.532591): 90, (31.109649, 121.402441): 91, (31.31394, 121.382812): 92, (30.97935, 121.815249): 93, (31.380093, 121.517779): 94, (31.123394, 121.278015): 95, (31.146522, 121.352501): 96, (31.17939, 121.754087): 97, (30.74139, 121.380141): 98, (31.142218, 121.413767): 99, (30.948712, 121.365593): 100, (31.158515, 121.1384): 101, (31.28711, 121.226904): 102, (31.062583, 121.44986): 103, (31.057696, 121.207516): 104, (31.221591, 121.351862): 105, (31.269868, 121.533249): 106, (31.150797, 121.728702): 107, (31.242948, 121.608734): 108, (31.131416, 121.429704): 109, (31.114038, 121.415096): 110, (31.228014, 121.477667): 111, (31.32712, 121.254377): 112, (31.236104, 121.375177): 113, (31.217074, 121.510123): 114, (30.899117, 121.175731): 115, (31.042021, 121.204506): 116, (31.109666, 121.052906): 117, (31.103853, 121.810034): 118, (31.456615, 121.221022): 119, (31.36124, 121.451839): 120, (31.321416, 121.215): 121, (31.27975, 121.497791): 122, (31.011886, 121.234971): 123, (31.273767, 121.222048): 124, (30.746286, 121.344028): 125, (31.254912, 121.371838): 126, (29.263844, 115.023159): 127, (30.751838, 121.360056): 128, (31.300119, 121.532041): 129, (30.716044, 121.291037): 130, (31.188696, 121.181232): 131, (31.265321, 121.503333): 132, (31.100465, 121.401931): 133, (31.042149, 121.539262): 134, (31.272151, 121.375814): 135, (31.038201, 121.216468): 136, (31.353347, 121.441038): 137, (31.241274, 121.36272): 138, (31.116103, 121.43571): 139, (31.357773, 121.508693): 140, (31.17255, 121.363745): 141, (31.382497, 121.548578): 142, (31.246106, 121.443663): 143, (36.406412, 102.003965): 144, (31.200783, 121.446011): 145, (31.180753, 121.749174): 146, (31.00153, 121.235235): 147, (30.892415, 121.023553): 148, (31.155403, 121.57642): 149, (31.342336, 121.605688): 150, (31.177646, 121.555355): 151, (31.013227, 121.552205): 152, (31.221052, 121.503967): 153, (31.050708, 121.217094): 154, (31.150662, 121.434931): 155, (31.269517, 121.498812): 156, (31.305522, 121.483135): 157, (30.919334, 121.475563): 158, (31.162868, 121.448422): 159, (31.292661, 121.305822): 160, (31.30092, 121.16892): 161, (31.250271, 121.406561): 162, (31.337091, 121.543621): 163, (31.339114, 121.321261): 164, (31.038291, 121.194794): 165, (31.370654, 121.459272): 166, (31.360292, 121.583955): 167, (30.915722, 121.763195): 168, (31.203978, 121.423912): 169, (31.235929, 121.449652): 170, (31.108567, 121.766873): 171, (31.162282, 121.426688): 172, (31.127217, 121.335543): 173, (31.312192, 121.456456): 174, (30.991472, 121.430726): 175, (31.260959, 121.562594): 176, (31.209182, 121.752433): 177, (31.098558, 121.377147): 178, (31.240601, 121.442307): 179, (30.824298, 121.363989): 180, (31.112344, 121.480257): 181, (31.182198, 121.390214): 182, (31.019691, 121.249497): 183, (31.24822, 121.296297): 184, (31.112548, 121.538798): 185, (31.242841, 121.391309): 186, (31.254166, 121.224476): 187, (31.152177, 121.172752): 188, (31.154923, 121.36045): 189, (31.301132, 121.177821): 190, (31.166408, 121.569203): 191, (31.247155, 121.23112): 192, (31.146311, 121.399951): 193, (31.054594, 121.339133): 194, (31.273019, 121.307617): 195, (31.355885, 121.375057): 196, (31.119557, 121.036331): 197, (31.268642, 121.332479): 198, (31.357217, 121.439635): 199, (22.522803, 114.218796): 200, (31.069261, 121.375484): 201, (31.279012, 121.514374): 202, (31.203552, 121.371544): 203, (31.19206, 121.438501): 204, (31.120864, 121.389141): 205, (31.280262, 121.364633): 206, (31.266666, 121.598298): 207, (31.343106, 121.455682): 208, (31.198093, 121.126168): 209, (31.27795, 121.538718): 210, (31.232095, 121.20583): 211, (31.207125, 121.299924): 212, (30.739797, 121.32739): 213, (31.10888, 121.390919): 214, (31.155848, 121.383402): 215, (30.936752, 121.470783): 216, (31.030308, 121.650922): 217, (31.254308, 121.459288): 218, (31.321039, 121.453913): 219, (31.232639, 121.705254): 220, (31.147783, 121.366475): 221, (31.378285, 121.436137): 222, (31.178359, 121.279284): 223, (31.209025, 121.616514): 224, (31.175781, 121.643301): 225, (31.160079, 121.37079): 226, (31.308964, 121.403345): 227, (30.718302, 121.314644): 228, (31.190389, 121.457678): 229, (31.252404, 121.435035): 230, (31.202075, 121.187355): 231, (31.251584, 121.4917): 232, (31.235454, 121.172523): 233, (31.205759, 121.26316): 234, (30.939177, 121.223182): 235, (31.325382, 121.553285): 236, (31.176998, 121.423586): 237, (31.069946, 121.719641): 238, (31.254981, 121.623923): 239, (31.031203, 121.65009): 240, (31.115176, 121.36579): 241, (31.2077, 121.669382): 242, (31.314038, 121.509762): 243, (31.148848, 121.314851): 244, (31.260465, 121.461228): 245, (31.211485, 121.403238): 246, (31.274169, 121.221302): 247, (31.262756, 121.625447): 248, (31.230109, 121.434393): 249, (31.286022, 121.497436): 250, (31.163705, 121.503747): 251, (30.910136, 121.843044): 252, (30.960682, 121.447242): 253, (31.263651, 121.573401): 254, (31.042056, 121.720903): 255, (30.919796, 121.459621): 256, (31.202395, 121.6278): 257, (31.045101, 121.223676): 258, (31.227795, 121.254172): 259, (31.326648, 121.304442): 260, (31.099642, 121.449578): 261, (31.368461, 121.372259): 262, (30.891482, 121.739107): 263, (31.051898, 121.769904): 264, (31.262048, 121.458128): 265, (31.159756, 121.525433): 266, (30.88883, 121.130607): 267, (31.402828, 121.48779): 268, (31.270626, 121.5908): 269, (31.276718, 121.371696): 270, (31.355737, 121.403678): 271, (31.337218, 121.465181): 272, (31.215428, 121.660427): 273, (31.015724, 121.40907): 274, (31.324464, 121.387532): 275, (31.273045, 121.546148): 276, (31.134786, 121.444969): 277, (31.175318, 121.181299): 278, (30.934254, 121.279662): 279, (31.143965, 121.573343): 280, (31.201259, 121.40143): 281, (30.967774, 121.447499): 282, (31.252105, 121.56398): 283, (31.201251, 121.561484): 284, (31.030356, 121.535809): 285, (31.260814, 121.490337): 286, (31.423448, 121.433415): 287, (30.988441, 121.87486): 288, (31.086136, 121.507054): 289, (31.211591, 121.499553): 290, (31.268894, 121.692432): 291, (30.900013, 121.049649): 292, (31.247688, 121.528106): 293, (31.176978, 121.499522): 294, (31.303976, 121.474504): 295, (31.294508, 121.537685): 296, (31.018176, 121.418004): 297, (31.228755, 121.523626): 298, (30.723654, 121.330778): 299, (30.970389, 121.451558): 300, (31.359311, 121.310645): 301, (31.260835, 121.366953): 302, (31.234496, 121.521516): 303, (31.137509, 121.40768): 304, (31.211598, 121.439406): 305, (31.104981, 121.360938): 306, (31.188821, 121.387044): 307, (31.143774, 121.519955): 308, (31.168853, 121.394657): 309, (31.346682, 121.49352): 310, (34.689694, 112.407126): 311, (31.053362, 121.431405): 312, (30.999098, 121.447826): 313, (31.311329, 121.495756): 314, (31.255942, 121.38141): 315, (31.246345, 121.45371): 316, (31.096325, 121.341209): 317, (31.382812, 121.242351): 318, (31.19706, 121.095538): 319, (26.139329, 103.078562): 320, (31.210891, 121.3779): 321, (31.125695, 121.46401): 322, (30.889117, 121.319998): 323, (31.242837, 121.455083): 324, (31.142941, 121.443942): 325, (31.163459, 121.574025): 326, (31.250332, 121.594504): 327, (31.233973, 121.41641): 328, (30.847557, 121.226776): 329, (31.26484, 121.498029): 330, (31.273183, 121.488391): 331, (31.216359, 121.397227): 332, (31.297527, 121.376): 333, (31.286632, 121.552094): 334, (31.141245, 121.500503): 335, (31.192915, 121.682688): 336, (31.121932, 121.33481): 337, (31.069676, 121.773361): 338, (31.06623, 121.321933): 339, (30.983162, 121.054057): 340, (31.206194, 121.456569): 341, (30.887807, 121.346419): 342, (31.204999, 121.471193): 343, (31.131231, 121.398758): 344, (30.995864, 121.568444): 345, (31.218201, 121.487151): 346, (31.090076, 121.406999): 347, (31.007327, 121.431151): 348, (31.182483, 121.5762): 349, (31.111607, 121.324796): 350, (31.2355, 121.417315): 351, (31.276178, 121.553996): 352, (31.184319, 121.230946): 353, (31.267266, 121.385292): 354, (31.261563, 121.376836): 355, (31.214683, 121.551574): 356, (31.318661, 121.194317): 357, (31.238593, 121.450062): 358, (31.019246, 121.250749): 359, (31.000658, 121.659325): 360, (31.176379, 121.397706): 361, (31.13107, 121.298444): 362, (31.214561, 121.298904): 363, (31.2676, 121.541803): 364, (31.273242, 121.499598): 365, (30.79706, 121.431713): 366, (31.231687, 121.32576): 367, (30.932475, 121.692651): 368, (31.233761, 121.670586): 369, (31.26604, 121.399004): 370, (31.140979, 121.586245): 371, (31.284148, 121.481157): 372, (31.194194, 121.456589): 373, (30.739639, 121.373695): 374, (31.288777, 121.390078): 375, (31.405231, 121.506638): 376, (31.31724, 121.237444): 377, (31.056596, 121.721736): 378, (31.348418, 121.427796): 379, (31.133956, 121.526289): 380, (31.224385, 121.143509): 381, (31.456258, 121.413525): 382, (30.900531, 121.399787): 383, (30.986462, 121.409757): 384, (31.092058, 121.38753): 385, (31.04139, 121.741651): 386, (31.018376, 121.40507): 387, (31.095884, 121.350551): 388, (31.105352, 121.438535): 389, (31.230287, 121.557116): 390, (31.149677, 121.729053): 391, (31.246527, 121.534982): 392, (31.395915, 121.363062): 393, (31.27431, 121.448589): 394, (31.102445, 121.413911): 395, (31.174053, 121.282683): 396, (31.189335, 121.781953): 397, (31.287814, 121.355886): 398, (31.242514, 121.493696): 399, (31.225266, 121.444323): 400, (31.370388, 121.187891): 401, (31.214209, 121.659493): 402, (31.109806, 121.242241): 403, (31.119806, 121.071074): 404, (31.206547, 121.306923): 405, (31.190168, 121.467092): 406, (31.306089, 121.474725): 407, (31.260555, 121.410883): 408, (31.173856, 121.350522): 409, (31.243013, 121.416207): 410, (30.877822, 121.548349): 411, (31.184979, 121.31046): 412, (31.337262, 121.473928): 413, (31.33167, 121.440751): 414, (30.73407, 121.350673): 415, (31.395438, 121.472743): 416, (30.98383, 121.235077): 417, (31.181381, 121.688934): 418, (31.073061, 120.970637): 419, (31.378631, 121.277965): 420, (31.319077, 121.331853): 421, (30.90387, 121.433071): 422, (31.326138, 121.48973): 423, (31.255905, 121.430231): 424, (31.157536, 121.203337): 425, (31.238561, 121.455041): 426, (31.120602, 121.581473): 427, (31.365408, 121.245027): 428, (31.206089, 121.105074): 429, (31.108876, 121.345256): 430, (31.262746, 121.517701): 431, (31.229334, 121.368291): 432, (31.260165, 121.541462): 433, (31.209905, 121.37472): 434, (31.198952, 121.70244): 435, (31.189701, 121.543143): 436, (31.344291, 121.487858): 437, (31.265, 121.48958): 438, (31.208108, 121.41483): 439, (31.280339, 121.402957): 440, (31.015959, 121.227509): 441, (31.197909, 121.426542): 442, (30.938138, 121.327668): 443, (31.19232, 121.142889): 444, (31.009423, 121.245898): 445, (31.163432, 121.354451): 446, (35.379598, 116.072359): 447, (31.207205, 121.482691): 448, (31.019982, 121.244295): 449, (31.086916, 121.387535): 450, (31.233201, 121.469528): 451, (31.311942, 121.54204): 452, (31.217372, 121.460724): 453, (30.908583, 121.656169): 454, (31.204614, 121.432408): 455, (31.057069, 121.599172): 456, (31.164195, 121.330502): 457, (31.200882, 121.446511): 458, (31.269929, 121.453419): 459, (31.182415, 121.381272): 460, (31.276317, 121.524794): 461, (31.12408, 121.716759): 462, (31.277512, 121.287969): 463, (31.25007, 121.546845): 464, (31.080824, 121.390896): 465, (31.196266, 121.537132): 466, (31.13756, 121.339005): 467, (31.263762, 121.481215): 468, (31.356238, 121.568226): 469, (31.266679, 121.539067): 470, (31.167947, 121.670231): 471, (30.87706, 121.856429): 472, (31.075231, 121.436269): 473, (31.252264, 121.38269): 474, (30.844397, 121.528845): 475, (31.186465, 121.429887): 476, (30.998611, 121.216212): 477, (31.270362, 121.612773): 478, (31.396351, 121.44222): 479, (30.874287, 121.67603): 480, (31.344675, 121.443963): 481, (31.211107, 121.59082): 482, (31.455174, 121.413981): 483, (31.241739, 121.537288): 484, (30.941447, 121.293964): 485, (31.371694, 121.443832): 486, (31.25393, 121.455087): 487, (31.19107, 121.554321): 488, (31.237872, 121.470259): 489, (31.125467, 121.581832): 490, (30.715526, 121.351073): 491, (31.259633, 121.594477): 492, (31.058092, 121.235964): 493, (30.903508, 121.331287): 494, (31.214059, 121.37764): 495, (31.025763, 121.541983): 496, (31.24448, 121.37062): 497, (30.93817, 121.865652): 498, (31.352538, 121.490272): 499, (31.208346, 121.544892): 500, (31.195139, 121.385973): 501, (31.285698, 121.612262): 502, (31.265326, 121.394456): 503, (31.246097, 121.39928): 504, (31.143297, 121.132883): 505, (31.251239, 121.428578): 506, (31.244102, 121.437744): 507, (31.155384, 121.771585): 508, (30.867598, 121.198612): 509, (31.17023, 121.337413): 510, (31.274536, 121.097082): 511, (31.382771, 121.291756): 512, (31.154875, 121.131226): 513, (31.296405, 121.392803): 514, (31.116903, 121.595022): 515, (30.983141, 121.540945): 516, (31.282514, 121.343407): 517, (31.163201, 121.11139): 518, (31.058407, 121.330724): 519, (31.294393, 121.497298): 520, (31.150483, 121.491401): 521, (30.860668, 121.470193): 522, (31.190301, 121.566862): 523, (31.158559, 121.319801): 524, (31.29992, 121.669771): 525, (31.201279, 121.709485): 526, (31.073134, 121.317814): 527, (31.179835, 121.429502): 528, (31.268663, 121.428101): 529, (31.134548, 121.39095): 530, (30.900529, 121.246804): 531, (31.148773, 121.387158): 532, (31.151539, 121.41109): 533, (31.152182, 121.564661): 534, (31.132785, 121.413457): 535, (31.272162, 121.459351): 536, (31.452354, 121.33861): 537, (31.213015, 121.547755): 538, (31.312291, 121.46631): 539, (31.03732, 121.18963): 540, (31.220649, 121.35994): 541, (31.118723, 121.248449): 542, (31.236728, 121.42407): 543, (31.223004, 121.465956): 544, (31.222233, 121.623547): 545, (31.042415, 121.482986): 546, (31.315825, 121.532151): 547, (31.220434, 121.408625): 548, (31.16346, 121.405127): 549, (31.036623, 121.377512): 550, (31.258255, 121.471554): 551, (31.045516, 121.216817): 552, (31.29092, 121.359453): 553, (31.236862, 121.355595): 554, (31.196135, 121.558528): 555, (31.26338, 121.433161): 556, (31.142865, 121.330843): 557, (31.159304, 121.358718): 558, (31.10114, 121.181649): 559, (31.323091, 121.364299): 560, (31.223665, 121.458791): 561, (31.248587, 121.277434): 562, (31.211574, 121.366182): 563, (31.276982, 121.460649): 564, (31.324905, 121.29416): 565, (30.87366, 121.082631): 566, (31.327488, 121.437837): 567, (31.327049, 121.443238): 568, (31.072213, 121.658885): 569, (31.020065, 121.23311): 570, (31.015452, 121.154814): 571, (31.384186, 121.484871): 572, (31.197573, 121.454035): 573, (31.048845, 121.231719): 574, (38.052584, 114.484137): 575, (31.311763, 121.21253): 576, (31.044065, 121.537499): 577, (30.705762, 121.33512): 578, (30.982046, 121.207872): 579, (31.17605, 121.431225): 580, (31.111821, 121.385462): 581, (31.488248, 121.349833): 582, (31.099755, 121.009542): 583, (31.366638, 121.43543): 584, (31.387389, 121.431922): 585, (31.213659, 121.504661): 586, (31.145387, 121.61623): 587, (31.044667, 121.46191): 588, (31.230395, 121.46599): 589, (31.320121, 121.459018): 590, (31.190185, 121.443475): 591, (31.155297, 121.573504): 592, (31.130862, 121.091425): 593, (31.254111, 121.408424): 594, (31.403259, 121.447841): 595, (31.209644, 121.56447): 596, (30.935837, 121.561311): 597, (31.277766, 121.433047): 598, (31.216371, 121.601926): 599, (31.158202, 121.389411): 600, (31.05799, 121.626154): 601, (30.996346, 121.297687): 602, (31.215381, 121.439262): 603, (31.155344, 121.2025): 604, (31.094914, 121.43682): 605, (31.395973, 121.258897): 606, (31.367578, 121.31858): 607, (31.210843, 121.491168): 608, (31.124501, 121.32791): 609, (30.999472, 121.378674): 610, (30.922611, 121.473581): 611, (31.232002, 121.359292): 612, (31.168938, 121.54333): 613, (31.177169, 121.445517): 614, (31.045313, 121.844791): 615, (31.175621, 121.317835): 616, (31.23593, 121.220028): 617, (31.13623, 121.77848): 618, (31.235177, 121.451385): 619, (31.228847, 121.338653): 620, (31.262448, 121.504717): 621, (31.217476, 121.494788): 622, (31.234004, 121.276012): 623, (31.311631, 121.354326): 624, (31.217968, 121.41467): 625, (31.241014, 121.48529): 626, (31.224226, 121.43839): 627, (31.224718, 121.344451): 628, (31.159569, 121.145966): 629, (31.023902, 121.350511): 630, (31.220809, 121.305363): 631, (31.294194, 121.316828): 632, (31.222119, 121.454078): 633, (31.265705, 121.420959): 634, (31.430211, 121.28239): 635, (31.180466, 121.441251): 636, (31.281813, 121.354285): 637, (31.121363, 121.369095): 638, (31.245624, 121.522839): 639, (31.125764, 121.562057): 640, (31.195131, 121.418788): 641, (30.829105, 121.180127): 642, (31.3397, 121.298473): 643, (30.799332, 121.268825): 644, (31.16807, 121.488657): 645, (31.047975, 121.785062): 646, (30.832601, 121.445485): 647, (31.039301, 121.593594): 648, (31.374842, 121.264224): 649, (31.275929, 121.473484): 650, (31.181485, 121.520026): 651, (30.895699, 121.478289): 652, (31.252974, 121.502684): 653, (31.074013, 121.407487): 654, (30.960478, 121.057529): 655, (31.102, 121.462298): 656, (31.204343, 121.407018): 657, (31.402222, 121.493387): 658, (31.153746, 121.504388): 659, (31.137143, 121.545715): 660, (30.832683, 121.481868): 661, (31.01294, 121.637001): 662, (31.191674, 121.699103): 663, (31.11674, 121.586334): 664, (31.248146, 121.441241): 665, (30.945561, 121.150834): 666, (31.318695, 121.410448): 667, (31.235453, 121.653548): 668, (31.32899, 121.323419): 669, (30.926464, 121.682732): 670, (31.24641, 121.548541): 671, (31.034322, 121.601024): 672, (31.145439, 121.417433): 673, (31.226966, 121.53174): 674, (31.182371, 121.526439): 675, (31.219133, 121.524846): 676, (31.459469, 121.412299): 677, (30.954604, 121.333773): 678, (31.331242, 121.225906): 679, (30.884387, 121.408648): 680, (31.203031, 121.443586): 681, (31.128227, 121.45441): 682, (31.30647, 121.492925): 683, (31.214767, 121.444427): 684, (31.292717, 121.482235): 685, (31.239726, 121.139516): 686, (31.294378, 121.567937): 687, (31.121152, 121.141999): 688, (31.186628, 121.372116): 689, (31.213701, 121.41802): 690, (31.063923, 121.353076): 691, (31.227933, 121.45361): 692, (30.797557, 121.356113): 693, (31.280867, 121.656939): 694, (31.294155, 121.418063): 695, (31.423181, 121.387917): 696, (31.098701, 121.582178): 697, (31.300693, 121.496182): 698, (31.214115, 121.406884): 699, (31.22121, 121.411568): 700, (31.250674, 121.47282): 701, (31.232857, 121.475333): 702, (31.22135, 121.391319): 703, (31.17665, 121.132941): 704, (30.950067, 121.449083): 705, (31.077825, 121.33981): 706, (46.777465, 131.812182): 707, (31.2317, 121.546506): 708, (31.082614, 121.523874): 709, (31.127837, 121.380654): 710, (31.414938, 121.486809): 711, (31.194792, 121.466884): 712, (31.318974, 121.412838): 713, (31.24143, 121.475172): 714, (31.274535, 121.48034): 715, (31.122623, 121.433813): 716, (31.318369, 121.407014): 717, (30.918205, 121.509992): 718, (31.13264, 121.32443): 719, (31.299261, 121.578516): 720, (31.276268, 121.683198): 721, (31.292008, 121.36941): 722, (30.730296, 121.342643): 723, (31.01613, 121.203009): 724, (31.19382, 121.49524): 725, (31.342417, 121.40922): 726, (31.23301, 121.442524): 727, (31.28023, 121.543164): 728, (31.32719, 121.53013): 729, (31.373346, 121.135761): 730, (31.037985, 121.752781): 731, (31.140855, 121.360032): 732, (30.898527, 121.22367): 733, (31.012212, 121.413472): 734, (31.224452, 121.511537): 735, (31.274564, 121.51979): 736, (31.205813, 121.413544): 737, (31.202883, 121.476296): 738, (31.18851, 121.403115): 739, (31.177858, 121.346956): 740, (30.719537, 121.355948): 741, (31.160199, 121.557134): 742, (31.25563, 121.508513): 743, (31.132311, 121.362691): 744, (31.171972, 121.140685): 745, (30.866245, 121.343018): 746, (31.217516, 121.452872): 747, (31.146551, 121.235394): 748, (31.298752, 121.491919): 749, (31.204179, 121.538588): 750, (31.191847, 121.381117): 751, (30.909885, 121.195459): 752, (31.281567, 121.557955): 753, (30.814573, 121.352883): 754, (31.012926, 121.423939): 755, (31.260429, 121.426787): 756, (31.371366, 121.236234): 757, (31.256151, 121.557845): 758, (31.122978, 121.704554): 759, (31.035283, 121.246093): 760, (31.201303, 121.434409): 761, (31.183323, 121.447822): 762, (31.281363, 121.459935): 763, (31.397749, 121.267217): 764, (31.285861, 121.507007): 765, (31.274417, 121.257132): 766, (31.219239, 121.418564): 767, (31.045068, 121.232113): 768, (31.120822, 121.29001): 769, (31.198048, 121.604048): 770, (31.301877, 121.353206): 771, (31.255293, 121.49343): 772, (31.236935, 121.106513): 773, (31.225092, 121.38464): 774, (31.051247, 121.480621): 775, (31.225207, 121.452948): 776, (31.303478, 121.55082): 777, (30.8406, 121.251681): 778, (31.156874, 121.348458): 779, (31.245814, 121.411472): 780, (30.918282, 121.639066): 781, (31.072138, 121.661831): 782, (31.312494, 121.157796): 783, (31.282237, 121.42091): 784, (31.225831, 121.412695): 785, (31.239319, 121.265377): 786, (30.920347, 121.057024): 787, (31.278892, 121.710661): 788, (31.085146, 121.589124): 789, (31.127879, 121.452728): 790, (31.185228, 121.1697): 791, (31.401391, 121.463758): 792, (31.312825, 121.491112): 793, (30.984047, 121.726879): 794, (31.00953, 121.052076): 795, (31.207679, 121.489691): 796, (30.816155, 121.298392): 797, (31.192975, 121.449755): 798, (31.126339, 121.573213): 799, (31.113159, 121.06421): 800, (31.240587, 121.402322): 801, (31.317294, 121.460019): 802, (31.324375, 121.433451): 803, (31.19332, 121.430385): 804, (31.220416, 121.529149): 805, (31.216526, 121.326846): 806, (31.122449, 121.582926): 807, (31.195907, 121.410659): 808, (31.194567, 121.453929): 809, (31.266743, 121.481644): 810, (30.975655, 121.228194): 811, (31.268263, 121.394134): 812, (31.049556, 121.600672): 813, (31.208384, 121.466365): 814, (31.335151, 121.590721): 815, (31.241036, 121.374063): 816, (30.7834, 121.299618): 817, (31.242991, 121.515376): 818, (31.299851, 121.163211): 819, (31.01217, 121.265575): 820, (30.831909, 121.397355): 821, (31.304918, 121.497837): 822, (31.215081, 121.481286): 823, (31.043159, 121.420296): 824, (31.307198, 121.511929): 825, (31.159667, 121.515041): 826, (31.038911, 121.226883): 827, (31.161027, 121.722478): 828, (31.088099, 121.430111): 829, (31.300545, 121.170082): 830, (31.030806, 121.396875): 831, (31.273276, 121.1303): 832, (31.276349, 121.39526): 833, (31.230201, 121.308739): 834, (31.087306, 121.275545): 835, (31.240478, 121.526884): 836, (31.284042, 121.583676): 837, (31.017385, 121.715431): 838, (31.24708, 121.419525): 839, (31.197656, 121.463533): 840, (31.213728, 121.386102): 841, (31.388641, 121.444174): 842, (31.039193, 121.604274): 843, (31.242365, 121.503571): 844, (31.15228, 121.118001): 845, (31.144212, 121.50825): 846, (31.193409, 121.393885): 847, (31.185812, 121.457923): 848, (31.258677, 121.392956): 849, (31.016895, 121.42971): 850, (31.198836, 121.215066): 851, (31.241362, 121.395822): 852, (31.023331, 121.315006): 853, (31.260742, 121.399188): 854, (31.337749, 121.579688): 855, (31.149689, 121.531301): 856, (30.948533, 121.465504): 857, (31.257553, 121.311711): 858, (31.00948, 121.4998): 859, (31.26748, 121.57859): 860, (31.211617, 121.486703): 861, (31.244559, 121.724977): 862, (31.245663, 121.449725): 863, (30.947965, 121.640593): 864, (31.10366, 121.420217): 865, (31.23423, 121.119889): 866, (30.983966, 121.737727): 867, (31.354948, 121.496863): 868, (31.358748, 121.52806): 869, (31.201647, 121.420098): 870, (30.843004, 121.438792): 871, (31.25901, 121.65797): 872, (30.823953, 121.528367): 873, (31.172757, 121.425965): 874, (31.019533, 121.388705): 875, (31.052986, 121.506423): 876, (31.301307, 121.33124): 877, (31.295347, 121.515155): 878, (31.234545, 121.743402): 879, (31.324066, 121.536866): 880, (30.910085, 121.504947): 881, (31.248482, 121.53958): 882, (31.059755, 121.389723): 883, (31.318389, 121.653955): 884, (31.157241, 121.418919): 885, (30.92668, 121.466784): 886, (31.218812, 121.401309): 887, (31.288102, 121.122551): 888, (31.334449, 121.536876): 889, (31.221001, 121.731159): 890, (31.266813, 121.512658): 891, (31.298177, 121.473198): 892, (31.045834, 121.408322): 893, (30.865062, 121.662776): 894, (30.795602, 121.152307): 895, (31.294598, 121.206157): 896, (31.096053, 121.518157): 897, (30.977517, 121.669761): 898, (30.858416, 121.035215): 899, (30.93561, 121.517165): 900, (31.057816, 121.413943): 901, (31.327484, 121.406697): 902, (31.425786, 121.422506): 903, (31.250734, 121.487731): 904, (31.264477, 121.44249): 905, (31.38336, 121.420672): 906, (31.318281, 121.447698): 907, (31.032565, 121.409407): 908, (31.312069, 121.172955): 909, (31.313954, 121.499987): 910, (30.924981, 121.710847): 911, (31.139657, 121.298915): 912, (31.157677, 121.424531): 913, (30.904757, 121.467941): 914, (31.219088, 121.428934): 915, (31.149628, 121.43924): 916, (31.297281, 121.505124): 917, (31.315501, 121.535127): 918, (31.25821, 121.479271): 919, (31.069757, 121.399847): 920, (31.360948, 121.595293): 921, (31.351256, 121.519123): 922, (31.295335, 121.555781): 923, (30.872205, 121.530081): 924, (31.390234, 121.254572): 925, (31.277576, 121.460905): 926, (31.316557, 121.516135): 927, (31.120714, 121.42693): 928, (31.235072, 121.518805): 929, (31.255173, 121.403569): 930, (30.831429, 121.242309): 931, (31.30835, 121.657013): 932, (31.333408, 121.552846): 933, (31.253394, 121.389864): 934, (31.139721, 121.673912): 935, (31.261924, 121.568031): 936, (30.929919, 121.206039): 937, (31.070388, 121.532962): 938, (31.221319, 121.166272): 939, (31.383625, 121.268069): 940, (31.189723, 121.356104): 941, (31.460409, 121.401649): 942, (31.167279, 121.113649): 943, (31.242808, 121.420927): 944, (31.054324, 121.793035): 945, (31.081475, 121.523285): 946, (31.138371, 121.416975): 947, (31.180763, 121.476049): 948, (31.267654, 121.473299): 949, (31.260903, 121.618966): 950, (31.088801, 121.536584): 951, (31.313065, 121.417971): 952, (31.185065, 121.503655): 953, (31.219945, 121.285654): 954, (30.766975, 121.362906): 955, (30.934396, 121.369499): 956, (31.39034, 121.272165): 957, (30.741272, 121.3348): 958, (31.43737, 121.432451): 959, (31.283201, 121.665402): 960, (31.046524, 121.749155): 961, (31.370457, 121.470415): 962, (30.908378, 121.634574): 963, (31.255025, 121.199554): 964, (31.064763, 121.396913): 965, (31.005717, 121.202014): 966, (30.894639, 121.21016): 967, (30.893975, 121.016922): 968, (31.364407, 121.561457): 969, (30.997412, 121.247239): 970, (31.168922, 121.412651): 971, (30.834815, 121.197836): 972, (31.251358, 121.485526): 973, (31.174203, 121.431404): 974, (31.253808, 121.483756): 975, (31.262941, 121.602924): 976, (31.236381, 121.484201): 977, (31.239655, 121.478097): 978, (31.199721, 121.325753): 979, (31.468523, 121.425585): 980, (30.978506, 121.447689): 981, (31.176489, 121.188906): 982, (31.271472, 121.494721): 983, (30.989121, 121.127308): 984, (30.796696, 121.200272): 985, (31.150398, 121.537521): 986, (31.361645, 121.475443): 987, (31.166007, 121.457766): 988, (31.1744, 121.285083): 989, (30.855412, 121.569454): 990, (31.260816, 121.36983): 991, (31.136777, 121.546762): 992, (30.915122, 121.560642): 993, (31.301749, 121.457406): 994, (31.302, 121.670498): 995, (31.287632, 121.458734): 996, (31.317987, 120.619907): 997, (31.120753, 121.381579): 998, (31.039158, 121.239632): 999, (31.236177, 121.493181): 1000, (31.163393, 121.381735): 1001, (31.453478, 121.256429): 1002, (31.202118, 121.395128): 1003, (31.176807, 121.251063): 1004, (31.035151, 121.544702): 1005, (31.294885, 121.244609): 1006, (30.929412, 121.460403): 1007, (31.106759, 121.398956): 1008, (30.915686, 121.462098): 1009, (31.350616, 121.589116): 1010, (31.427978, 121.433301): 1011, (31.109271, 121.617421): 1012, (30.966714, 121.771375): 1013, (31.22714, 121.391029): 1014, (31.28386, 121.530324): 1015, (31.178278, 121.606216): 1016, (30.774911, 121.259927): 1017, (31.364503, 121.391608): 1018, (31.293561, 121.362372): 1019, (31.040309, 121.308498): 1020, (31.036987, 121.386554): 1021, (31.243446, 121.675041): 1022, (31.165339, 121.427732): 1023, (31.171648, 121.110276): 1024, (31.116535, 121.678055): 1025, (31.230748, 121.485488): 1026, (31.290434, 121.425041): 1027, (31.054947, 120.995325): 1028, (31.07195, 121.150279): 1029, (31.373432, 121.490334): 1030, (31.456227, 121.359857): 1031, (31.046945, 121.764826): 1032, (30.763589, 121.264689): 1033, (31.011339, 121.296295): 1034, (31.441487, 121.179013): 1035, (31.434069, 121.448741): 1036, (31.233674, 121.498294): 1037, (34.556383, 118.79231): 1038, (31.323113, 121.616609): 1039, (31.277484, 121.382842): 1040, (31.360977, 121.444195): 1041, (30.985081, 121.691229): 1042, (31.240412, 121.147662): 1043, (31.235461, 121.498056): 1044, (31.04822, 121.231928): 1045, (31.251883, 121.418603): 1046, (31.145641, 121.449009): 1047, (31.148334, 121.420448): 1048, (31.317417, 121.48383): 1049, (31.190828, 121.420662): 1050, (31.291192, 121.65263): 1051, (31.053198, 120.907525): 1052, (39.612987, 118.20604): 1053, (31.036404, 121.272345): 1054, (31.244292, 121.495265): 1055, (31.305433, 121.435633): 1056, (31.08906, 121.722984): 1057, (31.241762, 121.50497): 1058, (31.234259, 121.52572): 1059, (31.033659, 121.577846): 1060, (31.28627, 121.599397): 1061, (31.256559, 121.43557): 1062, (31.195709, 121.424093): 1063, (31.358014, 121.37617): 1064, (31.421831, 121.349166): 1065, (30.891157, 121.186557): 1066, (31.219455, 121.098597): 1067, (31.236138, 121.463199): 1068, (30.993883, 121.129758): 1069, (31.270302, 121.398224): 1070, (31.242078, 121.331128): 1071, (31.459342, 121.319364): 1072, (30.733903, 121.341904): 1073, (30.912905, 121.801126): 1074, (31.281917, 121.435906): 1075, (31.163339, 121.469291): 1076, (31.203473, 121.445984): 1077, (31.099085, 121.386207): 1078, (31.058968, 121.787072): 1079, (31.426865, 121.351198): 1080, (31.301954, 121.54744): 1081, (31.232729, 121.566935): 1082, (30.969483, 121.53972): 1083, (31.231068, 121.4021): 1084, (31.200224, 121.316463): 1085, (31.270279, 121.531241): 1086, (31.002291, 121.873286): 1087, (31.237022, 121.546184): 1088, (31.121127, 121.243829): 1089, (30.771941, 121.373524): 1090, (31.132833, 121.17281): 1091, (31.22255, 121.440445): 1092, (31.116791, 121.166516): 1093, (31.249162, 121.487899): 1094, (31.225318, 121.437806): 1095, (31.087507, 121.398717): 1096, (30.85108, 121.855994): 1097, (31.256071, 121.462096): 1098, (31.004198, 121.067198): 1099, (31.270369, 121.443633): 1100, (31.370992, 121.177459): 1101, (31.180175, 121.422303): 1102, (30.798179, 121.426469): 1103, (31.051162, 121.311884): 1104, (31.209913, 121.425292): 1105, (31.178033, 121.487803): 1106, (31.242893, 121.465382): 1107, (31.213582, 121.38377): 1108, (31.002479, 121.390889): 1109, (31.17762, 121.511042): 1110, (31.126777, 121.367557): 1111, (31.237944, 121.693732): 1112, (31.3065, 121.429743): 1113, (30.945306, 121.729327): 1114, (31.021822, 121.590566): 1115, (30.747024, 121.287991): 1116, (31.316181, 121.405014): 1117, (31.235247, 121.383101): 1118, (31.243338, 121.253922): 1119, (31.228047, 121.425893): 1120, (30.978588, 121.581677): 1121, (31.105808, 121.038742): 1122, (31.312672, 121.593011): 1123, (31.308902, 121.502016): 1124, (31.147903, 121.403379): 1125, (31.201965, 121.452457): 1126, (31.345443, 121.596458): 1127, (31.320829, 121.389518): 1128, (31.283959, 121.416644): 1129, (31.267478, 121.5468): 1130, (31.221876, 121.687381): 1131, (31.068209, 121.373076): 1132, (30.875057, 121.324697): 1133, (31.204349, 121.693531): 1134, (31.289489, 121.341656): 1135, (31.070388, 121.520258): 1136, (31.041762, 120.928733): 1137, (31.20901, 121.478403): 1138, (31.307215, 121.486962): 1139, (31.29099, 121.697503): 1140, (31.396687, 121.378844): 1141, (30.852446, 121.262447): 1142, (31.08111, 121.263537): 1143, (31.3691, 121.415317): 1144, (31.22258, 121.428599): 1145, (31.255256, 121.500499): 1146, (30.900763, 121.246509): 1147, (31.048503, 121.141872): 1148, (30.933365, 121.873737): 1149, (31.244859, 121.489305): 1150, (31.288233, 121.304952): 1151, (31.281665, 121.42621): 1152, (31.331055, 121.52962): 1153, (30.72456, 121.344043): 1154, (31.285059, 121.406807): 1155, (31.217237, 121.432824): 1156, (31.176813, 121.439114): 1157, (31.337637, 121.446597): 1158, (31.232407, 121.652986): 1159, (31.296656, 121.483976): 1160, (31.159536, 121.1442): 1161, (31.323598, 121.638883): 1162, (31.239454, 121.46057): 1163, (31.291143, 121.472012): 1164, (31.197573, 121.37641): 1165, (31.413754, 121.202263): 1166, (31.376717, 121.539395): 1167, (32.902077, 109.42358): 1168, (31.233516, 121.491431): 1169, (31.078365, 121.404647): 1170, (31.110093, 121.371253): 1171, (31.050344, 121.770082): 1172, (31.168241, 121.437559): 1173, (31.161635, 121.286602): 1174, (31.065206, 121.803371): 1175, (31.473557, 121.331298): 1176, (31.228927, 121.486896): 1177, (30.967591, 121.656531): 1178, (31.150193, 121.45419): 1179, (31.192275, 121.462826): 1180, (31.247009, 121.47942): 1181, (30.802409, 121.205114): 1182, (31.130891, 121.472384): 1183, (31.171686, 121.441221): 1184, (31.273982, 121.512661): 1185, (31.223114, 121.522857): 1186, (31.170097, 121.608906): 1187, (31.221777, 121.558508): 1188, (31.294609, 121.526886): 1189, (31.160355, 121.454785): 1190, (31.278008, 121.492049): 1191, (31.080002, 121.798389): 1192, (31.373175, 121.438741): 1193, (31.431346, 121.222574): 1194, (31.253346, 121.448039): 1195, (31.396658, 121.496972): 1196, (31.141982, 121.459872): 1197, (31.243709, 121.197504): 1198, (31.068626, 121.108326): 1199, (31.222168, 121.378951): 1200, (31.334186, 121.50238): 1201, (31.19868, 121.485666): 1202, (31.173163, 121.356571): 1203, (31.157022, 121.498865): 1204, (31.289793, 121.455017): 1205, (30.844717, 121.528288): 1206, (30.736093, 121.355957): 1207, (31.126179, 121.378046): 1208, (31.280857, 121.169253): 1209, (31.333315, 121.188402): 1210, (30.955208, 121.732358): 1211, (31.197114, 121.442853): 1212, (31.191818, 121.538713): 1213, (30.866409, 121.188512): 1214, (31.233629, 121.532011): 1215, (31.144653, 121.478451): 1216, (31.227167, 121.633295): 1217, (31.28915, 121.229167): 1218, (31.304036, 121.448053): 1219, (31.234867, 121.46325): 1220, (31.290887, 121.435064): 1221, (30.957633, 121.343012): 1222, (31.168296, 121.588625): 1223, (31.059304, 121.53566): 1224, (31.250339, 121.440196): 1225, (31.11794, 121.457471): 1226, (31.235682, 121.487831): 1227, (31.410741, 121.442349): 1228, (31.218716, 121.568667): 1229, (31.013082, 121.391811): 1230, (31.222609, 121.378494): 1231, (31.264433, 121.541398): 1232, (31.232518, 121.465177): 1233, (31.23271, 121.455345): 1234, (31.309342, 121.580807): 1235, (30.873326, 121.284335): 1236, (31.252495, 121.390194): 1237, (31.109998, 121.164842): 1238, (31.304576, 121.522177): 1239, (31.271819, 121.546358): 1240, (31.184883, 121.430689): 1241, (31.26338, 121.517162): 1242, (31.437104, 121.31285): 1243, (31.037068, 121.686977): 1244, (31.216752, 121.476401): 1245, (31.284953, 121.489071): 1246, (31.119287, 121.204986): 1247, (31.22622, 121.506261): 1248, (31.240869, 121.481603): 1249, (31.224731, 121.43464): 1250, (31.221346, 121.201454): 1251, (30.92643, 121.782514): 1252, (30.948124, 121.381501): 1253, (31.300338, 121.526827): 1254, (31.235847, 121.536101): 1255, (31.127539, 121.389284): 1256, (31.021245, 121.226791): 1257, (31.355364, 121.537096): 1258, (31.209872, 121.409627): 1259, (31.417324, 121.491388): 1260, (31.245393, 121.705496): 1261, (30.956624, 121.334663): 1262, (31.309645, 121.4911): 1263, (30.86683, 121.83302): 1264, (31.335141, 121.413142): 1265, (31.23543, 121.447652): 1266, (31.152749, 121.182589): 1267, (30.879349, 121.094749): 1268, (31.302288, 121.510464): 1269, (31.026816, 121.426094): 1270, (31.280389, 121.467635): 1271, (31.398623, 121.409041): 1272, (31.310614, 121.476617): 1273, (31.17247, 121.404203): 1274, (31.254124, 121.605142): 1275, (31.143835, 120.935676): 1276, (31.288583, 121.428686): 1277, (30.905367, 121.832974): 1278, (31.221201, 121.497347): 1279, (31.223175, 121.184646): 1280, (31.208589, 121.524447): 1281, (30.749894, 121.270796): 1282, (31.203987, 121.520591): 1283, (30.975236, 121.195313): 1284, (31.305269, 121.397452): 1285, (31.327299, 121.452936): 1286, (31.252241, 121.57094): 1287, (31.098359, 121.073778): 1288, (31.228627, 121.480756): 1289, (31.071955, 121.234457): 1290, (31.08093, 121.159291): 1291, (31.155342, 121.8105): 1292, (31.33454, 121.46611): 1293, (31.213849, 121.43028): 1294, (31.259319, 121.494159): 1295, (31.286501, 121.363758): 1296, (31.379455, 121.372876): 1297, (31.162395, 121.353836): 1298, (31.46933, 121.246736): 1299, (31.145654, 121.375598): 1300, (31.169796, 121.52847): 1301, (31.106089, 121.039819): 1302, (31.256426, 121.61001): 1303, (30.9152, 121.664095): 1304, (30.960195, 121.398632): 1305, (30.910239, 121.419143): 1306, (30.990942, 121.12541): 1307, (31.42225, 121.355698): 1308, (31.406426, 121.476433): 1309, (31.432457, 121.359456): 1310, (31.177067, 121.370844): 1311, (30.897085, 121.617832): 1312, (31.240874, 121.518086): 1313, (30.965531, 121.208487): 1314, (31.278808, 121.419722): 1315, (30.874563, 121.067902): 1316, (31.202262, 121.564812): 1317, (31.058805, 121.593285): 1318, (31.267263, 121.086225): 1319, (31.319422, 121.469372): 1320, (31.228726, 121.482748): 1321, (31.195321, 121.445546): 1322, (31.300493, 121.323029): 1323, (31.113798, 121.382691): 1324, (31.307142, 121.362041): 1325, (30.930023, 121.011969): 1326, (30.99055, 121.126163): 1327, (31.285944, 121.44442): 1328, (30.894923, 121.319564): 1329, (31.023064, 121.534267): 1330, (31.236405, 121.432864): 1331, (31.240561, 121.459431): 1332, (31.195613, 121.40425): 1333, (31.227762, 121.495598): 1334, (31.099126, 121.813976): 1335, (31.217682, 121.499709): 1336, (31.212718, 121.447853): 1337, (31.155349, 121.375527): 1338, (31.217512, 121.533844): 1339, (30.830149, 121.35652): 1340, (31.292462, 121.271212): 1341, (31.27328, 121.47376): 1342, (30.853798, 121.344272): 1343, (31.3038, 121.450721): 1344, (30.846647, 121.431405): 1345, (30.718108, 121.283169): 1346, (31.219437, 121.374523): 1347, (31.382369, 121.258579): 1348, (31.212113, 121.444193): 1349, (30.827125, 121.329382): 1350, (31.022923, 121.806437): 1351, (30.956973, 121.097333): 1352, (31.236009, 121.478941): 1353, (30.94602, 121.106186): 1354, (31.415226, 121.349387): 1355, (31.284675, 121.516006): 1356, (31.166058, 121.403691): 1357, (31.030657, 121.236586): 1358, (31.387989, 121.396749): 1359, (31.259763, 121.089338): 1360, (31.209182, 121.535827): 1361, (39.084494, 117.236165): 1362, (31.081917, 121.062248): 1363, (31.307911, 121.518921): 1364, (30.810888, 121.234763): 1365, (35.24116, 113.276351): 1366, (31.33876, 121.437747): 1367, (31.232893, 121.380242): 1368, (30.977355, 121.74808): 1369, (31.242867, 121.490229): 1370, (30.893492, 121.315973): 1371, (31.221325, 121.448881): 1372, (31.202103, 121.445912): 1373, (31.259267, 121.369254): 1374, (31.217323, 121.753758): 1375, (31.183859, 121.415119): 1376, (31.184347, 121.441392): 1377, (31.159523, 121.435774): 1378, (31.25423, 121.461072): 1379, (31.157362, 121.221594): 1380, (31.168519, 121.804669): 1381, (31.065995, 121.314532): 1382, (31.073706, 121.680729): 1383, (31.251614, 121.464007): 1384, (30.91796, 121.543057): 1385, (31.012108, 121.074488): 1386, (30.889117, 121.317123): 1387, (30.801265, 121.395878): 1388, (31.344797, 121.500999): 1389, (31.230277, 121.448573): 1390, (31.219533, 121.427561): 1391, (31.180994, 121.377204): 1392, (31.492475, 121.328439): 1393, (31.08532, 121.237002): 1394, (31.134917, 121.334281): 1395, (31.191702, 121.449143): 1396, (31.209479, 121.702069): 1397, (30.80226, 121.311879): 1398, (31.278028, 121.583992): 1399, (31.304081, 121.326517): 1400, (31.318258, 121.167266): 1401, (31.231663, 121.428471): 1402, (31.213061, 121.483541): 1403, (26.215115, 109.744661): 1404, (31.283733, 121.149832): 1405, (31.260888, 121.340352): 1406, (31.406017, 121.240101): 1407, (31.256232, 121.498254): 1408, (31.151421, 121.279419): 1409, (31.264503, 121.715846): 1410, (31.412982, 121.496282): 1411, (31.162111, 121.566673): 1412, (30.92405, 121.648972): 1413, (31.209904, 121.234203): 1414, (31.266413, 121.351003): 1415, (31.2716, 121.465223): 1416, (31.060659, 121.399957): 1417, (31.328545, 121.538592): 1418, (31.010467, 121.237408): 1419, (31.299799, 121.530915): 1420, (31.24478, 121.505751): 1421, (31.143754, 121.434357): 1422, (31.157151, 121.431115): 1423, (31.294057, 121.171316): 1424, (31.087224, 121.466989): 1425, (31.346801, 121.577398): 1426, (31.283289, 121.510576): 1427, (31.108792, 121.061126): 1428, (31.007639, 121.416247): 1429, (31.162463, 121.364262): 1430, (31.200249, 121.443811): 1431, (31.224154, 121.43807): 1432, (31.051731, 121.423756): 1433, (31.412391, 121.491247): 1434, (31.30048, 121.469206): 1435, (31.181785, 121.529159): 1436, (31.21852, 121.44019): 1437, (31.047441, 121.470836): 1438, (31.224117, 121.4732): 1439, (31.222272, 121.438997): 1440, (31.405792, 121.489703): 1441, (31.242584, 121.449606): 1442, (31.171916, 121.718029): 1443, (30.946702, 121.624426): 1444, (31.251894, 121.464026): 1445, (24.284812, 102.999068): 1446, (31.253206, 121.459958): 1447, (31.178962, 121.388264): 1448, (31.240806, 121.481992): 1449, (31.225275, 121.47164): 1450, (31.237635, 121.476519): 1451, (31.353624, 121.155902): 1452, (31.168125, 121.503534): 1453, (31.259739, 121.488421): 1454, (31.30685, 121.519579): 1455, (30.790922, 121.348126): 1456, (30.889897, 121.320413): 1457, (31.221662, 121.539905): 1458, (31.242152, 121.609858): 1459, (31.190483, 121.434806): 1460, (31.020849, 121.759757): 1461, (31.152045, 121.420017): 1462, (31.254042, 121.464758): 1463, (31.483764, 121.274813): 1464, (31.044697, 121.22462): 1465, (31.276567, 121.483483): 1466, (30.846072, 121.32313): 1467, (31.310089, 121.640057): 1468, (31.273024, 121.42572): 1469, (28.738742, 120.640606): 1470, (31.288667, 121.527375): 1471, (31.299749, 121.170321): 1472, (30.953524, 121.483682): 1473, (31.240013, 121.487148): 1474, (34.798608, 116.090395): 1475, (31.200719, 121.519009): 1476, (31.126748, 120.934108): 1477, (31.210582, 121.529652): 1478, (31.251161, 121.489743): 1479, (31.139255, 121.409759): 1480, (31.227666, 121.431017): 1481, (31.224551, 121.547983): 1482, (30.908341, 121.870538): 1483, (31.200432, 121.447381): 1484, (30.880562, 121.486561): 1485, (31.113501, 121.393037): 1486, (30.948258, 121.324472): 1487, (31.198799, 121.383701): 1488, (31.230363, 121.5056): 1489, (31.240686, 121.480699): 1490, (31.415595, 121.188844): 1491, (31.060038, 121.195223): 1492, (30.838466, 121.228977): 1493, (30.874727, 121.250912): 1494, (31.010124, 121.538644): 1495, (31.211634, 121.520845): 1496, (31.164838, 121.410811): 1497, (31.229071, 121.452449): 1498, (31.214958, 121.527054): 1499, (31.018662, 121.420209): 1500, (31.207348, 121.70185): 1501, (31.269521, 121.153127): 1502, (31.258903, 121.441965): 1503, (31.304737, 121.522227): 1504, (31.210579, 121.384648): 1505, (31.359545, 121.18156): 1506, (31.074814, 121.713455): 1507, (31.218022, 121.504024): 1508, (31.238742, 121.530031): 1509, (31.318438, 121.497035): 1510, (31.231468, 121.537215): 1511, (31.042348, 121.28911): 1512, (31.244568, 121.441817): 1513, (31.279322, 121.466136): 1514, (31.223446, 121.432052): 1515, (31.228852, 121.541643): 1516, (31.303224, 121.308726): 1517, (31.299874, 121.298501): 1518, (31.207247, 121.572933): 1519, (31.265054, 121.456413): 1520, (31.201064, 121.438699): 1521, (31.213903, 121.410198): 1522, (31.199522, 121.44697): 1523, (31.294834, 121.431554): 1524, (31.241577, 121.485099): 1525, (31.251986, 121.456773): 1526, (31.174287, 121.435995): 1527, (31.330908, 121.45516): 1528, (31.228017, 121.444777): 1529, (31.263495, 121.496653): 1530, (31.260217, 121.525816): 1531, (31.400853, 121.483671): 1532, (31.046753, 121.062205): 1533, (31.19124, 121.507961): 1534, (31.25037, 121.488549): 1535, (31.218516, 121.463689): 1536, (31.234499, 121.463079): 1537, (31.30446, 121.426575): 1538, (31.185189, 121.445449): 1539, (31.229057, 121.452254): 1540, (31.161066, 121.354642): 1541, (31.339206, 121.453188): 1542, (31.293519, 121.22306): 1543, (31.233644, 121.525549): 1544, (31.37273, 121.546117): 1545, (31.16872, 121.768142): 1546, (31.232853, 121.486408): 1547, (31.302355, 121.515812): 1548, (31.302746, 121.376994): 1549, (31.231585, 121.517218): 1550, (30.773575, 121.14093): 1551, (31.215347, 121.134685): 1552, (31.236828, 121.514196): 1553, (31.224539, 121.550163): 1554, (31.305114, 121.41898): 1555, (31.16584, 121.530742): 1556, (31.180581, 121.527653): 1557, (30.895162, 121.094602): 1558, (31.206958, 121.591146): 1559, (31.231369, 121.403139): 1560, (31.242261, 121.507912): 1561, (31.123621, 121.266561): 1562, (31.27369, 121.537119): 1563, (31.210887, 121.315496): 1564, (31.024911, 121.255017): 1565, (31.251619, 121.486701): 1566, (30.962059, 121.244644): 1567, (31.277147, 121.514954): 1568, (31.238815, 121.559352): 1569, (31.32087, 121.393055): 1570, (30.811433, 121.309343): 1571, (31.430745, 121.187414): 1572, (30.974035, 121.554233): 1573, (31.0887, 121.513069): 1574, (31.039411, 121.819385): 1575, (31.188514, 121.49841): 1576, (31.214193, 121.376945): 1577, (31.243974, 121.555758): 1578, (30.891109, 121.903749): 1579, (31.276315, 121.564096): 1580, (31.240837, 121.409164): 1581, (31.039155, 121.241352): 1582, (30.796356, 121.199264): 1583, (31.214342, 121.376038): 1584, (31.365913, 121.177965): 1585, (31.25372, 121.437778): 1586, (31.224373, 121.364172): 1587, (31.275331, 121.286529): 1588, (31.201237, 121.669336): 1589, (30.923134, 121.481921): 1590, (31.246946, 121.513919): 1591, (31.243738, 121.486223): 1592, (31.289509, 121.447288): 1593, (31.297689, 121.447357): 1594, (30.919514, 121.45936): 1595, (31.224126, 121.547664): 1596, (31.280348, 121.592755): 1597, (31.391131, 121.251789): 1598, (31.180897, 121.463904): 1599, (31.188512, 121.437591): 1600, (30.994999, 121.264867): 1601, (31.397648, 121.28551): 1602, (31.293762, 121.442609): 1603, (31.055129, 121.770244): 1604, (31.200811, 121.444401): 1605, (31.246387, 121.458214): 1606, (31.012788, 121.228073): 1607, (30.736314, 121.354852): 1608, (31.246573, 121.509144): 1609, (31.25079, 121.48933): 1610, (31.21245, 121.637072): 1611, (31.201966, 121.545756): 1612, (31.356868, 121.555161): 1613, (31.166046, 121.642137): 1614, (31.221705, 121.642288): 1615, (31.024417, 121.471297): 1616, (31.23936, 121.488752): 1617, (30.741693, 121.367052): 1618, (31.236991, 121.441554): 1619, (31.291242, 121.489761): 1620, (31.122643, 121.734179): 1621, (31.041182, 121.739041): 1622, (31.032045, 121.783775): 1623, (31.235154, 121.533056): 1624, (30.919873, 121.464262): 1625, (31.146285, 121.518893): 1626, (31.247569, 121.41575): 1627, (31.137582, 121.083916): 1628, (31.224552, 121.482535): 1629, (31.313444, 121.309439): 1630, (31.209598, 121.437679): 1631, (31.235043, 121.523389): 1632, (31.188068, 121.699562): 1633, (31.227165, 121.480195): 1634, (31.252877, 121.148181): 1635, (31.223981, 121.478274): 1636, (31.238801, 121.386263): 1637, (30.734301, 121.25722): 1638, (31.261132, 121.093167): 1639, (31.266232, 121.402951): 1640, (31.175983, 121.381611): 1641, (31.272774, 121.436412): 1642, (31.18026, 121.406339): 1643, (31.162714, 121.772242): 1644, (31.00816, 121.274391): 1645, (31.284086, 121.212427): 1646, (31.057607, 121.030715): 1647, (31.067235, 121.762108): 1648, (30.889225, 121.177092): 1649, (31.311405, 121.553882): 1650, (31.252485, 121.549613): 1651, (31.225653, 121.480878): 1652, (30.993467, 121.234309): 1653, (31.243463, 121.4907): 1654, (30.89659, 121.281933): 1655, (31.240412, 121.45723): 1656, (30.912581, 121.465142): 1657, (30.800042, 121.417303): 1658, (31.237435, 121.455917): 1659, (31.253657, 121.443859): 1660, (30.860242, 121.79388): 1661, (46.247857, 128.762232): 1662, (31.289282, 121.378597): 1663, (31.154675, 121.81374): 1664, (31.178088, 121.413515): 1665, (31.273966, 121.429374): 1666, (31.341417, 121.271089): 1667, (31.284292, 121.273506): 1668, (31.219656, 121.49234): 1669, (31.323457, 121.57815): 1670, (31.299073, 121.552202): 1671, (31.258982, 121.494994): 1672, (31.4163, 121.496785): 1673, (31.244555, 121.5099): 1674, (31.344366, 121.51915): 1675, (31.409644, 121.303936): 1676, (31.410089, 121.244982): 1677, (31.234796, 121.533464): 1678, (31.292699, 121.478195): 1679, (31.209704, 121.368486): 1680, (31.278161, 121.486144): 1681, (31.207139, 121.449578): 1682, (31.058917, 121.770251): 1683, (31.429482, 121.184088): 1684, (31.492988, 121.292044): 1685, (30.850119, 121.499982): 1686, (31.084634, 120.989297): 1687, (31.132602, 121.095863): 1688, (30.814394, 121.18284): 1689, (31.262032, 121.117499): 1690, (31.367392, 121.276877): 1691, (31.256595, 121.493953): 1692, (31.214719, 121.624972): 1693, (31.010331, 121.381275): 1694, (31.291773, 121.203129): 1695, (31.045964, 121.420076): 1696, (31.258656, 121.472538): 1697, (30.97533, 121.271449): 1698, (30.9269, 121.467678): 1699, (30.963494, 121.160188): 1700, (31.308192, 121.667576): 1701, (31.223134, 121.43817): 1702, (31.101299, 120.918138): 1703, (31.295004, 121.516367): 1704, (31.288044, 121.356098): 1705, (30.88378, 121.573239): 1706, (31.385543, 121.469415): 1707, (31.210579, 121.453849): 1708, (31.326418, 121.501541): 1709, (31.245242, 121.479617): 1710, (31.253405, 121.091261): 1711, (31.469563, 121.384552): 1712, (31.193515, 121.531662): 1713, (31.241223, 121.45891): 1714, (31.045842, 121.756555): 1715, (30.7653, 121.292527): 1716, (31.278478, 121.420747): 1717, (30.903041, 121.922883): 1718, (31.301967, 121.515226): 1719, (31.194928, 121.366998): 1720, (31.272659, 121.601779): 1721, (30.90618, 121.178287): 1722, (30.976182, 121.750557): 1723, (31.174217, 121.289644): 1724, (31.326619, 121.493433): 1725, (31.19183, 121.59097): 1726, (31.174304, 121.436093): 1727, (31.134484, 121.42764): 1728, (31.244229, 121.475231): 1729, (31.303373, 121.4429): 1730, (31.317584, 121.626266): 1731, (31.407391, 121.490586): 1732, (31.301057, 121.388942): 1733, (31.223107, 121.454329): 1734, (31.283403, 121.541263): 1735, (30.898697, 121.172576): 1736, (31.467705, 121.201637): 1737, (31.198372, 121.355749): 1738, (31.054506, 121.460002): 1739, (31.154909, 121.436989): 1740, (31.307614, 121.526541): 1741, (31.182189, 121.394088): 1742, (30.953548, 121.636227): 1743, (31.486771, 121.360405): 1744, (31.280452, 121.480426): 1745, (31.148785, 121.115114): 1746, (31.15702, 121.106352): 1747, (31.276616, 121.593917): 1748, (31.233452, 121.480931): 1749, (31.24256, 121.490511): 1750, (30.824719, 121.471519): 1751, (31.240453, 121.511773): 1752, (31.214641, 121.525998): 1753, (30.89577, 121.697282): 1754, (30.904648, 121.173227): 1755, (31.052639, 121.761696): 1756, (31.285391, 121.497527): 1757, (31.4193, 121.285094): 1758, (31.166365, 121.419792): 1759, (31.420803, 121.280523): 1760, (31.217162, 121.411803): 1761, (31.159771, 121.58896): 1762, (31.035335, 121.117893): 1763, (31.294215, 121.527852): 1764, (30.882016, 121.532008): 1765, (30.933573, 121.55651): 1766, (31.380359, 121.304458): 1767, (31.410197, 121.500761): 1768, (30.932543, 121.716591): 1769, (31.312117, 121.474859): 1770, (31.48259, 121.348661): 1771, (31.249918, 121.47846): 1772, (31.248195, 121.490929): 1773, (31.060535, 121.274528): 1774, (31.307339, 121.534119): 1775, (30.918284, 120.995861): 1776, (31.181629, 121.510713): 1777, (31.229094, 121.480266): 1778, (31.119755, 121.577952): 1779, (31.304538, 121.526838): 1780, (30.956183, 121.908274): 1781, (31.21899, 121.492002): 1782, (31.20172, 121.743118): 1783, (31.006332, 121.579916): 1784, (25.222206, 117.086322): 1785, (31.18078, 121.423677): 1786, (31.265918, 121.428644): 1787, (31.314684, 121.522729): 1788, (31.211641, 121.474534): 1789, (31.038964, 121.2261): 1790, (31.257756, 121.601633): 1791, (31.19458, 121.436178): 1792, (31.441726, 121.246593): 1793, (31.233151, 121.530498): 1794, (31.298418, 121.539277): 1795, (31.332113, 121.451373): 1796, (31.233984, 121.524823): 1797, (31.378141, 121.503801): 1798, (31.436148, 121.224416): 1799, (31.139656, 121.378972): 1800, (31.235631, 121.567852): 1801, (31.012479, 121.240994): 1802, (31.018041, 121.055413): 1803, (31.165389, 121.119468): 1804, (31.327657, 121.544364): 1805, (31.317512, 121.404518): 1806, (30.990721, 121.803969): 1807, (30.758612, 121.320541): 1808, (31.179594, 121.55951): 1809, (31.264874, 121.459338): 1810, (31.292519, 121.549839): 1811, (31.436901, 121.294601): 1812, (31.169488, 121.349667): 1813, (31.247955, 121.495452): 1814, (31.184426, 121.15811): 1815, (31.193811, 121.373792): 1816, (31.306306, 121.28988): 1817, (31.217261, 121.391532): 1818, (31.039725, 121.225427): 1819, (30.935029, 121.186594): 1820, (30.77648, 121.358227): 1821, (31.258551, 121.584794): 1822, (31.257123, 121.503291): 1823, (31.192397, 121.384407): 1824, (30.901978, 121.452316): 1825, (31.010877, 121.240031): 1826, (31.216472, 121.413336): 1827, (31.174748, 121.39247): 1828, (31.297887, 121.62077): 1829, (31.234688, 121.502656): 1830, (31.275801, 121.484797): 1831, (31.395694, 121.391302): 1832, (30.837343, 121.275733): 1833, (30.957104, 121.146044): 1834, (31.369006, 121.25552): 1835, (31.243266, 121.443875): 1836, (31.156928, 121.121902): 1837, (30.893084, 121.252519): 1838, (31.124812, 121.508263): 1839, (31.340851, 121.63262): 1840, (31.450256, 121.254671): 1841, (31.209424, 121.530284): 1842, (31.267737, 121.529422): 1843, (30.802978, 121.397313): 1844, (31.219647, 121.446669): 1845, (31.248715, 121.481228): 1846, (31.32755, 121.15343): 1847, (31.267826, 121.521146): 1848, (31.261655, 121.474716): 1849, (31.038725, 121.464203): 1850, (31.234225, 121.525481): 1851, (31.217249, 121.504369): 1852, (31.348473, 121.384381): 1853, (31.402193, 121.367083): 1854, (31.274336, 121.39328): 1855, (30.844718, 121.256228): 1856, (31.266646, 121.609157): 1857, (31.186057, 121.44872): 1858, (31.085713, 120.966216): 1859, (31.091117, 121.549904): 1860, (31.19979, 121.389906): 1861, (30.903899, 121.906187): 1862, (31.154462, 121.123826): 1863, (30.838141, 121.189035): 1864, (31.265902, 121.537589): 1865, (30.993285, 121.36769): 1866, (31.21138, 121.728982): 1867, (31.212027, 121.491516): 1868, (31.156454, 121.815297): 1869, (30.898128, 121.536836): 1870, (30.876032, 121.466173): 1871, (31.262331, 121.563212): 1872, (31.038898, 121.23984): 1873, (30.897457, 121.36324): 1874, (31.129482, 121.192266): 1875, (31.320837, 121.542217): 1876, (31.039851, 121.603151): 1877, (31.139017, 121.545465): 1878, (30.869886, 121.901843): 1879, (31.397562, 121.15864): 1880, (31.107362, 121.020176): 1881, (31.21309, 121.531691): 1882, (31.358732, 121.508333): 1883, (31.052253, 121.347624): 1884, (30.781368, 121.418953): 1885, (31.281994, 121.631556): 1886, (31.341175, 121.618019): 1887, (31.41351, 121.367585): 1888, (31.310235, 121.437455): 1889, (31.09599, 121.238111): 1890, (30.931335, 121.472783): 1891, (30.949238, 121.021134): 1892, (31.070433, 121.467199): 1893, (30.992078, 121.31207): 1894, (31.054051, 121.741771): 1895, (31.186698, 121.44855): 1896, (31.267542, 121.685192): 1897, (31.023241, 121.226517): 1898, (31.186176, 121.53911): 1899, (31.280091, 121.519684): 1900, (31.42718, 121.33925): 1901, (31.200179, 121.551597): 1902, (30.935874, 121.692581): 1903, (30.873376, 121.040678): 1904, (31.008378, 121.431928): 1905, (31.370243, 121.576881): 1906, (30.975316, 121.349759): 1907, (31.189366, 121.443482): 1908, (31.217315, 121.635295): 1909, (31.307814, 121.504168): 1910, (30.903117, 121.146985): 1911, (31.285798, 121.546427): 1912, (30.889354, 121.488288): 1913, (31.180526, 121.349428): 1914, (31.198268, 121.383605): 1915, (31.211846, 121.475732): 1916, (31.180824, 121.525363): 1917, (31.20694, 121.459659): 1918, (31.200248, 121.474414): 1919, (31.172778, 121.53453): 1920, (31.266257, 121.506521): 1921, (31.238943, 121.498554): 1922, (31.259098, 121.373134): 1923, (31.198673, 121.479377): 1924, (31.182725, 121.41492): 1925, (31.217729, 121.472441): 1926, (31.288572, 121.534233): 1927, (31.231119, 121.484436): 1928, (31.340297, 121.26652): 1929, (31.236369, 121.476308): 1930, (30.991693, 121.639711): 1931, (31.254643, 121.569349): 1932, (31.18713, 121.451332): 1933, (31.501733, 121.334549): 1934, (30.781759, 121.168333): 1935, (31.196975, 121.440602): 1936, (31.251746, 121.488779): 1937, (31.054254, 121.087156): 1938, (31.200293, 121.400145): 1939, (30.86245, 121.124035): 1940, (30.855815, 121.311882): 1941, (31.302901, 121.553485): 1942, (31.279258, 121.474153): 1943, (31.2798, 121.459715): 1944, (31.291293, 121.559335): 1945, (31.242512, 121.481432): 1946, (31.172866, 121.434994): 1947, (31.292718, 121.410291): 1948, (31.248429, 121.463698): 1949, (31.252347, 121.459317): 1950, (31.268106, 121.49354): 1951, (31.204601, 121.484087): 1952, (31.29962, 121.536202): 1953, (31.327041, 121.661258): 1954, (31.256767, 121.438713): 1955, (31.1634, 121.396418): 1956, (31.150303, 121.430423): 1957, (30.897459, 121.151158): 1958, (31.25142, 121.449186): 1959, (31.444231, 121.225828): 1960, (31.267528, 121.490264): 1961, (31.111061, 121.395692): 1962, (31.223297, 121.430839): 1963, (30.978325, 121.70767): 1964, (31.234221, 121.462296): 1965, (31.112806, 121.462647): 1966, (31.420118, 121.349715): 1967, (30.828106, 121.187081): 1968, (31.19907, 121.517372): 1969, (31.290758, 121.514018): 1970, (31.182122, 121.377455): 1971, (31.206201, 121.085991): 1972, (31.062883, 121.744224): 1973, (31.188332, 121.405187): 1974, (30.918525, 121.74387): 1975, (31.220133, 121.471187): 1976, (31.196602, 121.461665): 1977, (31.248325, 121.158122): 1978, (31.256807, 121.518545): 1979, (31.299376, 121.670896): 1980, (31.211337, 121.413801): 1981, (31.223117, 121.504741): 1982, (31.111358, 121.020928): 1983, (31.241188, 121.483341): 1984, (31.092304, 120.912151): 1985, (30.956486, 121.40597): 1986, (31.203019, 121.443614): 1987, (31.24583, 121.392643): 1988, (31.26905, 121.627732): 1989, (31.243292, 121.518833): 1990, (31.223411, 121.428559): 1991, (31.233617, 121.523688): 1992, (31.356305, 121.293716): 1993, (31.251005, 121.418683): 1994, (31.292709, 121.497635): 1995, (30.738649, 121.382934): 1996, (31.371973, 121.278978): 1997, (31.325552, 121.473746): 1998, (31.286066, 121.474323): 1999, (31.229191, 121.453189): 2000, (30.807278, 121.304136): 2001, (31.219947, 121.467027): 2002, (31.211492, 121.571765): 2003, (31.260723, 121.490166): 2004, (31.195442, 121.754406): 2005, (31.175943, 121.424363): 2006, (30.841016, 121.482504): 2007, (30.940176, 121.086747): 2008, (30.963144, 121.39936): 2009, (31.247628, 121.510061): 2010, (31.1171, 121.391692): 2011, (30.921604, 121.471336): 2012, (31.243738, 121.475784): 2013, (31.392798, 121.246646): 2014, (30.852916, 121.364997): 2015, (31.296619, 121.497055): 2016, (31.404161, 121.47109): 2017, (31.237961, 121.477253): 2018, (31.30361, 121.492602): 2019, (31.242355, 121.490982): 2020, (31.233645, 121.53018): 2021, (31.116086, 121.488253): 2022, (31.38417, 121.344688): 2023, (30.841553, 121.161299): 2024, (31.220424, 121.383546): 2025, (31.268884, 121.44402): 2026, (31.045319, 121.846169): 2027, (31.259841, 121.618221): 2028, (31.150287, 121.471783): 2029, (31.289072, 121.481426): 2030, (31.25867, 121.428896): 2031, (31.260365, 121.454262): 2032, (31.290502, 121.434311): 2033, (31.234105, 121.34044): 2034, (31.332279, 121.286543): 2035, (31.220334, 121.367655): 2036, (31.339843, 121.232133): 2037, (30.85104, 121.524116): 2038, (31.262045, 121.446891): 2039, (31.236742, 121.385193): 2040, (30.902581, 121.167864): 2041, (31.206573, 121.701043): 2042, (31.278711, 121.514214): 2043, (31.252686, 121.491508): 2044, (31.11447, 121.460831): 2045, (31.142588, 121.756503): 2046, (31.243532, 121.330166): 2047, (31.024958, 120.918706): 2048, (30.442177, 120.618727): 2049, (30.815977, 121.436091): 2050, (41.835279, 123.498927): 2051, (31.286253, 121.531109): 2052, (31.195623, 121.398393): 2053, (30.721392, 121.345766): 2054, (31.155148, 121.597536): 2055, (30.872829, 121.424028): 2056, (31.314113, 121.110731): 2057, (31.183064, 121.407773): 2058, (31.242039, 121.687875): 2059, (31.247026, 121.448423): 2060, (31.31764, 121.496784): 2061, (31.246573, 121.495702): 2062, (30.793053, 121.176224): 2063, (31.259114, 121.376147): 2064, (31.272769, 121.57809): 2065, (31.217073, 121.524494): 2066, (31.298712, 121.617334): 2067, (31.225269, 121.491889): 2068, (31.204036, 121.437644): 2069, (31.260715, 121.622366): 2070, (31.267571, 121.5032): 2071, (31.250978, 121.44639): 2072, (31.241054, 121.449606): 2073, (31.206668, 121.449678): 2074, (31.260709, 121.087935): 2075, (31.268862, 121.434292): 2076, (31.348925, 121.178176): 2077, (31.384742, 121.277311): 2078, (30.993594, 121.348984): 2079, (30.94469, 121.299291): 2080, (31.23909, 121.474138): 2081, (31.128344, 121.629697): 2082, (31.497375, 121.324955): 2083, (31.242082, 121.522779): 2084, (31.50595, 121.282812): 2085, (31.279562, 121.467095): 2086, (30.779826, 121.376806): 2087, (31.222754, 121.459663): 2088, (31.415037, 121.260851): 2089, (31.291587, 121.511561): 2090, (31.242967, 121.51048): 2091, (31.312979, 121.545236): 2092, (31.204763, 121.526768): 2093, (31.239615, 121.489026): 2094, (30.929761, 121.587992): 2095, (31.332214, 121.21858): 2096, (31.239474, 121.495166): 2097, (31.225477, 121.448153): 2098, (31.268538, 121.49713): 2099, (31.246371, 121.513215): 2100, (31.216686, 121.493011): 2101, (31.193686, 121.423417): 2102, (31.030606, 121.225475): 2103, (31.262004, 121.350318): 2104, (31.244701, 121.514543): 2105, (31.285418, 121.47179): 2106, (31.250986, 121.460075): 2107, (31.234709, 121.436318): 2108, (31.201321, 121.404211): 2109, (31.236743, 121.36086): 2110, (31.038799, 121.216584): 2111, (31.216659, 121.391301): 2112, (31.226998, 121.556375): 2113, (31.193643, 121.402511): 2114, (31.270785, 121.465575): 2115, (31.110606, 121.517194): 2116, (31.248657, 121.537875): 2117, (31.188145, 121.432275): 2118, (31.02877, 121.456878): 2119, (31.249226, 121.448568): 2120, (31.234382, 121.531106): 2121, (31.22278, 121.450373): 2122, (31.292867, 121.42296): 2123, (31.150587, 121.499019): 2124, (31.362367, 121.461011): 2125, (31.227544, 121.537532): 2126, (31.241242, 121.517216): 2127, (31.20367, 121.543991): 2128, (31.168501, 121.402505): 2129, (31.203981, 121.483421): 2130, (31.29711, 121.519384): 2131, (31.188877, 121.43412): 2132, (31.258321, 121.496097): 2133, (31.194107, 121.399831): 2134, (31.190103, 121.485774): 2135, (31.216092, 121.508679): 2136, (31.016211, 121.412788): 2137, (31.184951, 121.409317): 2138, (31.185044, 121.414303): 2139, (31.19927, 121.6228): 2140, (30.750457, 121.376839): 2141, (31.255248, 121.416487): 2142, (31.198113, 121.590353): 2143, (31.200428, 121.437655): 2144, (31.08705, 121.503604): 2145, (31.10346, 121.419988): 2146, (31.243213, 121.504331): 2147, (31.238774, 121.496149): 2148, (31.310425, 121.439368): 2149, (31.264391, 121.519923): 2150, (31.245488, 121.513767): 2151, (31.302086, 121.510034): 2152, (31.244931, 121.50832): 2153, (31.238392, 121.428006): 2154, (31.233051, 121.455285): 2155, (31.225831, 121.36681): 2156, (31.271455, 121.513192): 2157, (31.462643, 121.331794): 2158, (31.112899, 121.367429): 2159, (31.121996, 121.393805): 2160, (31.273189, 121.472745): 2161, (31.284385, 121.511361): 2162, (31.364338, 121.251014): 2163, (31.178673, 121.521509): 2164, (31.218016, 121.568018): 2165, (31.24018, 121.506063): 2166, (31.195877, 121.597043): 2167, (31.239763, 121.540092): 2168, (31.25863, 121.490496): 2169, (31.24062, 121.428563): 2170, (31.245005, 121.517227): 2171, (31.106732, 121.290566): 2172, (31.250767, 121.458177): 2173, (31.111986, 121.38521): 2174, (31.216528, 121.439617): 2175, (31.229645, 121.481533): 2176, (31.213561, 121.634923): 2177, (30.990222, 121.082742): 2178, (30.918528, 121.113765): 2179, (31.247971, 121.513421): 2180, (31.18458, 121.420921): 2181, (31.236173, 121.466734): 2182, (31.23537, 121.522706): 2183, (31.2106, 121.411728): 2184, (31.21535, 121.46606): 2185, (31.241765, 121.496726): 2186, (31.236197, 121.485283): 2187, (31.389487, 121.259676): 2188, (31.237926, 121.46594): 2189, (31.143681, 121.353698): 2190, (31.197512, 121.470198): 2191, (31.188258, 121.301297): 2192, (31.10221, 121.277743): 2193, (31.087108, 121.371206): 2194, (31.195887, 121.591747): 2195, (31.301584, 121.490912): 2196, (31.098201, 121.323149): 2197, (31.214741, 121.427168): 2198, (31.211757, 121.46438): 2199, (31.266483, 121.545252): 2200, (31.207251, 121.407122): 2201, (31.185126, 121.451633): 2202, (31.463063, 121.345403): 2203, (31.4189, 121.295725): 2204, (31.27726, 121.460289): 2205, (31.169703, 121.412163): 2206, (31.225675, 121.364645): 2207, (31.25879, 121.627202): 2208, (31.143351, 121.352765): 2209, (31.256584, 121.4705): 2210, (31.343386, 121.208271): 2211, (31.250428, 121.460956): 2212, (31.230981, 121.542285): 2213, (31.20414, 121.406657): 2214, (31.212328, 121.407081): 2215, (31.283777, 121.379482): 2216, (31.245183, 121.484965): 2217, (31.475425, 121.356951): 2218, (31.222328, 121.461659): 2219, (31.236576, 121.465894): 2220, (31.234168, 121.473173): 2221, (31.170967, 121.40072): 2222, (30.770905, 121.407376): 2223, (31.198953, 121.516066): 2224, (31.09201, 121.395696): 2225, (31.103957, 121.456645): 2226, (31.236339, 121.52576): 2227, (31.314136, 121.549429): 2228, (31.230406, 121.482326): 2229, (31.22132, 121.438917): 2230, (31.447176, 121.385936): 2231, (31.290912, 121.498587): 2232, (31.277632, 121.380997): 2233, (31.278753, 121.505741): 2234, (31.299613, 121.442959): 2235, (30.975347, 121.032558): 2236, (31.232268, 121.541982): 2237, (31.365012, 121.504701): 2238, (31.391154, 121.246483): 2239, (31.270809, 121.543999): 2240, (31.307358, 121.524928): 2241, (31.235341, 121.477273): 2242, (31.326513, 121.490258): 2243, (31.189816, 121.430881): 2244, (31.140348, 121.210964): 2245, (31.192457, 121.443575): 2246, (31.210657, 121.474053): 2247, (31.209934, 121.604472): 2248, (31.265413, 121.564718): 2249, (31.238839, 121.438593): 2250, (31.280003, 121.361419): 2251, (31.284348, 121.564303): 2252, (31.229151, 121.484838): 2253, (31.235908, 121.57067): 2254, (31.248866, 121.478438): 2255, (31.270571, 121.476897): 2256, (31.227254, 121.402789): 2257, (31.231328, 121.487479): 2258, (31.217037, 121.43649): 2259, (31.229203, 121.449776): 2260, (31.256537, 121.625952): 2261, (31.207094, 121.713617): 2262, (31.2754, 121.485024): 2263, (31.30379, 121.455146): 2264, (31.240865, 121.426827): 2265, (31.379546, 121.260455): 2266, (31.333313, 121.333805): 2267, (31.240405, 121.569186): 2268, (31.206761, 121.399923): 2269, (31.23787, 121.543207): 2270, (31.236531, 121.491325): 2271, (31.281993, 121.464732): 2272, (31.156811, 121.236841): 2273, (31.317624, 121.405225): 2274, (31.193771, 121.546521): 2275, (30.901446, 121.904248): 2276, (31.214969, 121.586763): 2277, (31.234737, 121.513936): 2278, (30.930847, 121.490112): 2279, (31.07252, 121.404373): 2280, (31.305124, 121.454071): 2281, (31.265604, 121.567367): 2282, (31.036007, 121.46539): 2283, (31.224855, 121.480154): 2284, (30.967758, 121.08302): 2285, (31.240661, 121.512436): 2286, (31.243917, 121.496829): 2287, (31.223017, 121.465143): 2288, (30.979419, 121.022464): 2289, (31.262993, 121.08814): 2290, (31.18158, 121.295188): 2291, (31.225663, 121.452017): 2292, (30.940039, 121.548407): 2293, (31.258441, 121.461361): 2294, (31.217655, 121.534591): 2295, (30.891706, 121.171358): 2296, (30.763912, 121.333802): 2297, (31.177291, 121.405153): 2298, (31.152563, 121.460663): 2299, (31.199535, 121.456715): 2300, (31.195642, 121.446626): 2301, (31.399445, 121.464161): 2302, (31.161442, 121.507559): 2303, (31.270493, 121.353356): 2304, (31.211534, 121.510558): 2305, (31.299183, 121.628904): 2306, (31.136936, 121.325876): 2307, (31.267888, 121.46753): 2308, (31.197152, 121.444999): 2309, (31.296101, 121.505474): 2310, (35.522852, 102.0076): 2311, (31.169078, 121.433389): 2312, (31.233659, 121.565806): 2313, (30.97167, 121.185692): 2314, (31.177439, 121.108654): 2315, (31.235501, 121.402602): 2316, (31.299291, 121.454034): 2317, (31.169494, 121.302234): 2318, (31.301105, 121.430139): 2319, (31.15562, 121.130864): 2320, (31.22656, 121.490417): 2321, (31.219856, 121.568149): 2322, (31.283793, 121.518888): 2323, (31.236431, 121.460781): 2324, (31.113675, 121.383752): 2325, (31.236926, 121.454604): 2326, (30.981236, 121.488197): 2327, (31.054777, 121.166179): 2328, (31.237789, 121.461058): 2329, (31.240979, 121.677316): 2330, (31.220745, 121.438858): 2331, (31.23301, 121.556388): 2332, (31.17255, 121.405133): 2333, (31.263879, 121.49436): 2334, (30.987088, 121.588932): 2335, (31.1823, 121.394288): 2336, (31.235419, 121.486195): 2337, (31.217217, 121.558644): 2338, (30.747936, 121.348822): 2339, (31.234835, 121.527767): 2340, (31.237696, 121.382534): 2341, (31.269535, 121.4975): 2342, (31.25841, 121.392912): 2343, (31.228282, 121.556676): 2344, (31.264405, 121.489289): 2345, (31.22978, 121.540987): 2346, (31.238517, 121.429012): 2347, (31.292582, 121.508354): 2348, (31.206458, 121.562768): 2349, (31.23095, 121.537305): 2350, (31.138666, 121.398483): 2351, (31.246038, 121.531441): 2352, (31.187712, 121.432106): 2353, (31.232527, 121.530359): 2354, (31.182032, 121.388195): 2355, (31.193646, 121.449596): 2356, (31.202004, 121.591317): 2357, (31.208153, 121.41083): 2358, (31.025516, 121.30169): 2359, (31.239743, 121.512257): 2360, (31.235554, 121.479641): 2361, (31.419608, 121.431596): 2362, (31.266108, 121.341404): 2363, (31.19622, 121.468181): 2364, (31.233112, 121.442667): 2365, (31.17625, 121.418829): 2366, (31.21759, 121.40829): 2367, (31.180043, 121.609028): 2368, (31.182633, 121.439015): 2369, (31.248135, 121.539233): 2370, (31.150865, 121.336324): 2371, (31.234787, 121.498324): 2372, (31.218928, 121.402968): 2373, (31.233983, 121.455055): 2374, (31.177836, 121.408499): 2375, (31.348039, 121.147744): 2376, (31.386162, 121.255053): 2377, (31.215028, 121.438872): 2378, (31.256803, 121.522183): 2379, (31.241234, 121.460559): 2380, (31.258629, 121.491056): 2381, (30.806653, 121.257345): 2382, (31.228956, 121.481687): 2383, (31.258745, 121.370293): 2384, (31.23282, 121.468049): 2385, (31.228634, 121.536498): 2386, (31.209857, 121.644884): 2387, (31.264302, 121.428277): 2388, (31.210053, 121.464569): 2389, (31.296226, 121.494005): 2390, (31.328439, 121.384246): 2391, (31.179351, 121.31286): 2392, (31.187859, 121.444801): 2393, (31.235982, 121.466191): 2394, (31.37219, 121.448243): 2395, (31.213332, 121.619545): 2396, (31.23423, 121.436418): 2397, (31.237667, 121.434224): 2398, (31.293848, 121.528021): 2399, (31.039424, 121.225364): 2400, (31.244479, 121.429372): 2401, (31.237221, 121.467334): 2402, (31.200668, 121.480775): 2403, (31.230519, 121.520449): 2404, (31.281772, 121.484273): 2405, (31.228956, 121.535003): 2406, (31.203194, 121.46865): 2407, (31.216146, 121.472933): 2408, (31.192935, 121.39968): 2409, (31.270144, 121.354637): 2410, (31.199096, 121.401958): 2411, (31.234888, 121.447606): 2412, (31.228906, 121.451142): 2413, (30.972355, 121.610007): 2414, (31.214689, 121.426655): 2415, (31.21176, 121.45331): 2416, (47.35092, 130.301233): 2417, (31.249796, 121.582523): 2418, (31.359987, 121.417748): 2419, (31.06954, 120.928583): 2420, (31.260348, 121.542138): 2421, (30.881092, 121.162611): 2422, (31.246782, 121.528695): 2423, (30.941552, 121.1676): 2424, (31.154497, 121.501351): 2425, (31.043246, 120.924071): 2426, (31.239726, 121.510008): 2427, (31.156954, 121.146623): 2428, (31.252588, 121.463263): 2429, (31.315817, 121.359172): 2430, (30.899462, 121.164967): 2431, (31.211781, 121.405437): 2432, (31.012103, 121.414993): 2433, (31.186202, 121.54793): 2434, (31.193331, 121.599017): 2435, (31.249356, 121.410275): 2436, (31.161896, 121.135143): 2437, (31.251094, 121.416046): 2438, (31.249067, 121.543826): 2439, (31.256654, 121.599709): 2440, (31.170526, 121.34996): 2441, (31.209454, 121.524245): 2442, (31.232395, 121.564673): 2443, (31.149062, 121.392526): 2444, (31.21882, 121.436387): 2445, (31.236897, 121.363985): 2446, (31.1917, 121.440205): 2447, (31.294781, 121.494788): 2448, (31.231316, 121.54242): 2449, (30.83237, 121.439654): 2450, (31.200779, 121.533917): 2451, (31.236842, 121.433815): 2452, (31.275472, 121.436799): 2453, (31.23672, 121.488128): 2454, (31.260173, 121.348277): 2455, (31.193608, 121.448072): 2456, (31.221688, 121.462858): 2457, (31.242363, 121.480168): 2458, (31.238453, 121.377045): 2459, (31.209344, 121.414036): 2460, (31.306637, 121.523563): 2461, (31.17148, 121.411456): 2462, (31.290316, 121.457814): 2463, (31.186411, 121.402496): 2464, (31.265994, 121.519353): 2465, (31.276741, 121.514196): 2466, (31.278997, 121.510278): 2467, (31.308523, 121.423052): 2468, (31.240977, 121.494087): 2469, (31.188183, 121.440258): 2470, (31.233708, 121.454373): 2471, (31.243061, 121.517168): 2472, (31.248836, 121.463246): 2473, (31.177098, 121.525845): 2474, (31.201009, 121.610969): 2475, (31.063518, 121.215539): 2476, (31.212034, 121.476918): 2477, (31.191496, 121.41093): 2478, (31.227864, 121.638217): 2479, (31.295365, 121.16604): 2480, (31.242706, 121.49688): 2481, (31.123026, 120.926117): 2482, (31.218858, 121.465451): 2483, (31.303333, 121.455058): 2484, (30.879413, 121.704723): 2485, (31.198912, 121.444187): 2486, (31.052413, 121.218957): 2487, (31.192182, 121.466139): 2488, (31.154144, 121.118793): 2489, (31.236055, 121.434759): 2490, (31.256113, 121.510151): 2491, (31.26153, 121.582783): 2492, (31.111905, 121.05229): 2493, (31.197367, 121.479142): 2494, (31.233107, 121.417598): 2495, (31.129363, 121.425144): 2496, (31.267858, 121.495135): 2497, (31.410254, 121.496122): 2498, (31.203742, 121.461251): 2499, (31.338453, 121.449399): 2500, (31.216462, 121.677078): 2501, (31.061709, 121.771457): 2502, (31.173828, 121.781108): 2503, (31.228542, 121.538488): 2504, (31.210354, 121.595268): 2505, (31.275255, 121.45991): 2506, (31.258454, 121.440744): 2507, (31.272257, 121.513471): 2508, (31.233059, 121.458379): 2509, (31.241674, 121.485882): 2510, (31.226676, 121.555069): 2511, (31.211339, 121.406552): 2512, (31.201121, 121.445016): 2513, (31.295646, 121.498045): 2514, (31.156365, 121.41831): 2515, (31.234916, 121.447878): 2516, (30.866941, 121.595653): 2517, (31.261499, 121.457989): 2518, (31.238385, 121.521449): 2519, (31.246512, 121.514772): 2520, (31.213397, 121.413655): 2521, (31.241084, 121.467265): 2522, (31.289799, 121.455117): 2523, (31.146112, 121.504322): 2524, (31.200865, 121.448273): 2525, (31.230671, 121.552848): 2526, (31.249854, 121.497793): 2527, (31.350476, 121.519951): 2528, (31.205115, 121.492127): 2529, (31.211021, 121.569582): 2530, (30.910821, 121.465194): 2531, (31.404925, 121.473328): 2532, (30.864276, 121.112687): 2533, (31.276778, 121.542361): 2534, (31.115773, 120.907942): 2535, (31.25495, 121.739898): 2536, (31.242864, 121.426322): 2537, (31.375832, 121.389413): 2538, (31.244865, 121.549508): 2539, (31.299734, 121.531717): 2540, (30.719644, 121.345988): 2541, (31.246935, 121.49406): 2542, (31.194719, 121.448676): 2543, (31.246271, 121.425156): 2544, (31.17216, 121.347748): 2545, (31.258483, 121.368346): 2546, (31.334437, 121.640763): 2547, (31.234093, 121.475011): 2548, (31.257724, 121.595342): 2549, (30.939279, 121.074396): 2550, (31.262311, 121.449241): 2551, (31.316634, 121.657988): 2552, (31.200875, 121.481499): 2553, (31.234528, 121.528657): 2554, (31.19565, 121.439802): 2555, (31.259353, 121.427018): 2556, (30.940804, 121.073937): 2557, (31.145979, 121.376079): 2558, (31.2114, 121.476689): 2559, (31.233185, 121.481944): 2560, (31.233011, 121.523809): 2561, (31.239149, 121.488022): 2562, (31.239862, 121.490491): 2563, (31.239492, 121.486003): 2564, (31.21351, 121.3728): 2565, (31.235309, 121.529189): 2566, (31.25316, 121.459116): 2567, (31.184752, 121.387556): 2568, (31.213212, 121.470589): 2569, (31.223367, 121.46595): 2570, (31.267673, 121.346341): 2571, (31.264645, 121.446818): 2572, (31.103599, 121.416145): 2573, (29.151779, 120.985563): 2574, (31.230405, 121.534439): 2575, (31.19601, 121.444227): 2576, (31.297053, 121.52596): 2577, (31.311026, 121.456026): 2578, (31.203621, 121.393816): 2579, (31.242132, 121.488632): 2580, (31.139021, 121.323325): 2581, (31.014893, 121.047477): 2582, (31.39714, 121.507729): 2583, (31.246176, 121.494923): 2584, (31.23478, 121.51668): 2585, (31.32589, 121.477565): 2586, (31.351912, 121.58074): 2587, (31.197034, 121.391103): 2588, (31.241675, 121.441049): 2589, (31.237596, 121.488614): 2590, (31.24672, 121.38645): 2591, (31.246871, 121.409978): 2592, (31.199128, 121.540588): 2593, (31.215311, 121.523574): 2594, (31.24188, 121.487961): 2595, (31.224841, 121.440899): 2596, (31.226708, 121.481109): 2597, (31.203225, 121.399229): 2598, (31.263574, 121.586365): 2599, (31.247886, 121.391323): 2600, (31.279298, 121.485756): 2601, (30.735389, 121.358248): 2602, (31.258361, 121.594427): 2603, (31.192395, 121.748164): 2604, (31.223306, 121.378382): 2605, (31.239804, 121.545118): 2606, (31.189897, 121.354305): 2607, (31.301024, 121.509195): 2608, (31.224391, 121.489143): 2609, (31.212572, 121.537172): 2610, (31.160324, 121.508559): 2611, (31.215893, 121.558154): 2612, (31.238616, 121.477514): 2613, (31.226286, 121.600124): 2614, (30.679943, 104.067923): 2615, (31.194364, 121.400164): 2616, (31.253467, 121.447489): 2617, (31.242324, 121.221984): 2618, (31.255872, 121.493382): 2619, (31.012822, 121.051558): 2620, (31.276883, 121.507533): 2621, (31.212793, 121.494061): 2622, (31.189, 121.431937): 2623, (31.249972, 121.422237): 2624, (31.251211, 121.491022): 2625, (31.034225, 121.254609): 2626, (31.240285, 121.546323): 2627, (31.231815, 121.464062): 2628, (31.203422, 121.526246): 2629, (31.191178, 121.46648): 2630, (31.216269, 121.435171): 2631, (31.27495, 121.506381): 2632, (31.233679, 121.494765): 2633, (31.240588, 121.379145): 2634, (31.229886, 121.536598): 2635, (30.95485, 121.328077): 2636, (31.087196, 121.505435): 2637, (31.247703, 121.536476): 2638, (31.152571, 121.412981): 2639, (31.203911, 121.590509): 2640, (31.082557, 121.430065): 2641, (31.224487, 121.452917): 2642, (31.23976, 121.482648): 2643, (31.180728, 121.386832): 2644, (31.210101, 121.414922): 2645, (31.208403, 121.441631): 2646, (31.187492, 121.452395): 2647, (31.135275, 121.765517): 2648, (30.867838, 121.737782): 2649, (31.243189, 121.515482): 2650, (31.187384, 121.470686): 2651, (31.218005, 121.430759): 2652, (30.831962, 121.534676): 2653, (30.730789, 121.343497): 2654, (31.33881, 121.604229): 2655, (31.27688, 121.420858): 2656, (31.18674, 121.380533): 2657, (31.251313, 121.488307): 2658, (31.190941, 121.420437): 2659, (31.271086, 121.549327): 2660, (31.030352, 120.990008): 2661, (31.229148, 121.495962): 2662, (31.19914, 121.438003): 2663, (31.241369, 121.458067): 2664, (31.229998, 121.538807): 2665, (31.230429, 121.468099): 2666, (31.376534, 121.563363): 2667, (30.735272, 121.380359): 2668, (31.287524, 121.51338): 2669, (31.211093, 121.409091): 2670, (31.239094, 121.519821): 2671, (31.259135, 121.616618): 2672, (31.233431, 121.471698): 2673, (31.210632, 121.477007): 2674, (31.23923, 121.483027): 2675, (31.095422, 121.393617): 2676, (31.287554, 121.539): 2677, (31.242208, 121.509707): 2678, (31.245215, 121.601813): 2679, (31.331954, 121.451325): 2680, (31.231035, 121.555602): 2681, (30.929541, 121.616421): 2682, (31.260724, 121.469924): 2683, (31.226563, 121.483486): 2684, (31.184327, 121.381012): 2685, (31.085245, 121.533086): 2686, (31.038935, 121.465427): 2687, (31.232548, 121.478971): 2688, (30.735262, 121.351726): 2689, (31.278145, 121.452159): 2690, (31.264003, 121.489834): 2691, (31.232971, 121.47882): 2692, (31.191797, 121.442896): 2693, (31.274419, 121.475879): 2694, (31.183039, 121.434356): 2695, (31.151008, 121.324166): 2696, (31.245569, 121.5156): 2697, (31.177248, 121.51249): 2698, (31.123763, 120.901878): 2699, (31.242744, 121.425797): 2700, (31.187829, 121.452956): 2701, (31.21953, 121.455395): 2702, (31.167548, 121.425927): 2703, (31.237955, 121.493402): 2704, (31.027984, 121.267724): 2705, (31.260191, 121.617048): 2706, (31.23341, 121.483804): 2707, (31.169908, 121.418183): 2708, (31.203252, 121.441646): 2709, (31.2146, 121.522908): 2710, (31.236337, 121.490883): 2711, (31.40369, 121.255993): 2712, (31.234238, 121.521932): 2713, (31.106411, 121.450195): 2714, (28.812629, 115.952954): 2715, (31.26999, 121.472205): 2716, (31.23811, 121.493447): 2717, (30.995671, 121.095344): 2718, (31.003024, 121.254796): 2719, (31.2419, 121.506952): 2720, (31.168688, 121.487892): 2721, (31.216531, 121.541197): 2722, (31.19053, 121.445372): 2723, (31.198273, 121.445101): 2724, (31.224758, 121.446392): 2725, (31.279618, 121.439189): 2726, (31.250629, 121.598677): 2727, (31.22903, 121.3909): 2728, (31.201959, 121.61095): 2729, (31.231339, 121.543694): 2730, (30.976672, 121.459345): 2731, (31.201817, 121.452116): 2732, (31.110847, 121.509512): 2733, (31.410799, 121.48934): 2734, (31.250145, 121.450312): 2735, (31.22716, 121.386851): 2736, (31.269951, 121.336766): 2737, (31.22626, 121.551311): 2738, (31.315735, 121.534779): 2739, (30.829073, 121.470254): 2740, (31.231401, 121.497622): 2741, (31.234885, 121.529933): 2742, (31.223721, 121.482534): 2743, (31.244249, 121.675329): 2744, (31.309, 121.525098): 2745, (31.213659, 121.455113): 2746, (31.250814, 121.449225): 2747, (31.204322, 121.455438): 2748, (31.294157, 121.432501): 2749, (31.571904, 121.158978): 2750, (31.174223, 121.436719): 2751, (31.170804, 121.278846): 2752, (31.216682, 121.468605): 2753, (31.223648, 121.467523): 2754, (31.245938, 121.422548): 2755, (31.343354, 121.59331): 2756, (30.880213, 121.907656): 2757, (31.198142, 121.446688): 2758, (31.350554, 121.391849): 2759, (31.289528, 121.412608): 2760, (31.243869, 121.440506): 2761, (31.395845, 121.450008): 2762, (31.246433, 121.443184): 2763, (31.231749, 121.486203): 2764, (31.224006, 121.423572): 2765, (31.286523, 121.538876): 2766, (31.239836, 121.497502): 2767, (31.20907, 121.436851): 2768, (31.254745, 121.556081): 2769}\n",
            "{528: 41, 874: 32, 237: 25, 580: 61, 1102: 45, 180: 20, 764: 20, 1638: 29, 558: 137, 652: 96, 158: 73, 446: 63, 1325: 45, 59: 24, 1595: 18, 1009: 21, 886: 16, 285: 32, 1310: 2, 713: 6, 211: 37, 82: 10, 1585: 44, 757: 23, 428: 48, 1194: 52, 1407: 71, 1101: 56, 906: 13, 222: 73, 3: 12, 377: 50, 161: 17, 1549: 18, 260: 200, 2096: 7, 1602: 30, 1572: 12, 120: 16, 2267: 33, 100: 2, 471: 26, 169: 50, 1023: 3, 172: 24, 870: 8, 641: 10, 442: 21, 761: 47, 737: 12, 1241: 12, 2368: 4, 802: 84, 151: 44, 86: 111, 257: 75, 2387: 1, 725: 202, 114: 64, 778: 24, 798: 35, 762: 25, 591: 23, 1105: 5, 1294: 10, 2688: 5, 978: 41, 2242: 2, 305: 9, 1933: 6, 765: 25, 1139: 64, 531: 43, 1658: 19, 654: 38, 389: 65, 385: 183, 865: 133, 139: 115, 70: 87, 2573: 2, 224: 39, 555: 29, 284: 28, 596: 23, 1217: 28, 545: 30, 273: 84, 770: 14, 1134: 53, 320: 156, 668: 30, 807: 11, 242: 60, 264: 166, 225: 40, 917: 45, 749: 48, 698: 68, 520: 82, 1160: 46, 157: 100, 822: 42, 46: 52, 1620: 8, 444: 58, 704: 112, 1422: 58, 214: 102, 673: 98, 77: 48, 1404: 13, 109: 128, 947: 99, 275: 25, 842: 100, 166: 72, 486: 102, 1707: 41, 458: 45, 1030: 33, 173: 129, 1395: 16, 1263: 12, 1273: 26, 407: 42, 800: 36, 1430: 45, 115: 50, 1649: 17, 523: 46, 229: 23, 1396: 2, 848: 5, 1180: 6, 1599: 4, 1126: 45, 712: 5, 977: 23, 614: 11, 948: 10, 453: 36, 747: 8, 1390: 13, 1858: 1, 1581: 6, 1046: 21, 801: 16, 1195: 14, 1737: 8, 780: 27, 2438: 2, 219: 102, 208: 110, 272: 87, 1129: 21, 81: 93, 1528: 21, 907: 55, 586: 51, 1770: 7, 372: 50, 414: 168, 1634: 15, 1158: 24, 1664: 7, 178: 104, 1309: 71, 711: 49, 1411: 109, 376: 51, 1532: 14, 20: 53, 1706: 6, 2485: 2, 491: 104, 2541: 7, 313: 117, 1539: 1, 958: 76, 581: 143, 998: 39, 968: 1, 1270: 40, 99: 21, 306: 72, 74: 48, 221: 54, 231: 82, 1280: 21, 234: 113, 429: 12, 851: 103, 363: 110, 631: 79, 954: 68, 233: 2, 1414: 55, 1022: 41, 1159: 17, 957: 24, 2078: 19, 1011: 25, 2059: 36, 1112: 46, 1397: 69, 1261: 74, 388: 80, 317: 35, 519: 51, 706: 40, 745: 48, 339: 104, 1499: 31, 1756: 22, 1589: 29, 1042: 26, 336: 70, 418: 29, 2501: 1, 2003: 1, 1229: 7, 218: 62, 2036: 8, 1553: 4, 2648: 1, 618: 11, 663: 70, 450: 99, 979: 45, 1088: 35, 441: 64, 123: 100, 477: 43, 595: 42, 903: 45, 382: 31, 980: 16, 1712: 4, 483: 32, 942: 12, 1967: 3, 1065: 14, 1308: 12, 1685: 6, 1144: 31, 379: 170, 24: 57, 71: 19, 1490: 5, 1249: 16, 1286: 26, 611: 28, 721: 13, 1625: 18, 1605: 28, 479: 12, 1031: 13, 537: 6, 2231: 8, 2203: 8, 290: 171, 36: 109, 195: 252, 198: 156, 463: 99, 1415: 1, 858: 161, 769: 108, 15: 53, 468: 24, 680: 4, 14: 75, 390: 131, 1629: 10, 207: 56, 73: 125, 976: 32, 750: 28, 1475: 9, 1778: 4, 1636: 14, 65: 22, 773: 36, 928: 87, 110: 71, 1055: 25, 399: 28, 304: 107, 1037: 27, 844: 28, 1486: 51, 1378: 20, 703: 15, 699: 6, 1981: 14, 246: 40, 690: 48, 2157: 2, 41: 20, 627: 33, 1437: 49, 370: 19, 439: 11, 52: 76, 1048: 5, 1423: 3, 1962: 3, 2011: 4, 793: 89, 638: 46, 1480: 39, 205: 96, 241: 33, 616: 19, 133: 72, 496: 37, 325: 28, 535: 70, 344: 109, 885: 83, 650: 52, 926: 6, 564: 34, 1277: 12, 1342: 49, 1231: 2, 394: 25, 1587: 11, 1593: 18, 1825: 27, 1590: 5, 1699: 9, 1871: 80, 1306: 52, 193: 137, 1184: 21, 1125: 46, 532: 72, 1336: 35, 922: 7, 37: 47, 1049: 55, 331: 28, 983: 11, 715: 17, 1246: 50, 27: 30, 1709: 29, 810: 19, 1000: 11, 1675: 6, 927: 42, 243: 66, 226: 178, 1145: 61, 1156: 15, 1353: 27, 159: 8, 1515: 2, 156: 51, 365: 47, 132: 42, 609: 113, 719: 65, 22: 73, 467: 182, 606: 30, 925: 70, 1135: 19, 121: 15, 1506: 60, 679: 32, 415: 105, 2283: 13, 1462: 3, 1154: 104, 127: 30, 1331: 23, 825: 42, 878: 44, 1269: 6, 2090: 1, 1548: 16, 727: 14, 311: 16, 1619: 15, 1068: 21, 2032: 2, 949: 17, 919: 20, 1446: 38, 1230: 16, 348: 114, 387: 53, 405: 44, 21: 92, 91: 56, 1369: 21, 1252: 15, 171: 57, 1973: 8, 794: 32, 867: 8, 961: 8, 1032: 19, 1013: 9, 56: 123, 80: 140, 1885: 10, 1844: 4, 1103: 42, 741: 78, 2223: 8, 723: 27, 2297: 4, 98: 65, 125: 70, 557: 53, 350: 17, 1379: 11, 1442: 2, 658: 56, 358: 23, 249: 17, 1250: 26, 1402: 6, 476: 14, 7: 44, 1413: 44, 877: 54, 28: 46, 202: 69, 1356: 27, 1900: 2, 364: 6, 461: 23, 1185: 23, 1357: 14, 590: 24, 1421: 3, 1058: 38, 1603: 18, 396: 115, 527: 70, 337: 103, 572: 24, 281: 24, 50: 33, 707: 44, 521: 29, 1216: 11, 701: 31, 1445: 38, 1463: 39, 245: 67, 44: 35, 505: 10, 2429: 5, 1107: 22, 551: 45, 639: 109, 2352: 4, 440: 20, 230: 19, 60: 59, 1202: 7, 714: 36, 1949: 6, 1006: 23, 1040: 17, 2002: 17, 1924: 2, 1311: 19, 648: 8, 1017: 46, 1033: 43, 577: 5, 1224: 28, 13: 32, 775: 7, 51: 36, 876: 23, 946: 51, 134: 12, 549: 20, 971: 22, 254: 25, 182: 5, 457: 57, 530: 36, 913: 45, 1496: 15, 1008: 40, 1441: 28, 753: 30, 2252: 2, 837: 25, 261: 107, 395: 100, 1305: 1, 300: 11, 1728: 14, 605: 210, 1455: 7, 1364: 7, 1239: 38, 836: 102, 1059: 47, 1734: 21, 1432: 5, 548: 10, 1120: 7, 1561: 16, 176: 47, 936: 35, 882: 55, 758: 17, 1255: 15, 625: 29, 341: 30, 48: 20, 1942: 3, 777: 1, 592: 6, 49: 50, 371: 13, 170: 91, 1686: 22, 1206: 25, 475: 17, 2007: 13, 873: 5, 1743: 86, 672: 26, 1723: 3, 748: 16, 2273: 1, 2245: 3, 425: 111, 989: 32, 403: 38, 189: 93, 1298: 7, 1021: 81, 550: 48, 883: 58, 920: 89, 4: 41, 2637: 5, 993: 96, 1312: 56, 83: 78, 588: 74, 734: 89, 755: 59, 1429: 17, 2433: 1, 312: 65, 1433: 41, 850: 98, 1791: 16, 456: 12, 669: 66, 421: 108, 565: 70, 9: 69, 771: 43, 634: 35, 398: 74, 1019: 48, 2377: 1, 401: 120, 164: 36, 92: 89, 17: 126, 1630: 18, 1400: 57, 632: 33, 589: 33, 923: 42, 474: 41, 432: 50, 186: 26, 497: 61, 816: 6, 1200: 12, 934: 73, 113: 15, 2175: 1, 367: 37, 433: 27, 702: 27, 149: 116, 1383: 28, 1057: 53, 1996: 7, 938: 61, 1115: 15, 1784: 8, 152: 19, 283: 115, 651: 88, 32: 50, 1283: 76, 466: 27, 1516: 21, 298: 22, 626: 31, 1592: 5, 1617: 17, 2769: 1, 252: 89, 498: 3, 6: 95, 64: 71, 820: 57, 760: 40, 445: 24, 1985: 1, 2253: 1, 1170: 85, 640: 78, 813: 17, 427: 59, 613: 35, 62: 17, 1436: 6, 742: 25, 1368: 48, 289: 14, 490: 16, 1152: 1, 1018: 11, 1359: 12, 1997: 18, 1993: 7, 1242: 13, 1531: 18, 1086: 36, 122: 53, 633: 10, 544: 45, 1295: 15, 413: 67, 1245: 54, 1450: 9, 61: 68, 392: 53, 1293: 14, 324: 9, 1077: 8, 1484: 40, 451: 45, 1633: 5, 1438: 33, 829: 118, 1132: 51, 965: 46, 347: 46, 1739: 20, 889: 43, 720: 22, 16: 25, 1313: 40, 1990: 2, 763: 23, 996: 15, 1: 60, 1211: 60, 963: 117, 1769: 15, 1114: 24, 1964: 18, 25: 69, 138: 65, 19: 68, 1679: 10, 554: 4, 454: 161, 781: 81, 1883: 4, 1304: 40, 2295: 6, 1537: 12, 1234: 25, 1458: 62, 1361: 24, 891: 29, 1110: 48, 1643: 27, 1497: 10, 1274: 34, 1508: 9, 2006: 15, 2101: 5, 579: 64, 1665: 24, 493: 23, 361: 49, 570: 39, 827: 13, 1647: 7, 1024: 4, 513: 8, 2620: 1, 1405: 23, 404: 6, 845: 22, 1837: 2, 872: 105, 326: 131, 843: 57, 662: 29, 478: 24, 1330: 7, 892: 18, 568: 99, 2390: 22, 562: 105, 623: 89, 834: 35, 806: 10, 184: 70, 1119: 34, 766: 71, 192: 57, 786: 55, 974: 11, 1362: 4, 967: 35, 509: 16, 267: 7, 841: 12, 563: 9, 659: 63, 635: 27, 85: 44, 1758: 10, 1812: 10, 1793: 16, 1760: 4, 1999: 13, 1667: 21, 346: 11, 69: 6, 649: 27, 1078: 44, 183: 77, 187: 84, 964: 27, 124: 63, 277: 19, 716: 167, 322: 19, 682: 80, 1966: 29, 309: 30, 181: 12, 103: 16, 1166: 9, 119: 25, 352: 53, 1130: 8, 1399: 7, 2282: 6, 276: 56, 410: 14, 2471: 3, 1266: 35, 751: 36, 179: 17, 1467: 5, 1941: 11, 746: 46, 1340: 29, 1371: 8, 1655: 15, 1133: 52, 1350: 24, 78: 127, 209: 57, 908: 72, 710: 35, 2074: 4, 375: 82, 1285: 44, 1323: 81, 57: 89, 861: 8, 1626: 2, 826: 50, 251: 97, 1869: 10, 1137: 54, 732: 109, 1096: 50, 656: 22, 315: 48, 973: 14, 1479: 1, 2714: 5, 302: 18, 2546: 3, 1071: 11, 1193: 1, 987: 22, 962: 10, 1613: 14, 516: 64, 1495: 28, 256: 56, 345: 26, 696: 59, 1035: 37, 524: 47, 859: 51, 1616: 53, 282: 132, 881: 22, 901: 20, 1501: 18, 288: 2, 1718: 2, 369: 22, 526: 65, 68: 51, 1939: 6, 612: 22, 1705: 3, 637: 75, 351: 16, 1157: 15, 1676: 29, 893: 93, 95: 134, 1111: 10, 2305: 3, 1786: 7, 2407: 2, 53: 39, 244: 13, 593: 39, 1562: 8, 2393: 4, 1171: 8, 1977: 6, 1236: 38, 1608: 14, 1697: 27, 1736: 16, 1151: 33, 1341: 21, 1817: 15, 1518: 26, 318: 93, 1646: 29, 247: 75, 2211: 28, 1210: 40, 2077: 6, 1391: 5, 201: 70, 1183: 36, 567: 66, 1003: 6, 1412: 5, 2029: 4, 803: 30, 402: 16, 1615: 6, 815: 24, 1426: 36, 1127: 48, 150: 29, 1010: 36, 167: 6, 1670: 17, 855: 3, 933: 46, 1650: 19, 2756: 1, 969: 23, 818: 24, 1457: 37, 1329: 26, 875: 61, 297: 54, 831: 22, 238: 83, 217: 22, 435: 27, 89: 8, 666: 31, 1834: 19, 1284: 36, 1406: 10, 2363: 1, 88: 18, 1327: 11, 473: 32, 541: 13, 47: 106, 500: 35, 356: 45, 538: 19, 2307: 2, 517: 47, 607: 93, 717: 92, 952: 7, 1889: 43, 1056: 5, 1201: 22, 869: 14, 729: 73, 38: 38, 759: 55, 31: 16, 797: 18, 506: 22, 1896: 1, 424: 23, 1465: 2, 409: 11, 1143: 43, 835: 44, 87: 30, 162: 52, 1627: 5, 1640: 9, 852: 38, 147: 8, 154: 24, 574: 90, 2056: 35, 522: 78, 72: 26, 1766: 3, 1282: 8, 299: 96, 1861: 3, 569: 42, 601: 28, 1060: 21, 1025: 2, 731: 11, 1461: 4, 1172: 16, 1223: 18, 492: 42, 34: 34, 540: 24, 362: 64, 1409: 24, 542: 37, 1751: 26, 1367: 95, 1730: 15, 481: 147, 1542: 63, 507: 8, 1233: 23, 2187: 1, 944: 9, 400: 19, 692: 57, 700: 14, 785: 11, 740: 17, 1014: 11, 2559: 2, 144: 38, 534: 20, 1485: 38, 924: 93, 1192: 31, 945: 17, 744: 47, 1226: 14, 743: 8, 1979: 6, 1227: 11, 1146: 5, 1082: 23, 887: 10, 1818: 2, 332: 37, 148: 8, 2654: 2, 1522: 7, 657: 6, 1403: 20, 1789: 11, 814: 14, 1012: 12, 617: 62, 430: 48, 1394: 13, 236: 10, 58: 10, 1316: 5, 1892: 9, 1722: 22, 12: 25, 1443: 5, 26: 16, 508: 41, 1682: 12, 684: 13, 1618: 49, 128: 42, 374: 21, 160: 82, 1372: 10, 2122: 5, 863: 4, 1880: 25, 1684: 6, 112: 4, 896: 8, 1773: 1, 1147: 37, 1113: 15, 902: 34, 174: 19, 1538: 16, 1251: 17, 101: 41, 1746: 10, 1004: 86, 1178: 20, 314: 87, 310: 75, 423: 57, 436: 25, 1321: 19, 18: 32, 438: 18, 271: 29, 792: 13, 196: 16, 163: 24, 416: 9, 2091: 1, 206: 57, 556: 17, 594: 24, 200: 29, 54: 30, 1188: 54, 1898: 2, 1419: 4, 966: 14, 274: 63, 1866: 26, 194: 111, 1694: 67, 2079: 14, 1588: 10, 1131: 10, 1290: 10, 738: 24, 2076: 1, 1642: 6, 910: 41, 2094: 1, 735: 5, 1622: 2, 1489: 2, 1810: 1, 1424: 21, 459: 16, 1919: 19, 1248: 14, 1520: 26, 145: 42, 2133: 1, 718: 50, 1767: 30, 301: 65, 1691: 7, 890: 24, 1867: 7, 1887: 7, 1272: 7, 512: 54, 1820: 11, 752: 12, 1358: 7, 1601: 15, 1354: 1, 235: 21, 1352: 2, 384: 6, 677: 36, 393: 62, 1141: 33, 959: 30, 1854: 33, 1355: 14, 1888: 21, 582: 6, 265: 25, 2218: 20, 1771: 19, 999: 8, 472: 19, 1093: 23, 1983: 6, 645: 12, 1047: 13, 294: 23, 266: 10, 278: 101, 212: 31, 912: 31, 45: 88, 11: 39, 1554: 4, 805: 6, 1905: 1, 1472: 20, 1089: 10, 373: 31, 1320: 6, 539: 19, 2205: 3, 578: 70, 1493: 32, 2001: 10, 1571: 10, 213: 40, 1808: 5, 817: 11, 821: 27, 342: 33, 1657: 20, 494: 54, 2015: 29, 2339: 2, 323: 32, 598: 14, 693: 7, 1929: 5, 23: 8, 155: 14, 2300: 6, 840: 13, 916: 6, 2288: 2, 518: 42, 2: 31, 688: 12, 1628: 18, 1747: 6, 1360: 1, 117: 5, 1199: 1, 559: 14, 55: 17, 286: 14, 683: 35, 191: 94, 2427: 1, 1678: 17, 2147: 4, 2550: 3, 1635: 10, 1986: 18, 2335: 13, 1253: 1, 2009: 16, 1209: 27, 511: 6, 1639: 3, 1319: 7, 279: 20, 2641: 5, 1366: 21, 986: 10, 2098: 3, 1109: 18, 1104: 27, 259: 61, 135: 23, 587: 21, 1510: 23, 1725: 16, 1466: 5, 1198: 17, 788: 11, 1140: 43, 63: 47, 1663: 35, 333: 50, 560: 44, 1668: 7, 1492: 32, 116: 41, 165: 12, 1763: 5, 130: 6, 860: 33, 1469: 1, 1315: 4, 784: 15, 1524: 18, 1075: 20, 250: 43, 1666: 6, 1213: 14, 1586: 3, 2408: 7, 1094: 10, 988: 5, 1931: 7, 1783: 10, 240: 54, 253: 19, 981: 111, 1687: 2, 1262: 37, 510: 63, 1123: 15, 1954: 5, 465: 78, 223: 26, 2185: 3, 1673: 33, 354: 9, 1221: 16, 1027: 8, 726: 44, 137: 7, 129: 31, 628: 20, 514: 43, 296: 12, 1215: 12, 529: 12, 111: 21, 1726: 9, 1281: 13, 502: 41, 681: 4, 420: 49, 929: 9, 1680: 13, 2188: 10, 2204: 3, 1901: 2, 1754: 18, 670: 10, 695: 4, 644: 31, 604: 60, 96: 137, 1653: 3, 1238: 55, 287: 51, 1444: 9, 597: 11, 2095: 1, 1121: 6, 1385: 9, 447: 58, 10: 85, 1081: 13, 1811: 22, 426: 2, 408: 20, 1671: 16, 1451: 7, 1525: 2, 561: 44, 462: 53, 1729: 11, 1416: 24, 1926: 6, 1007: 14, 2596: 1, 1205: 34, 460: 12, 619: 9, 689: 5, 1708: 1, 914: 41, 1264: 6, 1661: 55, 1002: 7, 1299: 8, 1237: 11, 503: 12, 321: 9, 1347: 20, 767: 8, 1714: 1, 603: 13, 1918: 7, 489: 16, 1498: 10, 2073: 5, 1142: 3, 1470: 13, 1755: 3, 1494: 4, 756: 7, 239: 55, 2040: 3, 1118: 2, 722: 24, 1856: 11, 2382: 4, 536: 22, 1344: 5, 2039: 7, 1700: 1, 1659: 6, 904: 13, 232: 8, 1535: 6, 1564: 4, 849: 39, 854: 7, 790: 12, 355: 42, 29: 22, 1275: 31, 146: 15, 1375: 17, 177: 5, 1303: 43, 708: 11, 1701: 13, 655: 36, 694: 75, 291: 55, 1410: 7, 1517: 14, 733: 27, 215: 58, 921: 7, 1546: 9, 386: 4, 799: 8, 1644: 5, 2327: 4, 5: 8, 838: 18, 1039: 20, 955: 18, 1529: 12, 1677: 43, 126: 29, 897: 4, 856: 11, 796: 14, 76: 45, 30: 83, 2325: 3, 1052: 5, 175: 143, 1029: 26, 104: 17, 782: 24, 204: 2, 1398: 3, 1365: 5, 1583: 6, 67: 48, 220: 20, 2081: 1, 2536: 13, 862: 39, 258: 67, 984: 22, 1069: 47, 1148: 15, 552: 33, 1785: 8, 1872: 2, 768: 7, 642: 2, 1116: 35, 1090: 30, 2130: 4, 1913: 32, 1333: 32, 501: 69, 2219: 14, 107: 47, 1045: 28, 1247: 33, 1302: 37, 1349: 37, 2552: 4, 1291: 29, 1257: 3, 960: 73, 819: 1, 783: 5, 2197: 1, 1925: 1, 1256: 12, 1972: 6, 335: 17, 674: 97, 546: 13, 1850: 30, 1324: 7, 504: 35, 866: 2, 1452: 4, 935: 13, 411: 31, 1765: 39, 1870: 7, 1346: 5, 280: 24, 533: 61, 2410: 2, 1874: 30, 956: 18, 1085: 15, 1099: 21, 340: 5, 795: 1, 2500: 2, 1806: 1, 1175: 16, 2012: 2, 1838: 12, 2378: 1, 900: 49, 661: 17, 270: 4, 2068: 1, 664: 11, 2089: 3, 349: 31, 1187: 9, 1155: 10, 584: 32, 199: 20, 2736: 1, 353: 112, 188: 18, 2141: 5, 2054: 4, 2251: 13, 585: 32, 2395: 1, 1832: 10, 2484: 2, 772: 6, 2049: 8, 2166: 1, 1505: 1, 328: 31, 1186: 8, 499: 71, 437: 42, 1067: 17, 982: 4, 1847: 5, 868: 106, 1196: 60, 2048: 4, 1136: 23, 1957: 3, 2274: 1, 269: 16, 1912: 8, 1015: 54, 1735: 35, 884: 7, 1503: 5, 378: 7, 1702: 1, 1481: 3, 1189: 17, 1292: 11, 1153: 15, 905: 8, 1565: 6, 136: 7, 482: 17, 1596: 7, 1511: 6, 140: 34, 1464: 3, 620: 14, 643: 15, 615: 19, 515: 23, 452: 13, 1471: 23, 1258: 9, 880: 43, 1001: 70, 1338: 11, 1300: 27, 1884: 22, 1521: 13, 1645: 43, 1212: 28, 168: 41, 1975: 5, 610: 27, 602: 24, 1072: 14, 1478: 3, 1016: 6, 2183: 12, 1597: 1, 1339: 9, 2037: 2, 576: 8, 39: 21, 1041: 24, 94: 7, 1363: 18, 909: 11, 1388: 18, 1393: 4, 2461: 1, 1328: 8, 2734: 1, 636: 56, 2376: 4, 75: 6, 1243: 12, 2527: 1, 1418: 2, 2092: 3, 2691: 2, 90: 40, 210: 53, 1563: 66, 728: 31, 106: 3, 1865: 2, 368: 29, 839: 16, 624: 61, 975: 1, 262: 2, 1149: 10, 2146: 3, 1733: 31, 1594: 44, 2496: 3, 888: 4, 832: 20, 939: 4, 488: 22, 553: 33, 697: 30, 1779: 3, 2022: 2, 1218: 16, 327: 21, 1757: 2, 779: 10, 316: 8, 1606: 10, 1334: 3, 1169: 8, 1550: 7, 1287: 5, 2189: 1, 2220: 5, 1163: 12, 754: 23, 1821: 16, 105: 5, 1431: 20, 102: 3, 575: 17, 455: 20, 1100: 7, 1530: 3, 2660: 1, 812: 4, 1600: 2, 804: 7, 573: 18, 1559: 6, 1401: 8, 830: 15, 228: 28, 1288: 3, 1689: 15, 1968: 9, 1182: 7, 1073: 3, 203: 5, 1681: 2, 1875: 22, 1662: 8, 1054: 6, 1092: 27, 1191: 4, 898: 11, 864: 30, 1833: 3, 1064: 3, 1799: 22, 248: 36, 1674: 3, 1740: 4, 1380: 27, 449: 10, 2388: 3, 1150: 9, 268: 6, 2020: 2, 1168: 10, 990: 5, 2517: 2, 2293: 3, 329: 45, 422: 23, 383: 4, 1318: 3, 685: 12, 2030: 1, 2236: 7, 2027: 11, 833: 8, 1575: 14, 547: 14, 1482: 2, 2127: 27, 334: 12, 1044: 60, 366: 20, 871: 24, 2050: 4, 647: 13, 1748: 2, 1220: 3, 1932: 3, 2119: 17, 2137: 24, 216: 4, 709: 2, 84: 17, 1063: 4, 2114: 9, 847: 21, 307: 77, 1742: 43, 808: 14, 1908: 4, 1534: 7, 2730: 1, 1976: 8, 2753: 1, 292: 2, 2213: 1, 2449: 1, 2254: 3, 2401: 1, 1652: 1, 2014: 12, 487: 8, 2173: 10, 1098: 20, 970: 16, 2342: 4, 1948: 8, 1573: 22, 255: 6, 1720: 1, 1190: 2, 724: 17, 1846: 5, 2086: 4, 1578: 1, 2754: 1, 360: 27, 2414: 7, 2509: 2, 2453: 3, 2633: 1, 1951: 5, 2512: 1, 600: 47, 2444: 4, 1084: 45, 1987: 1, 543: 11, 263: 5, 131: 40, 2063: 2, 1509: 6, 622: 19, 1823: 4, 2181: 1, 972: 9, 571: 9, 2038: 9, 1864: 5, 2422: 4, 1268: 4, 1911: 4, 2533: 3, 1698: 14, 2024: 3, 1488: 7, 1824: 34, 1641: 1, 1969: 1, 789: 9, 940: 11, 1381: 6, 143: 25, 293: 24, 79: 78, 1556: 38, 2583: 7, 1732: 6, 1091: 7, 943: 17, 1428: 9, 2489: 1, 1545: 14, 1648: 17, 1083: 15, 621: 21, 330: 6, 1621: 23, 1176: 5, 1768: 36, 1796: 2, 1992: 2, 675: 10, 2477: 1, 1301: 30, 2434: 4, 1998: 2, 646: 23, 295: 42, 1244: 18, 2280: 1, 359: 42, 1207: 6, 2535: 2, 1507: 13, 1079: 22, 823: 21, 338: 25, 118: 12, 1279: 1, 2301: 4, 1204: 3, 1557: 3, 853: 9, 2344: 1, 380: 3, 1036: 10, 1434: 2, 434: 2, 1197: 33, 190: 3, 774: 6, 1228: 3, 1828: 13, 1917: 11, 1713: 8, 2374: 7, 1106: 5, 2135: 3, 2488: 3, 2435: 5, 1460: 1, 1138: 6, 1420: 3, 1903: 7, 1020: 25, 2057: 2, 2129: 1, 824: 6, 2217: 1, 2071: 4, 1759: 10, 1117: 7, 1173: 10, 1937: 1, 412: 6, 791: 26, 2225: 9, 2476: 1, 1271: 3, 66: 2, 1982: 2, 1314: 33, 2314: 4, 630: 42, 2372: 7, 153: 3, 1408: 7, 1576: 7, 671: 32, 1902: 4, 2313: 3, 42: 27, 419: 56, 1028: 4, 1859: 12, 1895: 1, 227: 31, 417: 20, 953: 6, 1693: 8, 1519: 4, 2018: 3, 141: 5, 1208: 11, 1798: 9, 2238: 1, 1772: 1, 1855: 7, 1574: 8, 608: 36, 1897: 5, 1792: 5, 899: 4, 448: 25, 1566: 1, 1278: 18, 1448: 6, 2016: 2, 776: 3, 2499: 2, 525: 37, 1980: 6, 995: 7, 1710: 3, 1326: 1, 480: 40, 1845: 3, 2258: 2, 43: 1, 2131: 2, 185: 46, 1296: 16, 1940: 2, 566: 5, 1540: 4, 1074: 23, 1097: 16, 931: 35, 1934: 2, 1080: 5, 730: 4, 2668: 1, 1370: 6, 1179: 15, 676: 7, 2160: 8, 1752: 3, 2472: 2, 2601: 2, 2061: 1, 2562: 2, 997: 1, 660: 8, 2450: 4, 828: 31, 391: 36, 1345: 2, 1533: 1, 1502: 2, 994: 29, 1831: 1, 1745: 13, 1807: 10, 1351: 15, 1623: 5, 93: 5, 1122: 10, 895: 7, 1607: 9, 2075: 2, 932: 17, 2354: 1, 2580: 2, 319: 24, 381: 10, 1552: 22, 2371: 5, 2381: 1, 1804: 2, 1604: 39, 142: 14, 1167: 2, 33: 3, 1066: 7, 1958: 12, 2031: 2, 2725: 1, 930: 20, 1567: 16, 937: 21, 1800: 1, 2595: 1, 2289: 11, 1222: 8, 678: 3, 2112: 2, 2158: 3, 1960: 2, 1841: 1, 2750: 1, 2356: 5, 2456: 1, 1297: 14, 1526: 1, 1656: 3, 2478: 19, 2621: 1, 1440: 10, 1164: 17, 2548: 2, 1816: 3, 1165: 5, 846: 2, 108: 5, 2557: 3, 1776: 1, 787: 4, 1043: 2, 686: 22, 1062: 2, 2190: 21, 2209: 6, 8: 3, 1034: 8, 2359: 13, 1835: 19, 1456: 29, 1921: 9, 1439: 3, 1513: 1, 1624: 3, 2340: 1, 1427: 1, 2346: 4, 1551: 6, 1609: 2, 1881: 10, 1061: 17, 495: 11, 705: 15, 691: 19, 2285: 2, 1343: 41, 2448: 1, 857: 1, 1128: 11, 2673: 1, 2692: 1, 1879: 5, 1387: 1, 2178: 3, 2599: 1, 1914: 1, 1527: 3, 1727: 1, 811: 22, 485: 8, 443: 2, 1894: 4, 1978: 4, 2350: 1, 2066: 1, 197: 35, 1203: 2, 1491: 3, 2309: 7, 2758: 1, 2097: 8, 2717: 1, 1076: 5, 406: 2, 1809: 1, 464: 32, 1254: 5, 2694: 1, 2380: 1, 2689: 1, 397: 3, 1953: 1, 1307: 14, 2576: 2, 303: 1, 2162: 2, 665: 14, 2013: 2, 1660: 1, 1944: 2, 911: 9, 1174: 3, 1459: 3, 2270: 2, 1232: 5, 1805: 2, 2276: 1, 879: 20, 1731: 2, 1569: 17, 1651: 2, 1373: 2, 2000: 1, 2044: 2, 1909: 15, 1890: 1, 1848: 1, 2064: 1, 2343: 1, 1715: 1, 1504: 1, 1267: 12, 2538: 1, 2391: 1, 1570: 11, 1614: 1, 1181: 4, 1780: 5, 1580: 3, 1794: 3, 1873: 4, 2228: 2, 2128: 1, 1240: 1, 951: 4, 2328: 1, 1989: 1, 1579: 3, 2104: 8, 2047: 1, 2111: 5, 35: 2, 1514: 1, 1541: 3, 1598: 2, 1289: 14, 97: 2, 1386: 1, 2266: 2, 1468: 2, 1026: 1, 1547: 3, 2058: 5, 629: 1, 484: 2, 343: 7, 2364: 1, 2224: 4, 2604: 1, 2284: 2, 1337: 4, 1435: 1, 2087: 3, 1842: 1, 1750: 2, 2161: 1, 2208: 2, 2159: 6, 2676: 1, 2568: 1, 1882: 9, 2600: 1, 2142: 3, 2025: 1, 2269: 1, 991: 8, 2113: 7, 2622: 4, 1523: 1, 2035: 1, 1852: 1, 918: 18, 2560: 1, 1830: 1, 2539: 6, 739: 1, 1214: 8, 1695: 1, 894: 73, 2053: 1, 1950: 1, 1558: 20, 1276: 3, 2628: 9, 2246: 1, 2767: 1, 469: 2, 2419: 3, 2682: 1, 2430: 3, 1920: 1, 2534: 1, 2431: 3, 1265: 7, 2351: 2, 1317: 1, 736: 14, 1782: 4, 2070: 4, 1787: 1, 1764: 2, 2677: 1, 2326: 3, 2051: 6, 2619: 1, 1005: 1, 2136: 1, 2532: 9, 1744: 1, 1868: 2, 1984: 1, 1259: 1, 2413: 11, 992: 2, 2103: 3, 1857: 1, 687: 3, 1417: 10, 2623: 1, 2613: 2, 1703: 1, 1688: 1, 1795: 1, 2460: 2, 2510: 1, 1749: 1, 2740: 1, 2260: 5, 2591: 1, 2469: 1, 653: 2, 2639: 1, 2421: 2, 2415: 1, 2713: 1, 1235: 2, 2194: 2, 1927: 1, 2264: 1, 1970: 1, 2085: 1, 667: 1, 1654: 1, 2588: 1, 2582: 7, 1819: 1, 950: 34, 2005: 1, 2763: 3, 2262: 1, 2464: 1, 1839: 3, 1916: 1, 2716: 1, 1815: 18, 2177: 2, 2226: 1, 1425: 1, 599: 1, 2505: 2, 1335: 19, 1893: 1, 1051: 8, 2631: 1, 2603: 1, 2360: 3, 1384: 1, 1683: 1, 1738: 1}\n",
            "2183\n",
            "2769\n",
            "min number of connections for each class 1 2387\n",
            "max number of connections for each class 252 195\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Approach"
      ],
      "metadata": {
        "id": "la6oh-00LxD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "oEoCaV7OLBmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inputs = [inp.tolist() for inp in inputs]\n",
        "# outputs = [out.tolist() for out in outputs]\n",
        "print(inputs[0])\n",
        "print(outputs[0])\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.3, random_state=1)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "zINuSgqbMD8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f314d3-430f-4a83-ef0e-8e6141045b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02,  6.0000e+00,\n",
            "         1.0000e+00,  2.0000e+00,  3.7000e+01,  7.0000e+00, -2.7597e-01,\n",
            "         2.6717e-01])\n",
            "tensor(874)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'max_depth': 3, 'colsample_bytree': 0.8, 'eta': 0.01, 'subsample': 1.0, 'alpha': 0, 'lambda': 1, 'min_child_weight': 5}\n",
        "\n",
        "# model = xgb.XGBRegressor(**parameters)\n",
        "model = xgb.XGBClassifier(max_depth=3, eta=0.01, subsample=1.0, alpha=0, min_child_weight=5,  colsample_bytree=0.8, gpu_id=0)\n",
        "print(model)\n",
        "\n",
        "model.fit(X_train, y_train_encoded)\n",
        "\n",
        "score = model.score(X_test, y_test_encoded)\n",
        "print(\"score: \",score)\n",
        "# cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=5)\n",
        "# print(\"cross score: \",cv_scores)\n",
        "\n",
        "#Predict the next position\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "id": "eZLG_8xU2vif",
        "outputId": "ab21783a-d4a8-415d-8ef3-9eec5c5651b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(alpha=0, base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
            "              feature_types=None, gamma=None, gpu_id=0, grow_policy=None,\n",
            "              importance_type=None, interaction_constraints=None,\n",
            "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
            "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
            "              max_leaves=None, min_child_weight=5, missing=nan,\n",
            "              monotone_constraints=None, multi_strategy=None, n_estimators=None, ...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [10:47:53] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [10:47:53] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score:  0.006807481867922127\n",
            "Accuracy: 0.006807481867922127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode the target\n",
        "\n",
        "\n",
        "#go to GPU\n",
        "# X_train = X_train.to('cuda')\n",
        "# X_test = X_test.to('cuda')\n",
        "# y_train_encoded = torch.tensor(y_train_encoded).to('cuda')\n",
        "# y_test_encoded = torch.tensor(y_test_encoded).to('cuda')\n",
        "#Search for the best parameters\n",
        "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
        "# 'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
        "# 'n_estimators': [100, 200, 300, 400, 500],\n",
        "# 'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
        "# 'min_child_weight': [1, 2, 3, 4, 5]\n",
        "# parameters = {'max_depth': [30, 40, 50, 60, 70, 80, 90, 100], 'learning_rate': [0.05, 0.1, 0.15, 0.2], 'n_estimators': [50, 70, 100, 130, 170], 'gamma': [0, 0.1, 0.2, 0.3, 0.4], 'min_child_weight': [5, 10, 20, 30, 40]}\n",
        "parameters = {'max_depth': [10, 20], 'learning_rate': [0.1, 0.2], 'n_estimators': [50, 70, 100], 'gamma': [0.2, 0.3, 0.4], 'min_child_weight': [10, 5]}\n",
        "# parameters = {'max_depth': [10, 20], 'learning_rate': [0.1, 0.2], 'n_estimators': [30, 50, 10], 'gamma': [0.2], 'min_child_weight': [10, 5]}\n",
        "\n",
        "best_score = -1\n",
        "\n",
        "for max_depth in parameters['max_depth']:\n",
        "    for learning_rate in parameters['learning_rate']:\n",
        "        for n_estimators in parameters['n_estimators']:\n",
        "            for gamma in parameters['gamma']:\n",
        "                for min_child_weight in parameters['min_child_weight']:\n",
        "                    model = xgb.XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, gamma=gamma, min_child_weight=min_child_weight, eta = 0.005,  gpu_id=0) #, gpu_id=0\n",
        "                    model.fit(X_train, y_train_encoded)\n",
        "                    score = model.score(X_test, y_test_encoded)\n",
        "                    print(score)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_parameters = {'max_depth': max_depth, 'learning_rate': learning_rate, 'n_estimators': n_estimators, 'gamma': gamma, 'min_child_weight': min_child_weight}\n",
        "\n",
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "# cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=5)\n",
        "print(\"cross score: \",cv_scores)\n",
        "\n",
        "#Predict the next position\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "id": "R-25EdbFMF0i",
        "outputId": "0eda04ad-07db-43a3-f4e3-756241d9f1bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:09] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:09] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008525257666369766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:15:00] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010115790813080545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:23:17] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008270772362896042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:30:51] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009861305509606821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:39:51] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008461636340501335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:47:34] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009861305509606821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:55:51] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009161470925054078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:06:18] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010115790813080545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:18:16] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009352334902659371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:28:58] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0104338974424227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:41:21] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fd5e016d6aca>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmin_child_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_child_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, gpu_id=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "HKtluODb1CLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Séparation des données par utilisateur et organisation en séquences temporelles\n",
        "user_sequences = {}\n",
        "for entry in inputs_with_pos_id_user_id_target:\n",
        "    user_id, position, time = entry[0], entry[1::2], entry[2::2]\n",
        "    if user_id not in user_sequences:\n",
        "        user_sequences[user_id] = []\n",
        "    user_sequences[user_id].append((position, time))\n",
        "\n",
        "# Normalisation des données\n",
        "scaler = MinMaxScaler()\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    positions = [pos for seq in sequences for pos in seq[0]]\n",
        "    times = [time for seq in sequences for time in seq[1]]\n",
        "    positions_scaled = scaler.fit_transform(np.array(positions).reshape(-1, 1))\n",
        "    times_scaled = scaler.fit_transform(np.array(times).reshape(-1, 1))\n",
        "    user_sequences[user_id] = [(positions_scaled[i], times_scaled[i]) for i in range(len(positions_scaled))]\n",
        "\n",
        "# Padding des séquences\n",
        "max_sequence_length = max(len(seq) for seq in user_sequences.values())\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "# Encodage des séquences (si nécessaire)\n",
        "\n",
        "# Fractionnement des données\n",
        "train_data, val_data, test_data = {}, {}, {}\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    train_size = int(0.7 * len(sequences))\n",
        "    val_size = int(0.15 * len(sequences))\n",
        "    train_data[user_id] = sequences[:train_size]\n",
        "    val_data[user_id] = sequences[train_size:train_size+val_size]\n",
        "    test_data[user_id] = sequences[train_size+val_size:]\n"
      ],
      "metadata": {
        "id": "t24nQ6zkMSyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "f5D9IoBtGX6R",
        "dDvAwpD4GrJu",
        "S_mzoE-MHqLa",
        "ptycyS7FWE4b",
        "3Bbp1dVXWQs3",
        "zZpbR8rG8kBn"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}