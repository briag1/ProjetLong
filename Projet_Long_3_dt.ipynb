{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briag1/ProjetLong/blob/main/Projet_Long_3_dt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# @title device\n",
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "  return device\n",
        "device=get_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZV8IJFGT9T",
        "outputId": "687eae70-eace-4e25-ca4d-da2d3020f910"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Using CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_LSRYqGMO2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qh5nnjRIFe0h"
      },
      "outputs": [],
      "source": [
        "# @title code\n",
        "from os import makedirs\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_x(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[0])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def get_y(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[1])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def read_dataframe(name):\n",
        "  if not os.path.exists(name+\".pkl\"):\n",
        "    print(\"reading dataframe: \"+name+\".xlsx\")\n",
        "    df=pd.read_excel(name+\".xlsx\")\n",
        "    df.to_pickle(name+\".pkl\")\n",
        "  else:\n",
        "    print(\"using already read daframe\")\n",
        "\n",
        "def get_vocab(poses,vocab):\n",
        "  for pos in poses:\n",
        "    if pos not in vocab and not any(isinstance(n, float) and math.isnan(n) for n in pos):\n",
        "        vocab[pos]=len(vocab)+1\n",
        "  return vocab\n",
        "\n",
        "def get_fix_time_encoding(df):\n",
        "\n",
        "  df['month_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "  df['month_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "\n",
        "  df['day_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "  df['day_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "\n",
        "  df['hour_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "  df['hour_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "\n",
        "  df['minute_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "  df['minute_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "\n",
        "  df['second_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "  df['second_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "def get_time_data(df):\n",
        "  df['month'] =  df[\"start time\"].dt.month\n",
        "  df['day'] =  df[\"start time\"].dt.day\n",
        "  df['hour'] =  df[\"start time\"].dt.hour\n",
        "  df['minute'] = df[\"start time\"].dt.minute\n",
        "  df['second'] = df[\"start time\"].dt.second\n",
        "  return df\n",
        "\n",
        "\n",
        "def tokenize_pos(pos,vocab):\n",
        "\n",
        "  if math.isnan(pos[0]) and math.isnan(pos[1]):\n",
        "    return len(vocab)\n",
        "  else:\n",
        "    return vocab[pos]\n",
        "\n",
        "def get_coordinates(df,input_position,full_dataset):\n",
        "\n",
        "  if full_dataset:\n",
        "    df['x'] = df['latitude']\n",
        "    df['y'] = df['longitude']\n",
        "  else:\n",
        "    df['x'] = df['location(latitude/lontitude)'].apply(get_x)\n",
        "    df['y'] = df['location(latitude/lontitude)'].apply(get_y)\n",
        "\n",
        "\n",
        "  if input_position:\n",
        "    df['x_normalised']=(df['x']-df['x'].mean())/(df['x'].std())\n",
        "    df['y_normalised']=(df['y']-df['y'].mean())/df['y'].std()\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_joined_coordinates(df):\n",
        "\n",
        "  df['pos']= list(zip(df['x'],df['y']))\n",
        "  poses=df['pos'].unique()\n",
        "\n",
        "  return poses\n",
        "\n",
        "def get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset):\n",
        "  col_to_drop_in_df=['date', 'end time','pos']\n",
        "  col_to_drop_in_dict=['x','y', 'time_to_end', 'time_to_next','start time', 'user id']\n",
        "  col_to_add_to_dict=[]\n",
        "  col_in_input=[]\n",
        "  if not full_dataset:\n",
        "    col_to_drop_in_df+=['location(latitude/lontitude)']\n",
        "  else:\n",
        "    col_to_drop_in_df+=['latitude','longitude']\n",
        "  if fixed_time_encoding:\n",
        "    col_to_drop_in_df+=[]\n",
        "    col_to_drop_in_dict+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "    col_in_input+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "  else:\n",
        "    col_to_add_to_dict+=['month','day','hour','minute','second']\n",
        "  if input_position:\n",
        "    col_to_drop_in_dict += ['x_normalised', 'y_normalised']\n",
        "    col_in_input+=['x_normalised', 'y_normalised']\n",
        "  return col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict\n",
        "\n",
        "def process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections):\n",
        "  #get the time to next connection\n",
        "  df_user[\"time_to_next\"] =  df_user[\"start time\"].diff(-1).dt.total_seconds()\n",
        "  dict_user=df_user.to_dict('list')\n",
        "  #create input\n",
        "  dict_user[\"pos_id\"],dict_user[\"pos_id_target\"]=torch.tensor(dict_user[\"pos_id\"][:-1]),torch.tensor(dict_user[\"pos_id\"][1:])\n",
        "\n",
        "  if col_in_input:\n",
        "    dict_user[\"input\"]=torch.tensor([dict_user[col] for col in col_in_input]).T\n",
        "    dict_user[\"input\"]=dict_user[\"input\"][:-1]\n",
        "\n",
        "  if col_to_add_to_dict:\n",
        "    for col in col_to_add_to_dict:\n",
        "      dict_user[col]=torch.tensor(dict_user[col])\n",
        "      dict_user[col]=dict_user[col][:-1]\n",
        "\n",
        "  dict_user[\"time_target\"]=torch.tensor([dict_user[\"time_to_end\"],dict_user[\"time_to_next\"]]).T\n",
        "  dict_user[\"time_target\"]=dict_user[\"time_target\"][:-1]\n",
        "  for e in col_to_drop_in_dict:\n",
        "    dict_user.pop(e)\n",
        "\n",
        "  if not with_repeated_connections:\n",
        "    dict_user=combine_repeated_connections_in_sequence_user(dict_user)\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine_repeated_connections_in_sequence_user(dict_user):\n",
        "  index=0\n",
        "  for key in dict_user:\n",
        "    if dict_user[key].shape[0]!=dict_user[\"pos_id\"].shape[0]:\n",
        "      print(key)\n",
        "  #print(dict_user[\"pos_id\"],dict_user[\"pos_id_target\"])\n",
        "  while index < len(dict_user[\"pos_id\"])-1:\n",
        "    #print(dict_user[\"pos_id\"][index],dict_user[\"pos_id_target\"][index])\n",
        "    if dict_user[\"pos_id\"][index]==dict_user[\"pos_id_target\"][index]:\n",
        "      #print(\"equal\")\n",
        "      dict_user[\"pos_id_target\"][index]=dict_user[\"pos_id_target\"][index+1]\n",
        "      dict_user[\"time_target\"][index]=dict_user[\"time_target\"][index+1]\n",
        "      for key in dict_user:\n",
        "\n",
        "        dict_user[key]=torch.cat((dict_user[key][:index+1],dict_user[key][index+2:]))\n",
        "      #print()\n",
        "    else:\n",
        "      #print(\"different\")\n",
        "      index+=1\n",
        "\n",
        "\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def normalize_output(list_users):\n",
        "  #get means and stds\n",
        "  time_targets=torch.cat([dict_user[\"time_target\"] for dict_user in list_users],dim=0)\n",
        "  time_targets_mean=time_targets.mean(dim=0)\n",
        "  time_targets_std=time_targets.std(dim=0)\n",
        "  #normalize\n",
        "  for i in range(len(list_users)):\n",
        "    list_users[i][\"time_target\"]=(list_users[i][\"time_target\"]-time_targets_mean)/time_targets_std\n",
        "  return list_users\n",
        "\n",
        "\n",
        "\n",
        "def process_dataframe(name,vocab,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,format=\".pkl\"):\n",
        "  df= pd.read_pickle(name+format)\n",
        "  df=df.sort_values('start time')\n",
        "  df=df.drop(['month'],axis=1)\n",
        "\n",
        "  df=get_coordinates(df,input_position,full_dataset)\n",
        "\n",
        "  poses=get_joined_coordinates(df)\n",
        "  vocab=get_vocab(poses,vocab)\n",
        "  df['pos_id'] = df['pos'].apply(lambda pos: tokenize_pos(pos,vocab))\n",
        "\n",
        "  df['time_to_end']=df['end time']-df['start time']\n",
        "  df['time_to_end']=df['time_to_end'].dt.total_seconds()\n",
        "  if fixed_time_encoding:\n",
        "    df=get_fix_time_encoding(df)\n",
        "  else:\n",
        "    df=get_time_data(df)\n",
        "\n",
        "  col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict=get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset)\n",
        "  df=df.drop(col_to_drop_in_df, axis=1)\n",
        "\n",
        "  df_user_group = df.groupby('user id')\n",
        "  list_users=[]\n",
        "  for user, df_user in df_user_group:\n",
        "    if len(df_user)>=2 and not df_user['x'].isnull().values.any():\n",
        "        list_users.append(process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections))\n",
        "  list_users=normalize_output(list_users)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "def get_raw_data(directory,src_directory,full_dataset):\n",
        "  if  full_dataset:\n",
        "    shutil.copytree(src_directory,directory)#telecomDataset6mont\n",
        "  else:\n",
        "    runcmd('wget http://sguangwang.com/dataset/telecom.zip', verbose = False)\n",
        "    runcmd('unzip /content/telecom.zip')\n",
        "\n",
        "def get_processed_dataset(load_dataset_path):\n",
        "  saved_list_user_path = os.path.join(load_dataset_path,\"list_users\")\n",
        "  saved_vocab_path = os.path.join(load_dataset_path,\"vocab\")\n",
        "  print(\"loading already preprocessed data: \")\n",
        "  print(saved_list_user_path)\n",
        "  print(saved_vocab_path)\n",
        "  list_users=torch.load(saved_list_user_path)\n",
        "  vocab=torch.load(saved_vocab_path)\n",
        "  return list_users,vocab\n",
        "\n",
        "def process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections):\n",
        "  list_users=[]\n",
        "  vocab={}\n",
        "  if not os.path.exists(directory_raw_data):\n",
        "    print('getting raw data at: '+src_directory_raw_data)\n",
        "    get_raw_data(directory_raw_data,src_directory_raw_data,full_dataset)\n",
        "  if full_dataset:\n",
        "    for name in os.listdir(directory_raw_data):\n",
        "      if not name.endswith(\".pkl\"):\n",
        "        complete_name=os.path.join(directory_raw_data,\".\".join(name.split(\".\")[:-1]))\n",
        "        print(\"processing dataframe: \"+complete_name)\n",
        "        read_dataframe(complete_name)\n",
        "        new_list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "        list_users+=new_list_users\n",
        "  else:\n",
        "    complete_name = \"/content/dataset-telecom/data_6.1~6.30_\"\n",
        "    read_dataframe(complete_name)\n",
        "    list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def split_long_sequences(list_users,max_sequence_length):\n",
        "  new_list_users=[]\n",
        "  for i in range(len(list_users)):\n",
        "    seq_length=list_users[i][\"input\"].shape[0]\n",
        "    if seq_length>=max_sequence_length:\n",
        "      nb_of_seq=seq_length//max_sequence_length\n",
        "      rest=seq_length%max_sequence_length\n",
        "      list_splitted_seq=nb_of_seq*[{}]\n",
        "      rest_splitted={}\n",
        "      for key in list_users[i]:\n",
        "        for j in range(nb_of_seq):\n",
        "          list_splitted_seq[j][key]=list_users[i][key][max_sequence_length*j:max_sequence_length*(j+1)]\n",
        "        if rest>2:\n",
        "          rest_splitted[key]= list_users[i][key][-rest:]\n",
        "      new_list_users=new_list_users+list_splitted_seq\n",
        "      if len(rest_splitted)>0:\n",
        "        new_list_users+=[rest_splitted]\n",
        "    else:\n",
        "      new_list_users.append(list_users[i])\n",
        "\n",
        "  return new_list_users\n",
        "\n",
        "\n",
        "\n",
        "def save_processed_data(list_users,vocab,path_to_save_dataset):\n",
        "    print(\"creating directory: \"+path_to_save_dataset)\n",
        "    os.makedirs(path_to_save_dataset,exist_ok=True)\n",
        "    print(\"saving processed data at: \")\n",
        "    save_list_user_path = os.path.join(path_to_save_dataset,\"list_users\")\n",
        "    save_vocab_path = os.path.join(path_to_save_dataset,\"vocab\")\n",
        "    print(save_list_user_path)\n",
        "    print(save_vocab_path)\n",
        "    torch.save(list_users,save_list_user_path)\n",
        "    torch.save(vocab,save_vocab_path)\n",
        "\n",
        "def get_processed_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,spliting_long_sequences,with_repeated_connections,max_sequence_length=100,min_sequence_length=3,save=False,path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month\",download=False,load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month\"):\n",
        "  if not download:\n",
        "    list_users,vocab = get_processed_dataset(load_dataset_path)\n",
        "  else:\n",
        "    list_users,vocab=process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections)\n",
        "  if spliting_long_sequences:\n",
        "    print(\"spliting sequences longuer than : \"+str(max_sequence_length)+ \" steps\")\n",
        "    list_users=split_long_sequences(list_users,max_sequence_length)\n",
        "  if save:\n",
        "    save_processed_data(list_users,vocab,path_to_save_dataset)\n",
        "  return list_users,vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sSRA1WX8UcsV"
      },
      "outputs": [],
      "source": [
        "# list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "#                                     directory_raw_data='/content/dataset-telecom-6month',\n",
        "#                                     fixed_time_encoding=False,\n",
        "#                                     input_position=True,\n",
        "#                                     full_dataset=False,\n",
        "#                                     spliting_long_sequences=False,\n",
        "#                                     with_repeated_connections=False,\n",
        "#                                     max_sequence_length=100,\n",
        "#                                     min_sequence_length=3,\n",
        "#                                     save=False,\n",
        "#                                     path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",\n",
        "#                                     download=True,\n",
        "#                                     load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip /home/archive.zip\n"
      ],
      "metadata": {
        "id": "swOpL-WiECVR",
        "outputId": "1ff70b75-9311-4e31-eb10-0d24604639e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /home/archive.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of /home/archive.zip or\n",
            "        /home/archive.zip.zip, and cannot find /home/archive.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SFqa2F6YwndZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={}\n",
        "name_data = \"/content/data_9.169.30\"\n",
        "name_data2 = \"/content/data_9.19.15\"\n",
        "complete_name = name_data\n",
        "read_dataframe(name_data)\n"
      ],
      "metadata": {
        "id": "MTyCLcFRWPwG",
        "outputId": "f315fed9-7a1e-4e22-83cd-f2504b3702f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using already read daframe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory_raw_data='/content/dataset-telecom-6month'\n",
        "fixed_time_encoding=False\n",
        "input_position=True\n",
        "full_dataset=True\n",
        "spliting_long_sequences=False\n",
        "with_repeated_connections=False\n",
        "max_sequence_length=100\n",
        "min_sequence_length=3\n",
        "save=False\n",
        "download=True\n",
        "load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\"\n",
        "path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\"\n",
        "list_users1,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "list_users2,vocab= process_dataframe(name_data2,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "list_users = list_users1 + list_users2"
      ],
      "metadata": {
        "id": "BWDWzCzXYvpG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_data_less_than_nb_min(inputs, nb_min):\n",
        "    return [inp for inp in inputs if inp.shape[0]>=nb_min]\n",
        "\n",
        "\n",
        "def inputs_for_classification(list_users, pos_id=True, user_id=True, date_time=False, time_to_end=False, time_to_end_and_target=False, delete_data_less_nb_min=True, nb_min=5):\n",
        "    inputs=[]\n",
        "    pos_id_targets=[]\n",
        "    pos_ids=[]\n",
        "    time_targets=[]\n",
        "    if date_time:\n",
        "        month = []\n",
        "        day = []\n",
        "        hour = []\n",
        "        minute = []\n",
        "        second = []\n",
        "\n",
        "        for user in list_users:\n",
        "            inputs.append(user[\"input\"])\n",
        "            pos_id_targets.append(user[\"pos_id_target\"])\n",
        "            time_targets.append(user[\"time_target\"])\n",
        "            pos_ids.append(user[\"pos_id\"])\n",
        "            month.append(user[\"month\"])\n",
        "            day.append(user[\"day\"])\n",
        "            hour.append(user[\"hour\"])\n",
        "            minute.append(user[\"minute\"])\n",
        "            second.append(user[\"second\"])\n",
        "        # print(pos_ids)\n",
        "        # print(month)\n",
        "    else:\n",
        "        for user in list_users:\n",
        "          inputs.append(user[\"input\"])\n",
        "          pos_id_targets.append(user[\"pos_id_target\"])\n",
        "          time_targets.append(user[\"time_target\"])\n",
        "          pos_ids.append(user[\"pos_id\"])\n",
        "    # print(inputs[0].shape,pos_id_targets[0].shape,pos_ids[0].shape)\n",
        "\n",
        "    if delete_data_less_nb_min:\n",
        "        inputs=delete_data_less_than_nb_min(inputs,nb_min)\n",
        "        pos_id_targets=delete_data_less_than_nb_min(pos_id_targets,nb_min)\n",
        "        pos_ids=delete_data_less_than_nb_min(pos_ids,nb_min)\n",
        "        time_targets=delete_data_less_than_nb_min(time_targets,nb_min)\n",
        "        if date_time:\n",
        "            month=delete_data_less_than_nb_min(month,nb_min)\n",
        "            day=delete_data_less_than_nb_min(day,nb_min)\n",
        "            hour=delete_data_less_than_nb_min(hour,nb_min)\n",
        "            minute=delete_data_less_than_nb_min(minute,nb_min)\n",
        "            second=delete_data_less_than_nb_min(second,nb_min)\n",
        "\n",
        "\n",
        "    if pos_id:\n",
        "        inputs_with_pos_id=[]\n",
        "        for inp,pos in zip(inputs,pos_ids):\n",
        "            inputs_with_pos_id.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))\n",
        "        inputs=inputs_with_pos_id\n",
        "    if date_time:\n",
        "        inputs_with_pos_id_and_date_time=[]\n",
        "        for i in range(len(inputs)):\n",
        "            inputs_with_pos_id_and_date_time.append(torch.cat([inputs[i], month[i].unsqueeze(1), day[i].unsqueeze(1), hour[i].unsqueeze(1), minute[i].unsqueeze(1), second[i].unsqueeze(1)],dim=1))\n",
        "        inputs=inputs_with_pos_id_and_date_time\n",
        "    if time_to_end:\n",
        "        time_to_end =  []\n",
        "        for i in range(len(time_targets)):\n",
        "            tmp = []\n",
        "            for j in range(len(time_targets[i])):\n",
        "                tmp.append(time_targets[i][j][0])\n",
        "            time_to_end.append(tmp)\n",
        "        time_to_end = [torch.tensor(time_to_end[i]) for i in range(len(time_to_end))]\n",
        "\n",
        "        inputs_with_pos_id_and_time_to_end=[]\n",
        "        for inp,pos in zip(inputs,time_to_end):\n",
        "            inputs_with_pos_id_and_time_to_end.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))\n",
        "        inputs=inputs_with_pos_id_and_time_to_end\n",
        "    if time_to_end_and_target:\n",
        "        inputs_with_pos_id_and_time_target=[]\n",
        "        for inp,tim in zip(inputs,time_targets):\n",
        "            inputs_with_pos_id_and_time_target.append(torch.cat((inp, tim), dim=1))\n",
        "        inputs=inputs_with_pos_id_and_time_target\n",
        "    if user_id:\n",
        "        inputs_with_pos_id_user_id=[]\n",
        "        for idx,inp in enumerate(inputs):\n",
        "            inputs_with_pos_id_user_id.append(torch.cat([torch.tensor([idx]*inp.shape[0]).unsqueeze(1),inp],dim=1))\n",
        "        inputs=inputs_with_pos_id_user_id\n",
        "    return inputs,pos_id_targets\n",
        "\n",
        "def inputs_outputs_for_classification(data_entry, targets):\n",
        "  inputs = torch.cat(all_inputs,dim=0)\n",
        "  outputs = torch.cat(pos_id_targets,dim=0)\n",
        "  print(\"len of the dataset\")\n",
        "  print(len(inputs), len(outputs) )\n",
        "  return inputs, outputs\n",
        "\n",
        "def trouver_cle_par_valeur(dictionnaire, valeur_recherchee):\n",
        "    for cle, valeur in dictionnaire.items():\n",
        "        if valeur == valeur_recherchee:\n",
        "            return cle\n",
        "    return None\n",
        "\n",
        "def get_x_y_target_from_outputs(outputs, vocab):\n",
        "    print(outputs[1].item())\n",
        "    x_y = [trouver_cle_par_valeur(vocab, i.item()) for i in outputs]\n",
        "    return [torch.tensor(x_y[i]) for i in range(len(outputs))]\n",
        "\n",
        "def inputs_group_by_user(list_users):\n",
        "  inputs_for_group,pos_id_targets=inputs_for_classification(list_users, pos_id=True, user_id=True, time_to_end=False, time_to_end_and_target=False, delete_data_less_nb_min=True, nb_min=5)\n",
        "\n",
        "  #for each user_id, concatenate by columns the inputs\n",
        "  inputs_group_by_user_id=[]\n",
        "\n",
        "  for i in range(len(inputs_for_group)):\n",
        "    inp = torch.stack(tuple(inputs_for_group[i][j] for j in range(len(inputs_for_group[i]))), dim=1)\n",
        "    inputs_group_by_user_id.append(inp)\n",
        "\n",
        "  inputs_group = [inputs_group_by_user_id[i].numpy() for i in range(len(inputs_group_by_user_id))]\n",
        "  inputs_group = [inputs_group[i].tolist() for i in range(len(inputs_group))]\n",
        "\n",
        "  return inputs_group\n",
        "\n",
        "def group_inputs_by_number_connections_and_users_id(inputs_group, pas_group=5):\n",
        "  nb_connections=[]\n",
        "  for user in inputs_group:\n",
        "    nb_connections.append(len(user[0]))\n",
        "  #group users by number of connections\n",
        "  group = [i for i in range(0, max(nb_connections)+pas_group, pas_group)]\n",
        "  group_users = []\n",
        "  group_users_id = []\n",
        "  for i in range(len(group)-1):\n",
        "    tmp = []\n",
        "    tmp_id = []\n",
        "    for j in range(len(nb_connections)):\n",
        "      if nb_connections[j] >= group[i] and nb_connections[j] < group[i+1]:\n",
        "        tmp.append(nb_connections[j])\n",
        "        tmp_id.append(j)\n",
        "    group_users.append(tmp)\n",
        "    group_users_id.append(tmp_id)\n",
        "  return group_users_id, group_users\n"
      ],
      "metadata": {
        "id": "L4WiObfV-oQL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list_users[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVgwKJjuRdB4",
        "outputId": "72e5dccb-2a8d-4905-9625-e58595c47449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'pos_id': tensor([ 528,  874,  237,  528,  580,  528,  237,  528, 1102,  237,  874]), 'month': tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]), 'day': tensor([ 1,  1,  1,  1,  2,  4,  5,  6, 13, 14, 14]), 'hour': tensor([ 2,  3, 10, 11, 20, 23,  3, 14,  9, 15, 18]), 'minute': tensor([37,  1, 21, 18, 23,  1, 33,  1, 39, 54, 51]), 'second': tensor([ 7,  2, 56, 13,  1, 40, 15, 16, 58, 39, 32]), 'pos_id_target': tensor([ 874,  237,  528,  580,  528,  237,  528, 1102,  237,  874, 1157]), 'input': tensor([[-0.0264,  0.0821],\n",
            "        [-0.0368,  0.0794],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0320,  0.0835],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0264,  0.0821],\n",
            "        [-0.0260,  0.0766],\n",
            "        [-0.0306,  0.0776],\n",
            "        [-0.0368,  0.0794]]), 'time_target': tensor([[-0.2760,  0.2672],\n",
            "        [-0.6802, -0.0909],\n",
            "        [ 0.0460,  0.2394],\n",
            "        [-0.2387, -1.4165],\n",
            "        [-0.6762, -2.3213],\n",
            "        [-0.4034,  0.0545],\n",
            "        [-0.6181, -0.9830],\n",
            "        [-0.6331, -8.1427],\n",
            "        [-0.5198, -1.2704],\n",
            "        [-0.6887,  0.1368],\n",
            "        [-0.3630, -0.8243]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plots"
      ],
      "metadata": {
        "id": "xQPyWItnFg1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def plot_nb_users_by_group_of_nuber_of_connections(list_users):\n",
        "  inputs_group = inputs_group_by_user(list_users)\n",
        "  group_users_id, group_users = group_inputs_by_number_connections_and_users_id(inputs_group)\n",
        "  #plot the number of users in each group\n",
        "  plt.bar([i for i in range(len(group_users))], [len(group_users[i]) for i in range(len(group_users))])\n",
        "  plt.xlabel(\"number of connections\")\n",
        "  plt.ylabel(\"number of users\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_itinerary_users_by_pos_id(list_users, tab_user_id_plot = [444, 445, 446, 447], number_group_to_plot = 6 ):\n",
        "  inputs_group = inputs_group_by_user(list_users)\n",
        "  group_users_id, group_users = group_inputs_by_number_connections_and_users_id(inputs_group)\n",
        "  number_user_plot = len(tab_user_id_plot)\n",
        "\n",
        "  group_of_10 = [[list_users[i] for i in group_users_id[j]] for j in range(len(group_users_id))]\n",
        "\n",
        "  cmap = plt.cm.jet\n",
        "  norm = colors.Normalize(vmin=0, vmax=number_user_plot)\n",
        "  group_3 = [list_users[i] for i in tab_user_id_plot]\n",
        "\n",
        "  # plot the itinerary of 10 users in the dataset\n",
        "  fig, ax = plt.subplots()\n",
        "  for i in range(number_user_plot):\n",
        "      user = group_3[i]\n",
        "      # user = np.random.choice(group_of_10[number_group_to_plot])\n",
        "      pos_id = user[\"pos_id\"]\n",
        "      color = cmap(norm(i))\n",
        "      ax.plot([i for i in range(len(pos_id))], pos_id,  'x', color=color)\n",
        "  ax.set_xlabel('evolution')\n",
        "  ax.set_ylabel('pos_id')\n",
        "  ax.set_title('Itinerary of ' + str(number_user_plot)+ ' users in the dataset')\n",
        "  plt.show()\n",
        "\n",
        "def plot_itinerary_users_by_x_y(list_users, tab_user_id_plot = [444, 445, 446, 447], number_group_to_plot = 6 ):\n",
        "    inputs_group = inputs_group_by_user(list_users)\n",
        "    group_users_id, group_users = group_inputs_by_number_connections_and_users_id(inputs_group)\n",
        "    print(group_users_id)\n",
        "    number_user_plot = len(tab_user_id_plot)\n",
        "\n",
        "    cmap = plt.cm.jet\n",
        "    norm = colors.Normalize(vmin=0, vmax=number_user_plot)\n",
        "    group_3 = [list_users[i] for i in tab_user_id_plot]\n",
        "\n",
        "    # plot the itinerary of 10 users in the dataset\n",
        "    fig, ax = plt.subplots()\n",
        "    for i in range(number_user_plot):\n",
        "        user = group_3[i]\n",
        "        # user = np.random.choice(group_of_10[number_group_to_plot])\n",
        "        x = user['input'][:,0].numpy()\n",
        "        y = user['input'][:,1].numpy()\n",
        "        color = cmap(norm(i))\n",
        "        for i, txt in enumerate(range(1, len(x) + 1)):\n",
        "          plt.annotate(txt, (x[i], y[i]), textcoords=\"offset points\", xytext=(0,0), ha='center', color=color)\n",
        "\n",
        "        ax.plot(x, y,  'x', color=color)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title('Itinerary of ' + str(number_user_plot)+ ' users in the dataset')\n",
        "    plt.show()\n",
        "\n",
        "def plot_pos_id_with_time_to_end_for_user(list_users, user_id):\n",
        "  inputs_with_pos_id_user_id, targets = inputs_for_classification(list_users, time_to_end=True)\n",
        "  tab_user_id = []\n",
        "  for t in range(len(inputs_with_pos_id_user_id)):\n",
        "      if inputs_with_pos_id_user_id[t][0][0].item() == user_id:\n",
        "          tab_user_id = inputs_with_pos_id_user_id[t]\n",
        "  tab_user_id = tab_user_id.numpy()\n",
        "  plt.bar([i for i in range(0, 2*len(tab_user_id), 2)],tab_user_id[:,3], width=tab_user_id[:,4])\n",
        "  plt.xlabel(\"number of connections with its time to end\")\n",
        "  plt.ylabel(\"pos_id\")\n",
        "  plt.title(\"pos_id with time_to_end for user \"+str(user_id))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "s5skrqXLIBHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "# plot_nb_users_by_group_of_nuber_of_connections(list_users)\n",
        "\n",
        "plot_itinerary_users_by_pos_id(list_users, tab_user_id_plot=[1184, 1198, 1206, 1210])\n",
        "plot_itinerary_users_by_x_y(list_users, tab_user_id_plot=[1184, 1198, 1206, 1210])\n",
        "\n",
        "# plot_pos_id_with_time_to_end_for_user(list_users, 123)\n",
        "# plot_pos_id_with_time_to_end_for_user(list_users,124)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 999
        },
        "id": "LR9dxkt9AVNC",
        "outputId": "8cabe121-6dd1-4610-91dc-dcc89071c9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 2]) torch.Size([11]) torch.Size([11])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 2]) torch.Size([11]) torch.Size([11])\n",
            "[[], [10, 15, 30, 38, 40, 44, 46, 47, 49, 50, 55, 58, 65, 69, 70, 72, 74, 76, 81, 86, 99, 109, 111, 118, 120, 126, 128, 131, 135, 145, 148, 155, 158, 160, 162, 163, 179, 189, 192, 199, 200, 202, 205, 208, 210, 215, 216, 219, 223, 226, 227, 230, 234, 240, 244, 251, 252, 256, 259, 262, 265, 270, 278, 279, 284, 289, 294, 296, 304, 306, 308, 311, 314, 315, 317, 342, 351, 352, 354, 356, 357, 358, 363, 366, 367, 369, 387, 391, 400, 402, 403, 404, 406, 407, 413, 419, 421, 424, 426, 437, 440, 448, 460, 461, 486, 487, 499, 512, 526, 537, 538, 541, 548, 550, 555, 556, 558, 559, 565, 568, 570, 576, 578, 586, 588, 590, 598, 603, 611, 618, 619, 628, 631, 637, 638, 649, 652, 657, 662, 665, 675, 681, 693, 697, 700, 704, 708, 710, 712, 725, 727, 729, 730, 738, 739, 746, 750, 753, 754, 758, 761, 771, 778, 779, 786, 791, 792, 795, 798, 800, 803, 804, 805, 806, 809, 811, 817, 819, 828, 834, 840, 842, 844, 852, 853, 858, 863, 873, 874, 875, 877, 880, 888, 892, 899, 902, 905, 913, 919, 921, 925, 927, 928, 931, 936, 941, 946, 950, 953, 954, 955, 956, 963, 966, 968, 976, 978, 988, 993, 998, 1009, 1012, 1013, 1017, 1019, 1021, 1025, 1034, 1036, 1037, 1039, 1040, 1042, 1049, 1061, 1062, 1063, 1065, 1066, 1067, 1068, 1076, 1081, 1087, 1094, 1098, 1102, 1108, 1117, 1118, 1123, 1132, 1134, 1141, 1146, 1147, 1157, 1159, 1161, 1170, 1175, 1183, 1184, 1198, 1206, 1210, 1221, 1222, 1226, 1232, 1243, 1248, 1249, 1252, 1253, 1255, 1258, 1264, 1272, 1273, 1281, 1299, 1302, 1310, 1313, 1318, 1321, 1323, 1329, 1330, 1331, 1332, 1333, 1336, 1339, 1343, 1344, 1345, 1361, 1362, 1371, 1373, 1375, 1376, 1382, 1384, 1388, 1397, 1401, 1403, 1409, 1414, 1418, 1423, 1425, 1427, 1439, 1443, 1444, 1450, 1452, 1453, 1465, 1466, 1467, 1472, 1473, 1478, 1480, 1484, 1492, 1497, 1502, 1505, 1525, 1528, 1529, 1538, 1552, 1553], [0, 1, 5, 8, 12, 14, 27, 29, 31, 36, 68, 77, 90, 97, 103, 112, 116, 123, 127, 129, 137, 147, 149, 166, 167, 204, 207, 212, 214, 218, 220, 222, 247, 255, 268, 269, 273, 282, 290, 293, 313, 326, 328, 332, 335, 343, 345, 347, 349, 350, 353, 392, 405, 408, 427, 428, 438, 443, 449, 452, 457, 469, 477, 478, 480, 481, 483, 488, 489, 490, 491, 492, 493, 504, 505, 521, 533, 534, 539, 543, 545, 552, 561, 573, 575, 587, 589, 591, 592, 597, 602, 609, 612, 614, 615, 617, 620, 624, 627, 634, 641, 651, 653, 654, 664, 670, 671, 676, 677, 699, 722, 723, 731, 734, 735, 736, 740, 743, 748, 751, 765, 766, 774, 781, 789, 796, 799, 826, 827, 835, 843, 845, 850, 862, 867, 872, 894, 898, 903, 915, 917, 918, 933, 967, 973, 984, 985, 992, 999, 1000, 1002, 1004, 1005, 1007, 1008, 1014, 1015, 1016, 1022, 1026, 1027, 1044, 1046, 1052, 1064, 1072, 1073, 1086, 1088, 1091, 1106, 1112, 1120, 1131, 1135, 1138, 1140, 1156, 1165, 1167, 1194, 1196, 1199, 1202, 1212, 1230, 1231, 1242, 1244, 1245, 1250, 1262, 1263, 1277, 1278, 1300, 1314, 1315, 1317, 1340, 1349, 1351, 1354, 1357, 1364, 1367, 1378, 1385, 1387, 1392, 1398, 1407, 1408, 1411, 1429, 1431, 1438, 1454, 1457, 1461, 1481, 1486, 1501, 1503, 1504, 1509, 1516, 1518, 1543, 1547, 1551, 1555], [4, 43, 51, 54, 79, 91, 93, 95, 96, 100, 113, 115, 141, 142, 150, 153, 169, 182, 194, 206, 211, 225, 235, 236, 242, 249, 250, 258, 266, 271, 276, 286, 292, 301, 316, 319, 327, 329, 331, 338, 348, 355, 368, 374, 376, 378, 382, 389, 393, 401, 409, 453, 465, 466, 495, 503, 506, 509, 525, 529, 530, 547, 551, 584, 596, 601, 604, 610, 613, 626, 643, 674, 678, 685, 689, 696, 702, 703, 711, 715, 741, 768, 772, 783, 810, 832, 838, 857, 860, 866, 868, 869, 878, 879, 882, 890, 907, 911, 937, 951, 958, 986, 990, 991, 995, 1003, 1006, 1054, 1055, 1058, 1070, 1078, 1090, 1105, 1107, 1116, 1125, 1143, 1155, 1179, 1186, 1228, 1235, 1241, 1246, 1265, 1271, 1280, 1282, 1288, 1290, 1304, 1326, 1328, 1337, 1342, 1348, 1352, 1353, 1358, 1386, 1412, 1420, 1422, 1483, 1487, 1493, 1512, 1532], [13, 16, 23, 35, 37, 45, 52, 53, 59, 60, 82, 98, 108, 136, 143, 151, 164, 180, 181, 185, 238, 243, 253, 254, 277, 280, 281, 309, 322, 330, 339, 377, 379, 381, 395, 397, 429, 430, 445, 456, 473, 475, 485, 498, 544, 567, 599, 605, 639, 640, 656, 658, 663, 672, 673, 705, 773, 787, 797, 815, 820, 824, 831, 841, 870, 871, 883, 889, 896, 908, 962, 971, 975, 979, 994, 1010, 1018, 1031, 1035, 1047, 1048, 1059, 1082, 1093, 1115, 1144, 1150, 1160, 1163, 1187, 1197, 1205, 1211, 1219, 1220, 1225, 1251, 1267, 1283, 1316, 1334, 1360, 1370, 1372, 1374, 1390, 1404, 1419, 1432, 1433, 1458, 1460, 1462, 1464, 1474, 1489, 1490, 1494, 1500, 1510, 1519, 1520, 1533, 1535, 1554], [33, 56, 73, 78, 84, 89, 101, 140, 144, 152, 175, 184, 186, 190, 198, 229, 263, 264, 272, 303, 334, 359, 384, 390, 410, 412, 417, 434, 444, 450, 500, 553, 564, 582, 621, 633, 646, 650, 669, 687, 688, 692, 713, 714, 720, 724, 733, 745, 749, 755, 759, 777, 790, 851, 859, 861, 864, 886, 923, 929, 972, 1020, 1023, 1028, 1050, 1100, 1103, 1110, 1137, 1164, 1171, 1174, 1176, 1192, 1203, 1208, 1214, 1234, 1286, 1293, 1303, 1320, 1338, 1377, 1399, 1434, 1447, 1522, 1537, 1548, 1550], [64, 83, 134, 154, 171, 231, 233, 245, 287, 297, 324, 333, 340, 344, 371, 386, 396, 398, 422, 431, 458, 467, 479, 484, 496, 508, 522, 554, 560, 593, 625, 632, 644, 648, 684, 695, 717, 719, 728, 742, 769, 833, 855, 856, 885, 939, 942, 977, 1033, 1045, 1074, 1075, 1096, 1114, 1121, 1126, 1181, 1182, 1188, 1189, 1207, 1227, 1257, 1287, 1289, 1308, 1309, 1312, 1322, 1359, 1380, 1383, 1391, 1410, 1428, 1436, 1446, 1451, 1463, 1479, 1485, 1511, 1541, 1549, 1556], [24, 32, 41, 104, 124, 125, 177, 195, 209, 232, 298, 310, 383, 420, 441, 463, 482, 516, 532, 536, 557, 566, 594, 607, 690, 691, 718, 726, 737, 752, 775, 814, 830, 848, 897, 904, 912, 922, 932, 944, 949, 959, 982, 1051, 1092, 1101, 1104, 1154, 1173, 1200, 1209, 1218, 1236, 1237, 1259, 1276, 1284, 1285, 1292, 1294, 1296, 1297, 1365, 1381, 1389, 1406, 1435, 1440, 1445, 1448, 1468, 1471, 1476, 1477, 1491, 1499, 1513, 1531, 1540, 1542, 1545], [6, 9, 19, 20, 25, 42, 61, 75, 94, 106, 107, 117, 173, 178, 183, 188, 302, 323, 360, 364, 433, 464, 474, 476, 524, 562, 600, 655, 661, 698, 747, 793, 802, 839, 865, 881, 906, 926, 957, 960, 965, 1041, 1085, 1133, 1213, 1217, 1223, 1256, 1306, 1325, 1394, 1400, 1415, 1421, 1424, 1426, 1488, 1507, 1527, 1539, 1544], [66, 92, 119, 121, 161, 224, 299, 305, 375, 447, 470, 502, 523, 563, 571, 583, 636, 660, 668, 763, 812, 813, 829, 916, 934, 981, 1038, 1097, 1109, 1111, 1113, 1122, 1127, 1149, 1168, 1229, 1238, 1301, 1369, 1430, 1437, 1456, 1469, 1482, 1523], [11, 28, 85, 193, 201, 213, 248, 442, 514, 540, 623, 645, 683, 707, 716, 721, 757, 823, 893, 914, 938, 945, 964, 974, 1053, 1056, 1089, 1139, 1142, 1145, 1172, 1177, 1191, 1195, 1201, 1216, 1307, 1449, 1470, 1475], [3, 18, 21, 34, 67, 88, 146, 217, 257, 312, 336, 346, 372, 416, 418, 432, 439, 497, 511, 531, 569, 580, 666, 694, 706, 762, 891, 924, 947, 969, 1024, 1030, 1060, 1152, 1178, 1215, 1266, 1268, 1327, 1346, 1396, 1495, 1496, 1534, 1536], [2, 48, 170, 191, 288, 321, 415, 425, 472, 630, 635, 647, 732, 744, 770, 788, 822, 849, 854, 876, 895, 920, 989, 1001, 1029, 1071, 1084, 1095, 1124, 1130, 1136, 1151, 1169, 1180, 1185, 1291, 1319, 1379, 1515], [71, 157, 197, 237, 275, 370, 414, 423, 435, 446, 455, 507, 577, 579, 682, 756, 764, 821, 837, 961, 997, 1128, 1162, 1193, 1239, 1260, 1270, 1298, 1324, 1350, 1355, 1441], [7, 130, 168, 261, 295, 494, 515, 572, 595, 608, 679, 686, 900, 930, 987, 1057, 1099, 1204, 1233, 1240, 1275, 1395, 1402, 1455, 1524], [239, 318, 337, 380, 399, 528, 574, 606, 780, 782, 784, 794, 807, 808, 847, 901, 909, 1011, 1119, 1158, 1224, 1247, 1254, 1274, 1341, 1363, 1368], [17, 57, 203, 246, 260, 291, 365, 388, 454, 471, 767, 887, 948, 970, 1043, 1077, 1148, 1335, 1356, 1366, 1506, 1521], [174, 196, 285, 362, 394, 462, 581, 667, 709, 818, 836, 952, 1279, 1413, 1416, 1417, 1459], [165, 518, 519, 520, 549, 616, 659, 680, 996, 1069, 1083, 1153, 1269, 1347, 1442], [110, 385, 436, 585, 642, 1393, 1517], [114, 133, 501, 801, 910, 940, 1295, 1305], [172, 283, 341, 468, 517, 1166], [102, 274, 361, 459, 510, 622, 884, 1498], [221, 325, 411, 546, 776, 785, 846, 943, 1032, 1557], [105, 122, 156, 159, 187, 1514], [26, 39, 139, 241, 307, 513, 980, 1261], [320, 1311, 1405, 1526], [80, 132], [62, 63, 1129], [983, 1530], [228, 267, 527, 542, 1508], [176, 300], [22, 451], [373, 935], [1190], [], [138, 1080], [], [825, 1079], [87, 535, 760], [], [701], [], [], [1546], [816], [], [], [], [629]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5D9IoBtGX6R"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvAwpD4GrJu"
      },
      "source": [
        "## Reproducibility seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p5d4mp9GDah"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import string\n",
        "import random\n",
        "def get_reproducible_seeds(name=\"ProjectLong\",nb_seeds=100):\n",
        "    # Calculate SHA-256 hash\n",
        "    sha256_hash = hashlib.sha256(name.encode()).hexdigest()\n",
        "    # Define character sets\n",
        "    digits = string.digits\n",
        "    # Use the hash to seed the random number generator\n",
        "    hash_as_int = int(sha256_hash, 16)\n",
        "    random.seed(hash_as_int)\n",
        "    # Generate a random list of seed of desired length\n",
        "    reproducibility_seeds = [random.randint(0,10000) for _ in range(nb_seeds)]\n",
        "\n",
        "    return reproducibility_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn8U0p9TGJXK"
      },
      "outputs": [],
      "source": [
        "reproducibility_seed=get_reproducible_seeds()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mzoE-MHqLa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nhxCSLNHsd9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class VariableLengthDatasetWithPosID(Dataset):\n",
        "    def __init__(self, time_series, transform=None):\n",
        "        self.times_series=time_series\n",
        "    def __len__(self):\n",
        "        return len(self.times_series)\n",
        "    def __getitem__(self, idx):\n",
        "        user_dict=self.times_series[idx]\n",
        "        return  user_dict\n",
        "\n",
        "def create_dataset(list_users,split=[0.8,0.1,0.1]):\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "  return dataset_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGgl2XnIhDQ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nke01dG-KJxO"
      },
      "outputs": [],
      "source": [
        "def collate_fn_padd(batch_dict):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "\n",
        "\n",
        "    dict_batch={key: [d[key] for d in batch_dict] for key in batch_dict[0]}\n",
        "    dict_batch[\"lengths\"] = torch.tensor([ user[\"input\"].shape[0] for user in batch_dict ])\n",
        "    if \"input\" in dict_batch:\n",
        "      dict_batch[\"input\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"input\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"month\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"month\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"day\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"day\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"hour\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"hour\"],batch_first=True,padding_value=24)\n",
        "    dict_batch[\"minute\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"minute\"],batch_first=True,padding_value=60)\n",
        "    dict_batch[\"second\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"second\"],batch_first=True,padding_value=60)\n",
        "\n",
        "    dict_batch[\"time_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"time_target\"],batch_first=True,padding_value=-1)\n",
        "    dict_batch[\"pos_id\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id\"],batch_first=True,padding_value=len(vocab))\n",
        "    dict_batch[\"pos_id_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id_target\"],batch_first=True,padding_value=len(vocab))\n",
        "    #print(dict_batch[\"input\"])\n",
        "    return dict_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Yl1E6gY8_P"
      },
      "source": [
        "## Instanciate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHpriSipY7Kz"
      },
      "outputs": [],
      "source": [
        "dataset_list=create_dataset(list_users)\n",
        "train_dataset=dataset_list[0]\n",
        "valid_dataset=dataset_list[1]\n",
        "test_dataset=dataset_list[2]\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=64,collate_fn=collate_fn_padd,shuffle=True)\n",
        "valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HodJbvKeMe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptycyS7FWE4b"
      },
      "source": [
        "## Transformer Encoder followed by LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oeWr0HDhJfo"
      },
      "source": [
        "### transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_bACPajeQx"
      },
      "outputs": [],
      "source": [
        "def get_mask(bath_size,sequence_length,lengths,device):\n",
        "  mask=torch.zeros(bath_size,sequence_length).to(device)\n",
        "  for i, length in enumerate(lengths):\n",
        "    mask[i,length:]=1\n",
        "  return mask.bool()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsaggvjghDsq"
      },
      "source": [
        "#### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yFXZxqeHMwi"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class VanillaPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = (x.transpose(0,1) + self.pe[:x.transpose(0,1).size(0)]).transpose(0,1)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX-kk1ScG-_n"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.positional_embedding=nn.Embedding(num_embeddings=max_len,embedding_dim= d_model)\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(self.parameters()).device\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size,seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x += self.positional_embedding(torch.arange(0,x.shape[1]).to(self.device))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzKYvfaWG6cH"
      },
      "outputs": [],
      "source": [
        "def get_PositionalEncoding(d_model: int, dropout: float = 0.1, max_len: int = 2000, learnable=False):\n",
        "  if learnable:\n",
        "    return LearnablePositionalEncoding(d_model, dropout, max_len)\n",
        "  else:\n",
        "    return VanillaPositionalEncoding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-pr0W6xhOkZ"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZQnLdN4UxY"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,  dropout=dropout, batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only=False,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,batch_first=batch_first)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers,nhead,dropout,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCkeqmkhRnT"
      },
      "source": [
        "### feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmqQtP0UhZqg"
      },
      "outputs": [],
      "source": [
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=embedding_dim)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=embedding_dim)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=embedding_dim)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "\n",
        "  def forward(self,dict_batch):\n",
        "    embedding= self.month_embedding(dict_batch['month'])\n",
        "    embedding=+ self.day_embedding(dict_batch['day'])\n",
        "    embedding=+ self.hour_embedding(dict_batch['hour'])\n",
        "    embedding=+ self.minute_embedding(dict_batch['minute'])\n",
        "    embedding=+ self.second_embedding(dict_batch['second'])\n",
        "    return self.dropout(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCp5Pz6uy-FG"
      },
      "outputs": [],
      "source": [
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bbp1dVXWQs3"
      },
      "source": [
        "#### graph_deepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW174iJrhFC",
        "outputId": "6ad9be4e-f4cd-40d0-a6fb-0cd7c1b561d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpysal\n",
            "  Downloading libpysal-4.9.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.12.3)\n",
            "Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from libpysal) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.25.2)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (23.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.11.4)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.10->libpysal) (2.5)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (1.9.5)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (67.7.2)\n",
            "Installing collected packages: libpysal\n",
            "Successfully installed libpysal-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install libpysal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQvAPDZyWNtg",
        "outputId": "c0fe4dde-5968-49ea-c87f-398c269f8a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=3049d4ae756209f0ed6d2a1856d71ae9d30d88b08233d9124d9ba4a0cbc02455\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdSTBOc3sKsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, hidden_dim1, hidden_dim2, output_dim,vocab,dropout,device):\n",
        "    super(GCN, self).__init__()\n",
        "    net=get_net(vocab)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "    self.conv1 = GCNConv(2, hidden_dim1)\n",
        "    self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
        "    self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.conv1(self.coordinates, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "    x = self.conv2(x, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv3(x, self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozXR4sW0W0Y"
      },
      "source": [
        " #### Combine feature embeddng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6mU1qWOjRP3"
      },
      "outputs": [],
      "source": [
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+use_gcn\n",
        "    self.concatenate_features=concatenate_features\n",
        "    self.embedding_dim=d_model\n",
        "    self.keep_input_positions=keep_input_positions\n",
        "    if keep_input_positions:\n",
        "      self.embedding_dim=self.embedding_dim-2\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,nb_of_pos_ids,dropout),TimeStampEmbedding(self.embedding_dim,dropout)]\n",
        "    if use_gcn:\n",
        "      list_feature_embedding.append(GCN(hidden_dim1, hidden_dim2, self.embedding_dim, vocab, dropout,device))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "    if self.keep_input_positions:\n",
        "      embedding=torch.cat((dic_batch[\"input\"],embedding),dim=2)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svMRI0xeji-7"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSt_zuJRKgBh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(F.relu(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.layer_normalisation:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,d_model,nb_of_pos_ids,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,num_layers_transformer,encoder_only,nhead,learnable_pos_encoding,new_station_binary_classification,use_gcn,vocab,hidden_dim1, hidden_dim2,max_len,dropout,batch_first,concatenate_features,keep_input_positions,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.feature_embedding=Feature_embedding(d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device)\n",
        "\n",
        "    self.num_layers_transformer=num_layers_transformer\n",
        "    if num_layers_transformer>0:\n",
        "      self.pos_encoder = get_PositionalEncoding(d_model, dropout, max_len,learnable_pos_encoding)\n",
        "      self.transformer_model=get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,nhead,dropout,batch_first)\n",
        "\n",
        "    self.num_layers_lstm=num_layers_lstm\n",
        "    if num_layers_lstm>0:\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(d_model,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first) for layer in range(num_layers_lstm)])\n",
        "    self.linear_reg=nn.Linear(d_model,output_regression_size)\n",
        "    self.classifier=nn.Linear(d_model,output_classfication_size)\n",
        "\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.binary_classifier=nn.Linear(d_model,1)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.num_layers_transformer>0:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.num_layers_lstm>0:\n",
        "      if self.num_layers_transformer>0:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.num_layers_lstm>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=F.relu(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(torch.exp(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.new_station_binary_classification:\n",
        "      out[\"new_station\"]=  torch.nn.utils.rnn.PackedSequence( self.binary_classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baselines"
      ],
      "metadata": {
        "id": "zZpbR8rG8kBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class  Baseline_model(nn.Module):\n",
        "  def __init__(self,nb_of_pos_ids):\n",
        "    super().__init__()\n",
        "    self.nb_of_pos_ids=nb_of_pos_ids\n",
        "  def forward(self,dic_batch,reg):\n",
        "    out={}\n",
        "    out[\"next_station\"]=  torch.nn.utils.rnn.pack_padded_sequence(F.one_hot(dic_batch[\"pos_id\"],self.nb_of_pos_ids).float(), lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    return out"
      ],
      "metadata": {
        "id": "jn0xR-ME8tRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Baseline_model(len(vocab)+1)\n",
        "criterion=Total_loss(False)\n",
        "evaluate(model,valid_dataloader,criterion,device,reg=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYH5OMnmELSa",
        "outputId": "fb668202-96cb-41cb-9f16-df7516a96334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classification': 0.0021053161556483374,\n",
              " 'total': 0.0021053161556483374,\n",
              " 'acc': 0.044894637279486165}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28s2GCFETdYS"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujoc4c2mQh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "\n",
        "  def forward(self, out, target_pos_ids, index_training_element):\n",
        "    loss_classification=self.criterion(out.data[index_training_element],target_pos_ids.data[index_training_element])\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,out,dict_batch):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Loss_new_station_binary_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion =  nn.BCEWithLogitsLoss()\n",
        "  def forward(self,out,target):\n",
        "    loss_classification=self.criterion(out.data.squeeze(),target.float())\n",
        "    return loss_classification\n",
        "\n",
        "def get_repetition_labels(target_pos_ids,pos_ids):\n",
        "\n",
        "  return (target_pos_ids.data==pos_ids.data).type(torch.LongTensor)\n",
        "\n",
        "def upsampling_strategy(target, epoch, epochs_new_station_only,pourcentage_of_repeat_training_elment):\n",
        "\n",
        "    index_non_repeat =(target==0).nonzero()\n",
        "    coeff=pourcentage_of_repeat_training_elment/(1-pourcentage_of_repeat_training_elment)\n",
        "    index_for_training= index_non_repeat\n",
        "    if epoch>= epochs_new_station_only:\n",
        "      index_repeat = target.nonzero().squeeze()\n",
        "      nb_non_repeat= index_non_repeat.shape[0]\n",
        "      slice_repeat=index_repeat[torch.randperm(index_repeat.shape[0])[:int(coeff*nb_non_repeat)]].squeeze()\n",
        "      index_for_training = torch.cat((index_non_repeat.squeeze(),slice_repeat))\n",
        "    return index_for_training.squeeze()\n",
        "\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self,new_station_binary_classification) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.loss_new_station_binary_classification=Loss_new_station_binary_classification()\n",
        "\n",
        "  def forward(self, out, dict_batch, upsampling,upsampling_strategy, reg=False):\n",
        "    loss={}\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    if self.new_station_binary_classification or upsampling:\n",
        "      pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "      target=get_repetition_labels(target_pos_ids,pos_ids)\n",
        "    else:\n",
        "      pos_ids=None\n",
        "      target=None\n",
        "\n",
        "    if upsampling:\n",
        "      index_training_element=upsampling_strategy(target)\n",
        "    else:\n",
        "      index_training_element=torch.arange(0,target_pos_ids.data.shape[0])\n",
        "\n",
        "    loss[\"classification\"]=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids,index_training_element)\n",
        "    loss[\"total\"]=loss[\"classification\"]\n",
        "    if self.new_station_binary_classification:\n",
        "      loss[\"new_station\"]=self.loss_new_station_binary_classification(out[\"new_station\"],target)\n",
        "      loss[\"total\"]+=loss[\"new_station\"]\n",
        "\n",
        "    if reg:\n",
        "      loss[\"time_regression\"]=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss[\"total\"]+=loss[\"time_regression\"]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHCyYC32ToKU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title evaluation\n",
        "from torch import autocast\n",
        "def evaluate(model,dataloader,upsampling,criterion,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        valid_result=criterion(out,dict_batch,upsampling,None,reg=reg)\n",
        "        valid_results=get_sum_valid_results(valid_results,valid_result)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    valid_results=get_mean_valid_results(valid_results,nb_points)\n",
        "    valid_results[\"acc\"]=acc/nb_points\n",
        "\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLu25E-eTcbT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title training\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "def train(\n",
        "          epochs_classifcation_only,\n",
        "          epochs_complete_problem,\n",
        "          input_size,\n",
        "          num_heads,\n",
        "          d_model,\n",
        "          nb_of_pos_ids,\n",
        "          num_layers_lstm,\n",
        "          lstm_layer_with_perceptron,\n",
        "          lstm_layer_with_layer_norm,\n",
        "          num_layers_transformer,\n",
        "          encoder_only,\n",
        "          output_regression_size,\n",
        "          output_classfication_size,\n",
        "          nb_batchs,\n",
        "          dropout,\n",
        "          max_len,\n",
        "          weight_decay,\n",
        "          lr,\n",
        "          learnable_pos_encoding,\n",
        "          new_station_binary_classification,\n",
        "          use_gcn,\n",
        "          vocab,hidden_dim1, hidden_dim2,\n",
        "          batch_first,\n",
        "          concatenate_features,\n",
        "          keep_input_positions,\n",
        "          upsampling,\n",
        "          upsampling_strategy,\n",
        "          epochs_new_station_only,\n",
        "          pourcentage_of_repeat_training_elment,\n",
        "          save_best_model,\n",
        "          path_best_model,\n",
        "          batch_size,\n",
        "          device):\n",
        "\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n",
        "                                         nb_of_pos_ids=nb_of_pos_ids,\n",
        "                                         output_regression_size=output_regression_size,\n",
        "                                         output_classfication_size=output_classfication_size,\n",
        "                                         num_layers_lstm=num_layers_lstm,\n",
        "                                         lstm_layer_with_perceptron=lstm_layer_with_perceptron,\n",
        "                                         lstm_layer_with_layer_norm=lstm_layer_with_perceptron,\n",
        "                                         num_layers_transformer=num_layers_transformer,\n",
        "                                         encoder_only=encoder_only,\n",
        "                                         nhead=num_heads,\n",
        "                                         learnable_pos_encoding=learnable_pos_encoding,\n",
        "                                         new_station_binary_classification=new_station_binary_classification,\n",
        "                                         use_gcn=use_gcn,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=hidden_dim1,\n",
        "                                         hidden_dim2=hidden_dim2,\n",
        "                                         max_len=max_len,\n",
        "                                         dropout=dropout,\n",
        "                                         batch_first = batch_first,\n",
        "                                         concatenate_features = concatenate_features,\n",
        "                                         keep_input_positions = keep_input_positions,device=device\n",
        "                                         ).to(device)\n",
        "  if save_best_model:\n",
        "    os.makedirs(path_best_model,exist_ok =True)\n",
        "  optimizer_encoder = optim.Adam( model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  criterion = Total_loss( new_station_binary_classification = new_station_binary_classification)\n",
        "  train_losses, valid_results = {},{}\n",
        "  best_results={}\n",
        "  for epoch in range(epochs):\n",
        "    reg=epoch >= epochs_classifcation_only\n",
        "    epoch_losses={}\n",
        "    model.train()\n",
        "    i=0\n",
        "    for dict_batch in train_dataloader:\n",
        "      optimizer_encoder.zero_grad()\n",
        "      i+=1\n",
        "      if i>=nb_batchs:\n",
        "        break\n",
        "      dict_batch=set_dic_to(dict_batch,device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch, reg)\n",
        "        loss=criterion(out, dict_batch,upsampling,lambda target: upsampling_strategy(target,epoch,epochs_new_station_only,pourcentage_of_repeat_training_elment) ,reg)\n",
        "        loss[\"total\"].backward()\n",
        "        optimizer_encoder.step()\n",
        "      epoch_losses=update_epoch_losses(epoch_losses,loss)\n",
        "      dict_batch.clear()\n",
        "      loss.clear()\n",
        "      out.clear()\n",
        "      del out, loss,dict_batch\n",
        "    epoch_loss=get_epoch_loss(epoch_losses,batch_size)\n",
        "    train_losses=update_train_losses(train_losses,epoch_loss,epoch)\n",
        "    valid_result = evaluate(model,valid_dataloader,upsampling,criterion,device)\n",
        "    best_results = update_best(model,valid_result,best_results,save_best_model,path_best_model)\n",
        "    valid_results = update_valid_results(valid_results,valid_result)\n",
        "    print_results(epoch_loss,valid_result,epoch)\n",
        "\n",
        "  return best_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "\n",
        "def set_dic_to(dict_batch,device):\n",
        "  for key in dict_batch:\n",
        "    if key!=\"lengths\":\n",
        "      dict_batch[key]=dict_batch[key].to(device)\n",
        "  return dict_batch\n",
        "\n",
        "def is_better(valid_result,best_result,key):\n",
        "  match key:\n",
        "    case \"acc\":\n",
        "      return valid_result>best_result\n",
        "    case _:\n",
        "      return valid_result<best_result\n",
        "\n",
        "def update_best(model,valid_result,best_results,save_best_model,path_best_model):\n",
        "  if best_results:\n",
        "    for key in valid_result:\n",
        "      if is_better(valid_result[key],best_results[key],key):\n",
        "        best_results[key]=valid_result[key]\n",
        "        if save_best_model:\n",
        "          save_model(model,path_best_model,key)\n",
        "  else:\n",
        "    for key in valid_result:\n",
        "      best_results[key]=valid_result[key]\n",
        "      if save_best_model:\n",
        "        save_model(model,path_best_model,key)\n",
        "  return best_results\n",
        "\n",
        "def save_model(model,path_best_model,key):\n",
        "  path=os.path.join(path_best_model,key)\n",
        "  torch.save(model.state_dict(), path+\".pth\")\n",
        "\n",
        "def get_sum_valid_results(valid_result,valid_result_batch):\n",
        "  if valid_result:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]+=valid_result_batch[key].item()\n",
        "  else:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]=valid_result_batch[key].item()\n",
        "  return valid_result\n",
        "\n",
        "def get_mean_valid_results(sum_valid_result,nb_element):\n",
        "  for key in sum_valid_result:\n",
        "    sum_valid_result[key]/=nb_element\n",
        "\n",
        "  return sum_valid_result\n",
        "\n",
        "def update_epoch_losses(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key].item())\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key].item()]\n",
        "  return dict_of_list\n",
        "\n",
        "def update_valid_results(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key])\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key]]\n",
        "  return dict_of_list\n",
        "\n",
        "def get_epoch_loss(epoch_losses,batch_size):\n",
        "\n",
        "  epoch_loss={}\n",
        "  for key in epoch_losses:\n",
        "    epoch_loss[key]=np.array(epoch_losses[key]).mean()/batch_size\n",
        "  return epoch_loss\n",
        "\n",
        "def print_results(epoch_loss,valid_result,epoch):\n",
        "\n",
        "  print(\"\\nepoch: \",epoch)\n",
        "  print(\"train :\", end=\"\\t\")\n",
        "  for key in epoch_loss:\n",
        "    print(key,epoch_loss[key], end=\"\\t\")\n",
        "  print(\"\\nvalid :\", end=\"\\t\")\n",
        "  for key in valid_result:\n",
        "    print(key,valid_result[key], end=\"\\t\")\n",
        "\n",
        "def update_train_losses(train_losses,epoch_loss,epoch):\n",
        "\n",
        "  if train_losses:\n",
        "    for key in epoch_loss:\n",
        "      if key in train_losses:\n",
        "        train_losses[key].append(epoch_loss[key])\n",
        "      else:\n",
        "        train_losses[key]=[float('nan')]*(epoch+1)+[epoch_loss[key]]\n",
        "  else:\n",
        "    for key in epoch_loss:\n",
        "      train_losses[key]=[epoch_loss[key]]\n",
        "  return train_losses"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8OL2WGr7cGZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4FEU_h1YtX"
      },
      "source": [
        "## Instance of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "HL0AZ-YJChyE",
        "outputId": "ccfdf139-20a5-452c-fe6c-6e36be5a95fd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-eb8eac0884b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Titre par défaut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model=train(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs_complete_problem\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-d40eb1db95f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs_classifcation_only, epochs_complete_problem, input_size, num_heads, d_model, nb_of_pos_ids, num_layers_lstm, lstm_layer_with_perceptron, lstm_layer_with_layer_norm, num_layers_transformer, encoder_only, output_regression_size, output_classfication_size, nb_batchs, dropout, max_len, weight_decay, lr, learnable_pos_encoding, new_station_binary_classification, use_gcn, vocab, hidden_dim1, hidden_dim2, batch_first, concatenate_features, keep_input_positions, upsampling, upsampling_strategy, epochs_new_station_only, pourcentage_of_repeat_training_elment, save_best_model, path_best_model, batch_size, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mdict_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_dic_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupsampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupsampling_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_new_station_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpourcentage_of_repeat_training_elment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2dbafaf65fc1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dic_batch, reg)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmask_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_lstm\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_transformer\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-a8f6ed0bd9ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       return self.transformer(x,\n\u001b[0m\u001b[1;32m      7\u001b[0m                        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                        \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    204\u001b[0m         memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask,\n\u001b[1;32m    205\u001b[0m                               is_causal=src_is_causal)\n\u001b[0;32m--> 206\u001b[0;31m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[0m\u001b[1;32m    207\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                               \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    461\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    863\u001b[0m     def _mha_block(self, x: Tensor, mem: Tensor,\n\u001b[1;32m    864\u001b[0m                    attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 865\u001b[0;31m         x = self.multihead_attn(x, mem, mem,\n\u001b[0m\u001b[1;32m    866\u001b[0m                                 \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5438\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5440\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5441\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# @title Titre par défaut\n",
        "model=train(\n",
        "          epochs_classifcation_only=100,\n",
        "          epochs_complete_problem =100,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=1200,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=2,\n",
        "          lstm_layer_with_perceptron=False,\n",
        "          lstm_layer_with_layer_norm=False,\n",
        "          num_layers_transformer=6,\n",
        "          encoder_only=False,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=100,\n",
        "          dropout=0.1,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=1e-3,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=False,\n",
        "          use_gcn=False,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          batch_first= True,\n",
        "          concatenate_features = True,\n",
        "          keep_input_positions = False,\n",
        "          upsampling=False,\n",
        "          upsampling_strategy=upsampling_strategy,\n",
        "          epochs_new_station_only=0,\n",
        "          pourcentage_of_repeat_training_elment=0.1,\n",
        "          save_best_model=True,\n",
        "          path_best_model=\"test_0.5\",\n",
        "          device=device,\n",
        "          batch_size=64\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_repeat(model,dataloader,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    repeat=0\n",
        "    not_repeat=0\n",
        "    correct_not_repeat=0\n",
        "    correct_repeat=0\n",
        "    incorrect_not_repeat_as_repeat=0\n",
        "    incorrect_not_repeat=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "        pred=out[\"next_station\"].data.argmax(dim=1)\n",
        "        pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        for i in range(len(target_pos_ids.data)):\n",
        "          if target_pos_ids.data[i]==pos_ids.data[i]:\n",
        "            repeat+=1\n",
        "\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_repeat+=1\n",
        "          else:\n",
        "            not_repeat+=1\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_not_repeat+=1\n",
        "            if target_pos_ids.data[i]!=pred[i]:\n",
        "              incorrect_not_repeat+=1\n",
        "\n",
        "          if pred[i]==pos_ids.data[i] and target_pos_ids.data[i]!=pos_ids.data[i]:\n",
        "            incorrect_not_repeat_as_repeat+=1\n",
        "    print(nb_points,\"repeat: \",repeat,\" not_repeat: \",not_repeat,\" correct_repeat/repeat: \",correct_repeat/repeat,\" correct_not_repeat/not_repeat: \",correct_not_repeat/not_repeat,incorrect_not_repeat_as_repeat/incorrect_not_repeat)\n",
        "    return valid_results"
      ],
      "metadata": {
        "id": "HClPVCKb59Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=768,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=0,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqswVm-Flq",
        "outputId": "4f8a2d24-1d1d-4d5e-da84-30957736cbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6509191238701378  correct_not_repeat/not_repeat:  0.2762021385930769 0.4746223564954683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=12,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufqL-c1e27Vm",
        "outputId": "5e436410-45a5-4c71-da92-f5b40a030112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.572683570872406  correct_not_repeat/not_repeat:  0.24545712973693992 0.38929461542920074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=10,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqLMoo9hpA82",
        "outputId": "cc34ad4e-47d5-4efd-d99e-5c06837a607e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5703307491790515  correct_not_repeat/not_repeat:  0.22293411471430757 0.40792435839711844\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYc14HpTkU2z",
        "outputId": "36544e2a-7d0b-4787-df90-4160a18680cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.44894038389925184  correct_not_repeat/not_repeat:  0.29502962979160746 0.2873538261112317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "tI1PRax29Fqw",
        "outputId": "d9d5ea9f-4378-49ec-be25-2fe2ce5e37f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d5fdb97ca0b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-17df1714a1da>\u001b[0m in \u001b[0;36mevaluate_repeat\u001b[0;34m(model, dataloader, device, reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpos_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrepeat\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucdMSE-0T3-s",
        "outputId": "4f766291-2702-431e-d059-241a1a1ccfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7462507193879279  correct_not_repeat/not_repeat:  0.23080623646979073 0.5669490561746645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEt-Bw9FgvDq",
        "outputId": "70b7fed7-4cc6-495a-de11-57721bf5d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4839957344527574  correct_not_repeat/not_repeat:  0.35011261507511315 0.2974764468371467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaphByRZOIo",
        "outputId": "13cfb863-5bb6-43a5-f104-7ce207f93c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6290835844138258  correct_not_repeat/not_repeat:  0.26852681988148086 0.44345460524349045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGE1rXtIIEpE",
        "outputId": "775055f6-3dae-4158-fdc4-078b4adba7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5781001387995531  correct_not_repeat/not_repeat:  0.3046073779274453 0.4137291280148423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=3,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "QPBs0PUrKUra",
        "outputId": "196fe7cd-25e3-479d-8c4b-b6a83ccd3977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.linear1.weight\", \"transformer_model.transformer.decoder.layers.5.linear1.bias\", \"transformer_model.transformer.decoder.layers.5.linear2.weight\", \"transformer_model.transformer.decoder.layers.5.linear2.bias\", \"transformer_model.transformer.decoder.layers.5.norm1.weight\", \"transformer_model.transformer.decoder.layers.5.norm1.bias\", \"transformer_model.transformer.decoder.layers.5.norm2.weight\", \"transformer_model.transformer.decoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.norm3.weight\", \"transformer_model.transformer.decoder.layers.5.norm3.bias\", \"transformer_lstm__list.2.layer_normalisation.weight\", \"transformer_lstm__list.2.layer_normalisation.bias\", \"transformer_lstm__list.2.lstm.weight_ih_l0\", \"transformer_lstm__list.2.lstm.weight_hh_l0\", \"transformer_lstm__list.2.lstm.bias_ih_l0\", \"transformer_lstm__list.2.lstm.bias_hh_l0\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.bias\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-10bf83ce1050>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                          ).to(device)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cIy9KQFjv8",
        "outputId": "3ea5d475-a56f-4118-d568-2c020806ed64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8564101696062832  correct_not_repeat/not_repeat:  0.09422492401215805 0.8021341316208778\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.643730131126935, 2.2387092113494873, 4.385555267333984)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06MhCDWcaF9",
        "outputId": "4a84b9d7-b594-41b9-bca7-acacf62010b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6269508107925116  correct_not_repeat/not_repeat:  0.24020904856661782 0.4275024463247568\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5190344566683142, 3.329756021499634, 2.0473709106445312)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonRxU5b96oc",
        "outputId": "baba1be5-861e-44f1-95fb-808d9cd6b5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7019025694844104  correct_not_repeat/not_repeat:  0.2555815529946863 0.5174044590664747\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5773612306040138, 2.5278093814849854, 2.7385761737823486)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz2zFYSsAroq",
        "outputId": "10b15668-6b3b-44d9-eb4f-c28a02cbd09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7179745421307424  correct_not_repeat/not_repeat:  0.2436203013273272 0.5562012142237641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5856108172094187, 2.1441447734832764, 1.2037302255630493)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlGehwEQtfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378de12f-1ebf-44f6-b4b6-e57d19a2bb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8789227800534886  correct_not_repeat/not_repeat:  0.11254947409853272 0.8136211314803864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6650741059388481, 1.82258939743042, 2.1341636180877686)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejz7LOwNTZ-k",
        "outputId": "5d16ae15-48ea-4ff8-a2e7-718c1be73c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8187565591252243  correct_not_repeat/not_repeat:  0.1462683956178522 0.7372573126376722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6311055788439596, 2.027265787124634, 2.6414260864257812)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9yHv6HjJ-N",
        "outputId": "ba6f7b9e-b88f-407c-d22b-1e6913f0e56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.3789143166661024  correct_not_repeat/not_repeat:  0.3284642802475345 0.28316509280364704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3648367472709855, 2.700303077697754, 3.602348566055298)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLWBTILrc94",
        "outputId": "c04e1913-1724-4df4-e563-1801d24ca813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4562527506009005  correct_not_repeat/not_repeat:  0.30379829874702063 0.3240153275959545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4137118868488654, 2.806699752807617, 3.5753602981567383)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtscCJTUkD0R",
        "outputId": "d2e3c16a-065e-4503-fa5c-4f991dea2ff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7411692032"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if None:\n",
        "  print(1)"
      ],
      "metadata": {
        "id": "42I-z2ls4-Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree classifier"
      ],
      "metadata": {
        "id": "wIjYebkGJ8i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list_users"
      ],
      "metadata": {
        "id": "iec_zdeKwrBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_inputs,pos_id_targets=inputs_for_classification(list_users, pos_id=True, user_id=True, date_time=True, time_to_end=True, time_to_end_and_target=False, delete_data_less_nb_min=True, nb_min=5)"
      ],
      "metadata": {
        "id": "MAQ_RqjZz4Mo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs = inputs_outputs_for_classification(all_inputs,pos_id_targets)\n",
        "# outputs = get_x_y_target_from_outputs(outputs, vocab)\n",
        "print(inputs[0])\n",
        "outputs[0]\n",
        "print(list_users1[0])\n",
        "print(list_users2[0])"
      ],
      "metadata": {
        "id": "PjrSYgChroTj",
        "outputId": "32182c35-5125-4708-80af-3f590bfcc72e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of the dataset\n",
            "67085 67085\n",
            "tensor([0.0000e+00, 5.7183e-02, 8.8420e-02, 4.0500e+02, 9.0000e+00, 1.7000e+01,\n",
            "        2.3000e+01, 2.9000e+01, 2.4000e+01, 1.0267e+00])\n",
            "{'pos_id': tensor([604]), 'month': tensor([9]), 'day': tensor([24]), 'hour': tensor([18]), 'minute': tensor([11]), 'second': tensor([1]), 'pos_id_target': tensor([604]), 'input': tensor([[0.1300, 0.1295]]), 'time_target': tensor([[-0.5447,  0.3086]])}\n",
            "{'pos_id': tensor([276, 198, 405, 276, 405]), 'month': tensor([9, 9, 9, 9, 9]), 'day': tensor([ 8, 12, 12, 13, 14]), 'hour': tensor([12, 20, 20, 10,  3]), 'minute': tensor([36, 38, 40, 57, 39]), 'second': tensor([23, 51, 28, 38, 21]), 'pos_id_target': tensor([198, 405, 276, 405, 405]), 'input': tensor([[0.0589, 0.0930],\n",
            "        [0.0580, 0.0901],\n",
            "        [0.0513, 0.0848],\n",
            "        [0.0589, 0.0930],\n",
            "        [0.0513, 0.0848]]), 'time_target': tensor([[-0.6675,  0.2957],\n",
            "        [-0.6564,  0.3187],\n",
            "        [ 1.7115,  0.2037],\n",
            "        [-0.6651,  0.2340],\n",
            "        [-0.6234,  0.3201]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def get_correlation(inputs):\n",
        "    df = pd.DataFrame(inputs)\n",
        "    corr = df.corr()\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=2)\n",
        "    return corr\n",
        "\n",
        "corr = get_correlation(inputs)"
      ],
      "metadata": {
        "id": "kQqvIv24gzeW",
        "outputId": "e0d6e00a-316d-4e5b-e0e4-bc0b68b412bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "ExA8oZy4KjUW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do cross validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.15, random_state=1)"
      ],
      "metadata": {
        "id": "DCa_zL0Oe3hI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#got best decision tree classifier\n",
        "# 'criterion': ['gini', 'entropy'],\n",
        "parameters = { 'criterion': ['gini', 'entropy'], 'max_depth': [ i for i in range(5,50,10)], 'min_samples_split': [ i for i in range(5,40,10)], 'splitter':['best', 'random']}\n",
        "best_score = -1\n",
        "# for criterion in parameters['criterion']:\n",
        "for criterion in parameters['criterion']:\n",
        "  for max_depth in parameters['max_depth']:\n",
        "    for min_samples_split in parameters['min_samples_split']:\n",
        "      for splitter in parameters['splitter']:\n",
        "        clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, splitter=splitter, random_state=12)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        score = f1_score(y_test, y_pred, average='weighted')\n",
        "        print(score)\n",
        "        if score > best_score:\n",
        "          best_score = score\n",
        "          best_parameters = {'criterion': criterion, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'splitter':splitter}"
      ],
      "metadata": {
        "id": "gLpUTDQXKkg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7def85-f797-4ecf-8f28-c81e2bd8785d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.014641973770076026\n",
            "0.005361314728962395\n",
            "0.01463536515802652\n",
            "0.004197506335050218\n",
            "0.01463536515802652\n",
            "0.004900703612929171\n",
            "0.01463536515802652\n",
            "0.004900703612929171\n",
            "0.2643063347035141\n",
            "0.23567731138510478\n",
            "0.27188562991236326\n",
            "0.2349521699723383\n",
            "0.2725680517141168\n",
            "0.24280935424517808\n",
            "0.27182991746055796\n",
            "0.20049915670189158\n",
            "0.29781107947001095\n",
            "0.31455047740357844\n",
            "0.32314382719006784\n",
            "0.34861043037874057\n",
            "0.3270308995393475\n",
            "0.3167626556819343\n",
            "0.327315233359562\n",
            "0.3010756474258258\n",
            "0.29634409716911964\n",
            "0.31511528450605614\n",
            "0.3233575576763776\n",
            "0.35061676119617036\n",
            "0.3274664182826994\n",
            "0.3156284113179177\n",
            "0.3272968938332389\n",
            "0.3136489293413735\n",
            "0.29442642529487734\n",
            "0.31511528450605614\n",
            "0.32356430182990215\n",
            "0.35061676119617036\n",
            "0.3274664182826994\n",
            "0.3156284113179177\n",
            "0.3272968938332389\n",
            "0.3136489293413735\n",
            "0.012318865337938948\n",
            "0.004283624188623537\n",
            "0.012318865337938948\n",
            "0.004283624188623537\n",
            "0.012318865337938948\n",
            "0.004283624188623537\n",
            "0.012318865337938948\n",
            "0.004283624188623537\n",
            "0.3115807314738316\n",
            "0.2993659387646265\n",
            "0.3268807611781579\n",
            "0.3029151734616861\n",
            "0.3233271539628124\n",
            "0.27508279029262817\n",
            "0.3138434936704621\n",
            "0.24636309615124993\n",
            "0.29635877606926353\n",
            "0.3284490448841918\n",
            "0.32601968339056503\n",
            "0.3479610080350992\n",
            "0.32387629364228365\n",
            "0.32164582360201394\n",
            "0.3146414025037305\n",
            "0.28513734746915603\n",
            "0.29635877606926353\n",
            "0.3315340329053372\n",
            "0.32601968339056503\n",
            "0.3204386808806417\n",
            "0.32387629364228365\n",
            "0.32164582360201394\n",
            "0.3146414025037305\n",
            "0.28513734746915603\n",
            "0.29635877606926353\n",
            "0.3315340329053372\n",
            "0.32601968339056503\n",
            "0.3204386808806417\n",
            "0.32387629364228365\n",
            "0.32164582360201394\n",
            "0.3146414025037305\n",
            "0.28513734746915603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "6OCspM3UK62N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5336db6b-9e87-4f10-cb8d-8d7f56b2b83e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score:  0.6807267331679286\n",
            "{'criterion': 'entropy', 'max_depth': 15, 'min_samples_split': 35, 'splitter': 'best'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross score:  [0.67047553 0.67142358 0.67007634 0.65748503 0.66706587]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = { 'n_estimators': [ i for i in range(50,150,30)], 'max_depth': [ i for i in range(10,30,10)], 'min_samples_split': [ i for i in range(20,40,10)]}\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "clf = GridSearchCV(clf, parameters, cv=5)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)\n",
        "print(clf.best_estimator_)\n",
        "print(clf.best_index_)\n"
      ],
      "metadata": {
        "id": "wVnwYXsOe_3L",
        "outputId": "b3b36769-3aa9-415d-c7b7-c0ecaa48df27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Créer un modèle d'arbre de décision\n",
        "model = RandomForestClassifier(n_estimators=10, min_samples_leaf = 5, max_features = 0.7, random_state=42)\n",
        "# model = DecisionTreeClassifier(criterion='entropy')\n",
        "# model = SVC(random_state=42)\n",
        "# Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(\"Exactitude :\", accuracy)\n",
        "print(model.get_params())\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "6lIeX2pDQx9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install catboost\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Créer un modèle d'arbre de décision\n",
        "# model = RandomForestClassifier(n_estimators=50, min_samples_leaf = 5, max_features = 0.7, random_state=42)\n",
        "model = GradientBoostingClassifier(n_estimators=20, learning_rate=0.1, max_depth=10)\n",
        "# model = SVC(random_state=42)\n",
        "# Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Exactitude :\", accuracy)\n",
        "print(model.get_params())\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "0a_EcGZkqiFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "inputs = [inp.tolist() for inp in inputs]\n",
        "outputs = [inp.tolist() for inp in outputs]\n",
        "#Do cross validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.25, random_state=1)\n",
        "# Créer un modèle d'arbre de décision\n",
        "# model = KNeighborsClassifier(n_neighbors=5, metric='l2')\n",
        "# model = DecisionTreeClassifier(criterion='entropy')\n",
        "model = CatBoostClassifier(iterations=50, learning_rate=0.1, loss_function='MultiClass')\n",
        "# Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Exactitude :\", accuracy)\n",
        "print(model.get_params())\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUXy-DJKpyPx",
        "outputId": "c9670685-cd2a-47e3-b00e-a782fc7b5e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 7.2019131\ttotal: 3m 33s\tremaining: 2h 54m 33s\n",
            "1:\tlearn: 6.8811740\ttotal: 7m 2s\tremaining: 2h 48m 54s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOmBJAknRSHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary with the number of data for each class\n",
        "class_count={}\n",
        "print(vocab)\n",
        "pos_ids = [i[3] for i in inputs]\n",
        "for pos_id in pos_ids:\n",
        "    if pos_id.item() in class_count:\n",
        "      class_count[pos_id.item()]+=1\n",
        "    else:\n",
        "      class_count[pos_id.item()]=1\n",
        "print(class_count)\n",
        "print(len(class_count))\n",
        "print(max(class_count.keys()))\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#See the tendency of the number of connections for each class\n",
        "print(\"min number of connections for each class\"\n",
        "      ,min(class_count.values())\n",
        "      ,min(class_count,key=class_count.get))\n",
        "print(\"max number of connections for each class\"\n",
        "      ,max(class_count.values())\n",
        "      ,max(class_count,key=class_count.get))\n",
        "\n",
        "plt.plot(list(class_count.values()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2UOJ4O63xDu",
        "outputId": "af831a4f-a8d9-4cca-e2aa-ef50bee5352b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(31.32719, 121.53013): 1, (31.11794, 121.457471): 2, (31.245393, 121.705496): 3, (31.094914, 121.43682): 4, (31.155344, 121.2025): 5, (31.044667, 121.46191): 6, (31.112344, 121.480257): 7, (31.128227, 121.45441): 8, (31.118723, 121.248449): 9, (31.125695, 121.46401): 10, (30.824719, 121.471519): 11, (31.146551, 121.235394): 12, (30.832601, 121.445485): 13, (31.298668, 121.541944): 14, (30.950067, 121.449083): 15, (31.130891, 121.472384): 16, (31.01217, 121.265575): 17, (31.130862, 121.091425): 18, (31.25821, 121.479271): 19, (31.225831, 121.412695): 20, (31.264503, 121.715846): 21, (31.121127, 121.243829): 22, (31.019691, 121.249497): 23, (31.205813, 121.413544): 24, (31.214115, 121.406884): 25, (31.287632, 121.458734): 26, (31.294609, 121.526886): 27, (31.121152, 121.141999): 28, (31.327299, 121.452936): 29, (31.109998, 121.164842): 30, (31.260742, 121.399188): 31, (31.198372, 121.355749): 32, (31.177169, 121.445517): 33, (31.138371, 121.416975): 34, (31.137509, 121.40768): 35, (31.22258, 121.428599): 36, (31.174203, 121.431404): 37, (31.24708, 121.419525): 38, (31.159523, 121.435774): 39, (31.123394, 121.278015): 40, (31.268894, 121.692432): 41, (31.236862, 121.355595): 42, (31.08532, 121.237002): 43, (30.967774, 121.447499): 44, (31.183323, 121.447822): 45, (30.83237, 121.439654): 46, (31.219114, 121.423499): 47, (31.323457, 121.57815): 48, (30.910085, 121.504947): 49, (31.243446, 121.675041): 50, (31.024911, 121.255017): 51, (31.260555, 121.410883): 52, (31.213849, 121.43028): 53, (31.247569, 121.41575): 54, (31.08111, 121.263537): 55, (31.333408, 121.552846): 56, (31.228847, 121.338653): 57, (30.858416, 121.035215): 58, (30.801265, 121.395878): 59, (31.148773, 121.387158): 60, (31.096325, 121.341209): 61, (31.218005, 121.430759): 62, (31.106089, 121.039819): 63, (31.290887, 121.435064): 64, (31.33775, 121.306733): 65, (31.071955, 121.234457): 66, (31.088099, 121.430111): 67, (31.246059, 121.362706): 68, (31.242808, 121.420927): 69, (31.241274, 121.36272): 70, (31.19458, 121.436178): 71, (31.141982, 121.459872): 72, (30.824298, 121.363989): 73, (31.319806, 121.526248): 74, (31.075231, 121.436269): 75, (31.154909, 121.436989): 76, (31.162868, 121.448422): 77, (31.151421, 121.279419): 78, (31.149628, 121.43924): 79, (31.284953, 121.489071): 80, (31.00816, 121.274391): 81, (31.243013, 121.416207): 82, (31.157362, 121.221594): 83, (31.338059, 121.289933): 84, (31.32712, 121.254377): 85, (31.268263, 121.394134): 86, (31.22135, 121.391319): 87, (34.689694, 112.407126): 88, (31.224226, 121.43839): 89, (31.216659, 121.391301): 90, (30.98383, 121.235077): 91, (31.145641, 121.449009): 92, (31.339114, 121.321261): 93, (31.284385, 121.511361): 94, (31.285861, 121.507007): 95, (31.30685, 121.519579): 96, (31.267266, 121.385292): 97, (31.304576, 121.522177): 98, (31.295347, 121.515155): 99, (31.099085, 121.386207): 100, (31.263495, 121.496653): 101, (31.201259, 121.40143): 102, (31.193409, 121.393885): 103, (31.182189, 121.394088): 104, (31.326648, 121.304442): 105, (31.209913, 121.425292): 106, (31.208108, 121.41483): 107, (31.213701, 121.41802): 108, (31.134917, 121.334281): 109, (31.117005, 121.271194): 110, (31.123621, 121.266561): 111, (31.331055, 121.52962): 112, (31.274336, 121.39328): 113, (31.255248, 121.416487): 114, (31.156874, 121.348458): 115, (31.254111, 121.408424): 116, (31.220649, 121.35994): 117, (31.147903, 121.403379): 118, (31.16807, 121.488657): 119, (30.779826, 121.376806): 120, (31.441487, 121.179013): 121, (31.220334, 121.367655): 122, (31.312672, 121.593011): 123, (30.970389, 121.451558): 124, (31.323113, 121.616609): 125, (31.327041, 121.661258): 126, (31.323598, 121.638883): 127, (31.209479, 121.702069): 128, (31.07195, 121.150279): 129, (31.18713, 121.451332): 130, (31.074013, 121.407487): 131, (31.186057, 121.44872): 132, (30.832683, 121.481868): 133, (31.250271, 121.406561): 134, (30.960682, 121.447242): 135, (31.102, 121.462298): 136, (31.112806, 121.462647): 137, (31.241362, 121.395822): 138, (31.252495, 121.390194): 139, (31.150662, 121.434931): 140, (31.340851, 121.63262): 141, (31.311329, 121.495756): 142, (31.30647, 121.492925): 143, (31.300084, 121.504034): 144, (31.318646, 121.53899): 145, (30.975655, 121.228194): 146, (31.311942, 121.54204): 147, (31.295335, 121.555781): 148, (31.134254, 121.280533): 149, (31.212113, 121.444193): 150, (31.116021, 121.287621): 151, (31.27795, 121.538718): 152, (31.324066, 121.536866): 153, (31.036404, 121.272345): 154, (30.892415, 121.023553): 155, (31.019246, 121.250749): 156, (31.166007, 121.457766): 157, (31.177858, 121.346956): 158, (46.247857, 128.762232): 159, (31.285944, 121.44442): 160, (31.219533, 121.427561): 161, (31.157151, 121.431115): 162, (31.24448, 121.37062): 163, (31.229334, 121.368291): 164, (30.938138, 121.327668): 165, (30.941447, 121.293964): 166, (31.245624, 121.522839): 167, (31.246527, 121.534982): 168, (31.236856, 121.40554): 169, (30.986462, 121.409757): 170, (30.997412, 121.247239): 171, (30.948712, 121.365593): 172, (31.198836, 121.215066): 173, (31.344675, 121.443963): 174, (31.157536, 121.203337): 175, (31.222168, 121.378951): 176, (31.224373, 121.364172): 177, (30.919796, 121.459621): 178, (31.18026, 121.406339): 179, (31.155848, 121.383402): 180, (31.249162, 121.487899): 181, (31.202075, 121.187355): 182, (31.224385, 121.143509): 183, (31.184319, 121.230946): 184, (31.381763, 121.301878): 185, (30.790922, 121.348126): 186, (31.248587, 121.277434): 187, (31.243338, 121.253922): 188, (31.247155, 121.23112): 189, (31.254166, 121.224476): 190, (31.213728, 121.386102): 191, (31.245814, 121.411472): 192, (31.219437, 121.374523): 193, (30.731903, 121.334173): 194, (31.253394, 121.389864): 195, (31.252264, 121.38269): 196, (31.246106, 121.443663): 197, (31.245663, 121.449725): 198, (31.380359, 121.304458): 199, (30.898697, 121.172576): 200, (31.38417, 121.344688): 201, (31.217237, 121.432824): 202, (31.228927, 121.486896): 203, (31.171686, 121.441221): 204, (31.313444, 121.309439): 205, (31.31724, 121.237444): 206, (31.371973, 121.278978): 207, (31.176813, 121.439114): 208, (31.150193, 121.45419): 209, (31.217372, 121.460724): 210, (30.800042, 121.417303): 211, (31.142941, 121.443942): 212, (31.134786, 121.444969): 213, (30.903508, 121.331287): 214, (31.188512, 121.437591): 215, (31.299749, 121.170321): 216, (31.415595, 121.188844): 217, (31.164195, 121.330502): 218, (31.098558, 121.377147): 219, (30.893975, 121.016922): 220, (31.17023, 121.337413): 221, (31.232877, 121.48753): 222, (31.162463, 121.364262): 223, (31.163393, 121.381735): 224, (31.134548, 121.39095): 225, (31.150797, 121.728702): 226, (31.251584, 121.4917): 227, (30.860668, 121.470193): 228, (31.00153, 121.235235): 229, (31.227933, 121.45361): 230, (31.166058, 121.403691): 231, (30.74139, 121.380141): 232, (31.224552, 121.482535): 233, (31.242867, 121.490229): 234, (30.797557, 121.356113): 235, (31.011886, 121.234971): 236, (30.746286, 121.344028): 237, (31.081475, 121.523285): 238, (31.284292, 121.273506): 239, (31.223175, 121.184646): 240, (31.165339, 121.427732): 241, (31.08093, 121.159291): 242, (31.159704, 121.457647): 243, (31.199721, 121.325753): 244, (31.161635, 121.286602): 245, (39.612987, 118.20604): 246, (31.19868, 121.485666): 247, (31.19382, 121.49524): 248, (31.364503, 121.391608): 249, (31.260709, 121.087935): 250, (31.176489, 121.188906): 251, (31.111821, 121.385462): 252, (31.269521, 121.153127): 253, (31.01613, 121.203009): 254, (31.143297, 121.132883): 255, (31.275929, 121.473484): 256, (31.139657, 121.298915): 257, (31.237435, 121.455917): 258, (31.254042, 121.464758): 259, (31.25423, 121.461072): 260, (31.318258, 121.167266): 261, (31.207205, 121.482691): 262, (31.38336, 121.420672): 263, (31.232002, 121.359292): 264, (31.315825, 121.532151): 265, (31.182198, 121.390214): 266, (31.175621, 121.317835): 267, (31.216526, 121.326846): 268, (30.873326, 121.284335): 269, (31.207679, 121.489691): 270, (31.203978, 121.423912): 271, (31.2355, 121.417315): 272, (31.254308, 121.459288): 273, (31.25393, 121.455087): 274, (31.251986, 121.456773): 275, (31.246345, 121.45371): 276, (31.06623, 121.321933): 277, (30.939279, 121.074396): 278, (30.814573, 121.352883): 279, (31.102445, 121.413911): 280, (30.852446, 121.262447): 281, (30.89659, 121.281933): 282, (30.934254, 121.279662): 283, (31.087306, 121.275545): 284, (31.207247, 121.572933): 285, (31.109806, 121.242241): 286, (31.175318, 121.181299): 287, (31.17665, 121.132941): 288, (31.215347, 121.134685): 289, (31.239726, 121.139516): 290, (31.252877, 121.148181): 291, (30.8406, 121.251681): 292, (31.265326, 121.394456): 293, (31.280857, 121.169253): 294, (31.274169, 121.221302): 295, (31.275331, 121.286529): 296, (31.273019, 121.307617): 297, (31.257553, 121.311711): 298, (31.247519, 121.322265): 299, (30.718108, 121.283169): 300, (31.231687, 121.32576): 301, (31.236104, 121.375177): 302, (31.28711, 121.226904): 303, (31.277484, 121.382842): 304, (31.22714, 121.391029): 305, (31.096053, 121.518157): 306, (31.144812, 121.119606): 307, (31.054254, 121.087156): 308, (31.262331, 121.563212): 309, (31.068626, 121.108326): 310, (31.058092, 121.235964): 311, (31.011339, 121.296295): 312, (31.240478, 121.526884): 313, (31.30092, 121.16892): 314, (30.799332, 121.268825): 315, (30.807278, 121.304136): 316, (31.258656, 121.472538): 317, (31.255942, 121.38141): 318, (31.228017, 121.444777): 319, (30.741272, 121.3348): 320, (31.024417, 121.471297): 321, (31.240869, 121.481603): 322, (30.716044, 121.291037): 323, (31.229057, 121.452254): 324, (31.349748, 121.506246): 325, (31.030356, 121.535809): 326, (26.215115, 109.744661): 327, (31.224731, 121.43464): 328, (31.16691, 121.115166): 329, (31.236138, 121.463199): 330, (31.230395, 121.46599): 331, (31.237872, 121.470259): 332, (31.154675, 121.81374): 333, (30.963494, 121.160188): 334, (31.148785, 121.115114): 335, (31.206547, 121.306923): 336, (31.387989, 121.396749): 337, (31.217074, 121.510123): 338, (31.230933, 121.347295): 339, (31.122623, 121.433813): 340, (31.256071, 121.462096): 341, (31.286066, 121.474323): 342, (31.237022, 121.546184): 343, (31.25563, 121.508513): 344, (31.430745, 121.187414): 345, (31.382812, 121.242351): 346, (31.36124, 121.451839): 347, (31.321039, 121.453913): 348, (31.131416, 121.429704): 349, (31.256971, 121.509351): 350, (31.188696, 121.181232): 351, (31.230201, 121.308739): 352, (31.255173, 121.403569): 353, (31.214561, 121.298904): 354, (31.232729, 121.566935): 355, (31.218022, 121.504024): 356, (31.467705, 121.201637): 357, (31.1634, 121.396418): 358, (31.210887, 121.315496): 359, (31.162395, 121.353836): 360, (31.132833, 121.17281): 361, (31.318661, 121.194317): 362, (31.255905, 121.430231): 363, (31.239025, 121.551622): 364, (30.771941, 121.373524): 365, (30.978506, 121.447689): 366, (31.374842, 121.264224): 367, (31.239655, 121.478097): 368, (31.234499, 121.463079): 369, (31.334449, 121.536876): 370, (31.124664, 121.323455): 371, (31.051898, 121.769904): 372, (30.915122, 121.560642): 373, (30.922611, 121.473581): 374, (31.120822, 121.29001): 375, (31.224126, 121.547664): 376, (31.223981, 121.478274): 377, (31.185228, 121.1697): 378, (31.152177, 121.172752): 379, (30.766975, 121.362906): 380, (31.156454, 121.815297): 381, (31.406426, 121.476433): 382, (31.258677, 121.392956): 383, (31.099642, 121.449578): 384, (31.204179, 121.538588): 385, (31.235929, 121.449652): 386, (31.260959, 121.562594): 387, (31.178088, 121.413515): 388, (31.413754, 121.202263): 389, (31.054506, 121.460002): 390, (31.257756, 121.601633): 391, (31.359545, 121.18156): 392, (30.999098, 121.447826): 393, (31.042021, 121.204506): 394, (31.348418, 121.427796): 395, (31.284148, 121.481157): 396, (31.203987, 121.520591): 397, (31.059304, 121.53566): 398, (31.285698, 121.612262): 399, (31.370992, 121.177459): 400, (31.038201, 121.216468): 401, (31.174661, 121.52822): 402, (31.208384, 121.466365): 403, (31.127837, 121.380654): 404, (31.240601, 121.442307): 405, (31.155403, 121.57642): 406, (31.253601, 121.4782): 407, (31.272774, 121.436412): 408, (31.246097, 121.39928): 409, (29.526266, 119.910488): 410, (31.167279, 121.113649): 411, (30.954604, 121.333773): 412, (31.217476, 121.494788): 413, (31.163705, 121.503747): 414, (31.25901, 121.65797): 415, (31.10366, 121.420217): 416, (31.078365, 121.404647): 417, (31.11674, 121.586334): 418, (31.387987, 121.495216): 419, (31.190168, 121.467092): 420, (31.03333, 121.427385): 421, (31.05815, 120.970188): 422, (31.056648, 121.404326): 423, (31.303976, 121.474504): 424, (31.297281, 121.505124): 425, (30.920156, 121.488943): 426, (31.270626, 121.5908): 427, (31.281813, 121.354285): 428, (31.122449, 121.582926): 429, (31.13756, 121.339005): 430, (31.307198, 121.511929): 431, (31.057696, 121.207516): 432, (31.288777, 121.390078): 433, (31.16872, 121.768142): 434, (30.715526, 121.351073): 435, (31.227167, 121.633295): 436, (30.860242, 121.79388): 437, (31.171101, 121.507145): 438, (31.198048, 121.604048): 439, (31.344291, 121.487858): 440, (30.947965, 121.640593): 441, (31.279012, 121.514374): 442, (31.261563, 121.376836): 443, (31.23423, 121.119889): 444, (31.31394, 121.382812): 445, (31.136777, 121.546762): 446, (30.967591, 121.656531): 447, (30.926464, 121.682732): 448, (31.292519, 121.549839): 449, (31.038291, 121.194794): 450, (31.337262, 121.473928): 451, (31.242039, 121.687875): 452, (31.307268, 121.521916): 453, (31.297527, 121.376): 454, (31.039301, 121.593594): 455, (31.324464, 121.387532): 456, (31.280262, 121.364633): 457, (31.214683, 121.551574): 458, (31.353347, 121.441038): 459, (31.01294, 121.637001): 460, (31.202395, 121.6278): 461, (31.145654, 121.375598): 462, (31.274564, 121.51979): 463, (31.045068, 121.232113): 464, (31.068209, 121.373076): 465, (31.16346, 121.405127): 466, (31.355465, 121.412867): 467, (31.221052, 121.503967): 468, (31.120753, 121.381579): 469, (31.291242, 121.489761): 470, (31.112548, 121.538798): 471, (31.316181, 121.405014): 472, (30.723654, 121.330778): 473, (30.718302, 121.314644): 474, (31.209182, 121.752433): 475, (31.186628, 121.372116): 476, (31.109649, 121.402441): 477, (31.214209, 121.659493): 478, (31.235682, 121.487831): 479, (31.031203, 121.65009): 480, (31.17762, 121.511042): 481, (31.204999, 121.471193): 482, (31.306306, 121.28988): 483, (31.259633, 121.594477): 484, (30.874727, 121.250912): 485, (30.843004, 121.438792): 486, (31.373432, 121.490334): 487, (31.215381, 121.439262): 488, (31.19143, 121.411411): 489, (31.294057, 121.171316): 490, (30.912905, 121.801126): 491, (31.292008, 121.36941): 492, (30.974035, 121.554233): 493, (31.183857, 121.637321): 494, (31.042056, 121.720903): 495, (30.872829, 121.424028): 496, (31.242991, 121.515376): 497, (31.317294, 121.460019): 498, (31.242627, 121.730079): 499, (31.444014, 121.25661): 500, (31.152157, 121.514858): 501, (31.17255, 121.363745): 502, (38.052584, 114.484137): 503, (31.127539, 121.389284): 504, (30.751838, 121.360056): 505, (31.318369, 121.407014): 506, (31.211574, 121.366182): 507, (31.025227, 121.44251): 508, (31.382497, 121.548578): 509, (31.232548, 121.478971): 510, (31.25007, 121.546845): 511, (31.260217, 121.525816): 512, (31.127879, 121.452728): 513, (30.955208, 121.732358): 514, (31.201647, 121.420098): 515, (31.266743, 121.481644): 516, (31.032565, 121.409407): 517, (30.939177, 121.223182): 518, (30.865062, 121.662776): 519, (31.378285, 121.436137): 520, (31.367578, 121.31858): 521, (31.030308, 121.650922): 522, (31.181485, 121.520026): 523, (30.816155, 121.298392): 524, (31.015959, 121.227509): 525, (31.174748, 121.39247): 526, (31.246387, 121.458214): 527, (31.305269, 121.397452): 528, (31.319501, 121.584673): 529, (31.274417, 121.257132): 530, (30.936752, 121.470783): 531, (31.236728, 121.42407): 532, (31.140855, 121.360032): 533, (31.327488, 121.437837): 534, (31.163459, 121.574025): 535, (31.211641, 121.474534): 536, (36.406412, 102.003965): 537, (30.872205, 121.530081): 538, (31.124501, 121.32791): 539, (31.305522, 121.483135): 540, (31.412982, 121.496282): 541, (31.157241, 121.418919): 542, (31.202883, 121.476296): 543, (31.312825, 121.491112): 544, (31.3065, 121.429743): 545, (46.777465, 131.812182): 546, (31.220416, 121.529149): 547, (31.119287, 121.204986): 548, (31.26604, 121.399004): 549, (30.834815, 121.197836): 550, (31.283403, 121.541263): 551, (31.213015, 121.547755): 552, (31.288667, 121.527375): 553, (31.017385, 121.715431): 554, (31.294194, 121.316828): 555, (31.127217, 121.335543): 556, (31.233973, 121.41641): 557, (31.288583, 121.428686): 558, (30.993883, 121.129758): 559, (31.105808, 121.038742): 560, (31.146311, 121.399951): 561, (30.734301, 121.25722): 562, (31.158559, 121.319801): 563, (30.866245, 121.343018): 564, (31.200882, 121.446511): 565, (31.321416, 121.215): 566, (31.221591, 121.351862): 567, (31.217512, 121.533844): 568, (31.488248, 121.349833): 569, (31.360292, 121.583955): 570, (31.234004, 121.276012): 571, (31.341175, 121.618019): 572, (31.215428, 121.660427): 573, (31.167947, 121.670231): 574, (26.139329, 103.078562): 575, (31.303478, 121.55082): 576, (31.209905, 121.37472): 577, (31.189335, 121.781953): 578, (31.211591, 121.499553): 579, (31.092058, 121.38753): 580, (22.522803, 114.218796): 581, (31.370388, 121.187891): 582, (31.205839, 121.462046): 583, (31.193646, 121.449596): 584, (31.313065, 121.417971): 585, (31.406017, 121.240101): 586, (31.256807, 121.518545): 587, (31.184888, 121.510295): 588, (30.889897, 121.320413): 589, (31.343106, 121.455682): 590, (31.379455, 121.372876): 591, (31.211485, 121.403238): 592, (31.273982, 121.512661): 593, (31.178359, 121.279284): 594, (30.738649, 121.382934): 595, (31.036987, 121.386554): 596, (31.111396, 121.379353): 597, (31.210843, 121.491168): 598, (30.898527, 121.22367): 599, (31.301057, 121.388942): 600, (31.395915, 121.363062): 601, (31.180466, 121.441251): 602, (31.021462, 121.321255): 603, (31.294393, 121.497298): 604, (31.337218, 121.465181): 605, (31.070388, 121.520258): 606, (31.256559, 121.43557): 607, (30.887807, 121.346419): 608, (31.034322, 121.601024): 609, (31.382771, 121.291756): 610, (30.831962, 121.534676): 611, (30.900529, 121.246804): 612, (31.010124, 121.538644): 613, (31.405231, 121.506638): 614, (35.379598, 116.072359): 615, (31.045313, 121.844791): 616, (31.149689, 121.531301): 617, (31.146522, 121.352501): 618, (31.114131, 121.394475): 619, (31.190389, 121.457678): 620, (31.078791, 121.422082): 621, (31.039193, 121.604274): 622, (31.300693, 121.496182): 623, (31.306089, 121.474725): 624, (31.221662, 121.539905): 625, (31.272151, 121.375814): 626, (31.244859, 121.489305): 627, (31.325382, 121.553285): 628, (31.243738, 121.486223): 629, (31.354948, 121.496863): 630, (31.015452, 121.154814): 631, (31.196266, 121.537132): 632, (30.994999, 121.264867): 633, (31.263651, 121.573401): 634, (30.808567, 121.29074): 635, (31.304918, 121.497837): 636, (31.200224, 121.316463): 637, (31.33167, 121.440751): 638, (31.069676, 121.773361): 639, (31.217516, 121.452872): 640, (31.267654, 121.473299): 641, (31.069946, 121.719641): 642, (31.262746, 121.517701): 643, (31.346801, 121.577398): 644, (31.151539, 121.41109): 645, (30.919514, 121.45936): 646, (31.273183, 121.488391): 647, (31.251239, 121.428578): 648, (31.232639, 121.705254): 649, (31.268642, 121.332479): 650, (31.166046, 121.642137): 651, (31.045516, 121.216817): 652, (31.174053, 121.282683): 653, (31.040378, 121.255563): 654, (31.331242, 121.225906): 655, (31.10888, 121.390919): 656, (31.296656, 121.483976): 657, (31.095884, 121.350551): 658, (31.216359, 121.397227): 659, (31.460409, 121.401649): 660, (30.935314, 121.733322): 661, (31.33876, 121.437747): 662, (31.358014, 121.37617): 663, (30.894923, 121.319564): 664, (31.319077, 121.331853): 665, (31.132785, 121.413457): 666, (31.2317, 121.546506): 667, (31.208346, 121.544892): 668, (31.177646, 121.555355): 669, (31.108567, 121.766873): 670, (31.4163, 121.496785): 671, (31.020065, 121.23311): 672, (31.002479, 121.390889): 673, (31.237944, 121.693732): 674, (30.847557, 121.226776): 675, (31.018376, 121.40507): 676, (30.988441, 121.87486): 677, (31.021822, 121.590566): 678, (31.211107, 121.59082): 679, (31.26484, 121.498029): 680, (31.190185, 121.443475): 681, (30.747024, 121.287991): 682, (41.835279, 123.498927): 683, (31.250332, 121.594504): 684, (31.147783, 121.366475): 685, (31.205759, 121.26316): 686, (31.219945, 121.285654): 687, (31.129955, 121.336848): 688, (31.226966, 121.53174): 689, (31.168241, 121.437559): 690, (31.228755, 121.523626): 691, (31.086916, 121.387535): 692, (30.908378, 121.634574): 693, (31.218201, 121.487151): 694, (31.160199, 121.557134): 695, (31.301602, 121.487289): 696, (31.192975, 121.449755): 697, (31.282237, 121.42091): 698, (31.042149, 121.539262): 699, (30.823953, 121.528367): 700, (31.121363, 121.369095): 701, (31.270369, 121.443633): 702, (31.277512, 121.287969): 703, (31.223665, 121.458791): 704, (31.054594, 121.339133): 705, (30.894639, 121.21016): 706, (31.186465, 121.429887): 707, (31.192915, 121.682688): 708, (31.227795, 121.254172): 709, (31.069757, 121.399847): 710, (31.140979, 121.586245): 711, (31.27975, 121.497791): 712, (31.145387, 121.61623): 713, (31.462643, 121.331794): 714, (30.908583, 121.656169): 715, (31.219455, 121.098597): 716, (31.314038, 121.509762): 717, (31.371694, 121.443832): 718, (31.116535, 121.678055): 719, (31.290434, 121.425041): 720, (31.155384, 121.771585): 721, (31.004198, 121.067198): 722, (31.007639, 121.416247): 723, (31.217682, 121.499709): 724, (31.414938, 121.486809): 725, (31.270279, 121.531241): 726, (30.918282, 121.639066): 727, (31.104981, 121.360938): 728, (31.492475, 121.328439): 729, (31.025763, 121.541983): 730, (31.180753, 121.749174): 731, (30.97935, 121.815249): 732, (31.120714, 121.42693): 733, (31.155297, 121.573504): 734, (31.158202, 121.389411): 735, (31.368461, 121.372259): 736, (31.327049, 121.443238): 737, (31.21351, 121.3728): 738, (30.829105, 121.180127): 739, (31.133956, 121.526289): 740, (31.264146, 121.532591): 741, (31.121932, 121.33481): 742, (31.356238, 121.568226): 743, (31.262941, 121.602924): 744, (31.085146, 121.589124): 745, (31.214958, 121.527054): 746, (30.899117, 121.175731): 747, (31.183064, 121.407773): 748, (31.209025, 121.616514): 749, (31.294155, 121.418063): 750, (31.223114, 121.522857): 751, (31.214767, 121.444427): 752, (31.355737, 121.403678): 753, (31.210582, 121.529652): 754, (31.203031, 121.443586): 755, (31.233201, 121.469528): 756, (31.252404, 121.435035): 757, (31.120864, 121.389141): 758, (34.638922, 119.467017): 759, (31.213659, 121.504661): 760, (31.215246, 121.604329): 761, (31.388641, 121.444174): 762, (30.86683, 121.83302): 763, (31.078561, 121.564629): 764, (31.140384, 121.620758): 765, (31.224117, 121.4732): 766, (31.278892, 121.710661): 767, (31.045101, 121.223676): 768, (30.442177, 120.618727): 769, (31.253346, 121.448039): 770, (31.201237, 121.669336): 771, (31.106732, 121.290566): 772, (31.228014, 121.477667): 773, (31.077825, 121.33981): 774, (31.367392, 121.276877): 775, (31.260835, 121.366953): 776, (31.366638, 121.43543): 777, (31.455174, 121.413981): 778, (31.244229, 121.475231): 779, (31.39034, 121.272165): 780, (30.935874, 121.692581): 781, (31.180763, 121.476049): 782, (31.292718, 121.410291): 783, (31.225207, 121.452948): 784, (30.719644, 121.345988): 785, (31.299261, 121.578516): 786, (31.125764, 121.562057): 787, (31.188821, 121.387044): 788, (31.141245, 121.500503): 789, (31.114038, 121.415096): 790, (31.010331, 121.381275): 791, (31.069261, 121.375484): 792, (31.24822, 121.296297): 793, (31.181381, 121.688934): 794, (31.131231, 121.398758): 795, (30.92643, 121.782514): 796, (31.162282, 121.426688): 797, (30.920347, 121.057024): 798, (31.254534, 121.34772): 799, (31.247628, 121.510061): 800, (31.403259, 121.447841): 801, (31.318974, 121.412838): 802, (31.05799, 121.626154): 803, (31.283959, 121.416644): 804, (31.176978, 121.499522): 805, (31.19979, 121.389906): 806, (31.301307, 121.33124): 807, (31.144212, 121.50825): 808, (30.92668, 121.466784): 809, (31.436901, 121.294601): 810, (31.039158, 121.239632): 811, (30.932475, 121.692651): 812, (30.901978, 121.452316): 813, (31.327484, 121.406697): 814, (31.21852, 121.44019): 815, (30.853798, 121.344272): 816, (30.966714, 121.771375): 817, (31.276178, 121.553996): 818, (31.221001, 121.731159): 819, (31.327319, 121.462047): 820, (31.270362, 121.612773): 821, (30.987088, 121.588932): 822, (31.210891, 121.3779): 823, (31.072213, 121.658885): 824, (31.176807, 121.251063): 825, (31.195139, 121.385973): 826, (31.08906, 121.722984): 827, (31.145439, 121.417433): 828, (31.060038, 121.195223): 829, (31.324905, 121.29416): 830, (31.19232, 121.142889): 831, (31.019533, 121.388705): 832, (31.221777, 121.558508): 833, (31.103853, 121.810034): 834, (31.122978, 121.704554): 835, (31.276317, 121.524794): 836, (31.067235, 121.762108): 837, (31.082701, 121.448694): 838, (31.468523, 121.425585): 839, (31.17605, 121.431225): 840, (31.182483, 121.5762): 841, (31.150483, 121.491401): 842, (31.155342, 121.8105): 843, (31.483764, 121.274813): 844, (31.073134, 121.317814): 845, (31.219239, 121.418564): 846, (31.179835, 121.429502): 847, (31.307911, 121.518921): 848, (31.150398, 121.537521): 849, (24.284812, 102.999068): 850, (31.251883, 121.418603): 851, (31.221325, 121.448881): 852, (30.910136, 121.843044): 853, (31.357773, 121.508693): 854, (31.007327, 121.431151): 855, (31.045834, 121.408322): 856, (31.210033, 121.405714): 857, (30.844397, 121.528845): 858, (30.758612, 121.320541): 859, (30.933365, 121.873737): 860, (31.209644, 121.56447): 861, (31.337749, 121.579688): 862, (31.244102, 121.437744): 863, (30.984047, 121.726879): 864, (31.312291, 121.46631): 865, (31.113744, 121.341878): 866, (31.326619, 121.493433): 867, (30.716187, 121.349498): 868, (31.32899, 121.323419): 869, (31.441726, 121.246593): 870, (31.197656, 121.463533): 871, (31.197909, 121.426542): 872, (31.20172, 121.743118): 873, (31.200248, 121.474414): 874, (31.28386, 121.530324): 875, (31.312117, 121.474859): 876, (31.265705, 121.420959): 877, (31.208589, 121.524447): 878, (31.300493, 121.323029): 879, (31.125467, 121.581832): 880, (31.194792, 121.466884): 881, (31.033659, 121.577846): 882, (31.28023, 121.543164): 883, (31.212718, 121.447853): 884, (31.201251, 121.561484): 885, (30.904757, 121.467941): 886, (31.233629, 121.532011): 887, (31.168519, 121.804669): 888, (31.227165, 121.480195): 889, (31.359311, 121.310645): 890, (31.286632, 121.552094): 891, (31.280339, 121.402957): 892, (30.87706, 121.856429): 893, (31.260465, 121.461228): 894, (31.158843, 121.127268): 895, (31.241762, 121.50497): 896, (31.132311, 121.362691): 897, (31.19332, 121.430385): 898, (31.139721, 121.673912): 899, (31.268862, 121.434292): 900, (31.27431, 121.448589): 901, (31.333313, 121.333805): 902, (31.410089, 121.244982): 903, (31.291143, 121.472012): 904, (31.346682, 121.49352): 905, (31.175781, 121.643301): 906, (31.339206, 121.453188): 907, (31.12408, 121.716759): 908, (30.838141, 121.189035): 909, (31.149677, 121.729053): 910, (31.1744, 121.285083): 911, (31.395973, 121.258897): 912, (31.219647, 121.446669): 913, (31.194928, 121.366998): 914, (31.155349, 121.375527): 915, (31.225266, 121.444323): 916, (31.23543, 121.447652): 917, (31.240412, 121.147662): 918, (30.996346, 121.297687): 919, (31.402222, 121.493387): 920, (31.273242, 121.499598): 921, (31.2077, 121.669382): 922, (31.254124, 121.605142): 923, (31.333315, 121.188402): 924, (31.195907, 121.410659): 925, (31.22121, 121.411568): 926, (31.423181, 121.387917): 927, (31.190301, 121.566862): 928, (31.173163, 121.356571): 929, (30.741693, 121.367052): 930, (31.19206, 121.438501): 931, (31.163432, 121.354451): 932, (31.420016, 121.44113): 933, (30.956624, 121.334663): 934, (31.166408, 121.569203): 935, (31.109666, 121.052906): 936, (31.260429, 121.426787): 937, (31.2114, 121.476689): 938, (31.198952, 121.70244): 939, (31.166365, 121.419792): 940, (31.266666, 121.598298): 941, (31.04139, 121.741651): 942, (31.23936, 121.488752): 943, (31.111607, 121.324796): 944, (31.170097, 121.608906): 945, (30.960195, 121.398632): 946, (31.211598, 121.439406): 947, (30.730296, 121.342643): 948, (31.3397, 121.298473): 949, (31.157677, 121.424531): 950, (31.280348, 121.592755): 951, (30.991472, 121.430726): 952, (31.201303, 121.434409): 953, (31.260903, 121.618966): 954, (31.116903, 121.595022): 955, (31.163339, 121.469291): 956, (31.178278, 121.606216): 957, (31.337091, 121.543621): 958, (31.052639, 121.761696): 959, (31.43737, 121.432451): 960, (31.046945, 121.764826): 961, (31.023064, 121.534267): 962, (31.173856, 121.350522): 963, (31.292717, 121.482235): 964, (31.285741, 121.626244): 965, (31.192275, 121.462826): 966, (31.308964, 121.403345): 967, (31.215081, 121.481286): 968, (31.241014, 121.48529): 969, (31.311405, 121.553882): 970, (31.224452, 121.511537): 971, (31.228726, 121.482748): 972, (31.048845, 121.231719): 973, (31.160355, 121.454785): 974, (31.276349, 121.39526): 975, (31.48259, 121.348661): 976, (31.236935, 121.106513): 977, (31.276268, 121.683198): 978, (31.312069, 121.172955): 979, (31.22622, 121.506261): 980, (31.263762, 121.481215): 981, (31.206089, 121.105074): 982, (31.343386, 121.208271): 983, (31.262048, 121.458128): 984, (31.222119, 121.454078): 985, (31.13264, 121.32443): 986, (31.172757, 121.425965): 987, (31.191847, 121.381117): 988, (31.297689, 121.447357): 989, (31.33454, 121.46611): 990, (30.855412, 121.569454): 991, (31.395438, 121.472743): 992, (31.031459, 121.300841): 993, (30.909885, 121.195459): 994, (31.414397, 121.481621): 995, (31.273451, 121.681542): 996, (30.900013, 121.049649): 997, (31.152182, 121.564661): 998, (31.080824, 121.390896): 999, (31.058407, 121.330724): 1000, (31.148848, 121.314851): 1001, (31.232857, 121.475333): 1002, (31.204614, 121.432408): 1003, (31.423448, 121.433415): 1004, (30.7653, 121.292527): 1005, (30.910239, 121.419143): 1006, (31.323091, 121.364299): 1007, (31.370654, 121.459272): 1008, (31.26338, 121.517162): 1009, (31.385543, 121.469415): 1010, (31.242893, 121.465382): 1011, (31.218812, 121.401309): 1012, (31.035335, 121.117893): 1013, (31.26338, 121.433161): 1014, (31.400853, 121.483671): 1015, (31.204036, 121.437644): 1016, (31.187254, 121.536259): 1017, (31.217261, 121.391532): 1018, (31.298177, 121.473198): 1019, (31.198093, 121.126168): 1020, (31.29099, 121.697503): 1021, (31.247688, 121.528106): 1022, (31.282271, 121.524511): 1023, (31.284042, 121.583676): 1024, (30.919334, 121.475563): 1025, (30.750457, 121.376839): 1026, (30.972355, 121.610007): 1027, (31.309342, 121.580807): 1028, (31.231585, 121.517218): 1029, (31.221876, 121.687381): 1030, (31.059755, 121.389723): 1031, (31.361645, 121.475443): 1032, (31.159304, 121.358718): 1033, (31.26748, 121.57859): 1034, (31.248146, 121.441241): 1035, (30.93817, 121.865652): 1036, (31.122643, 121.734179): 1037, (31.264477, 121.44249): 1038, (31.378631, 121.277965): 1039, (31.049556, 121.600672): 1040, (31.023902, 121.350511): 1041, (30.91796, 121.543057): 1042, (30.998611, 121.216212): 1043, (31.25037, 121.488549): 1044, (31.3691, 121.415317): 1045, (31.370457, 121.470415): 1046, (31.219133, 121.524846): 1047, (31.194194, 121.456589): 1048, (31.051247, 121.480621): 1049, (30.895699, 121.478289): 1050, (31.360977, 121.444195): 1051, (31.335151, 121.590721): 1052, (31.013227, 121.552205): 1053, (31.4193, 121.285094): 1054, (31.311631, 121.354326): 1055, (31.356305, 121.293716): 1056, (31.023331, 121.315006): 1057, (31.296619, 121.497055): 1058, (31.057069, 121.599172): 1059, (31.074814, 121.713455): 1060, (31.142865, 121.330843): 1061, (31.215664, 121.46829): 1062, (31.013082, 121.391811): 1063, (31.291293, 121.559335): 1064, (30.963144, 121.39936): 1065, (31.100465, 121.401931): 1066, (31.016895, 121.42971): 1067, (31.301132, 121.177821): 1068, (30.97533, 121.271449): 1069, (31.396351, 121.44222): 1070, (31.115176, 121.36579): 1071, (31.224855, 121.480154): 1072, (31.434069, 121.448741): 1073, (30.929412, 121.460403): 1074, (31.316557, 121.516135): 1075, (31.23593, 121.220028): 1076, (31.009423, 121.245898): 1077, (31.132602, 121.095863): 1078, (31.048503, 121.141872): 1079, (31.390234, 121.254572): 1080, (30.889117, 121.319998): 1081, (31.262756, 121.625447): 1082, (30.893492, 121.315973): 1083, (31.224718, 121.344451): 1084, (31.233761, 121.670586): 1085, (31.284675, 121.516006): 1086, (31.250674, 121.47282): 1087, (31.273276, 121.1303): 1088, (31.041762, 120.928733): 1089, (31.217323, 121.753758): 1090, (31.124812, 121.508263): 1091, (31.23301, 121.442524): 1092, (31.256232, 121.498254): 1093, (31.100034, 121.663489): 1094, (31.185812, 121.457923): 1095, (31.240874, 121.518086): 1096, (31.251894, 121.464026): 1097, (31.266257, 121.506521): 1098, (31.047441, 121.470836): 1099, (31.235247, 121.383101): 1100, (31.497375, 121.324955): 1101, (30.985081, 121.691229): 1102, (31.17247, 121.404203): 1103, (31.364407, 121.561457): 1104, (31.334186, 121.50238): 1105, (30.895162, 121.094602): 1106, (31.232407, 121.652986): 1107, (31.24641, 121.548541): 1108, (31.037985, 121.752781): 1109, (31.159756, 121.525433): 1110, (30.866289, 121.103027): 1111, (31.223107, 121.454329): 1112, (31.195613, 121.40425): 1113, (30.957633, 121.343012): 1114, (31.234259, 121.52572): 1115, (31.285059, 121.406807): 1116, (31.159569, 121.145966): 1117, (31.357217, 121.439635): 1118, (30.739639, 121.373695): 1119, (31.098701, 121.582178): 1120, (31.252105, 121.56398): 1121, (30.943662, 121.155665): 1122, (30.774911, 121.259927): 1123, (30.763589, 121.264689): 1124, (31.299073, 121.552202): 1125, (31.352538, 121.490272): 1126, (31.157022, 121.498865): 1127, (31.168125, 121.503534): 1128, (31.046524, 121.749155): 1129, (31.032045, 121.783775): 1130, (31.038911, 121.226883): 1131, (31.231508, 121.577691): 1132, (31.420118, 121.349715): 1133, (31.150287, 121.471783): 1134, (31.459469, 121.412299): 1135, (31.168922, 121.412651): 1136, (31.202103, 121.445912): 1137, (31.398623, 121.409041): 1138, (30.802409, 121.205114): 1139, (31.308192, 121.667576): 1140, (31.16584, 121.530742): 1141, (31.113159, 121.06421): 1142, (31.197114, 121.442853): 1143, (31.278161, 121.486144): 1144, (30.830149, 121.35652): 1145, (31.090076, 121.406999): 1146, (31.235461, 121.498056): 1147, (30.903899, 121.906187): 1148, (31.223117, 121.504741): 1149, (31.269517, 121.498812): 1150, (31.206201, 121.085991): 1151, (31.154923, 121.36045): 1152, (30.946702, 121.624426): 1153, (31.241036, 121.374063): 1154, (31.299799, 121.530915): 1155, (31.213582, 121.38377): 1156, (30.850119, 121.499982): 1157, (31.278008, 121.492049): 1158, (31.235154, 121.533056): 1159, (30.85108, 121.855994): 1160, (31.171916, 121.718029): 1161, (31.318281, 121.447698): 1162, (31.225275, 121.47164): 1163, (31.396687, 121.378844): 1164, (30.827125, 121.329382): 1165, (31.200811, 121.444401): 1166, (30.72456, 121.344043): 1167, (31.168296, 121.588625): 1168, (31.242157, 121.568785): 1169, (31.182725, 121.41492): 1170, (31.258255, 121.471554): 1171, (31.005717, 121.202014): 1172, (31.267826, 121.521146): 1173, (31.410197, 121.500761): 1174, (31.209704, 121.368486): 1175, (31.168938, 121.54333): 1176, (31.225653, 121.480878): 1177, (31.380093, 121.517779): 1178, (31.326138, 121.48973): 1179, (31.181785, 121.529159): 1180, (31.230287, 121.557116): 1181, (31.311763, 121.21253): 1182, (31.302901, 121.553485): 1183, (31.248482, 121.53958): 1184, (31.072138, 121.661831): 1185, (31.283201, 121.665402): 1186, (31.228852, 121.541643): 1187, (25.222206, 117.086322): 1188, (31.30446, 121.426575): 1189, (31.206194, 121.456569): 1190, (31.012212, 121.413472): 1191, (31.300338, 121.526827): 1192, (31.224154, 121.43807): 1193, (31.281567, 121.557955): 1194, (31.209182, 121.535827): 1195, (31.288233, 121.304952): 1196, (31.206668, 121.449678): 1197, (31.270493, 121.353356): 1198, (31.294885, 121.244609): 1199, (31.267478, 121.5468): 1200, (31.066368, 121.2082): 1201, (31.292462, 121.271212): 1202, (31.222233, 121.623547): 1203, (31.207139, 121.449578): 1204, (30.977517, 121.669761): 1205, (31.266232, 121.402951): 1206, (31.225092, 121.38464): 1207, (31.242948, 121.608734): 1208, (31.278808, 121.419722): 1209, (31.271455, 121.513192): 1210, (31.268106, 121.49354): 1211, (31.018176, 121.418004): 1212, (31.20694, 121.459659): 1213, (31.156928, 121.121902): 1214, (30.719537, 121.355948): 1215, (31.29092, 121.359453): 1216, (31.259319, 121.494159): 1217, (31.235072, 121.518805): 1218, (31.32755, 121.15343): 1219, (31.244559, 121.724977): 1220, (31.235453, 121.653548): 1221, (31.268663, 121.428101): 1222, (30.989121, 121.127308): 1223, (31.022923, 121.806437): 1224, (31.24143, 121.475172): 1225, (31.262004, 121.350318): 1226, (31.459342, 121.319364): 1227, (31.248195, 121.490929): 1228, (31.415226, 121.349387): 1229, (31.252974, 121.502684): 1230, (31.351256, 121.519123): 1231, (31.00948, 121.4998): 1232, (31.249796, 121.582523): 1233, (31.281363, 121.459935): 1234, (31.19124, 121.507961): 1235, (30.705762, 121.33512): 1236, (31.082614, 121.523874): 1237, (30.929919, 121.206039): 1238, (31.260814, 121.490337): 1239, (31.356868, 121.555161): 1240, (31.240453, 121.511773): 1241, (31.255025, 121.199554): 1242, (31.053362, 121.431405): 1243, (31.201064, 121.438699): 1244, (31.112899, 121.367429): 1245, (31.250986, 121.460075): 1246, (31.426865, 121.351198): 1247, (31.320121, 121.459018): 1248, (31.234796, 121.533464): 1249, (31.110093, 121.371253): 1250, (31.248715, 121.481228): 1251, (31.19706, 121.095538): 1252, (31.274535, 121.48034): 1253, (31.178033, 121.487803): 1254, (31.119755, 121.577952): 1255, (31.371366, 121.236234): 1256, (31.232893, 121.380242): 1257, (31.282514, 121.343407): 1258, (31.312494, 121.157796): 1259, (30.763912, 121.333802): 1260, (31.279562, 121.467095): 1261, (31.019982, 121.244295): 1262, (31.335141, 121.413142): 1263, (31.050708, 121.217094): 1264, (31.226242, 121.450105): 1265, (31.143965, 121.573343): 1266, (31.193643, 121.402511): 1267, (31.161896, 121.135143): 1268, (31.277766, 121.433047): 1269, (31.207348, 121.70185): 1270, (31.203552, 121.371544): 1271, (31.239319, 121.265377): 1272, (31.19107, 121.554321): 1273, (31.409644, 121.303936): 1274, (31.04822, 121.231928): 1275, (30.733903, 121.341904): 1276, (31.236009, 121.478941): 1277, (30.80226, 121.311879): 1278, (31.289509, 121.447288): 1279, (31.230748, 121.485488): 1280, (31.196135, 121.558528): 1281, (31.270809, 121.543999): 1282, (31.216752, 121.476401): 1283, (31.233248, 121.479537): 1284, (31.002291, 121.873286): 1285, (31.13107, 121.298444): 1286, (30.94602, 121.106186): 1287, (30.739797, 121.32739): 1288, (31.225318, 121.437806): 1289, (31.281917, 121.435906): 1290, (31.289282, 121.378597): 1291, (31.086136, 121.507054): 1292, (30.993467, 121.234309): 1293, (31.134484, 121.42764): 1294, (31.238801, 121.386263): 1295, (30.983141, 121.540945): 1296, (31.204349, 121.693531): 1297, (31.218716, 121.568667): 1298, (31.281994, 121.631556): 1299, (31.189701, 121.543143): 1300, (31.164838, 121.410811): 1301, (30.897085, 121.617832): 1302, (31.051731, 121.423756): 1303, (31.180175, 121.422303): 1304, (31.267542, 121.685192): 1305, (31.324375, 121.433451): 1306, (31.262448, 121.504717): 1307, (31.155148, 121.597536): 1308, (30.93561, 121.517165): 1309, (31.236991, 121.441554): 1310, (31.214719, 121.624972): 1311, (31.242967, 121.51048): 1312, (31.221346, 121.201454): 1313, (30.915722, 121.763195): 1314, (31.265, 121.48958): 1315, (31.2716, 121.465223): 1316, (31.410741, 121.442349): 1317, (31.328439, 121.384246): 1318, (31.137582, 121.083916): 1319, (31.234382, 121.531106): 1320, (31.088801, 121.536584): 1321, (31.344366, 121.51915): 1322, (31.10114, 121.181649): 1323, (31.244931, 121.50832): 1324, (30.993594, 121.348984): 1325, (31.15228, 121.118001): 1326, (31.22278, 121.450373): 1327, (31.254912, 121.371838): 1328, (31.453478, 121.256429): 1329, (35.24116, 113.276351): 1330, (31.180824, 121.525363): 1331, (31.223004, 121.465956): 1332, (31.080002, 121.798389): 1333, (30.796356, 121.199264): 1334, (31.287814, 121.355886): 1335, (31.456258, 121.413525): 1336, (31.187859, 121.444801): 1337, (31.256426, 121.61001): 1338, (31.397648, 121.28551): 1339, (31.040309, 121.308498): 1340, (31.203473, 121.445984): 1341, (31.116086, 121.488253): 1342, (30.90618, 121.178287): 1343, (31.24256, 121.490511): 1344, (31.396658, 121.496972): 1345, (31.425786, 121.422506): 1346, (31.182371, 121.526439): 1347, (31.240686, 121.480699): 1348, (31.286022, 121.497436): 1349, (31.298752, 121.491919): 1350, (31.310425, 121.439368): 1351, (31.242514, 121.493696): 1352, (30.876032, 121.466173): 1353, (31.279322, 121.466136): 1354, (31.250339, 121.440196): 1355, (31.326418, 121.501541): 1356, (31.341417, 121.271089): 1357, (31.258483, 121.368346): 1358, (31.244555, 121.5099): 1359, (31.254981, 121.623923): 1360, (31.081917, 121.062248): 1361, (41.677262, 125.960124): 1362, (31.177067, 121.370844): 1363, (31.299874, 121.298501): 1364, (31.035151, 121.544702): 1365, (31.313954, 121.499987): 1366, (31.222609, 121.378494): 1367, (31.231119, 121.484436): 1368, (31.301749, 121.457406): 1369, (31.211634, 121.520845): 1370, (31.235177, 121.451385): 1371, (31.235847, 121.536101): 1372, (31.063923, 121.353076): 1373, (31.265054, 121.456413): 1374, (31.216371, 121.601926): 1375, (31.238742, 121.530031): 1376, (31.289489, 121.341656): 1377, (31.242584, 121.449606): 1378, (30.978325, 121.70767): 1379, (31.253405, 121.091261): 1380, (31.025516, 121.30169): 1381, (31.116103, 121.43571): 1382, (31.188514, 121.49841): 1383, (31.255582, 121.363523): 1384, (31.25879, 121.627202): 1385, (31.082557, 121.430065): 1386, (31.312192, 121.456456): 1387, (31.232518, 121.465177): 1388, (30.781368, 121.418953): 1389, (30.77648, 121.358227): 1390, (31.129482, 121.192266): 1391, (31.261924, 121.568031): 1392, (31.337637, 121.446597): 1393, (31.246512, 121.514772): 1394, (31.293561, 121.362372): 1395, (31.345443, 121.596458): 1396, (31.24018, 121.506063): 1397, (31.236381, 121.484201): 1398, (31.126777, 121.367557): 1399, (31.171972, 121.140685): 1400, (31.317417, 121.48383): 1401, (31.143774, 121.519955): 1402, (31.201321, 121.404211): 1403, (31.19183, 121.59097): 1404, (31.246371, 121.513215): 1405, (31.073706, 121.680729): 1406, (31.231815, 121.464062): 1407, (31.108876, 121.345256): 1408, (31.143754, 121.434357): 1409, (31.242841, 121.391309): 1410, (31.296405, 121.392803): 1411, (31.109271, 121.617421): 1412, (31.211492, 121.571765): 1413, (31.200432, 121.447381): 1414, (31.389487, 121.259676): 1415, (31.195642, 121.446626): 1416, (31.280389, 121.467635): 1417, (31.094143, 121.488864): 1418, (31.211337, 121.413801): 1419, (31.176379, 121.397706): 1420, (31.216686, 121.493011): 1421, (31.444231, 121.225828): 1422, (31.42718, 121.33925): 1423, (31.036623, 121.377512): 1424, (31.015724, 121.40907): 1425, (31.065995, 121.314532): 1426, (31.180897, 121.463904): 1427, (31.057816, 121.413943): 1428, (31.236405, 121.432864): 1429, (31.110606, 121.517194): 1430, (31.234168, 121.473173): 1431, (31.275472, 121.436799): 1432, (31.116791, 121.166516): 1433, (31.184883, 121.430689): 1434, (31.280867, 121.656939): 1435, (31.473557, 121.331298): 1436, (31.271086, 121.549327): 1437, (31.342417, 121.40922): 1438, (31.178962, 121.388264): 1439, (31.238593, 121.450062): 1440, (31.148334, 121.420448): 1441, (31.201965, 121.452457): 1442, (31.162714, 121.772242): 1443, (31.227544, 121.537532): 1444, (31.19907, 121.517372): 1445, (31.317624, 121.405225): 1446, (31.13623, 121.77848): 1447, (31.152045, 121.420017): 1448, (31.255293, 121.49343): 1449, (31.087507, 121.398717): 1450, (31.229645, 121.481533): 1451, (31.054947, 120.995325): 1452, (31.246038, 121.531441): 1453, (31.137143, 121.545715): 1454, (31.203981, 121.483421): 1455, (31.197512, 121.470198): 1456, (31.070433, 121.467199): 1457, (31.281772, 121.484273): 1458, (31.219947, 121.467027): 1459, (31.240412, 121.45723): 1460, (31.207251, 121.407122): 1461, (31.203742, 121.461251): 1462, (31.230277, 121.448573): 1463, (31.291587, 121.511561): 1464, (31.402193, 121.367083): 1465, (31.254643, 121.569349): 1466, (31.303373, 121.4429): 1467, (31.266813, 121.512658): 1468, (31.150587, 121.499019): 1469, (31.276567, 121.483483): 1470, (31.241242, 121.517216): 1471, (31.113675, 121.383752): 1472, (31.242837, 121.455083): 1473, (31.016211, 121.412788): 1474, (31.020849, 121.759757): 1475, (31.353624, 121.155902): 1476, (31.305114, 121.41898): 1477, (31.051162, 121.311884): 1478, (31.319422, 121.469372): 1479, (31.236177, 121.493181): 1480, (31.236926, 121.454604): 1481, (31.185044, 121.414303): 1482, (31.436148, 121.224416): 1483, (30.882016, 121.532008): 1484, (31.225477, 121.448153): 1485, (30.941552, 121.1676): 1486, (31.299183, 121.628904): 1487, (31.242365, 121.503571): 1488, (30.935837, 121.561311): 1489, (31.120602, 121.581473): 1490, (28.738742, 120.640606): 1491, (31.195887, 121.591747): 1492, (31.310235, 121.437455): 1493, (31.221705, 121.642288): 1494, (31.370243, 121.576881): 1495, (31.10346, 121.419988): 1496, (31.188258, 121.301297): 1497, (31.197573, 121.454035): 1498, (31.250767, 121.458177): 1499, (31.18078, 121.423677): 1500, (31.236828, 121.514196): 1501, (31.248657, 121.537875): 1502, (31.201279, 121.709485): 1503, (31.258321, 121.496097): 1504, (31.232095, 121.20583): 1505, (31.277632, 121.380997): 1506, (31.19927, 121.6228): 1507, (31.204763, 121.526768): 1508, (31.158515, 121.1384): 1509, (31.196602, 121.461665): 1510, (31.026816, 121.426094): 1511, (31.193686, 121.423417): 1512, (31.227762, 121.495598): 1513, (31.146285, 121.518893): 1514, (31.238561, 121.455041): 1515, (31.087224, 121.466989): 1516, (31.023241, 121.226517): 1517, (31.203513, 121.422001): 1518, (31.242261, 121.507912): 1519, (31.245242, 121.479617): 1520, (31.247009, 121.47942): 1521, (31.176998, 121.423586): 1522, (31.228047, 121.425893): 1523, (31.184951, 121.409317): 1524, (31.121996, 121.393805): 1525, (30.781759, 121.168333): 1526, (31.239437, 121.470556): 1527, (31.191674, 121.699103): 1528, (30.960478, 121.057529): 1529, (31.042348, 121.28911): 1530, (31.207125, 121.299924): 1531, (30.990942, 121.12541): 1532, (30.981236, 121.488197): 1533, (31.231468, 121.537215): 1534, (31.299291, 121.454034): 1535, (31.239763, 121.540092): 1536, (31.272659, 121.601779): 1537, (31.230109, 121.434393): 1538, (31.143681, 121.353698): 1539, (31.244701, 121.514543): 1540, (31.26905, 121.627732): 1541, (31.241739, 121.537288): 1542, (31.301877, 121.353206): 1543, (31.073061, 120.970637): 1544, (31.192397, 121.384407): 1545, (31.307142, 121.362041): 1546, (31.330908, 121.45516): 1547, (31.233516, 121.491431): 1548, (31.301954, 121.54744): 1549, (31.161027, 121.722478): 1550, (31.220809, 121.305363): 1551, (31.219656, 121.49234): 1552, (31.293762, 121.442609): 1553, (31.214741, 121.427168): 1554, (31.217315, 121.635295): 1555, (31.360948, 121.595293): 1556, (31.154875, 121.131226): 1557, (31.195131, 121.418788): 1558, (30.948258, 121.324472): 1559, (31.364338, 121.251014): 1560, (31.245569, 121.5156): 1561, (31.213397, 121.413655): 1562, (31.250428, 121.460956): 1563, (31.241223, 121.45891): 1564, (31.201009, 121.610969): 1565, (31.301105, 121.430139): 1566, (31.231068, 121.4021): 1567, (30.94469, 121.299291): 1568, (31.144653, 121.478451): 1569, (31.273045, 121.546148): 1570, (31.304538, 121.526838): 1571, (31.210354, 121.595268): 1572, (31.233674, 121.498294): 1573, (31.429482, 121.184088): 1574, (31.238815, 121.559352): 1575, (31.206958, 121.591146): 1576, (31.050344, 121.770082): 1577, (30.914669, 121.471638): 1578, (31.203019, 121.443614): 1579, (31.198113, 121.590353): 1580, (31.180043, 121.609028): 1581, (30.844717, 121.528288): 1582, (31.139255, 121.409759): 1583, (31.070388, 121.532962): 1584, (31.304081, 121.326517): 1585, (31.239474, 121.495166): 1586, (31.420803, 121.280523): 1587, (31.310089, 121.640057): 1588, (31.315501, 121.535127): 1589, (31.318389, 121.653955): 1590, (30.945561, 121.150834): 1591, (31.193771, 121.546521): 1592, (30.948533, 121.465504): 1593, (31.25863, 121.490496): 1594, (31.169796, 121.52847): 1595, (30.879349, 121.094749): 1596, (31.105352, 121.438535): 1597, (31.243213, 121.504331): 1598, (31.306637, 121.523563): 1599, (31.310614, 121.476617): 1600, (31.259739, 121.488421): 1601, (31.269951, 121.336766): 1602, (31.294215, 121.527852): 1603, (30.932543, 121.716591): 1604, (30.73407, 121.350673): 1605, (31.106759, 121.398956): 1606, (31.195877, 121.597043): 1607, (31.239726, 121.510008): 1608, (31.236531, 121.491325): 1609, (31.23271, 121.455345): 1610, (30.965531, 121.208487): 1611, (31.22656, 121.490417): 1612, (31.273966, 121.429374): 1613, (31.247971, 121.513421): 1614, (31.101868, 121.461669): 1615, (31.198799, 121.383701): 1616, (31.209934, 121.604472): 1617, (31.177291, 121.405153): 1618, (31.294834, 121.431554): 1619, (31.216472, 121.413336): 1620, (31.475425, 121.356951): 1621, (31.207094, 121.713617): 1622, (30.89577, 121.697282): 1623, (31.243039, 121.478897): 1624, (31.358748, 121.52806): 1625, (31.258629, 121.491056): 1626, (31.253808, 121.483756): 1627, (31.262032, 121.117499): 1628, (31.217968, 121.41467): 1629, (31.365408, 121.245027): 1630, (31.317584, 121.626266): 1631, (31.236599, 121.469857): 1632, (31.226773, 121.480316): 1633, (30.810888, 121.234763): 1634, (31.222272, 121.438997): 1635, (31.087108, 121.371206): 1636, (31.387389, 121.431922): 1637, (31.300545, 121.170082): 1638, (31.200783, 121.446011): 1639, (31.1171, 121.391692): 1640, (31.228956, 121.535003): 1641, (31.291192, 121.65263): 1642, (31.175943, 121.424363): 1643, (29.263844, 115.023159): 1644, (31.195709, 121.424093): 1645, (30.993285, 121.36769): 1646, (31.309645, 121.4911): 1647, (31.187384, 121.470686): 1648, (31.252241, 121.57094): 1649, (31.463063, 121.345403): 1650, (31.184347, 121.441392): 1651, (31.232268, 121.541982): 1652, (31.276718, 121.371696): 1653, (31.276315, 121.564096): 1654, (30.9269, 121.467678): 1655, (31.384186, 121.484871): 1656, (31.043159, 121.420296): 1657, (31.251358, 121.485526): 1658, (31.211617, 121.486703): 1659, (31.053198, 120.907525): 1660, (31.194364, 121.400164): 1661, (31.27369, 121.537119): 1662, (31.22132, 121.438917): 1663, (31.224854, 121.476992): 1664, (39.084494, 117.236165): 1665, (30.956486, 121.40597): 1666, (30.953548, 121.636227): 1667, (31.222328, 121.461659): 1668, (31.492988, 121.292044): 1669, (31.168853, 121.394657): 1670, (31.437104, 121.31285): 1671, (31.233112, 121.442667): 1672, (31.195623, 121.398393): 1673, (31.047975, 121.785062): 1674, (30.815977, 121.436091): 1675, (31.383625, 121.268069): 1676, (31.250826, 121.578689): 1677, (31.216788, 121.299974): 1678, (31.397749, 121.267217): 1679, (31.4189, 121.295725): 1680, (31.282066, 121.49415): 1681, (31.427978, 121.433301): 1682, (31.202912, 121.712449): 1683, (31.307215, 121.486962): 1684, (31.260348, 121.542138): 1685, (31.156365, 121.41831): 1686, (31.21882, 121.436387): 1687, (47.35092, 130.301233): 1688, (31.103599, 121.416145): 1689, (31.188332, 121.405187): 1690, (31.239454, 121.46057): 1691, (31.276982, 121.460649): 1692, (31.169703, 121.412163): 1693, (31.248117, 121.466399): 1694, (31.289793, 121.455017): 1695, (31.214059, 121.37764): 1696, (31.271472, 121.494721): 1697, (30.831429, 121.242309): 1698, (30.992078, 121.31207): 1699, (31.00953, 121.052076): 1700, (31.187829, 121.452956): 1701, (31.052253, 121.347624): 1702, (31.204343, 121.407018): 1703, (31.23537, 121.522706): 1704, (31.08705, 121.503604): 1705, (31.184426, 121.15811): 1706, (31.296226, 121.494005): 1707, (30.935029, 121.186594): 1708, (31.263678, 121.468663): 1709, (31.20901, 121.478403): 1710, (31.244292, 121.495265): 1711, (31.355364, 121.537096): 1712, (31.231035, 121.555602): 1713, (31.213061, 121.483541): 1714, (31.342336, 121.605688): 1715, (31.370336, 121.265507): 1716, (31.402828, 121.48779): 1717, (31.248429, 121.463698): 1718, (31.291773, 121.203129): 1719, (31.286501, 121.363758): 1720, (31.339843, 121.232133): 1721, (31.30048, 121.469206): 1722, (31.251161, 121.489743): 1723, (30.897457, 121.36324): 1724, (30.838466, 121.228977): 1725, (30.933573, 121.55651): 1726, (31.405792, 121.489703): 1727, (31.20414, 121.406657): 1728, (30.891109, 121.903749): 1729, (30.889377, 121.176142): 1730, (31.260888, 121.340352): 1731, (31.119806, 121.071074): 1732, (31.03732, 121.18963): 1733, (30.930023, 121.011969): 1734, (31.214193, 121.376945): 1735, (30.995864, 121.568444): 1736, (31.099126, 121.813976): 1737, (31.280452, 121.480426): 1738, (31.058805, 121.593285): 1739, (30.747936, 121.348822): 1740, (31.243709, 121.197504): 1741, (31.332214, 121.21858): 1742, (30.889354, 121.488288): 1743, (31.224539, 121.550163): 1744, (31.248325, 121.158122): 1745, (31.265321, 121.503333): 1746, (31.211395, 121.465744): 1747, (31.126339, 121.573213): 1748, (31.355885, 121.375057): 1749, (31.172866, 121.434994): 1750, (30.897459, 121.151158): 1751, (31.239804, 121.545118): 1752, (31.237635, 121.476519): 1753, (31.24583, 121.392643): 1754, (31.28915, 121.229167): 1755, (31.17939, 121.754087): 1756, (31.229203, 121.449776): 1757, (31.317284, 121.46298): 1758, (31.135275, 121.765517): 1759, (31.234787, 121.498324): 1760, (31.456227, 121.359857): 1761, (30.919873, 121.464262): 1762, (31.320829, 121.389518): 1763, (30.969483, 121.53972): 1764, (31.27688, 121.420858): 1765, (30.903117, 121.146985): 1766, (31.294598, 121.206157): 1767, (31.275801, 121.484797): 1768, (31.284086, 121.212427): 1769, (31.035283, 121.246093): 1770, (31.40369, 121.255993): 1771, (31.229071, 121.452449): 1772, (31.233452, 121.480931): 1773, (31.242078, 121.331128): 1774, (30.99055, 121.126163): 1775, (31.030657, 121.236586): 1776, (31.260165, 121.541462): 1777, (31.301584, 121.490912): 1778, (31.273767, 121.222048): 1779, (30.956973, 121.097333): 1780, (31.269535, 121.4975): 1781, (31.098359, 121.073778): 1782, (31.401391, 121.463758): 1783, (30.852916, 121.364997): 1784, (31.318438, 121.497035): 1785, (31.469563, 121.384552): 1786, (30.770905, 121.407376): 1787, (31.188181, 121.442033): 1788, (30.736314, 121.354852): 1789, (30.736093, 121.355957): 1790, (30.891482, 121.739107): 1791, (31.267673, 121.346341): 1792, (31.23909, 121.474138): 1793, (30.869886, 121.901843): 1794, (31.245183, 121.484965): 1795, (31.169494, 121.302234): 1796, (31.301233, 121.167693): 1797, (31.229191, 121.453189): 1798, (31.225831, 121.36681): 1799, (30.945306, 121.729327): 1800, (31.248836, 121.463246): 1801, (31.229094, 121.480266): 1802, (31.000658, 121.659325): 1803, (31.253206, 121.459958): 1804, (31.292661, 121.305822): 1805, (30.912581, 121.465142): 1806, (30.983162, 121.054057): 1807, (31.23095, 121.537305): 1808, (30.898595, 121.030128): 1809, (31.216528, 121.439617): 1810, (31.273024, 121.42572): 1811, (31.142218, 121.413767): 1812, (30.798179, 121.426469): 1813, (31.218516, 121.463689): 1814, (31.270785, 121.465575): 1815, (31.174304, 121.436093): 1816, (31.200428, 121.437655): 1817, (31.237789, 121.461058): 1818, (31.18674, 121.380533): 1819, (31.216092, 121.508679): 1820, (31.235982, 121.466191): 1821, (30.866941, 121.595653): 1822, (31.237221, 121.467334): 1823, (30.880562, 121.486561): 1824, (30.918205, 121.509992): 1825, (31.166663, 121.412681): 1826, (31.200249, 121.443811): 1827, (31.039411, 121.819385): 1828, (31.267858, 121.495135): 1829, (31.233617, 121.523688): 1830, (31.231663, 121.428471): 1831, (31.198273, 121.445101): 1832, (31.191818, 121.538713): 1833, (31.431346, 121.222574): 1834, (31.052986, 121.506423): 1835, (31.161066, 121.354642): 1836, (31.203911, 121.590509): 1837, (31.010467, 121.237408): 1838, (31.230671, 121.552848): 1839, (31.232395, 121.564673): 1840, (31.052413, 121.218957): 1841, (31.376717, 121.539395): 1842, (31.292699, 121.478195): 1843, (30.905367, 121.832974): 1844, (31.056596, 121.721736): 1845, (31.27495, 121.506381): 1846, (31.11447, 121.460831): 1847, (30.9152, 121.664095): 1848, (31.365012, 121.504701): 1849, (31.2676, 121.541803): 1850, (31.265604, 121.567367): 1851, (31.233051, 121.455285): 1852, (31.197573, 121.37641): 1853, (31.210579, 121.384648): 1854, (31.397562, 121.15864): 1855, (31.217249, 121.504369): 1856, (31.217162, 121.411803): 1857, (31.271819, 121.546358): 1858, (31.294378, 121.567937): 1859, (31.384742, 121.277311): 1860, (31.224758, 121.446392): 1861, (31.28823, 121.512731): 1862, (31.288572, 121.534233): 1863, (30.891157, 121.186557): 1864, (30.990721, 121.803969): 1865, (31.211846, 121.475732): 1866, (31.308523, 121.423052): 1867, (31.044697, 121.22462): 1868, (31.062583, 121.44986): 1869, (31.302288, 121.510464): 1870, (30.831909, 121.397355): 1871, (31.358732, 121.508333): 1872, (31.256595, 121.493953): 1873, (31.189, 121.431937): 1874, (31.113501, 121.393037): 1875, (31.178085, 121.409564): 1876, (31.456615, 121.221022): 1877, (30.893084, 121.252519): 1878, (31.024958, 120.918706): 1879, (31.289799, 121.455117): 1880, (31.199522, 121.44697): 1881, (31.25372, 121.437778): 1882, (31.227254, 121.402789): 1883, (31.286253, 121.531109): 1884, (31.233659, 121.565806): 1885, (31.302355, 121.515812): 1886, (31.042415, 121.482986): 1887, (31.312979, 121.545236): 1888, (30.874563, 121.067902): 1889, (31.262045, 121.446891): 1890, (31.292709, 121.497635): 1891, (31.42225, 121.355698): 1892, (30.982046, 121.207872): 1893, (31.115773, 120.907942): 1894, (31.257724, 121.595342): 1895, (31.258903, 121.441965): 1896, (31.181629, 121.510713): 1897, (31.235454, 121.172523): 1898, (31.055129, 121.770244): 1899, (31.294508, 121.537685): 1900, (30.977355, 121.74808): 1901, (30.844718, 121.256228): 1902, (31.250734, 121.487731): 1903, (31.171648, 121.110276): 1904, (31.09599, 121.238111): 1905, (31.391131, 121.251789): 1906, (31.266413, 121.351003): 1907, (30.87366, 121.082631): 1908, (31.180581, 121.527653): 1909, (31.314684, 121.522729): 1910, (31.269929, 121.453419): 1911, (30.900531, 121.399787): 1912, (31.240587, 121.402322): 1913, (31.571904, 121.158978): 1914, (32.902077, 109.42358): 1915, (31.19622, 121.468181): 1916, (31.2798, 121.459715): 1917, (31.486771, 121.360405): 1918, (31.038725, 121.464203): 1919, (31.392798, 121.246646): 1920, (31.193811, 121.373792): 1921, (31.27328, 121.47376): 1922, (31.305433, 121.435633): 1923, (31.236055, 121.434759): 1924, (31.037068, 121.686977): 1925, (31.19053, 121.445372): 1926, (30.841016, 121.482504): 1927, (31.223648, 121.467523): 1928, (31.012926, 121.423939): 1929, (31.175983, 121.381611): 1930, (31.501733, 121.334549): 1931, (31.240405, 121.569186): 1932, (31.407391, 121.490586): 1933, (31.225269, 121.491889): 1934, (31.270144, 121.354637): 1935, (31.057607, 121.030715): 1936, (31.038964, 121.2261): 1937, (31.02877, 121.456878): 1938, (31.036429, 121.324198): 1939, (31.241188, 121.483341): 1940, (31.159771, 121.58896): 1941, (31.220434, 121.408625): 1942, (31.058968, 121.787072): 1943, (31.021245, 121.226791): 1944, (31.230363, 121.5056): 1945, (31.41351, 121.367585): 1946, (31.21535, 121.46606): 1947, (30.855815, 121.311882): 1948, (31.242864, 121.426322): 1949, (30.915686, 121.462098): 1950, (31.092304, 120.912151): 1951, (31.09201, 121.395696): 1952, (31.452354, 121.33861): 1953, (31.272162, 121.459351): 1954, (31.239094, 121.519821): 1955, (31.156341, 121.512599): 1956, (31.240661, 121.512436): 1957, (31.23787, 121.543207): 1958, (31.251746, 121.488779): 1959, (31.190103, 121.485774): 1960, (31.253657, 121.443859): 1961, (31.29992, 121.669771): 1962, (31.365913, 121.177965): 1963, (31.203194, 121.46865): 1964, (31.355417, 121.263102): 1965, (31.255256, 121.500499): 1966, (34.798608, 116.090395): 1967, (31.217655, 121.534591): 1968, (31.152563, 121.460663): 1969, (31.243974, 121.555758): 1970, (30.900763, 121.246509): 1971, (31.46933, 121.246736): 1972, (31.160324, 121.508559): 1973, (31.25495, 121.739898): 1974, (31.302086, 121.510034): 1975, (31.213903, 121.410198): 1976, (31.293519, 121.22306): 1977, (31.395694, 121.391302): 1978, (31.233431, 121.471698): 1979, (31.303224, 121.308726): 1980, (31.302, 121.670498): 1981, (31.150303, 121.430423): 1982, (31.189366, 121.443482): 1983, (31.230665, 121.449046): 1984, (31.344797, 121.500999): 1985, (31.062883, 121.744224): 1986, (31.281993, 121.464732): 1987, (31.302746, 121.376994): 1988, (31.19565, 121.439802): 1989, (31.265902, 121.537589): 1990, (31.259353, 121.427018): 1991, (30.92405, 121.648972): 1992, (31.221319, 121.166272): 1993, (31.292867, 121.42296): 1994, (30.884387, 121.408648): 1995, (31.206573, 121.701043): 1996, (31.184327, 121.381012): 1997, (31.314113, 121.110731): 1998, (31.10221, 121.277743): 1999, (31.264302, 121.428277): 2000, (30.949238, 121.021134): 2001, (31.25142, 121.449186): 2002, (30.940039, 121.548407): 2003, (31.223017, 121.465143): 2004, (31.234496, 121.521516): 2005, (31.212027, 121.491516): 2006, (31.184979, 121.31046): 2007, (31.300119, 121.532041): 2008, (30.95485, 121.328077): 2009, (30.953524, 121.483682): 2010, (31.198912, 121.444187): 2011, (30.88883, 121.130607): 2012, (30.814394, 121.18284): 2013, (31.30835, 121.657013): 2014, (31.369006, 121.25552): 2015, (31.242152, 121.609858): 2016, (31.209904, 121.234203): 2017, (31.187712, 121.432106): 2018, (31.318695, 121.410448): 2019, (31.30361, 121.492602): 2020, (31.246271, 121.425156): 2021, (31.236842, 121.433815): 2022, (31.252485, 121.549613): 2023, (30.79706, 121.431713): 2024, (31.273901, 121.616429): 2025, (31.307339, 121.534119): 2026, (31.165863, 121.398738): 2027, (31.183039, 121.434356): 2028, (31.154497, 121.501351): 2029, (31.244249, 121.675329): 2030, (31.21759, 121.40829): 2031, (30.904648, 121.173227): 2032, (31.182633, 121.439015): 2033, (31.450256, 121.254671): 2034, (30.903041, 121.922883): 2035, (30.795602, 121.152307): 2036, (31.241233, 121.484727): 2037, (31.328545, 121.538592): 2038, (31.065206, 121.803371): 2039, (31.348473, 121.384381): 2040, (30.846647, 121.431405): 2041, (31.256151, 121.557845): 2042, (30.828106, 121.187081): 2043, (31.295004, 121.516367): 2044, (31.299376, 121.670896): 2045, (30.873376, 121.040678): 2046, (30.880213, 121.907656): 2047, (30.88378, 121.573239): 2048, (31.274536, 121.097082): 2049, (31.18458, 121.420921): 2050, (31.194567, 121.453929): 2051, (31.227666, 121.431017): 2052, (31.264874, 121.459338): 2053, (30.866409, 121.188512): 2054, (31.185065, 121.503655): 2055, (31.189723, 121.356104): 2056, (31.260816, 121.36983): 2057, (31.169078, 121.433389): 2058, (31.036007, 121.46539): 2059, (31.256767, 121.438713): 2060, (31.281665, 121.42621): 2061, (31.054324, 121.793035): 2062, (31.046753, 121.062205): 2063, (31.239615, 121.489026): 2064, (31.25867, 121.428896): 2065, (31.091117, 121.549904): 2066, (31.217729, 121.472441): 2067, (30.976182, 121.750557): 2068, (31.233708, 121.454373): 2069, (31.258982, 121.494994): 2070, (31.272257, 121.513471): 2071, (31.018041, 121.055413): 2072, (31.151008, 121.324166): 2073, (31.244865, 121.549508): 2074, (31.108792, 121.061126): 2075, (31.201959, 121.61095): 2076, (30.999472, 121.378674): 2077, (31.249226, 121.448568): 2078, (30.929541, 121.616421): 2079, (31.192182, 121.466139): 2080, (31.193515, 121.531662): 2081, (31.23475, 121.523857): 2082, (31.186698, 121.44855): 2083, (31.113798, 121.382691): 2084, (31.223367, 121.46595): 2085, (31.188877, 121.43412): 2086, (31.278028, 121.583992): 2087, (31.099755, 121.009542): 2088, (31.254745, 121.556081): 2089, (31.265413, 121.564718): 2090, (30.877822, 121.548349): 2091, (31.240285, 121.546323): 2092, (31.243532, 121.330166): 2093, (31.211021, 121.569582): 2094, (31.362367, 121.461011): 2095, (31.261655, 121.474716): 2096, (31.25079, 121.48933): 2097, (31.241054, 121.449606): 2098, (31.194107, 121.399831): 2099, (31.259763, 121.089338): 2100, (31.20367, 121.543991): 2101, (30.749894, 121.270796): 2102, (31.203422, 121.526246): 2103, (31.250112, 121.449922): 2104, (31.18851, 121.403115): 2105, (31.23301, 121.556388): 2106, (31.224841, 121.440899): 2107, (30.934396, 121.369499): 2108, (31.230406, 121.482326): 2109, (31.305124, 121.454071): 2110, (31.168501, 121.402505): 2111, (31.195321, 121.445546): 2112, (31.255872, 121.493382): 2113, (31.17255, 121.405133): 2114, (31.223134, 121.43817): 2115, (30.940804, 121.073937): 2116, (30.773575, 121.14093): 2117, (31.201966, 121.545756): 2118, (30.796696, 121.200272): 2119, (31.199535, 121.456715): 2120, (31.226998, 121.556375): 2121, (31.261062, 121.500145): 2122, (31.327657, 121.544364): 2123, (31.268538, 121.49713): 2124, (31.299851, 121.163211): 2125, (31.308902, 121.502016): 2126, (31.233059, 121.458379): 2127, (31.212793, 121.494061): 2128, (31.243869, 121.440506): 2129, (30.811433, 121.309343): 2130, (31.486, 121.243442): 2131, (31.0887, 121.513069): 2132, (31.284348, 121.564303): 2133, (31.245005, 121.517227): 2134, (31.432457, 121.359456): 2135, (31.267571, 121.5032): 2136, (31.240588, 121.379145): 2137, (31.209424, 121.530284): 2138, (30.86245, 121.124035): 2139, (31.338453, 121.449399): 2140, (31.161442, 121.507559): 2141, (31.447176, 121.385936): 2142, (31.233679, 121.494765): 2143, (31.152749, 121.182589): 2144, (31.278997, 121.510278): 2145, (31.215311, 121.523574): 2146, (30.979419, 121.022464): 2147, (31.216146, 121.472933): 2148, (31.301967, 121.515226): 2149, (31.238774, 121.496149): 2150, (31.246871, 121.409978): 2151, (30.967758, 121.08302): 2152, (31.293848, 121.528021): 2153, (31.222617, 121.460214): 2154, (31.200779, 121.533917): 2155, (31.303333, 121.455058): 2156, (31.231328, 121.487479): 2157, (31.299613, 121.442959): 2158, (31.170967, 121.40072): 2159, (31.294657, 121.341816): 2160, (31.043246, 120.924071): 2161, (31.177836, 121.408499): 2162, (31.209454, 121.524245): 2163, (31.228542, 121.538488): 2164, (31.182122, 121.377455): 2165, (31.267888, 121.46753): 2166, (30.806653, 121.257345): 2167, (31.421831, 121.349166): 2168, (31.259098, 121.373134): 2169, (31.234916, 121.447878): 2170, (31.246109, 121.496577): 2171, (31.197034, 121.391103): 2172, (31.232527, 121.530359): 2173, (31.332113, 121.451373): 2174, (31.234238, 121.521932): 2175, (31.232853, 121.486408): 2176, (31.378141, 121.503801): 2177, (31.058917, 121.770251): 2178, (31.28627, 121.599397): 2179, (30.916416, 121.467057): 2180, (31.236197, 121.485283): 2181, (31.242512, 121.481432): 2182, (31.126179, 121.378046): 2183, (30.975316, 121.349759): 2184, (31.294781, 121.494788): 2185, (31.039155, 121.241352): 2186, (30.87528, 121.057676): 2187, (31.21309, 121.531691): 2188, (31.2419, 121.506952): 2189, (31.246176, 121.494923): 2190, (31.060659, 121.399957): 2191, (30.874287, 121.67603): 2192, (31.419608, 121.431596): 2193, (31.348925, 121.178176): 2194, (31.218928, 121.402968): 2195, (31.030806, 121.396875): 2196, (31.24478, 121.505751): 2197, (31.172778, 121.53453): 2198, (31.283289, 121.510576): 2199, (31.311026, 121.456026): 2200, (31.045842, 121.756555): 2201, (31.233011, 121.523809): 2202, (31.237696, 121.382534): 2203, (31.234225, 121.525481): 2204, (31.25841, 121.392912): 2205, (31.343354, 121.59331): 2206, (31.264003, 121.489834): 2207, (31.054051, 121.741771): 2208, (31.237955, 121.493402): 2209, (31.276616, 121.593917): 2210, (31.045319, 121.846169): 2211, (30.867598, 121.198612): 2212, (31.283733, 121.149832): 2213, (31.206761, 121.399923): 2214, (31.297887, 121.62077): 2215, (31.244568, 121.441817): 2216, (31.200719, 121.519009): 2217, (31.382369, 121.258579): 2218, (31.238616, 121.477514): 2219, (31.234709, 121.436318): 2220, (31.041182, 121.739041): 2221, (31.006332, 121.579916): 2222, (31.210101, 121.414922): 2223, (31.244479, 121.429372): 2224, (30.97167, 121.185692): 2225, (31.39714, 121.507729): 2226, (31.163201, 121.11139): 2227, (31.243266, 121.443875): 2228, (31.250363, 121.465343): 2229, (31.307358, 121.524928): 2230, (31.003024, 121.254796): 2231, (31.220424, 121.383546): 2232, (31.15702, 121.106352): 2233, (30.735473, 121.352751): 2234, (31.228906, 121.451142): 2235, (30.948124, 121.381501): 2236, (31.373346, 121.135761): 2237, (31.129363, 121.425144): 2238, (31.37273, 121.546117): 2239, (30.841553, 121.161299): 2240, (30.889225, 121.177092): 2241, (30.793053, 121.176224): 2242, (31.29711, 121.519384): 2243, (31.139017, 121.545465): 2244, (31.260723, 121.490166): 2245, (31.24453, 121.518623): 2246, (31.340297, 121.26652): 2247, (31.030606, 121.225475): 2248, (31.294157, 121.432501): 2249, (31.236743, 121.36086): 2250, (31.375832, 121.389413): 2251, (31.386162, 121.255053): 2252, (31.227908, 121.451169): 2253, (31.238839, 121.438593): 2254, (31.283793, 121.518888): 2255, (31.149062, 121.392526): 2256, (31.111358, 121.020928): 2257, (31.075694, 121.505294): 2258, (31.206458, 121.562768): 2259, (31.061709, 121.771457): 2260, (31.243292, 121.518833): 2261, (30.259244, 120.219375): 2262, (31.260365, 121.454262): 2263, (31.234867, 121.46325): 2264, (31.261499, 121.457989): 2265, (31.270302, 121.398224): 2266, (31.242363, 121.480168): 2267, (31.233107, 121.417598): 2268, (31.254617, 121.493846): 2269, (30.923134, 121.481921): 2270, (31.111061, 121.395692): 2271, (31.264645, 121.446818): 2272, (31.256537, 121.625952): 2273, (31.245215, 121.601813): 2274, (31.198673, 121.479377): 2275, (30.918549, 121.476398): 2276, (30.867838, 121.737782): 2277, (31.219856, 121.568149): 2278, (31.139021, 121.323325): 2279, (31.203252, 121.441646): 2280, (31.278753, 121.505741): 2281, (31.063518, 121.215539): 2282, (31.424101, 121.362592): 2283, (31.239492, 121.486003): 2284, (30.940176, 121.086747): 2285, (30.929761, 121.587992): 2286, (31.336058, 121.274257): 2287, (31.391154, 121.246483): 2288, (30.975236, 121.195313): 2289, (30.85104, 121.524116): 2290, (31.246573, 121.495702): 2291, (31.085713, 120.966216): 2292, (31.238199, 121.484427): 2293, (31.174217, 121.289644): 2294, (31.198149, 121.443929): 2295, (31.285798, 121.546427): 2296, (31.233644, 121.525549): 2297, (30.875057, 121.324697): 2298, (31.289846, 121.462957): 2299, (31.217073, 121.524494): 2300, (31.038799, 121.216584): 2301, (31.404161, 121.47109): 2302, (31.236576, 121.465894): 2303, (31.242132, 121.488632): 2304, (31.239149, 121.488022): 2305, (31.190483, 121.434806): 2306, (31.242355, 121.490982): 2307, (31.190828, 121.420662): 2308, (31.373175, 121.438741): 2309, (30.802978, 121.397313): 2310, (31.278711, 121.514214): 2311, (30.7834, 121.299618): 2312, (31.199167, 121.433919): 2313, (30.846072, 121.32313): 2314, (31.261132, 121.093167): 2315, (31.253373, 121.458187): 2316, (31.209857, 121.644884): 2317, (31.195619, 121.518297): 2318, (31.251005, 121.418683): 2319, (31.26153, 121.582783): 2320, (31.045964, 121.420076): 2321, (31.277147, 121.514954): 2322, (31.2146, 121.522908): 2323, (31.185189, 121.445449): 2324, (31.216683, 121.469647): 2325, (31.186202, 121.54793): 2326, (31.063443, 121.227299): 2327, (31.258454, 121.440744): 2328, (30.978588, 121.581677): 2329, (31.146112, 121.504322): 2330, (30.924981, 121.710847): 2331, (31.278478, 121.420747): 2332, (31.183859, 121.415119): 2333, (31.247955, 121.495452): 2334, (31.288102, 121.122551): 2335, (31.18158, 121.295188): 2336, (31.186176, 121.53911): 2337, (31.287524, 121.51338): 2338, (31.253467, 121.447489): 2339, (31.267263, 121.086225): 2340, (30.881092, 121.162611): 2341, (31.198953, 121.516066): 2342, (31.258551, 121.584794): 2343, (31.039725, 121.225427): 2344, (31.256113, 121.510151): 2345, (31.269868, 121.533249): 2346, (31.138666, 121.398483): 2347, (31.290912, 121.498587): 2348, (31.250978, 121.44639): 2349, (31.235908, 121.57067): 2350, (31.178673, 121.521509): 2351, (30.889117, 121.317123): 2352, (31.2754, 121.485024): 2353, (31.201817, 121.452116): 2354, (31.087196, 121.505435): 2355, (31.246573, 121.509144): 2356, (30.899462, 121.164967): 2357, (31.119557, 121.036331): 2358, (31.238275, 121.393667): 2359, (31.317512, 121.404518): 2360, (31.107362, 121.020176): 2361, (31.243689, 121.515122): 2362, (31.173828, 121.781108): 2363, (31.242283, 121.504967): 2364, (31.404925, 121.473328): 2365, (31.030352, 120.990008): 2366, (31.156954, 121.146623): 2367, (31.218016, 121.568018): 2368, (31.280091, 121.519684): 2369, (31.012479, 121.240994): 2370, (31.251313, 121.488307): 2371, (31.180526, 121.349428): 2372, (30.975347, 121.032558): 2373, (31.19248, 121.596297): 2374, (31.257123, 121.503291): 2375, (31.265918, 121.428644): 2376, (31.197152, 121.444999): 2377, (31.223446, 121.432052): 2378, (31.231369, 121.403139): 2379, (28.812629, 115.952954): 2380, (31.213332, 121.619545): 2381, (31.012108, 121.074488): 2382, (31.211534, 121.510558): 2383, (31.0233, 121.247714): 2384, (31.212328, 121.407081): 2385, (31.295646, 121.498045): 2386, (31.350403, 121.599242): 2387, (31.24062, 121.428563): 2388, (31.152571, 121.412981): 2389, (31.248135, 121.539233): 2390, (31.223411, 121.428559): 2391, (31.188068, 121.699562): 2392, (31.193331, 121.599017): 2393, (31.236173, 121.466734): 2394, (31.228627, 121.480756): 2395, (31.240865, 121.426827): 2396, (31.50595, 121.282812): 2397, (31.299734, 121.531717): 2398, (31.238385, 121.521449): 2399, (31.234885, 121.529933): 2400, (31.202262, 121.564812): 2401, (31.214641, 121.525998): 2402, (31.31764, 121.496784): 2403, (31.229148, 121.495962): 2404, (31.177248, 121.51249): 2405, (31.417324, 121.491388): 2406, (31.243061, 121.517168): 2407, (31.140348, 121.210964): 2408, (31.249918, 121.47846): 2409, (31.216651, 121.604749): 2410, (31.193608, 121.448072): 2411, (31.298712, 121.617334): 2412, (31.044065, 121.537499): 2413, (31.191702, 121.449143): 2414, (30.991693, 121.639711): 2415, (31.210632, 121.477007): 2416, (31.235341, 121.477273): 2417, (30.90387, 121.433071): 2418, (31.209545, 121.412006): 2419, (31.241577, 121.485099): 2420, (30.918525, 121.74387): 2421, (31.235508, 121.479001): 2422, (30.918528, 121.113765): 2423, (31.290316, 121.457814): 2424, (31.240806, 121.481992): 2425, (31.27523, 121.515265): 2426, (31.128344, 121.629697): 2427, (31.039424, 121.225364): 2428, (31.430211, 121.28239): 2429, (31.223297, 121.430839): 2430, (31.179594, 121.55951): 2431, (31.307614, 121.526541): 2432, (31.320837, 121.542217): 2433, (31.239862, 121.490491): 2434, (31.263474, 121.48934): 2435, (31.268884, 121.44402): 2436, (31.084634, 120.989297): 2437, (31.258441, 121.461361): 2438, (31.228282, 121.556676): 2439, (31.251614, 121.464007): 2440, (31.410799, 121.48934): 2441, (31.249356, 121.410275): 2442, (31.243463, 121.4907): 2443, (30.956183, 121.908274): 2444, (31.192395, 121.748164): 2445, (31.222697, 121.444861): 2446, (31.252347, 121.459317): 2447, (31.142588, 121.756503): 2448, (31.226286, 121.600124): 2449, (31.159536, 121.1442): 2450, (31.103957, 121.456645): 2451, (31.213212, 121.470589): 2452, (31.242296, 121.490613): 2453, (31.239604, 121.543386): 2454, (31.3038, 121.450721): 2455, (31.276741, 121.514196): 2456, (30.983966, 121.737727): 2457, (31.225488, 121.445983): 2458, (31.195442, 121.754406): 2459, (31.236693, 121.467316): 2460, (31.194719, 121.448676): 2461, (31.25003, 121.406481): 2462, (31.010877, 121.240031): 2463, (31.225878, 121.466952): 2464, (31.231401, 121.497622): 2465, (31.242082, 121.522779): 2466, (31.200179, 121.551597): 2467, (31.17216, 121.347748): 2468, (31.154462, 121.123826): 2469, (31.215893, 121.558154): 2470, (31.267528, 121.490264): 2471, (31.214342, 121.376038): 2472, (31.139656, 121.378972): 2473, (30.910821, 121.465194): 2474, (31.32589, 121.477565): 2475, (31.243189, 121.515482): 2476, (31.24595, 121.403327): 2477, (30.902744, 121.173518): 2478, (31.326671, 121.481254): 2479, (30.721392, 121.345766): 2480, (30.879413, 121.704723): 2481, (31.23811, 121.493447): 2482, (31.30379, 121.455146): 2483, (31.234528, 121.528657): 2484, (31.304036, 121.448053): 2485, (31.21138, 121.728982): 2486, (31.180994, 121.377204): 2487, (31.012788, 121.228073): 2488, (30.908341, 121.870538): 2489, (31.136936, 121.325876): 2490, (31.229818, 121.462466): 2491, (31.154144, 121.118793): 2492, (31.292275, 121.524795): 2493, (31.201121, 121.445016): 2494, (31.217217, 121.558644): 2495, (30.735389, 121.358248): 2496, (31.37219, 121.448243): 2497, (31.235043, 121.523389): 2498, (31.233185, 121.481944): 2499, (31.325552, 121.473746): 2500, (31.235414, 121.461313): 2501, (31.241234, 121.460559): 2502, (31.200865, 121.448273): 2503, (31.333953, 121.449114): 2504, (31.191178, 121.46648): 2505, (31.304737, 121.522227): 2506, (31.204322, 121.455438): 2507, (31.034225, 121.254609): 2508, (31.237961, 121.477253): 2509, (31.212572, 121.537172): 2510, (31.230981, 121.542285): 2511, (31.272769, 121.57809): 2512, (31.289072, 121.481426): 2513, (30.901446, 121.904248): 2514, (31.348039, 121.147744): 2515, (31.332279, 121.286543): 2516, (31.234737, 121.513936): 2517, (31.21176, 121.45331): 2518, (31.415037, 121.260851): 2519, (31.197367, 121.479142): 2520, (30.990222, 121.082742): 2521, (31.227348, 121.350438): 2522, (31.264433, 121.541398): 2523, (31.287554, 121.539): 2524, (31.014893, 121.047477): 2525, (31.251094, 121.416046): 2526, (31.237596, 121.488614): 2527, (31.27726, 121.460289): 2528, (31.286523, 121.538876): 2529, (31.247703, 121.536476): 2530, (31.195086, 121.440792): 2531, (30.962059, 121.244644): 2532, (31.242608, 121.419862): 2533, (31.177667, 121.410757): 2534, (31.234845, 121.530708): 2535, (31.233983, 121.455055): 2536, (31.245938, 121.422548): 2537, (31.216555, 121.409176): 2538, (31.123026, 120.926117): 2539, (31.202226, 121.399589): 2540, (31.241765, 121.496726): 2541, (31.216462, 121.677078): 2542, (31.21245, 121.637072): 2543, (31.233151, 121.530498): 2544, (31.239575, 121.50749): 2545, (31.255167, 121.491376): 2546, (31.250145, 121.450312): 2547, (31.234093, 121.475011): 2548, (31.252588, 121.463263): 2549, (31.412391, 121.491247): 2550, (31.156361, 121.131706): 2551, (31.228956, 121.481687): 2552, (31.126748, 120.934108): 2553, (35.522852, 102.0076): 2554, (31.249854, 121.497793): 2555, (31.098201, 121.323149): 2556, (31.252686, 121.491508): 2557, (31.263879, 121.49436): 2558, (31.085245, 121.533086): 2559, (31.234688, 121.502656): 2560, (31.235631, 121.567852): 2561, (31.209881, 121.416504): 2562, (31.169908, 121.418183): 2563, (31.316634, 121.657988): 2564, (30.901012, 121.167878): 2565, (31.204601, 121.484087): 2566, (31.111986, 121.38521): 2567, (31.210053, 121.464569): 2568, (31.198268, 121.383605): 2569, (31.231339, 121.543694): 2570, (30.898128, 121.536836): 2571, (31.271321, 121.521621): 2572, (31.233733, 121.49619): 2573, (31.221688, 121.462858): 2574, (31.231316, 121.54242): 2575, (31.1917, 121.440205): 2576, (31.22626, 121.551311): 2577, (31.257486, 121.491309): 2578, (31.210579, 121.453849): 2579, (31.350476, 121.519951): 2580, (31.189816, 121.430881): 2581, (31.210598, 121.449688): 2582, (31.258745, 121.370293): 2583, (31.22978, 121.540987): 2584, (31.155379, 121.765101): 2585, (30.837343, 121.275733): 2586, (31.169488, 121.349667): 2587, (31.237479, 121.462941): 2588, (31.236742, 121.385193): 2589, (31.217037, 121.43649): 2590, (31.264405, 121.489289): 2591, (31.262782, 121.215761): 2592, (31.247026, 121.448423): 2593, (31.2106, 121.411728): 2594, (31.106411, 121.450195): 2595, (31.012822, 121.051558): 2596, (31.216682, 121.468605): 2597, (30.730943, 121.320069): 2598, (31.17446, 121.798555): 2599, (31.165389, 121.119468): 2600, (31.262311, 121.449241): 2601, (31.192457, 121.443575): 2602, (31.225663, 121.452017): 2603, (31.241184, 121.516051): 2604, (31.307814, 121.504168): 2605, (31.428157, 121.260412): 2606, (31.263574, 121.586365): 2607, (31.23423, 121.436418): 2608, (31.278145, 121.452159): 2609, (31.143835, 120.935676): 2610, (31.039823, 121.280347): 2611, (31.216154, 121.557057): 2612, (31.410254, 121.496122): 2613, (31.315405, 121.532388): 2614, (31.236431, 121.460781): 2615, (31.236369, 121.476308): 2616, (31.270571, 121.476897): 2617, (31.200668, 121.480775): 2618, (31.24672, 121.38645): 2619, (31.265994, 121.519353): 2620, (31.162111, 121.566673): 2621, (31.245488, 121.513767): 2622, (31.226708, 121.481109): 2623, (31.230405, 121.534439): 2624, (31.280003, 121.361419): 2625, (31.377094, 121.500868): 2626, (31.101299, 120.918138): 2627, (31.251619, 121.486701): 2628, (31.376534, 121.563363): 2629, (34.556383, 118.79231): 2630, (31.348329, 121.593744): 2631, (31.241131, 121.487911): 2632, (31.259841, 121.618221): 2633, (31.223721, 121.482534): 2634, (31.246782, 121.528695): 2635, (31.060813, 121.789941): 2636, (31.213561, 121.634923): 2637, (31.15562, 121.130864): 2638, (31.234221, 121.462296): 2639, (30.772261, 121.378192): 2640, (31.23522, 121.498835): 2641, (30.730789, 121.343497): 2642, (31.257157, 121.398524): 2643, (31.23976, 121.482648): 2644, (31.244438, 121.531412): 2645, (31.254863, 121.509673): 2646, (31.244007, 121.482838): 2647, (31.167412, 121.436492): 2648, (31.188145, 121.432275): 2649, (31.266108, 121.341404): 2650, (31.231749, 121.486203): 2651, (31.237667, 121.434224): 2652, (31.279258, 121.474153): 2653, (31.212716, 121.419839): 2654, (31.290878, 121.429094): 2655, (30.891706, 121.171358): 2656, (31.266483, 121.545252): 2657, (31.246433, 121.443184): 2658, (31.224551, 121.547983): 2659, (31.203225, 121.399229): 2660, (31.168688, 121.487892): 2661, (31.298418, 121.539277): 2662, (31.224391, 121.489143): 2663, (31.266646, 121.609157): 2664, (31.06954, 120.928583): 2665, (31.359987, 121.417748): 2666, (31.177098, 121.525845): 2667, (31.288044, 121.356098): 2668, (31.1823, 121.394288): 2669, (31.289528, 121.412608): 2670, (31.228895, 121.46357): 2671, (31.227864, 121.638217): 2672, (31.282019, 121.494747): 2673, (31.231701, 121.454022): 2674, (31.294732, 121.496831): 2675, (31.256654, 121.599709): 2676, (31.228634, 121.536498): 2677, (31.234888, 121.447606): 2678, (31.243738, 121.475784): 2679, (31.192935, 121.39968): 2680, (31.315817, 121.359172): 2681, (31.238392, 121.428006): 2682, (31.249067, 121.543826): 2683, (31.25316, 121.459116): 2684, (31.19708, 121.382863): 2685, (31.238453, 121.377045): 2686, (31.174223, 121.436719): 2687, (31.208476, 121.466867): 2688, (31.265578, 121.48973): 2689, (30.921604, 121.471336): 2690, (31.285418, 121.47179): 2691, (31.21684, 121.593486): 2692, (31.23626, 121.493692): 2693, (31.240905, 121.489425): 2694, (31.237926, 121.46594): 2695, (31.290758, 121.514018): 2696, (31.259135, 121.616618): 2697, (31.264332, 121.379318): 2698, (31.317987, 120.619907): 2699, (31.167195, 121.389325): 2700, (31.299433, 121.524696): 2701}\n",
            "{405.0: 121, 197.0: 18, 276.0: 23, 1378.0: 38, 198.0: 39, 1691.0: 6, 780.0: 178, 1679.0: 86, 1415.0: 17, 1339.0: 48, 1422.0: 20, 1080.0: 163, 1505.0: 100, 1304.0: 52, 872.0: 15, 847.0: 129, 271.0: 7, 24.0: 79, 1522.0: 91, 707.0: 93, 1558.0: 31, 953.0: 6, 843.0: 17, 1739.0: 9, 321.0: 84, 204.0: 94, 898.0: 77, 45.0: 50, 2308.0: 2, 2414.0: 1, 681.0: 27, 272.0: 23, 1410.0: 24, 138.0: 91, 1659.0: 4, 694.0: 273, 1714.0: 3, 766.0: 13, 1961.0: 2, 741.0: 259, 174.0: 106, 512.0: 63, 1777.0: 52, 1990.0: 5, 1222.0: 12, 612.0: 255, 59.0: 109, 1813.0: 155, 749.0: 399, 552.0: 68, 1375.0: 9, 1203.0: 92, 461.0: 101, 439.0: 160, 922.0: 78, 573.0: 92, 668.0: 149, 885.0: 26, 1281.0: 34, 1523.0: 8, 1100.0: 80, 385.0: 7, 1154.0: 11, 2203.0: 12, 409.0: 23, 169.0: 9, 1249.0: 40, 833.0: 20, 456.0: 161, 718.0: 23, 762.0: 115, 777.0: 17, 520.0: 56, 1008.0: 52, 2309.0: 3, 233.0: 2, 1188.0: 66, 829.0: 20, 242.0: 32, 1323.0: 80, 1890.0: 1, 1261.0: 7, 1600.0: 72, 697.0: 27, 602.0: 4, 614.0: 172, 920.0: 28, 1684.0: 278, 421.0: 109, 1067.0: 98, 676.0: 63, 423.0: 33, 774.0: 182, 61.0: 56, 658.0: 29, 619.0: 301, 1875.0: 37, 338.0: 50, 1551.0: 25, 1084.0: 9, 1190.0: 20, 403.0: 161, 1283.0: 10, 981.0: 26, 547.0: 67, 691.0: 52, 689.0: 131, 861.0: 40, 941.0: 71, 588.0: 185, 248.0: 241, 1967.0: 5, 744.0: 43, 746.0: 22, 2402.0: 1, 484.0: 299, 755.0: 27, 759.0: 45, 1235.0: 6, 684.0: 372, 2055.0: 2, 2042.0: 76, 1850.0: 16, 831.0: 59, 288.0: 108, 982.0: 1, 1468.0: 15, 943.0: 7, 1352.0: 13, 1417.0: 43, 544.0: 367, 593.0: 30, 724.0: 64, 1820.0: 2, 921.0: 44, 666.0: 449, 140.0: 33, 1409.0: 190, 469.0: 80, 34.0: 248, 795.0: 426, 730.0: 25, 701.0: 22, 1071.0: 26, 1294.0: 49, 212.0: 16, 656.0: 35, 252.0: 375, 758.0: 306, 35.0: 138, 597.0: 38, 205.0: 56, 1575.0: 10, 1932.0: 5, 2306.0: 13, 2347.0: 5, 561.0: 647, 404.0: 47, 60.0: 96, 735.0: 101, 1454.0: 4, 118.0: 51, 645.0: 149, 2256.0: 13, 596.0: 167, 1496.0: 1, 104.0: 251, 806.0: 3, 1767.0: 7, 190.0: 7, 22.0: 127, 2191.0: 10, 1424.0: 8, 856.0: 255, 2100.0: 11, 1031.0: 49, 838.0: 324, 929.0: 18, 539.0: 145, 986.0: 108, 430.0: 332, 556.0: 182, 1033.0: 33, 566.0: 307, 392.0: 186, 924.0: 51, 425.0: 97, 99.0: 226, 636.0: 34, 144.0: 42, 1886.0: 16, 431.0: 166, 453.0: 331, 1092.0: 19, 1429.0: 3, 1644.0: 42, 1538.0: 145, 1253.0: 60, 1870.0: 7, 98.0: 28, 2221.0: 8, 637.0: 7, 354.0: 172, 336.0: 16, 670.0: 361, 1447.0: 5, 796.0: 30, 864.0: 41, 1901.0: 16, 834.0: 7, 372.0: 363, 1061.0: 264, 845.0: 408, 688.0: 337, 944.0: 57, 563.0: 241, 1167.0: 151, 194.0: 263, 1236.0: 108, 1605.0: 279, 232.0: 667, 1086.0: 65, 1023.0: 63, 1388.0: 11, 553.0: 25, 665.0: 168, 1543.0: 5, 704.0: 5, 1619.0: 10, 1155.0: 1, 260.0: 14, 1166.0: 19, 109.0: 2, 742.0: 443, 257.0: 215, 1656.0: 144, 541.0: 566, 1933.0: 4, 2550.0: 14, 419.0: 11, 546.0: 19, 618.0: 422, 685.0: 45, 462.0: 44, 286.0: 147, 115.0: 23, 533.0: 213, 1152.0: 133, 915.0: 5, 180.0: 103, 43.0: 27, 40.0: 247, 154.0: 17, 1532.0: 31, 1530.0: 4, 410.0: 203, 1059.0: 7, 2084.0: 2, 1136.0: 61, 466.0: 24, 1272.0: 46, 299.0: 109, 1218.0: 71, 543.0: 8, 1000.0: 63, 698.0: 47, 227.0: 8, 407.0: 5, 96.0: 55, 187.0: 52, 830.0: 83, 783.0: 68, 1116.0: 15, 1882.0: 9, 101.0: 5, 1470.0: 15, 384.0: 539, 4.0: 1005, 1597.0: 190, 1629.0: 65, 1315.0: 10, 1211.0: 1, 1692.0: 53, 1553.0: 11, 804.0: 29, 64.0: 38, 160.0: 4, 1234.0: 34, 1743.0: 25, 1302.0: 37, 313.0: 129, 1096.0: 39, 896.0: 36, 1662.0: 64, 867.0: 25, 808.0: 18, 878.0: 24, 325.0: 28, 168.0: 44, 595.0: 239, 1120.0: 329, 238.0: 89, 1835.0: 38, 745.0: 14, 2066.0: 3, 1584.0: 196, 613.0: 5, 2413.0: 13, 398.0: 109, 606.0: 472, 1490.0: 23, 1887.0: 59, 764.0: 19, 471.0: 147, 16.0: 36, 1321.0: 3, 386.0: 217, 151.0: 131, 580.0: 101, 1450.0: 28, 999.0: 95, 705.0: 28, 1270.0: 78, 1297.0: 214, 575.0: 115, 708.0: 188, 445.0: 447, 555.0: 42, 2252.0: 3, 1585.0: 63, 807.0: 33, 582.0: 83, 135.0: 21, 133.0: 95, 1675.0: 7, 211.0: 39, 11.0: 317, 13.0: 437, 46.0: 27, 6.0: 17, 1197.0: 3, 1112.0: 10, 2098.0: 2, 1915.0: 125, 393.0: 544, 757.0: 186, 416.0: 220, 1157.0: 57, 621.0: 334, 231.0: 7, 363.0: 93, 47.0: 25, 176.0: 8, 164.0: 76, 1774.0: 4, 70.0: 15, 799.0: 22, 244.0: 53, 628.0: 12, 727.0: 264, 693.0: 173, 715.0: 642, 1848.0: 84, 1309.0: 12, 1489.0: 3, 1610.0: 26, 1501.0: 7, 917.0: 158, 1893.0: 11, 1611.0: 4, 44.0: 512, 8.0: 193, 1225.0: 29, 1577.0: 30, 916.0: 65, 1964.0: 4, 2429.0: 57, 810.0: 400, 500.0: 79, 1587.0: 8, 1054.0: 21, 1227.0: 5, 1400.0: 51, 1286.0: 72, 213.0: 82, 1606.0: 274, 280.0: 46, 349.0: 144, 733.0: 347, 389.0: 36, 217.0: 50, 1877.0: 3, 346.0: 290, 912.0: 28, 1920.0: 28, 1613.0: 23, 900.0: 3, 157.0: 57, 33.0: 87, 1427.0: 1, 77.0: 46, 181.0: 26, 92.0: 18, 243.0: 13, 974.0: 1, 1014.0: 2, 1392.0: 76, 818.0: 66, 1851.0: 25, 387.0: 31, 236.0: 125, 100.0: 47, 17.0: 67, 1083.0: 18, 1145.0: 225, 1165.0: 214, 282.0: 49, 279.0: 186, 816.0: 256, 564.0: 427, 1948.0: 11, 589.0: 175, 1081.0: 84, 855.0: 63, 1929.0: 26, 517.0: 82, 1212.0: 134, 1243.0: 79, 1146.0: 42, 790.0: 114, 1191.0: 140, 1528.0: 18, 604.0: 597, 590.0: 58, 1046.0: 77, 1032.0: 14, 2177.0: 1, 29.0: 33, 1547.0: 4, 509.0: 2, 2497.0: 1, 534.0: 131, 1010.0: 20, 565.0: 81, 1360.0: 24, 679.0: 144, 1338.0: 46, 576.0: 103, 449.0: 6, 1200.0: 35, 391.0: 164, 428.0: 22, 1921.0: 4, 255.0: 63, 68.0: 2, 1363.0: 37, 914.0: 6, 902.0: 9, 2194.0: 5, 983.0: 235, 1256.0: 16, 1963.0: 17, 1066.0: 186, 2363.0: 1, 710.0: 217, 417.0: 204, 692.0: 250, 792.0: 114, 832.0: 263, 1063.0: 13, 1582.0: 38, 858.0: 36, 1325.0: 63, 1425.0: 221, 2184.0: 2, 2196.0: 8, 791.0: 37, 751.0: 54, 625.0: 11, 2077.0: 28, 673.0: 22, 458.0: 34, 669.0: 17, 1833.0: 9, 341.0: 16, 883.0: 60, 152.0: 314, 510.0: 7, 889.0: 2, 972.0: 103, 891.0: 26, 726.0: 115, 1805.0: 46, 1364.0: 9, 1914.0: 2, 635.0: 200, 1124.0: 35, 315.0: 148, 1634.0: 42, 54.0: 33, 38.0: 130, 851.0: 164, 648.0: 269, 1201.0: 399, 1264.0: 48, 654.0: 344, 1838.0: 15, 1770.0: 60, 973.0: 135, 1262.0: 13, 435.0: 394, 562.0: 23, 320.0: 241, 1276.0: 32, 237.0: 47, 1260.0: 10, 1215.0: 148, 474.0: 7, 473.0: 244, 882.0: 18, 622.0: 207, 609.0: 98, 803.0: 32, 824.0: 37, 455.0: 14, 1040.0: 6, 1406.0: 59, 460.0: 8, 739.0: 355, 550.0: 258, 2240.0: 24, 2043.0: 18, 2013.0: 23, 2341.0: 2, 1139.0: 180, 1751.0: 30, 1334.0: 34, 747.0: 34, 909.0: 3, 1109.0: 122, 1130.0: 111, 1475.0: 173, 1986.0: 54, 961.0: 177, 1102.0: 6, 754.0: 44, 1195.0: 107, 574.0: 39, 959.0: 226, 1448.0: 17, 828.0: 114, 797.0: 58, 532.0: 7, 987.0: 147, 542.0: 171, 381.0: 15, 292.0: 20, 753.0: 10, 467.0: 52, 395.0: 663, 1181.0: 432, 1187.0: 15, 1108.0: 38, 1277.0: 12, 2213.0: 27, 314.0: 5, 1517.0: 30, 672.0: 45, 1733.0: 25, 343.0: 130, 355.0: 237, 364.0: 60, 2074.0: 3, 110.0: 133, 1020.0: 82, 111.0: 9, 1511.0: 163, 1399.0: 4, 931.0: 17, 162.0: 3, 1939.0: 7, 1001.0: 141, 594.0: 8, 360.0: 12, 536.0: 4, 1710.0: 13, 1947.0: 1, 537.0: 238, 373.0: 99, 538.0: 67, 1824.0: 114, 1667.0: 43, 1995.0: 95, 1273.0: 6, 1463.0: 14, 202.0: 4, 888.0: 5, 498.0: 22, 540.0: 259, 876.0: 26, 399.0: 25, 667.0: 21, 821.0: 58, 332.0: 17, 2394.0: 1, 1772.0: 8, 1398.0: 25, 815.0: 35, 703.0: 82, 1258.0: 18, 1196.0: 80, 1377.0: 63, 650.0: 21, 298.0: 46, 1280.0: 80, 894.0: 50, 121.0: 8, 2376.0: 1, 259.0: 40, 203.0: 1, 1832.0: 1, 481.0: 103, 521.0: 511, 506.0: 171, 472.0: 104, 1387.0: 26, 1162.0: 154, 188.0: 23, 173.0: 131, 2017.0: 70, 709.0: 18, 433.0: 201, 1493.0: 7, 199.0: 80, 1274.0: 7, 1680.0: 4, 74.0: 12, 717.0: 353, 142.0: 343, 1785.0: 46, 1366.0: 11, 2369.0: 5, 1503.0: 41, 2353.0: 3, 1985.0: 24, 2455.0: 11, 131.0: 87, 1869.0: 70, 75.0: 274, 67.0: 238, 1303.0: 129, 220.0: 5, 1780.0: 3, 525.0: 98, 2289.0: 6, 261.0: 42, 221.0: 20, 465.0: 202, 2392.0: 2, 1401.0: 259, 1479.0: 116, 1917.0: 7, 605.0: 80, 696.0: 213, 1179.0: 38, 1488.0: 11, 1103.0: 5, 773.0: 9, 1248.0: 22, 147.0: 44, 624.0: 199, 990.0: 28, 342.0: 6, 1440.0: 24, 451.0: 10, 230.0: 99, 494.0: 333, 992.0: 25, 1637.0: 14, 801.0: 25, 263.0: 6, 964.0: 11, 725.0: 34, 1591.0: 18, 1708.0: 22, 994.0: 32, 1069.0: 12, 1776.0: 39, 1568.0: 7, 490.0: 16, 1357.0: 8, 1638.0: 10, 722.0: 164, 1782.0: 76, 1361.0: 238, 2358.0: 32, 560.0: 177, 305.0: 11, 1645.0: 53, 1868.0: 7, 768.0: 68, 1725.0: 61, 316.0: 12, 2130.0: 13, 317.0: 9, 868.0: 222, 1789.0: 22, 186.0: 7, 2314.0: 6, 809.0: 76, 235.0: 16, 664.0: 73, 930.0: 262, 2229.0: 8, 269.0: 88, 1699.0: 8, 2264.0: 5, 1002.0: 11, 1330.0: 163, 452.0: 28, 1057.0: 9, 1730.0: 15, 784.0: 43, 2004.0: 4, 79.0: 90, 2120.0: 2, 848.0: 348, 1144.0: 1, 470.0: 123, 153.0: 25, 1676.0: 16, 206.0: 17, 655.0: 40, 400.0: 40, 1906.0: 12, 1630.0: 23, 870.0: 49, 653.0: 100, 1259.0: 5, 1608.0: 25, 291.0: 15, 579.0: 38, 312.0: 87, 603.0: 90, 1041.0: 53, 1702.0: 11, 189.0: 20, 1076.0: 20, 1316.0: 10, 2109.0: 7, 1087.0: 163, 1923.0: 7, 1966.0: 12, 1369.0: 12, 545.0: 4, 1521.0: 6, 377.0: 5, 277.0: 213, 836.0: 88, 136.0: 357, 348.0: 90, 447.0: 408, 1800.0: 5, 819.0: 14, 1205.0: 38, 254.0: 82, 1172.0: 27, 1043.0: 130, 1131.0: 17, 401.0: 26, 146.0: 15, 365.0: 69, 1390.0: 7, 951.0: 57, 1374.0: 55, 191.0: 183, 1018.0: 1, 339.0: 72, 2298.0: 64, 429.0: 28, 1052.0: 3, 955.0: 58, 935.0: 225, 1184.0: 34, 928.0: 353, 946.0: 338, 646.0: 87, 897.0: 36, 1822.0: 10, 2068.0: 44, 991.0: 31, 480.0: 359, 519.0: 475, 1992.0: 12, 996.0: 5, 414.0: 71, 1438.0: 159, 737.0: 161, 219.0: 97, 2001.0: 3, 1349.0: 95, 290.0: 169, 444.0: 11, 1434.0: 21, 814.0: 159, 967.0: 111, 1746.0: 8, 585.0: 8, 370.0: 214, 112.0: 11, 145.0: 49, 647.0: 4, 80.0: 40, 934.0: 232, 822.0: 152, 2329.0: 1, 1113.0: 6, 925.0: 1, 1257.0: 14, 1736.0: 25, 1717.0: 2, 695.0: 19, 1421.0: 6, 2083.0: 3, 1149.0: 5, 1826.0: 4, 356.0: 9, 55.0: 275, 226.0: 93, 150.0: 144, 1449.0: 42, 1171.0: 4, 779.0: 18, 599.0: 52, 1971.0: 84, 706.0: 176, 256.0: 325, 1062.0: 26, 1738.0: 20, 2067.0: 7, 450.0: 97, 559.0: 114, 1079.0: 50, 1013.0: 21, 652.0: 41, 156.0: 3, 432.0: 149, 631.0: 47, 1223.0: 19, 581.0: 150, 662.0: 25, 820.0: 231, 1050.0: 154, 886.0: 23, 1353.0: 14, 1927.0: 11, 1578.0: 163, 1762.0: 84, 1328.0: 75, 163.0: 116, 195.0: 11, 139.0: 28, 383.0: 17, 293.0: 5, 1186.0: 30, 852.0: 16, 1327.0: 5, 1478.0: 6, 985.0: 6, 1206.0: 10, 975.0: 11, 1332.0: 29, 1305.0: 14, 7.0: 74, 2.0: 4, 2589.0: 3, 1698.0: 64, 2167.0: 37, 1902.0: 54, 675.0: 111, 281.0: 2, 1903.0: 16, 1911.0: 31, 382.0: 99, 995.0: 221, 2192.0: 7, 1373.0: 3, 1988.0: 113, 1980.0: 62, 879.0: 17, 1878.0: 14, 644.0: 98, 497.0: 108, 1396.0: 59, 570.0: 22, 1962.0: 7, 1556.0: 2, 862.0: 1, 375.0: 59, 1749.0: 23, 436.0: 10, 224.0: 175, 1070.0: 6, 1842.0: 2, 223.0: 9, 225.0: 255, 1843.0: 5, 2054.0: 11, 149.0: 422, 1473.0: 11, 1376.0: 4, 1132.0: 23, 1460.0: 7, 1515.0: 113, 1331.0: 22, 2278.0: 1, 1298.0: 6, 1885.0: 3, 18.0: 74, 83.0: 18, 623.0: 282, 2348.0: 1, 102.0: 150, 712.0: 335, 108.0: 312, 443.0: 28, 776.0: 20, 1942.0: 6, 105.0: 932, 1039.0: 2, 207.0: 24, 367.0: 14, 1056.0: 3, 775.0: 16, 952.0: 149, 30.0: 183, 586.0: 6, 134.0: 146, 196.0: 275, 192.0: 26, 3.0: 114, 674.0: 32, 649.0: 7, 1107.0: 4, 1085.0: 6, 475.0: 1, 1030.0: 5, 2237.0: 10, 397.0: 62, 805.0: 31, 1804.0: 70, 273.0: 45, 607.0: 8, 1355.0: 15, 177.0: 14, 193.0: 54, 57.0: 164, 1486.0: 28, 524.0: 52, 310.0: 79, 66.0: 34, 1275.0: 56, 548.0: 149, 129.0: 217, 1643.0: 6, 1213.0: 17, 249.0: 2, 663.0: 2, 371.0: 215, 1639.0: 4, 1414.0: 5, 1341.0: 22, 143.0: 12, 1763.0: 39, 1442.0: 42, 874.0: 2, 1408.0: 136, 1051.0: 6, 2625.0: 1, 411.0: 46, 895.0: 138, 2227.0: 8, 1904.0: 13, 2059.0: 26, 1919.0: 53, 1099.0: 10, 608.0: 169, 1724.0: 88, 214.0: 152, 1214.0: 14, 1326.0: 204, 335.0: 27, 1319.0: 3, 940.0: 31, 892.0: 49, 31.0: 7, 549.0: 20, 1216.0: 83, 489.0: 84, 2188.0: 2, 528.0: 231, 583.0: 67, 86.0: 2, 87.0: 4, 2169.0: 11, 1462.0: 3, 1660.0: 10, 2367.0: 8, 1557.0: 81, 329.0: 9, 2490.0: 2, 903.0: 13, 2519.0: 29, 124.0: 21, 366.0: 147, 939.0: 254, 1443.0: 79, 721.0: 51, 1683.0: 28, 1037.0: 60, 434.0: 4, 1161.0: 8, 910.0: 422, 2473.0: 4, 2271.0: 2, 82.0: 27, 2061.0: 2, 302.0: 12, 117.0: 9, 711.0: 6, 1571.0: 1, 1207.0: 2, 331.0: 14, 2464.0: 3, 97.0: 316, 2698.0: 4, 505.0: 43, 2480.0: 2, 120.0: 3, 958.0: 24, 56.0: 16, 1625.0: 3, 1240.0: 1, 615.0: 82, 1034.0: 93, 2115.0: 128, 89.0: 31, 1029.0: 44, 1561.0: 2, 969.0: 7, 501.0: 135, 1115.0: 48, 554.0: 164, 1047.0: 1, 887.0: 24, 27.0: 48, 875.0: 76, 1892.0: 78, 1946.0: 8, 984.0: 46, 2283.0: 7, 337.0: 28, 95.0: 11, 459.0: 39, 442.0: 290, 241.0: 95, 208.0: 40, 1.0: 207, 980.0: 16, 1356.0: 117, 523.0: 355, 402.0: 480, 1110.0: 20, 1254.0: 6, 247.0: 1, 1230.0: 2, 728.0: 219, 842.0: 5, 1428.0: 72, 1640.0: 3, 1459.0: 6, 1354.0: 2, 39.0: 12, 1247.0: 15, 1336.0: 71, 1845.0: 10, 642.0: 479, 957.0: 5, 427.0: 86, 616.0: 62, 1169.0: 58, 1649.0: 4, 218.0: 46, 267.0: 4, 103.0: 57, 1545.0: 17, 881.0: 5, 2036.0: 6, 1809.0: 13, 155.0: 23, 508.0: 199, 901.0: 7, 770.0: 11, 2447.0: 14, 1168.0: 57, 841.0: 235, 998.0: 5, 406.0: 203, 945.0: 20, 535.0: 21, 1288.0: 11, 1790.0: 173, 2234.0: 2, 2262.0: 12, 52.0: 17, 1119.0: 46, 1389.0: 48, 477.0: 107, 1474.0: 80, 476.0: 18, 789.0: 7, 890.0: 141, 179.0: 30, 251.0: 13, 970.0: 25, 483.0: 154, 84.0: 67, 426.0: 33, 2048.0: 47, 1413.0: 2, 1025.0: 246, 2091.0: 17, 1741.0: 4, 1872.0: 4, 2690.0: 2, 551.0: 43, 2346.0: 1, 529.0: 3, 503.0: 190, 2494.0: 2, 1244.0: 15, 14.0: 29, 785.0: 33, 1089.0: 7, 1204.0: 3, 1646.0: 21, 1439.0: 4, 1344.0: 5, 1836.0: 3, 963.0: 12, 1866.0: 5, 468.0: 8, 344.0: 14, 1711.0: 4, 557.0: 17, 408.0: 15, 340.0: 48, 69.0: 21, 1097.0: 17, 629.0: 8, 234.0: 5, 1382.0: 144, 1420.0: 16, 526.0: 8, 2106.0: 5, 1329.0: 71, 492.0: 61, 1395.0: 32, 1335.0: 16, 1720.0: 17, 2668.0: 2, 1170.0: 17, 1477.0: 27, 750.0: 1, 1546.0: 29, 1003.0: 10, 1534.0: 28, 1300.0: 23, 793.0: 8, 294.0: 18, 85.0: 14, 514.0: 249, 661.0: 93, 1379.0: 8, 817.0: 11, 1411.0: 11, 567.0: 5, 201.0: 9, 185.0: 103, 1245.0: 2, 2148.0: 29, 1346.0: 320, 1135.0: 1, 927.0: 243, 960.0: 43, 1677.0: 30, 2138.0: 2, 1466.0: 2, 756.0: 22, 1445.0: 1, 2217.0: 1, 328.0: 11, 2052.0: 1, 36.0: 120, 1026.0: 11, 1787.0: 37, 2493.0: 2, 627.0: 5, 905.0: 14, 300.0: 21, 1943.0: 18, 1674.0: 95, 968.0: 20, 1899.0: 69, 630.0: 382, 1015.0: 10, 440.0: 77, 1849.0: 1, 1009.0: 107, 270.0: 25, 438.0: 13, 19.0: 7, 971.0: 237, 322.0: 10, 262.0: 8, 1217.0: 160, 1896.0: 5, 330.0: 10, 948.0: 40, 611.0: 9, 1467.0: 9, 23.0: 10, 51.0: 32, 988.0: 31, 826.0: 34, 788.0: 151, 2102.0: 45, 2073.0: 30, 499.0: 198, 1974.0: 19, 1220.0: 128, 128.0: 17, 993.0: 66, 866.0: 59, 311.0: 24, 2344.0: 4, 811.0: 23, 171.0: 14, 91.0: 8, 1082.0: 191, 1221.0: 5, 1642.0: 3, 1604.0: 3, 441.0: 188, 2415.0: 6, 962.0: 9, 1533.0: 4, 522.0: 68, 200.0: 16, 2312.0: 3, 1060.0: 41, 827.0: 345, 495.0: 229, 137.0: 35, 1615.0: 2, 2065.0: 3, 1202.0: 104, 1182.0: 168, 601.0: 373, 1229.0: 240, 2168.0: 26, 1090.0: 11, 266.0: 8, 1953.0: 16, 1682.0: 30, 933.0: 26, 1242.0: 170, 1779.0: 105, 743.0: 1, 1524.0: 21, 283.0: 6, 166.0: 5, 2532.0: 9, 657.0: 122, 1019.0: 319, 424.0: 58, 396.0: 81, 634.0: 68, 1570.0: 2, 1208.0: 7, 2149.0: 2, 1121.0: 14, 1194.0: 11, 1024.0: 32, 1158.0: 23, 2528.0: 40, 904.0: 33, 965.0: 72, 454.0: 27, 265.0: 8, 1589.0: 8, 572.0: 3, 761.0: 26, 1791.0: 13, 37.0: 11, 1147.0: 185, 1174.0: 91, 1727.0: 23, 1783.0: 8, 1913.0: 3, 353.0: 27, 1960.0: 5, 1537.0: 3, 640.0: 19, 1622.0: 10, 731.0: 2, 2542.0: 7, 678.0: 12, 58.0: 39, 268.0: 3, 25.0: 26, 1371.0: 11, 1088.0: 87, 2335.0: 7, 1159.0: 5, 2125.0: 4, 65.0: 271, 1423.0: 143, 2135.0: 4, 1465.0: 24, 1978.0: 13, 1671.0: 5, 1133.0: 93, 2185.0: 6, 1709.0: 2, 1011.0: 3, 2053.0: 2, 1499.0: 2, 638.0: 272, 274.0: 1, 504.0: 54, 600.0: 71, 1291.0: 56, 700.0: 3, 326.0: 202, 78.0: 57, 911.0: 238, 63.0: 14, 837.0: 155, 1549.0: 153, 1192.0: 24, 1654.0: 30, 148.0: 8, 2008.0: 50, 222.0: 15, 479.0: 4, 2286.0: 2, 507.0: 47, 1150.0: 82, 1781.0: 6, 1350.0: 30, 1290.0: 10, 1811.0: 7, 1209.0: 8, 558.0: 9, 297.0: 68, 1716.0: 9, 1742.0: 4, 1975.0: 1, 502.0: 7, 592.0: 7, 323.0: 2, 1224.0: 36, 2127.0: 17, 1118.0: 16, 239.0: 240, 1232.0: 54, 1007.0: 92, 2087.0: 2, 1164.0: 9, 736.0: 2, 1177.0: 2, 2016.0: 1, 1074.0: 195, 73.0: 23, 415.0: 6, 2675.0: 2, 932.0: 25, 1700.0: 3, 2147.0: 136, 2373.0: 4, 380.0: 30, 610.0: 236, 287.0: 277, 379.0: 25, 361.0: 27, 378.0: 7, 351.0: 37, 28.0: 151, 307.0: 12, 1391.0: 12, 284.0: 36, 1255.0: 4, 1173.0: 1, 587.0: 41, 175.0: 153, 825.0: 16, 184.0: 63, 1806.0: 10, 2024.0: 5, 1825.0: 4, 15.0: 16, 289.0: 14, 250.0: 12, 1311.0: 29, 285.0: 4, 511.0: 93, 376.0: 10, 1581.0: 3, 2456.0: 4, 2322.0: 2, 1703.0: 2, 1853.0: 1, 1857.0: 9, 1810.0: 1, 333.0: 11, 1472.0: 3, 1695.0: 64, 116.0: 12, 1048.0: 12, 72.0: 156, 1569.0: 1, 10.0: 11, 209.0: 9, 871.0: 10, 2097.0: 2, 2272.0: 20, 1269.0: 66, 1925.0: 78, 639.0: 111, 2439.0: 2, 1160.0: 31, 1333.0: 19, 159.0: 79, 5.0: 2, 12.0: 4, 1045.0: 20, 686.0: 4, 1651.0: 2, 1412.0: 338, 713.0: 13, 20.0: 4, 1831.0: 2, 53.0: 14, 2378.0: 7, 659.0: 3, 1012.0: 1, 926.0: 26, 319.0: 266, 947.0: 37, 106.0: 5, 488.0: 12, 119.0: 4, 215.0: 7, 1337.0: 1, 1126.0: 44, 1697.0: 4, 989.0: 5, 720.0: 9, 702.0: 15, 752.0: 21, 913.0: 26, 2080.0: 5, 632.0: 32, 880.0: 11, 2081.0: 2, 1383.0: 5, 1897.0: 1, 906.0: 100, 1308.0: 1, 869.0: 38, 93.0: 807, 1855.0: 10, 1219.0: 3, 1950.0: 12, 71.0: 150, 48.0: 7, 1370.0: 18, 2133.0: 1, 760.0: 65, 1004.0: 172, 374.0: 43, 598.0: 212, 2006.0: 5, 1176.0: 10, 413.0: 18, 2186.0: 3, 394.0: 21, 2428.0: 3, 1143.0: 26, 1744.0: 24, 1731.0: 2, 296.0: 8, 1792.0: 7, 1652.0: 27, 2310.0: 2, 857.0: 55, 1123.0: 55, 1005.0: 14, 1592.0: 2, 1180.0: 6, 1125.0: 16, 1653.0: 17, 2488.0: 8, 1006.0: 10, 1912.0: 9, 1105.0: 29, 1231.0: 1, 626.0: 44, 81.0: 90, 633.0: 14, 1576.0: 3, 1512.0: 1, 446.0: 58, 1973.0: 6, 1748.0: 42, 787.0: 4, 1292.0: 3, 1134.0: 1, 1803.0: 4, 942.0: 79, 765.0: 135, 418.0: 3, 1345.0: 162, 908.0: 74, 359.0: 196, 358.0: 3, 327.0: 4, 161.0: 1, 1844.0: 9, 1129.0: 30, 2201.0: 104, 1550.0: 10, 2350.0: 2, 158.0: 390, 1021.0: 2, 2090.0: 56, 1263.0: 42, 1658.0: 6, 9.0: 36, 240.0: 11, 2408.0: 1, 2157.0: 34, 1548.0: 1, 1362.0: 5, 1372.0: 70, 2343.0: 5, 2176.0: 1, 2243.0: 1, 671.0: 20, 457.0: 15, 699.0: 6, 569.0: 100, 2233.0: 10, 2450.0: 1, 1306.0: 19, 2586.0: 18, 865.0: 6, 641.0: 44, 976.0: 98, 1621.0: 17, 493.0: 109, 1860.0: 59, 1965.0: 1, 2393.0: 1, 1594.0: 2, 2403.0: 3, 530.0: 177, 1122.0: 19, 515.0: 421, 107.0: 14, 1068.0: 5, 318.0: 113, 1358.0: 39, 1865.0: 40, 732.0: 3, 167.0: 5, 2136.0: 1, 183.0: 8, 2270.0: 4, 357.0: 3, 716.0: 285, 1151.0: 28, 1252.0: 148, 977.0: 24, 1156.0: 1, 304.0: 6, 1044.0: 9, 2145.0: 3, 308.0: 34, 1347.0: 21, 309.0: 3, 324.0: 7, 253.0: 7, 835.0: 14, 719.0: 1, 2208.0: 25, 42.0: 6, 853.0: 26, 1036.0: 38, 1807.0: 58, 2521.0: 1, 2382.0: 3, 1704.0: 6, 2399.0: 2, 1657.0: 6, 2331.0: 3, 1238.0: 264, 518.0: 258, 1976.0: 4, 2695.0: 1, 877.0: 1, 1734.0: 2, 412.0: 5, 295.0: 15, 345.0: 5, 2212.0: 2, 1017.0: 2, 1761.0: 58, 1650.0: 45, 1655.0: 34, 2571.0: 2, 778.0: 40, 839.0: 62, 1786.0: 8, 388.0: 33, 1693.0: 4, 1301.0: 4, 1266.0: 123, 918.0: 8, 1993.0: 7, 687.0: 31, 216.0: 47, 2568.0: 1, 1278.0: 14, 1485.0: 6, 1498.0: 9, 1760.0: 1, 2417.0: 2, 786.0: 651, 2123.0: 1, 2179.0: 32, 1127.0: 2, 1318.0: 3, 1994.0: 1, 1183.0: 2, 907.0: 7, 1073.0: 6, 2249.0: 2, 1766.0: 5, 2032.0: 30, 1343.0: 24, 2241.0: 23, 2034.0: 2, 1138.0: 49, 884.0: 5, 1881.0: 2, 513.0: 66, 1922.0: 15, 1596.0: 65, 1871.0: 97, 1784.0: 9, 486.0: 1, 487.0: 54, 1077.0: 33, 1093.0: 1, 2122.0: 1, 1504.0: 2, 620.0: 26, 2041.0: 1, 1064.0: 23, 2287.0: 5, 1991.0: 2, 859.0: 12, 2060.0: 1, 1828.0: 21, 2062.0: 26, 1484.0: 3, 2296.0: 7, 1142.0: 7, 50.0: 11, 899.0: 7, 794.0: 3, 2116.0: 5, 1573.0: 6, 2607.0: 1, 1812.0: 15, 1393.0: 1, 1239.0: 10, 1307.0: 6, 1706.0: 12, 2251.0: 1, 2040.0: 2, 1987.0: 1, 1446.0: 3, 2039.0: 4, 229.0: 3, 1128.0: 2, 860.0: 1, 2444.0: 1, 2023.0: 86, 2667.0: 4, 1856.0: 17, 2683.0: 4, 1163.0: 2, 2356.0: 1, 1670.0: 4, 369.0: 7, 516.0: 61, 2257.0: 15, 2361.0: 15, 2075.0: 1, 2316.0: 1, 578.0: 3, 1038.0: 19, 1889.0: 6, 1834.0: 26, 246.0: 4, 2088.0: 6, 2383.0: 2, 1404.0: 15, 740.0: 106, 1402.0: 46, 1141.0: 7, 1514.0: 3, 591.0: 37, 769.0: 3, 1647.0: 2, 1471.0: 196, 1955.0: 1, 680.0: 2, 1519.0: 5, 1098.0: 2, 32.0: 4, 1198.0: 2, 1935.0: 1, 2030.0: 3, 390.0: 4, 823.0: 3, 182.0: 105, 1628.0: 7, 1745.0: 1, 301.0: 12, 94.0: 86, 734.0: 1, 949.0: 5, 2685.0: 2, 782.0: 11, 210.0: 28, 2151.0: 3, 771.0: 14, 683.0: 7, 2152.0: 1, 1542.0: 2, 306.0: 33, 1509.0: 2, 2549.0: 2, 2440.0: 1, 1359.0: 2, 2139.0: 11, 1111.0: 1, 1989.0: 12, 863.0: 19, 88.0: 22, 1035.0: 1, 950.0: 12, 2379.0: 2, 264.0: 5, 531.0: 21, 966.0: 4, 437.0: 21, 1525.0: 52, 873.0: 1, 303.0: 1, 1199.0: 97, 464.0: 1, 2198.0: 1, 1880.0: 5, 114.0: 24, 2000.0: 4, 463.0: 71, 1516.0: 1, 1574.0: 1, 2642.0: 1, 347.0: 6, 1981.0: 1, 1603.0: 2, 1284.0: 4, 1314.0: 118, 2421.0: 6, 123.0: 27, 2144.0: 55, 1310.0: 2, 1342.0: 5, 2108.0: 11, 1381.0: 81, 1340.0: 12, 1432.0: 3, 849.0: 9, 2554.0: 2, 2565.0: 3, 1322.0: 3, 568.0: 1, 802.0: 4, 1433.0: 18, 893.0: 3, 763.0: 1, 491.0: 1, 2357.0: 61, 2636.0: 4, 122.0: 2, 1750.0: 6, 840.0: 17, 1491.0: 17, 1884.0: 1, 1228.0: 4, 368.0: 12, 1185.0: 22, 1719.0: 1, 2163.0: 1, 2282.0: 2, 2056.0: 2, 798.0: 2, 2294.0: 1, 448.0: 46, 781.0: 53, 1042.0: 2, 125.0: 10, 478.0: 4, 165.0: 12, 170.0: 2, 1713.0: 45, 2103.0: 4, 1476.0: 2, 228.0: 9, 1529.0: 9, 1661.0: 2, 682.0: 27, 923.0: 4, 748.0: 29, 1827.0: 3, 1452.0: 93, 1879.0: 2, 2419.0: 2, 1563.0: 5, 2460.0: 2, 2292.0: 1, 1951.0: 1, 422.0: 2, 1894.0: 1, 1801.0: 2, 1544.0: 1, 2418.0: 2, 1764.0: 138, 49.0: 24, 1296.0: 50, 2003.0: 16, 178.0: 80, 1614.0: 25, 936.0: 1, 1055.0: 29, 1669.0: 9, 1075.0: 4, 2126.0: 1, 2216.0: 1, 850.0: 2, 1689.0: 24, 1193.0: 2, 2183.0: 17, 1049.0: 44, 2451.0: 1, 1769.0: 4, 1058.0: 17, 1279.0: 4, 1094.0: 3, 2332.0: 4, 1226.0: 142, 2441.0: 1, 2057.0: 29, 1616.0: 2, 1930.0: 2, 2121.0: 6, 571.0: 10, 2267.0: 4, 1983.0: 2, 2105.0: 160, 1380.0: 1, 2049.0: 1, 978.0: 2, 937.0: 136, 21.0: 1, 1715.0: 4, 979.0: 1, 1909.0: 1, 1968.0: 1, 1788.0: 1, 1819.0: 11, 2248.0: 1, 278.0: 1, 2079.0: 23, 1867.0: 1, 1623.0: 14, 1106.0: 8, 1027.0: 21, 1635.0: 5, 2205.0: 1, 1541.0: 46, 690.0: 2, 1313.0: 1, 1444.0: 1, 1482.0: 173, 2349.0: 1, 2365.0: 49, 1696.0: 3, 2235.0: 8, 90.0: 2, 2506.0: 2, 2012.0: 30, 2544.0: 3, 1940.0: 1, 2334.0: 1, 2255.0: 1, 172.0: 1, 1295.0: 1, 2276.0: 6, 2211.0: 5, 1435.0: 21, 812.0: 222, 1775.0: 4, 997.0: 1, 1065.0: 3, 1189.0: 3, 76.0: 1, 2224.0: 10, 2671.0: 2, 1531.0: 5, 1944.0: 14, 113.0: 2, 1583.0: 52, 2681.0: 2, 1934.0: 2, 723.0: 1, 1153.0: 1, 2352.0: 25, 1510.0: 1, 1053.0: 4, 1237.0: 2, 2173.0: 5, 1483.0: 8, 1668.0: 26, 2337.0: 2, 617.0: 2, 1091.0: 1, 2564.0: 1, 2470.0: 2, 1095.0: 3, 130.0: 1, 1665.0: 3, 2385.0: 2, 1567.0: 33, 919.0: 63, 1293.0: 4}\n",
            "1972\n",
            "2698.0\n",
            "min number of connections for each class 1 2414.0\n",
            "max number of connections for each class 1005 4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Approach"
      ],
      "metadata": {
        "id": "la6oh-00LxD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "oEoCaV7OLBmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inputs = [inp.tolist() for inp in inputs]\n",
        "# outputs = [out.tolist() for out in outputs]\n",
        "print(inputs[0])\n",
        "print(outputs[0])\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.3, random_state=1)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)"
      ],
      "metadata": {
        "id": "zINuSgqbMD8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f314d3-430f-4a83-ef0e-8e6141045b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02,  6.0000e+00,\n",
            "         1.0000e+00,  2.0000e+00,  3.7000e+01,  7.0000e+00, -2.7597e-01,\n",
            "         2.6717e-01])\n",
            "tensor(874)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {'max_depth': 3, 'colsample_bytree': 0.8, 'eta': 0.01, 'subsample': 1.0, 'alpha': 0, 'lambda': 1, 'min_child_weight': 5}\n",
        "\n",
        "# model = xgb.XGBRegressor(**parameters)\n",
        "model = xgb.XGBClassifier(max_depth=3, eta=0.01, subsample=1.0, alpha=0, min_child_weight=5,  colsample_bytree=0.8, gpu_id=0)\n",
        "print(model)\n",
        "\n",
        "model.fit(X_train, y_train_encoded)\n",
        "\n",
        "score = model.score(X_test, y_test_encoded)\n",
        "print(\"score: \",score)\n",
        "# cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=5)\n",
        "# print(\"cross score: \",cv_scores)\n",
        "\n",
        "#Predict the next position\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "id": "eZLG_8xU2vif",
        "outputId": "ab21783a-d4a8-415d-8ef3-9eec5c5651b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBClassifier(alpha=0, base_score=None, booster=None, callbacks=None,\n",
            "              colsample_bylevel=None, colsample_bynode=None,\n",
            "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
            "              enable_categorical=False, eta=0.01, eval_metric=None,\n",
            "              feature_types=None, gamma=None, gpu_id=0, grow_policy=None,\n",
            "              importance_type=None, interaction_constraints=None,\n",
            "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
            "              max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
            "              max_leaves=None, min_child_weight=5, missing=nan,\n",
            "              monotone_constraints=None, multi_strategy=None, n_estimators=None, ...)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [10:47:53] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [10:47:53] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score:  0.006807481867922127\n",
            "Accuracy: 0.006807481867922127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode the target\n",
        "\n",
        "\n",
        "#go to GPU\n",
        "# X_train = X_train.to('cuda')\n",
        "# X_test = X_test.to('cuda')\n",
        "# y_train_encoded = torch.tensor(y_train_encoded).to('cuda')\n",
        "# y_test_encoded = torch.tensor(y_test_encoded).to('cuda')\n",
        "#Search for the best parameters\n",
        "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
        "# 'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
        "# 'n_estimators': [100, 200, 300, 400, 500],\n",
        "# 'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
        "# 'min_child_weight': [1, 2, 3, 4, 5]\n",
        "# parameters = {'max_depth': [30, 40, 50, 60, 70, 80, 90, 100], 'learning_rate': [0.05, 0.1, 0.15, 0.2], 'n_estimators': [50, 70, 100, 130, 170], 'gamma': [0, 0.1, 0.2, 0.3, 0.4], 'min_child_weight': [5, 10, 20, 30, 40]}\n",
        "parameters = {'max_depth': [10, 20], 'learning_rate': [0.1, 0.2], 'n_estimators': [50, 70, 100], 'gamma': [0.2, 0.3, 0.4], 'min_child_weight': [10, 5]}\n",
        "# parameters = {'max_depth': [10, 20], 'learning_rate': [0.1, 0.2], 'n_estimators': [30, 50, 10], 'gamma': [0.2], 'min_child_weight': [10, 5]}\n",
        "\n",
        "best_score = -1\n",
        "\n",
        "for max_depth in parameters['max_depth']:\n",
        "    for learning_rate in parameters['learning_rate']:\n",
        "        for n_estimators in parameters['n_estimators']:\n",
        "            for gamma in parameters['gamma']:\n",
        "                for min_child_weight in parameters['min_child_weight']:\n",
        "                    model = xgb.XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, gamma=gamma, min_child_weight=min_child_weight, eta = 0.005,  gpu_id=0) #, gpu_id=0\n",
        "                    model.fit(X_train, y_train_encoded)\n",
        "                    score = model.score(X_test, y_test_encoded)\n",
        "                    print(score)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_parameters = {'max_depth': max_depth, 'learning_rate': learning_rate, 'n_estimators': n_estimators, 'gamma': gamma, 'min_child_weight': min_child_weight}\n",
        "\n",
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "# cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=5)\n",
        "print(\"cross score: \",cv_scores)\n",
        "\n",
        "#Predict the next position\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "id": "R-25EdbFMF0i",
        "outputId": "0eda04ad-07db-43a3-f4e3-756241d9f1bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:09] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:07:09] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008525257666369766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:15:00] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010115790813080545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:23:17] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008270772362896042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:30:51] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009861305509606821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:39:51] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.008461636340501335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:47:34] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009861305509606821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [12:55:51] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009161470925054078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:06:18] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.010115790813080545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:18:16] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009352334902659371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:28:58] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0104338974424227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:41:21] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fd5e016d6aca>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmin_child_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_child_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, gpu_id=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "HKtluODb1CLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Séparation des données par utilisateur et organisation en séquences temporelles\n",
        "user_sequences = {}\n",
        "for entry in inputs_with_pos_id_user_id_target:\n",
        "    user_id, position, time = entry[0], entry[1::2], entry[2::2]\n",
        "    if user_id not in user_sequences:\n",
        "        user_sequences[user_id] = []\n",
        "    user_sequences[user_id].append((position, time))\n",
        "\n",
        "# Normalisation des données\n",
        "scaler = MinMaxScaler()\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    positions = [pos for seq in sequences for pos in seq[0]]\n",
        "    times = [time for seq in sequences for time in seq[1]]\n",
        "    positions_scaled = scaler.fit_transform(np.array(positions).reshape(-1, 1))\n",
        "    times_scaled = scaler.fit_transform(np.array(times).reshape(-1, 1))\n",
        "    user_sequences[user_id] = [(positions_scaled[i], times_scaled[i]) for i in range(len(positions_scaled))]\n",
        "\n",
        "# Padding des séquences\n",
        "max_sequence_length = max(len(seq) for seq in user_sequences.values())\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "# Encodage des séquences (si nécessaire)\n",
        "\n",
        "# Fractionnement des données\n",
        "train_data, val_data, test_data = {}, {}, {}\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    train_size = int(0.7 * len(sequences))\n",
        "    val_size = int(0.15 * len(sequences))\n",
        "    train_data[user_id] = sequences[:train_size]\n",
        "    val_data[user_id] = sequences[train_size:train_size+val_size]\n",
        "    test_data[user_id] = sequences[train_size+val_size:]\n"
      ],
      "metadata": {
        "id": "t24nQ6zkMSyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "f5D9IoBtGX6R",
        "dDvAwpD4GrJu",
        "S_mzoE-MHqLa",
        "ptycyS7FWE4b",
        "3Bbp1dVXWQs3",
        "zZpbR8rG8kBn"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}