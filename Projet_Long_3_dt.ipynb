{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briag1/ProjetLong/blob/main/Projet_Long_3_dt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# @title device\n",
        "def get_device():\n",
        "  if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda\")\n",
        "      print(\"CUDA is available. Using GPU.\")\n",
        "  else:\n",
        "      device = torch.device(\"cpu\")\n",
        "      print(\"CUDA is not available. Using CPU.\")\n",
        "  return device\n",
        "device=get_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1ZV8IJFGT9T",
        "outputId": "aa38d3de-9758-4869-a916-b15c007c9a3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Using CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV_LSRYqGMO2"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qh5nnjRIFe0h"
      },
      "outputs": [],
      "source": [
        "# @title code\n",
        "from os import makedirs\n",
        "import torch\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import shutil\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def get_x(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[0])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def get_y(value):\n",
        "\n",
        "    if isinstance(value, str):\n",
        "        return float(value.split(\"/\")[1])\n",
        "    elif isinstance(value, float):\n",
        "        return value\n",
        "\n",
        "def read_dataframe(name):\n",
        "  if not os.path.exists(name+\".pkl\"):\n",
        "    print(\"reading dataframe: \"+name+\".xlsx\")\n",
        "    df=pd.read_excel(name+\".xlsx\")\n",
        "    df.to_pickle(name+\".pkl\")\n",
        "  else:\n",
        "    print(\"using already read daframe\")\n",
        "\n",
        "def get_vocab(poses,vocab):\n",
        "  for pos in poses:\n",
        "    if pos not in vocab and not any(isinstance(n, float) and math.isnan(n) for n in pos):\n",
        "        vocab[pos]=len(vocab)+1\n",
        "  return vocab\n",
        "\n",
        "def get_fix_time_encoding(df):\n",
        "\n",
        "  df['month_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "  df['month_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.month / 12)\n",
        "\n",
        "  df['day_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "  df['day_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.day / 31)\n",
        "\n",
        "  df['hour_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "  df['hour_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.hour / 24)\n",
        "\n",
        "  df['minute_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "  df['minute_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.minute / 60)\n",
        "\n",
        "  df['second_sin'] = np.sin(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "  df['second_cos'] = np.cos(2 * np.pi * df[\"start time\"].dt.second / 60)\n",
        "def get_time_data(df):\n",
        "  df['month'] =  df[\"start time\"].dt.month\n",
        "  df['day'] =  df[\"start time\"].dt.day\n",
        "  df['hour'] =  df[\"start time\"].dt.hour\n",
        "  df['minute'] = df[\"start time\"].dt.minute\n",
        "  df['second'] = df[\"start time\"].dt.second\n",
        "  return df\n",
        "\n",
        "\n",
        "def tokenize_pos(pos,vocab):\n",
        "\n",
        "  if math.isnan(pos[0]) and math.isnan(pos[1]):\n",
        "    return len(vocab)\n",
        "  else:\n",
        "    return vocab[pos]\n",
        "\n",
        "def get_coordinates(df,input_position,full_dataset):\n",
        "\n",
        "  if full_dataset:\n",
        "    df['x'] = df['latitude']\n",
        "    df['y'] = df['longitude']\n",
        "  else:\n",
        "    df['x'] = df['location(latitude/lontitude)'].apply(get_x)\n",
        "    df['y'] = df['location(latitude/lontitude)'].apply(get_y)\n",
        "\n",
        "\n",
        "  if input_position:\n",
        "    df['x_normalised']=(df['x']-df['x'].mean())/(df['x'].std())\n",
        "    df['y_normalised']=(df['y']-df['y'].mean())/df['y'].std()\n",
        "\n",
        "  return df\n",
        "\n",
        "def get_joined_coordinates(df):\n",
        "\n",
        "  df['pos']= list(zip(df['x'],df['y']))\n",
        "  poses=df['pos'].unique()\n",
        "\n",
        "  return poses\n",
        "\n",
        "def get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset):\n",
        "  col_to_drop_in_df=['date', 'end time','pos']\n",
        "  col_to_drop_in_dict=['x','y', 'time_to_end', 'time_to_next','start time', 'user id']\n",
        "  col_to_add_to_dict=[]\n",
        "  col_in_input=[]\n",
        "  if not full_dataset:\n",
        "    col_to_drop_in_df+=['location(latitude/lontitude)']\n",
        "  else:\n",
        "    col_to_drop_in_df+=['latitude','longitude']\n",
        "  if fixed_time_encoding:\n",
        "    col_to_drop_in_df+=[]\n",
        "    col_to_drop_in_dict+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "    col_in_input+=['month_sin', 'month_cos', 'day_sin', 'day_cos', 'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'second_sin', 'second_cos']\n",
        "  else:\n",
        "    col_to_add_to_dict+=['month','day','hour','minute','second']\n",
        "  if input_position:\n",
        "    col_to_drop_in_dict += ['x_normalised', 'y_normalised']\n",
        "    col_in_input+=['x_normalised', 'y_normalised']\n",
        "  return col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict\n",
        "\n",
        "def process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections):\n",
        "  #get the time to next connection\n",
        "  df_user[\"time_to_next\"] =  df_user[\"start time\"].diff(-1).dt.total_seconds()\n",
        "  dict_user=df_user.to_dict('list')\n",
        "  #create input\n",
        "  dict_user[\"pos_id\"],dict_user[\"pos_id_target\"]=torch.tensor(dict_user[\"pos_id\"][:-1]),torch.tensor(dict_user[\"pos_id\"][1:])\n",
        "\n",
        "  if col_in_input:\n",
        "    dict_user[\"input\"]=torch.tensor([dict_user[col] for col in col_in_input]).T\n",
        "    dict_user[\"input\"]=dict_user[\"input\"][:-1]\n",
        "\n",
        "  if col_to_add_to_dict:\n",
        "    for col in col_to_add_to_dict:\n",
        "      dict_user[col]=torch.tensor(dict_user[col])\n",
        "      dict_user[col]=dict_user[col][:-1]\n",
        "\n",
        "  dict_user[\"time_target\"]=torch.tensor([dict_user[\"time_to_end\"],dict_user[\"time_to_next\"]]).T\n",
        "  dict_user[\"time_target\"]=dict_user[\"time_target\"][:-1]\n",
        "  for e in col_to_drop_in_dict:\n",
        "    dict_user.pop(e)\n",
        "\n",
        "  if not with_repeated_connections:\n",
        "    dict_user=combine_repeated_connections_in_sequence_user(dict_user)\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def combine_repeated_connections_in_sequence_user(dict_user):\n",
        "  index=0\n",
        "  for key in dict_user:\n",
        "    if dict_user[key].shape[0]!=dict_user[\"pos_id\"].shape[0]:\n",
        "      print(key)\n",
        "  #print(dict_user[\"pos_id\"],dict_user[\"pos_id_target\"])\n",
        "  while index < len(dict_user[\"pos_id\"])-1:\n",
        "    #print(dict_user[\"pos_id\"][index],dict_user[\"pos_id_target\"][index])\n",
        "    if dict_user[\"pos_id\"][index]==dict_user[\"pos_id_target\"][index]:\n",
        "      #print(\"equal\")\n",
        "      dict_user[\"pos_id_target\"][index]=dict_user[\"pos_id_target\"][index+1]\n",
        "      dict_user[\"time_target\"][index]=dict_user[\"time_target\"][index+1]\n",
        "      for key in dict_user:\n",
        "\n",
        "        dict_user[key]=torch.cat((dict_user[key][:index+1],dict_user[key][index+2:]))\n",
        "      #print()\n",
        "    else:\n",
        "      #print(\"different\")\n",
        "      index+=1\n",
        "\n",
        "\n",
        "  return dict_user\n",
        "\n",
        "\n",
        "def normalize_output(list_users):\n",
        "  #get means and stds\n",
        "  time_targets=torch.cat([dict_user[\"time_target\"] for dict_user in list_users],dim=0)\n",
        "  time_targets_mean=time_targets.mean(dim=0)\n",
        "  time_targets_std=time_targets.std(dim=0)\n",
        "  #normalize\n",
        "  for i in range(len(list_users)):\n",
        "    list_users[i][\"time_target\"]=(list_users[i][\"time_target\"]-time_targets_mean)/time_targets_std\n",
        "  return list_users\n",
        "\n",
        "\n",
        "\n",
        "def process_dataframe(name,vocab,fixed_time_encoding,input_position,full_dataset,with_repeated_connections,format=\".pkl\"):\n",
        "  df= pd.read_pickle(name+format)\n",
        "  df=df.sort_values('start time')\n",
        "  df=df.drop(['month'],axis=1)\n",
        "\n",
        "  df=get_coordinates(df,input_position,full_dataset)\n",
        "\n",
        "  poses=get_joined_coordinates(df)\n",
        "  vocab=get_vocab(poses,vocab)\n",
        "  df['pos_id'] = df['pos'].apply(lambda pos: tokenize_pos(pos,vocab))\n",
        "\n",
        "  df['time_to_end']=df['end time']-df['start time']\n",
        "  df['time_to_end']=df['time_to_end'].dt.total_seconds()\n",
        "  if fixed_time_encoding:\n",
        "    df=get_fix_time_encoding(df)\n",
        "  else:\n",
        "    df=get_time_data(df)\n",
        "\n",
        "  col_to_drop_in_df,col_to_drop_in_dict,col_in_input,col_to_add_to_dict=get_col_to_keep_and_drop(fixed_time_encoding,input_position,full_dataset)\n",
        "  df=df.drop(col_to_drop_in_df, axis=1)\n",
        "\n",
        "  df_user_group = df.groupby('user id')\n",
        "  list_users=[]\n",
        "  for user, df_user in df_user_group:\n",
        "    if len(df_user)>=2 and not df_user['x'].isnull().values.any():\n",
        "        list_users.append(process_user_data(df_user,vocab,col_in_input,col_to_drop_in_dict,col_to_add_to_dict,with_repeated_connections))\n",
        "  list_users=normalize_output(list_users)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def runcmd(cmd, verbose = False, *args, **kwargs):\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout = subprocess.PIPE,\n",
        "        stderr = subprocess.PIPE,\n",
        "        text = True,\n",
        "        shell = True\n",
        "    )\n",
        "    std_out, std_err = process.communicate()\n",
        "    if verbose:\n",
        "        print(std_out.strip(), std_err)\n",
        "    pass\n",
        "\n",
        "def get_raw_data(directory,src_directory,full_dataset):\n",
        "  if  full_dataset:\n",
        "    shutil.copytree(src_directory,directory)#telecomDataset6mont\n",
        "  else:\n",
        "    runcmd('wget http://sguangwang.com/dataset/telecom.zip', verbose = False)\n",
        "    runcmd('unzip /content/telecom.zip')\n",
        "\n",
        "def get_processed_dataset(load_dataset_path):\n",
        "  saved_list_user_path = os.path.join(load_dataset_path,\"list_users\")\n",
        "  saved_vocab_path = os.path.join(load_dataset_path,\"vocab\")\n",
        "  print(\"loading already preprocessed data: \")\n",
        "  print(saved_list_user_path)\n",
        "  print(saved_vocab_path)\n",
        "  list_users=torch.load(saved_list_user_path)\n",
        "  vocab=torch.load(saved_vocab_path)\n",
        "  return list_users,vocab\n",
        "\n",
        "def process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections):\n",
        "  list_users=[]\n",
        "  vocab={}\n",
        "  if not os.path.exists(directory_raw_data):\n",
        "    print('getting raw data at: '+src_directory_raw_data)\n",
        "    get_raw_data(directory_raw_data,src_directory_raw_data,full_dataset)\n",
        "  if full_dataset:\n",
        "    for name in os.listdir(directory_raw_data):\n",
        "      if not name.endswith(\".pkl\"):\n",
        "        complete_name=os.path.join(directory_raw_data,\".\".join(name.split(\".\")[:-1]))\n",
        "        print(\"processing dataframe: \"+complete_name)\n",
        "        read_dataframe(complete_name)\n",
        "        new_list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "        list_users+=new_list_users\n",
        "  else:\n",
        "    complete_name = \"/content/dataset-telecom/data_6.1~6.30_\"\n",
        "    read_dataframe(complete_name)\n",
        "    list_users,vocab= process_dataframe(complete_name,vocab,fixed_time_encoding=fixed_time_encoding,input_position=input_position,full_dataset=full_dataset,with_repeated_connections=with_repeated_connections)\n",
        "\n",
        "  return list_users,vocab\n",
        "\n",
        "def split_long_sequences(list_users,max_sequence_length):\n",
        "  new_list_users=[]\n",
        "  for i in range(len(list_users)):\n",
        "    seq_length=list_users[i][\"input\"].shape[0]\n",
        "    if seq_length>=max_sequence_length:\n",
        "      nb_of_seq=seq_length//max_sequence_length\n",
        "      rest=seq_length%max_sequence_length\n",
        "      list_splitted_seq=nb_of_seq*[{}]\n",
        "      rest_splitted={}\n",
        "      for key in list_users[i]:\n",
        "        for j in range(nb_of_seq):\n",
        "          list_splitted_seq[j][key]=list_users[i][key][max_sequence_length*j:max_sequence_length*(j+1)]\n",
        "        if rest>2:\n",
        "          rest_splitted[key]= list_users[i][key][-rest:]\n",
        "      new_list_users=new_list_users+list_splitted_seq\n",
        "      if len(rest_splitted)>0:\n",
        "        new_list_users+=[rest_splitted]\n",
        "    else:\n",
        "      new_list_users.append(list_users[i])\n",
        "\n",
        "  return new_list_users\n",
        "\n",
        "\n",
        "\n",
        "def save_processed_data(list_users,vocab,path_to_save_dataset):\n",
        "    print(\"creating directory: \"+path_to_save_dataset)\n",
        "    os.makedirs(path_to_save_dataset,exist_ok=True)\n",
        "    print(\"saving processed data at: \")\n",
        "    save_list_user_path = os.path.join(path_to_save_dataset,\"list_users\")\n",
        "    save_vocab_path = os.path.join(path_to_save_dataset,\"vocab\")\n",
        "    print(save_list_user_path)\n",
        "    print(save_vocab_path)\n",
        "    torch.save(list_users,save_list_user_path)\n",
        "    torch.save(vocab,save_vocab_path)\n",
        "\n",
        "def get_processed_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,spliting_long_sequences,with_repeated_connections,max_sequence_length=100,min_sequence_length=3,save=False,path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month\",download=False,load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month\"):\n",
        "  if not download:\n",
        "    list_users,vocab = get_processed_dataset(load_dataset_path)\n",
        "  else:\n",
        "    list_users,vocab=process_raw_data(src_directory_raw_data,directory_raw_data,fixed_time_encoding,input_position,full_dataset,with_repeated_connections)\n",
        "  if spliting_long_sequences:\n",
        "    print(\"spliting sequences longuer than : \"+str(max_sequence_length)+ \" steps\")\n",
        "    list_users=split_long_sequences(list_users,max_sequence_length)\n",
        "  if save:\n",
        "    save_processed_data(list_users,vocab,path_to_save_dataset)\n",
        "  return list_users,vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSRA1WX8UcsV",
        "outputId": "3ac53c7d-ad8e-453b-ad2a-173c2c315635"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "getting raw data at: drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\n",
            "reading dataframe: /content/dataset-telecom/data_6.1~6.30_.xlsx\n"
          ]
        }
      ],
      "source": [
        "list_users,vocab=get_processed_data(src_directory_raw_data=\"drive/MyDrive/Shanghai-Telcome-Six-Months-DataSet\",\n",
        "                                    directory_raw_data='/content/dataset-telecom-6month',\n",
        "                                    fixed_time_encoding=False,\n",
        "                                    input_position=True,\n",
        "                                    full_dataset=False,\n",
        "                                    spliting_long_sequences=False,\n",
        "                                    with_repeated_connections=False,\n",
        "                                    max_sequence_length=100,\n",
        "                                    min_sequence_length=3,\n",
        "                                    save=False,\n",
        "                                    path_to_save_dataset=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",\n",
        "                                    download=True,\n",
        "                                    load_dataset_path=\"/content/drive/MyDrive/telecomDataset6month-splited-100-without-repeated-elements\",)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs_without_pos_id=[]\n",
        "# pos_id_targets=[]\n",
        "# pos_ids=[]\n",
        "# time_targets=[]\n",
        "# for user in list_users:\n",
        "#   inputs_without_pos_id.append(user[\"input\"])\n",
        "#   pos_id_targets.append(user[\"pos_id_target\"])\n",
        "#   time_targets.append(user[\"time_target\"])\n",
        "#   pos_ids.append(user[\"pos_id\"])\n",
        "# print(inputs_without_pos_id[0].shape,pos_id_targets[0].shape,pos_ids[0].shape)\n",
        "\n",
        "# #delete the time_to_end of each time_target\n",
        "# # for i in range(len(time_targets)):\n",
        "# #   for j in range(len(time_targets[i])):\n",
        "# #     time_targets[i][j]=time_targets[i][j][1:]\n",
        "# print(pos_id_targets[0])\n",
        "\n",
        "# inputs_with_pos_id=[]\n",
        "# for inp,pos in zip(inputs_without_pos_id,pos_ids):\n",
        "#   inputs_with_pos_id.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))\n",
        "\n",
        "# inputs_with_pos_id_user_id=[]\n",
        "# for idx,inp in enumerate(inputs_with_pos_id):\n",
        "#   inputs_with_pos_id_user_id.append(torch.cat([torch.tensor([idx]*inp.shape[0]).unsqueeze(1),inp],dim=1))\n",
        "#   # inputs_with_pos_id_user_id.append(torch.cat([inp, torch.ones(inp.size(0),1)*idx],dim=1))\n",
        "\n",
        "# inputs_with_pos_id_user_id_target=[]\n",
        "# for idx,inp in zip(inputs_with_pos_id_user_id, pos_id_targets):\n",
        "#   inputs_with_pos_id_user_id_target.append(torch.cat([idx, inp.unsqueeze(1)],dim=1))\n",
        "# inputs_with_pos_id_user_id_target[0]"
      ],
      "metadata": {
        "id": "9zbFQ7xk6Ctx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Study homophily in the network by doing a clustering of the users based on their connections.\n",
        "\n",
        "# from sklearn.cluster import KMeans\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import torch\n",
        "\n",
        "\n",
        "# # inputs_with_pos_id_user_id = inputs_with_pos_id_user_id_target\n",
        "# for i in range(len(inputs_with_pos_id_user_id)):\n",
        "#   inputs_with_pos_id_user_id[i] = inputs_with_pos_id_user_id[i].view(-1)\n",
        "\n",
        "\n",
        "# max_length = max([len(inp) for inp in inputs_with_pos_id_user_id])\n",
        "# for i in range(len(inputs_with_pos_id_user_id)):\n",
        "#   inputs_with_pos_id_user_id[i] = torch.cat([inputs_with_pos_id_user_id[i], torch.zeros(max_length - len(inputs_with_pos_id_user_id[i]))], dim=0)\n",
        "\n",
        "# #delete inputs which have less than 5 elements\n",
        "# inputs_without_pos_id=[inputs for inputs in inputs_without_pos_id if inputs.shape[0]>=10]\n",
        "# pos_id_targets=[pos_id for pos_id in pos_id_targets if pos_id.shape[0]>=10]\n",
        "# time_targets=[time for time in time_targets if time.shape[0]>=10]\n",
        "# pos_ids=[pos_id for pos_id in pos_ids if pos_id.shape[0]>=10]\n",
        "\n",
        "# print(len(inputs_with_pos_id_user_id[1]))\n",
        "# inputs_with_pos_id_user_id[0]"
      ],
      "metadata": {
        "id": "PyCBdLosS6Ol",
        "outputId": "d7b8205e-9cfa-439c-c139-a181a2401417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "984\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02,  0.0000e+00,\n",
              "        -3.6774e-02,  7.9402e-02,  8.7400e+02,  0.0000e+00, -3.0587e-02,\n",
              "         7.7556e-02,  2.3700e+02,  0.0000e+00, -2.6449e-02,  8.2146e-02,\n",
              "         5.2800e+02,  0.0000e+00, -3.1970e-02,  8.3483e-02,  5.8000e+02,\n",
              "         0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02,  0.0000e+00,\n",
              "        -3.0587e-02,  7.7556e-02,  2.3700e+02,  0.0000e+00, -2.6449e-02,\n",
              "         8.2146e-02,  5.2800e+02,  0.0000e+00, -2.5953e-02,  7.6560e-02,\n",
              "         1.1020e+03,  0.0000e+00, -3.0587e-02,  7.7556e-02,  2.3700e+02,\n",
              "         0.0000e+00, -3.6774e-02,  7.9402e-02,  8.7400e+02,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
              "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Study homophily in the network by doing a clustering of the users based on their connections.\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "inputs_without_pos_id=[]\n",
        "pos_ids=[]\n",
        "for user in list_users:\n",
        "  inputs_without_pos_id.append(user[\"input\"])\n",
        "  pos_ids.append(user[\"pos_id\"])\n",
        "\n",
        "inputs_with_pos_id=[]\n",
        "for inp,pos in zip(inputs_without_pos_id,pos_ids):\n",
        "  inputs_with_pos_id.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))\n",
        "\n",
        "  # add user id to the inputs\n",
        "inputs_with_pos_id_user_id=[]\n",
        "# for idx,inp in enumerate(inputs_with_pos_id):\n",
        "for idx,inp in enumerate(inputs_with_pos_id):\n",
        "  inputs_with_pos_id_user_id.append(torch.cat([inp, torch.ones(inp.size(0),1)*idx],dim=1))\n",
        "\n",
        "\n",
        "#for each user_id, concatenate by columns the inputs\n",
        "inputs_group_by_user_id=[]\n",
        "print(inputs_with_pos_id_user_id[0][0])\n",
        "print(inputs_with_pos_id_user_id[0][1])\n",
        "n = torch.stack([inputs_with_pos_id_user_id[0][0], inputs_with_pos_id_user_id[0][1]], dim=1)\n",
        "# print(n)\n",
        "for i in range(len(inputs_with_pos_id_user_id)):\n",
        "  inp = torch.stack(tuple(inputs_with_pos_id_user_id[i][j] for j in range(len(inputs_with_pos_id_user_id[i]))), dim=1)\n",
        "  # inputs_group_by_user_id.append(np.array(inp.numpy()))\n",
        "  inputs_group_by_user_id.append(inp)\n",
        "\n",
        "\n",
        "# final_inputs_group_by_user_id = [inputs_group_by_user_id[i].T for i in range(len(inputs_group_by_user_id))]\n",
        "# print(final_inputs_group_by_user_id[0])\n",
        "# print(inputs_group_by_user_id[0])\n",
        "\n",
        "inputs_with_pos_id_user_id = [inputs_group_by_user_id[i].numpy() for i in range(len(inputs_group_by_user_id))]\n",
        "inputs_with_pos_id_user_id = [inputs_with_pos_id_user_id[i].tolist() for i in range(len(inputs_with_pos_id_user_id))]\n",
        "print(len(inputs_with_pos_id_user_id))\n",
        "inputs_with_pos_id_user_id[0]"
      ],
      "metadata": {
        "id": "s5skrqXLIBHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c46b874-9fcd-4872-b8b4-d3c161d453b2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-2.6449e-02,  8.2146e-02,  5.2800e+02,  0.0000e+00])\n",
            "tensor([-3.6774e-02,  7.9402e-02,  8.7400e+02,  0.0000e+00])\n",
            "3072\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[-0.026448775082826614,\n",
              "  -0.03677371144294739,\n",
              "  -0.0305872093886137,\n",
              "  -0.026448775082826614,\n",
              "  -0.03197009116411209,\n",
              "  -0.026448775082826614,\n",
              "  -0.0305872093886137,\n",
              "  -0.026448775082826614,\n",
              "  -0.02595280483365059,\n",
              "  -0.0305872093886137,\n",
              "  -0.03677371144294739],\n",
              " [0.08214598894119263,\n",
              "  0.07940151542425156,\n",
              "  0.0775555744767189,\n",
              "  0.08214598894119263,\n",
              "  0.08348292112350464,\n",
              "  0.08214598894119263,\n",
              "  0.0775555744767189,\n",
              "  0.08214598894119263,\n",
              "  0.07656005024909973,\n",
              "  0.0775555744767189,\n",
              "  0.07940151542425156],\n",
              " [528.0,\n",
              "  874.0,\n",
              "  237.0,\n",
              "  528.0,\n",
              "  580.0,\n",
              "  528.0,\n",
              "  237.0,\n",
              "  528.0,\n",
              "  1102.0,\n",
              "  237.0,\n",
              "  874.0],\n",
              " [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "nb_connections=[]\n",
        "for user in inputs_with_pos_id_user_id:\n",
        "  nb_connections.append(len(user[0]))\n",
        "\n",
        "# plt.plot([i for i in range(len(nb_connections))],nb_connections)\n",
        "# plt.xlabel(\"user id\")\n",
        "# plt.ylabel(\"number of connections\")\n",
        "# plt.show()\n",
        "\n",
        "#group users by number of connections\n",
        "group = [i for i in range(0, max(nb_connections)+5, 5)]\n",
        "group_users = []\n",
        "group_users_id = []\n",
        "for i in range(len(group)-1):\n",
        "  tmp = []\n",
        "  tmp_id = []\n",
        "  for j in range(len(nb_connections)):\n",
        "    if nb_connections[j] >= group[i] and nb_connections[j] < group[i+1]:\n",
        "      tmp.append(nb_connections[j])\n",
        "      tmp_id.append(j)\n",
        "  group_users.append(tmp)\n",
        "  group_users_id.append(tmp_id)\n",
        "\n",
        "\n",
        "print(group_users)\n",
        "#plot the number of users in each group\n",
        "plt.bar([i for i in range(len(group_users))], [len(group_users[i]) for i in range(len(group_users))])\n",
        "# plt.plot([i for i in range(len(group)-1)], [len(group_users[i]) for i in range(len(group)-1)], 'o')\n",
        "plt.xlabel(\"number of connections\")\n",
        "plt.ylabel(\"number of users\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "LR9dxkt9AVNC",
        "outputId": "be0a76c7-5aa0-4e6f-c888-4d02f376983a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 2, 1, 2, 1, 4, 2, 2, 1, 1, 1, 1, 3, 1, 3, 1, 2, 3, 1, 1, 1, 3, 3, 2, 1, 1, 2, 1, 1, 4, 1, 4, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 3, 2, 4, 1, 3, 1, 1, 2, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 3, 2, 1, 4, 3, 3, 1, 1, 2, 4, 1, 2, 3, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 4, 1, 1, 3, 1, 1, 4, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 4, 2, 1, 1, 1, 1, 3, 1, 1, 3, 2, 3, 4, 1, 1, 2, 4, 3, 1, 3, 2, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 4, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 4, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 3, 1, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 4, 2, 4, 1, 4, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 2, 4, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 3, 2, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 2, 4, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 3, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 4, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 2, 4, 1, 1, 1, 1, 3, 3, 1, 2, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 4, 1, 1, 1, 1, 2, 1, 1, 2, 1, 4, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 4, 1, 1, 4, 1, 1, 2, 1, 3, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 3, 3, 4, 1, 1, 1, 1, 3, 2, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 1, 2, 1, 2, 3, 1, 3, 1, 1, 1, 3, 4, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 1, 2, 4, 1, 1, 2, 3, 1, 1, 1, 2, 2, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 4, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1, 4, 2, 4, 1, 1, 1, 1, 3, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1, 1, 2, 4, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 2, 4, 3, 1, 1, 1, 2, 1, 4, 1, 1, 2, 2, 1, 3, 1, 3, 1, 1, 1, 3, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 3, 1, 1, 1, 2, 1, 2, 3, 4, 1, 3, 1, 1, 1, 1, 1, 3, 2, 4, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 4, 4, 3, 1, 2, 1, 4, 1, 4, 3, 1, 1, 1, 3, 1, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 3, 1, 2, 3, 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 2, 4, 1, 2, 3, 3, 1, 2, 1, 1, 1, 4, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 3, 3, 4, 4, 2, 1, 3, 1, 1, 3, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 4, 1, 2, 1, 2, 3, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 2, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 3, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 4, 1, 1, 4, 1, 3, 1, 1, 2, 3, 2, 1, 1, 2, 4, 1, 2, 1, 1, 3, 1, 4, 1, 1, 2, 1, 1, 3, 4, 1, 1, 3, 1, 1, 1, 2, 2, 4, 4, 3, 1, 1, 3, 2, 1, 2, 1, 3, 1, 1, 1, 1, 1, 3, 2, 3, 1, 1, 2, 2, 1, 4, 1, 2, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 2, 2, 1, 1, 4, 4, 1, 3, 1, 1, 1, 4, 1, 1, 3, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 3, 2, 3, 1, 2, 3, 1, 1, 1, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 4, 1, 2, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 3, 2, 2, 1, 1, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 4, 2, 1, 1, 2, 1, 3, 4, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 3, 1, 2, 1, 1, 4, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 4, 1, 4, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 4, 1, 1, 3, 2, 3, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 3, 4, 1, 1, 1, 3, 1, 1, 2, 3, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 1, 1, 2, 1, 1, 4, 1, 1, 1, 1, 4, 4, 2, 4, 1, 1, 1, 4, 1, 1, 2, 2, 1, 2, 1, 1, 3, 3, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 4, 4, 1, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 4, 1, 1, 1, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 3, 2, 1, 3, 4, 1, 3, 1, 2, 1, 1, 1, 1, 1, 3, 1, 1, 1, 2, 3, 3, 1, 1, 1], [5, 9, 8, 8, 8, 9, 9, 8, 6, 8, 5, 5, 7, 8, 6, 6, 7, 8, 8, 5, 5, 5, 7, 6, 7, 8, 9, 9, 8, 7, 5, 7, 9, 6, 7, 7, 6, 5, 5, 5, 8, 7, 5, 6, 9, 5, 7, 7, 5, 5, 5, 5, 7, 6, 9, 9, 9, 9, 5, 6, 5, 7, 9, 8, 5, 8, 7, 8, 6, 7, 6, 8, 8, 8, 7, 5, 5, 8, 6, 5, 6, 9, 9, 5, 5, 6, 6, 7, 5, 9, 8, 5, 6, 9, 9, 5, 5, 5, 8, 6, 6, 5, 8, 9, 5, 5, 5, 9, 5, 7, 6, 5, 6, 5, 9, 8, 8, 5, 6, 5, 5, 6, 9, 9, 7, 7, 8, 5, 6, 9, 8, 9, 7, 7, 6, 9, 8, 9, 7, 6, 9, 5, 8, 7, 5, 6, 8, 8, 7, 5, 8, 6, 9, 5, 5, 6, 9, 7, 6, 9, 7, 5, 7, 9, 7, 5, 8, 7, 9, 5, 5, 7, 9, 6, 8, 8, 7, 6, 9, 7, 8, 8, 8, 5, 7, 7, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 9, 5, 5, 7, 5, 6, 5, 6, 6, 5, 6, 5, 5, 7, 8, 8, 8, 6, 7, 6, 9, 8, 6, 5, 6, 8, 8, 8, 5, 9, 5, 6, 9, 5, 5, 7, 9, 9, 6, 5, 6, 6, 5, 5, 8, 5, 9, 5, 5, 5, 8, 5, 6, 5, 7, 9, 8, 5, 7, 5, 6, 6, 7, 7, 8, 6, 5, 9, 5, 9, 5, 7, 5, 7, 9, 7, 9, 5, 5, 7, 6, 5, 7, 8, 6, 9, 5, 5, 8, 6, 9, 7, 6, 8, 7, 5, 8, 9, 7, 7, 8, 5, 9, 8, 7, 7, 7, 8, 8, 9, 7, 7, 5, 5, 6, 6, 5, 6, 8, 5, 7, 8, 5, 7, 9, 6, 9, 7, 5, 9, 6, 6, 8, 9, 5, 5, 5, 7, 8, 6, 5, 9, 5, 5], [11, 10, 14, 13, 10, 10, 12, 14, 14, 10, 13, 13, 14, 11, 14, 10, 14, 11, 13, 14, 11, 10, 12, 14, 10, 10, 10, 12, 13, 12, 11, 14, 11, 13, 14, 14, 14, 11, 14, 11, 11, 14, 12, 13, 11, 12, 10, 13, 13, 11, 11, 11, 12, 14, 10, 10, 11, 10, 14, 10, 13, 11, 11, 13, 12, 10, 11, 11, 12, 10, 12, 12, 11, 10, 14, 12, 13, 13, 14, 13, 11, 12, 10, 12, 13, 14, 14, 10, 11, 14, 13, 10, 11, 10, 13, 10, 10, 13, 13, 11, 12, 11, 11, 12, 13, 11, 12, 12, 14, 11, 13, 14, 14, 12, 12, 14, 13, 14, 11, 14, 11, 13, 10, 10, 14, 14, 13, 10, 14, 14, 14, 12, 14, 10, 11, 12, 13, 11, 13, 14, 12, 12, 10, 13, 13, 13, 10, 12, 10, 10, 11, 10, 13, 13, 12, 12, 10, 10, 11, 14, 10, 13, 10, 14, 12, 10, 10, 10, 13, 11, 11, 11, 12, 10, 12, 14, 13, 13, 10, 12, 13, 13, 12, 11, 13, 11, 14, 12, 12, 12, 12, 12, 12, 10, 11, 14, 14, 13, 10, 12, 14, 10, 10, 10, 10, 13, 10, 10, 12, 13, 13, 12, 14, 11, 14, 14, 14, 13, 11, 10, 10, 11, 11, 14, 13, 10, 11, 14, 10, 12, 12, 12], [16, 15, 15, 19, 16, 15, 16, 17, 17, 16, 15, 17, 17, 15, 15, 17, 18, 17, 19, 18, 16, 18, 18, 18, 15, 15, 19, 15, 15, 15, 17, 17, 15, 18, 16, 15, 18, 18, 19, 19, 19, 15, 15, 16, 15, 17, 18, 18, 15, 17, 19, 15, 15, 15, 17, 19, 15, 18, 19, 18, 15, 17, 15, 18, 18, 18, 17, 17, 18, 18, 16, 18, 17, 15, 16, 15, 17, 18, 16, 18, 18, 15, 19, 15, 17, 17, 15, 17, 17, 15, 19, 18, 16, 19, 16, 19, 16, 19, 16, 19, 16, 16, 15, 17, 18, 18, 15, 15, 17, 17, 18, 18, 15, 15, 15, 15, 16, 15, 17, 17, 18, 17, 16, 17, 16, 19, 15, 16, 19, 19, 17, 15, 17, 16, 19, 16, 16, 16, 17, 17, 16, 16, 17, 15, 15, 19, 17, 18, 19], [22, 22, 23, 24, 23, 24, 24, 23, 23, 23, 23, 21, 21, 24, 21, 20, 24, 24, 21, 23, 20, 20, 23, 24, 24, 23, 23, 20, 21, 21, 21, 20, 24, 24, 23, 24, 23, 23, 21, 24, 21, 22, 22, 21, 24, 20, 24, 21, 23, 20, 24, 22, 24, 22, 23, 23, 20, 24, 22, 20, 23, 20, 23, 22, 21, 24, 22, 23, 23, 24, 20, 20, 20, 23, 20, 22, 23, 22, 22, 24, 21, 20, 24, 24, 24, 20, 21, 23, 21, 22, 20, 20, 24, 21, 23, 21, 21, 20, 21, 22, 24, 22, 22, 20, 21, 21, 20, 22, 24, 24, 23, 21, 20, 20, 20, 23, 21, 24, 20, 23, 23, 22, 21, 24, 20], [27, 28, 29, 28, 27, 25, 25, 27, 27, 27, 27, 26, 26, 26, 26, 25, 26, 27, 26, 29, 27, 28, 25, 28, 25, 26, 25, 25, 25, 27, 25, 26, 26, 26, 26, 29, 25, 26, 28, 26, 25, 26, 27, 26, 26, 25, 26, 29, 29, 29, 27, 28, 26, 25, 27, 28, 25, 25, 27, 26, 26, 27, 26, 28, 27, 28, 29, 28, 29, 25, 29, 25, 29, 25, 25, 28, 27, 25, 27, 28, 27, 29, 27, 28, 25, 29, 27, 25, 25, 28, 27], [30, 30, 34, 32, 30, 30, 32, 31, 30, 30, 32, 32, 30, 30, 31, 31, 31, 30, 34, 33, 30, 33, 31, 31, 32, 33, 30, 34, 33, 34, 34, 32, 31, 33, 33, 31, 32, 31, 31, 33, 32, 32, 31, 33, 33, 30, 30, 33, 32, 30, 33, 30, 33, 31, 34, 32, 30, 30, 33, 33, 30, 33, 33, 30, 30, 31, 30, 34, 34, 34, 30, 31, 31, 32, 30, 31, 30, 34, 32, 32, 30, 34, 30, 30, 31], [36, 39, 36, 35, 35, 37, 36, 35, 36, 38, 36, 35, 38, 36, 37, 38, 37, 38, 35, 38, 37, 37, 37, 35, 39, 39, 37, 37, 35, 37, 35, 39, 36, 36, 36, 39, 35, 37, 39, 37, 37, 35, 36, 37, 38, 36, 36, 37, 37, 35, 39, 38, 35, 38, 35, 35, 38, 37, 35, 36, 38, 35, 38, 36, 37, 35, 35, 35, 38, 36, 35, 38, 36, 39, 36, 35, 37, 36, 37, 35, 39], [42, 42, 44, 44, 41, 41, 40, 40, 40, 42, 44, 41, 40, 41, 40, 41, 43, 42, 40, 42, 43, 43, 43, 41, 40, 43, 41, 40, 42, 43, 40, 44, 40, 40, 42, 41, 40, 42, 43, 44, 41, 43, 43, 40, 44, 40, 42, 43, 40, 42, 42, 42, 44, 44, 41, 43, 44, 42, 41, 44, 42], [48, 48, 48, 49, 48, 49, 46, 46, 47, 49, 48, 45, 48, 49, 45, 49, 49, 49, 48, 49, 45, 49, 45, 48, 48, 45, 46, 45, 47, 45, 47, 45, 47, 46, 47, 47, 46, 48, 45, 46, 46, 47, 47, 45, 49], [50, 54, 52, 51, 53, 54, 52, 54, 54, 50, 54, 54, 54, 52, 54, 53, 52, 54, 51, 50, 50, 53, 52, 53, 51, 53, 50, 50, 51, 53, 52, 54, 50, 50, 52, 52, 51, 51, 53, 52], [56, 57, 57, 59, 57, 55, 59, 59, 57, 56, 55, 55, 57, 56, 59, 57, 57, 59, 59, 59, 59, 56, 56, 56, 56, 58, 58, 59, 56, 58, 57, 55, 59, 57, 59, 59, 58, 57, 56, 55, 55, 56, 57, 57, 57], [61, 63, 61, 60, 61, 63, 61, 62, 64, 60, 61, 61, 60, 64, 61, 64, 61, 61, 63, 61, 60, 62, 63, 63, 60, 60, 61, 63, 63, 60, 62, 64, 63, 64, 61, 60, 61, 60, 62], [65, 66, 66, 69, 69, 66, 69, 65, 66, 65, 68, 66, 68, 69, 65, 67, 69, 69, 68, 66, 69, 65, 65, 68, 66, 65, 67, 67, 69, 68, 69, 68], [71, 73, 74, 73, 70, 71, 72, 70, 74, 71, 71, 70, 71, 72, 74, 71, 71, 70, 73, 70, 74, 73, 74, 74, 74], [78, 76, 77, 76, 75, 75, 76, 78, 75, 79, 76, 75, 75, 75, 79, 75, 75, 76, 79, 76, 77, 79, 77, 77, 78, 77, 76], [81, 82, 84, 80, 84, 80, 82, 83, 83, 83, 80, 81, 80, 82, 80, 82, 84, 80, 84, 82, 81, 83], [86, 88, 88, 87, 87, 85, 87, 88, 85, 85, 89, 88, 85, 89, 87, 87, 89], [90, 92, 91, 93, 90, 90, 91, 93, 93, 92, 94, 93, 92, 92, 93], [98, 99, 97, 96, 97, 95, 96], [100, 100, 104, 100, 103, 104, 104, 100], [105, 105, 106, 105, 109, 106], [111, 110, 112, 114, 111, 113, 110, 114], [118, 115, 116, 116, 117, 119, 116, 115, 116, 116], [122, 124, 120, 121, 124, 122], [126, 128, 128, 126, 126, 128, 125, 127], [133, 132, 131, 132], [135, 138], [140, 141, 142], [148, 147], [153, 150, 153, 151, 150], [157, 158], [163, 164], [165, 165], [170], [], [182, 183], [], [192, 194], [197, 195, 198], [], [209], [], [], [220], [225], [], [], [], [246]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import silhouette_score\n",
        "# from sklearn.metrics import davies_bouldin_score\n",
        "# from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# params = {\n",
        "#     \"n_clusters\": range(int(1218/10), int(1218/4), 30),\n",
        "#     \"init\": [\"k-means++\"],# , \"random\"\n",
        "#     \"n_init\": [10],\n",
        "#     \"max_iter\": [350, 400],\n",
        "# }\n",
        "# score = -1\n",
        "# for n in params[\"n_clusters\"]:\n",
        "#     for i in params[\"init\"]:\n",
        "#         for ni in params[\"n_init\"]:\n",
        "#             for mi in params[\"max_iter\"]:\n",
        "#                 kmeans = KMeans(n_clusters=n, init=i, n_init=ni, max_iter=mi)\n",
        "#                 # kmeans = AgglomerativeClustering(n_clusters=n)\n",
        "#                 kmeans.fit(inputs_with_pos_id_user_id)\n",
        "#                 s = silhouette_score(inputs_with_pos_id_user_id, kmeans.labels_)\n",
        "#                 print(s)\n",
        "#                 if s > score:\n",
        "#                     score = s\n",
        "#                     best_params = {\"n_clusters\": n, \"init\": i, \"n_init\": ni, \"max_iter\": mi}\n",
        "# print(best_params)\n",
        "# print(score)\n",
        "# # Create kmeans model with best parameters\n",
        "# kmeans = KMeans(n_clusters=best_params[\"n_clusters\"], init=best_params[\"init\"], n_init=best_params[\"n_init\"], max_iter=best_params[\"max_iter\"])\n",
        "\n",
        "# # Fit the model\n",
        "# kmeans.fit(inputs_with_pos_id_user_id)"
      ],
      "metadata": {
        "id": "mnZSTx7RTtHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Nombre de clusters:\", kmeans.n_clusters_)\n",
        "# print(\"Liaison utilisée:\", kmeans.linkage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXoVJlug78qn",
        "outputId": "f83fa0ae-a3ca-440f-b00d-e852f37a25af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de clusters: 2\n",
            "Liaison utilisée: ward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# params = {\n",
        "#     \"n_clusters\": range(int(3072/20), int(3072/10), 40),\n",
        "#     \"init\": [\"k-means++\"],\n",
        "#     \"n_init\": [10],\n",
        "#     \"max_iter\": [400],\n",
        "# }\n",
        "\n",
        "# # Create a kmeans model\n",
        "# kmeans = KMeans()\n",
        "\n",
        "# # Create a grid search\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# grid_search = GridSearchCV(kmeans, params, cv=5)\n",
        "\n",
        "# # Fit the grid search\n",
        "# grid_search.fit(inputs_with_pos_id_user_id)\n",
        "\n",
        "# # Get the best parameters\n",
        "# print(grid_search.best_params_)\n",
        "# print(grid_search.best_score_)\n",
        "\n",
        "# # Create kmeans model with best parameters\n",
        "# kmeans = KMeans(n_clusters=grid_search.best_params_[\"n_clusters\"], init=grid_search.best_params_[\"init\"], n_init=grid_search.best_params_[\"n_init\"], max_iter=grid_search.best_params_[\"max_iter\"])\n",
        "\n",
        "# # Fit the model\n",
        "# kmeans.fit(inputs_with_pos_id_user_id)"
      ],
      "metadata": {
        "id": "ISKuAJn5ewHz",
        "outputId": "df9e6d73-e612-4381-a56c-49bf91024326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'init': 'k-means++', 'max_iter': 400, 'n_clusters': 273, 'n_init': 10}\n",
            "-1072146663.8881695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# s = silhouette_score(inputs_with_pos_id_user_id, kmeans.labels_)\n",
        "# print(s)"
      ],
      "metadata": {
        "id": "4WTr5ilug0sa",
        "outputId": "d0a9f74a-c2c7-4588-b8c4-846e308d0c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.39034376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5D9IoBtGX6R"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDvAwpD4GrJu"
      },
      "source": [
        "## Reproducibility seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p5d4mp9GDah"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import string\n",
        "import random\n",
        "def get_reproducible_seeds(name=\"ProjectLong\",nb_seeds=100):\n",
        "    # Calculate SHA-256 hash\n",
        "    sha256_hash = hashlib.sha256(name.encode()).hexdigest()\n",
        "    # Define character sets\n",
        "    digits = string.digits\n",
        "    # Use the hash to seed the random number generator\n",
        "    hash_as_int = int(sha256_hash, 16)\n",
        "    random.seed(hash_as_int)\n",
        "    # Generate a random list of seed of desired length\n",
        "    reproducibility_seeds = [random.randint(0,10000) for _ in range(nb_seeds)]\n",
        "\n",
        "    return reproducibility_seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mn8U0p9TGJXK"
      },
      "outputs": [],
      "source": [
        "reproducibility_seed=get_reproducible_seeds()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_mzoE-MHqLa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nhxCSLNHsd9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class VariableLengthDatasetWithPosID(Dataset):\n",
        "    def __init__(self, time_series, transform=None):\n",
        "        self.times_series=time_series\n",
        "    def __len__(self):\n",
        "        return len(self.times_series)\n",
        "    def __getitem__(self, idx):\n",
        "        user_dict=self.times_series[idx]\n",
        "        return  user_dict\n",
        "\n",
        "def create_dataset(list_users,split=[0.8,0.1,0.1]):\n",
        "  dataset=VariableLengthDatasetWithPosID(list_users)\n",
        "  generator = torch.Generator().manual_seed(reproducibility_seed)\n",
        "  dataset_list=torch.utils.data.random_split(dataset,[0.8,0.1,0.1],generator)\n",
        "  return dataset_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGgl2XnIhDQ"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nke01dG-KJxO"
      },
      "outputs": [],
      "source": [
        "def collate_fn_padd(batch_dict):\n",
        "    '''\n",
        "    Padds batch of variable length\n",
        "\n",
        "    note: it converts things ToTensor manually here since the ToTensor transform\n",
        "    assume it takes in images rather than arbitrary tensors.\n",
        "    '''\n",
        "\n",
        "\n",
        "    dict_batch={key: [d[key] for d in batch_dict] for key in batch_dict[0]}\n",
        "    dict_batch[\"lengths\"] = torch.tensor([ user[\"input\"].shape[0] for user in batch_dict ])\n",
        "    if \"input\" in dict_batch:\n",
        "      dict_batch[\"input\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"input\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"month\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"month\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"day\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"day\"],batch_first=True,padding_value=0)\n",
        "    dict_batch[\"hour\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"hour\"],batch_first=True,padding_value=24)\n",
        "    dict_batch[\"minute\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"minute\"],batch_first=True,padding_value=60)\n",
        "    dict_batch[\"second\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"second\"],batch_first=True,padding_value=60)\n",
        "\n",
        "    dict_batch[\"time_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"time_target\"],batch_first=True,padding_value=-1)\n",
        "    dict_batch[\"pos_id\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id\"],batch_first=True,padding_value=len(vocab))\n",
        "    dict_batch[\"pos_id_target\"] = torch.nn.utils.rnn.pad_sequence(dict_batch[\"pos_id_target\"],batch_first=True,padding_value=len(vocab))\n",
        "    #print(dict_batch[\"input\"])\n",
        "    return dict_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5Yl1E6gY8_P"
      },
      "source": [
        "## Instanciate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHpriSipY7Kz"
      },
      "outputs": [],
      "source": [
        "dataset_list=create_dataset(list_users)\n",
        "train_dataset=dataset_list[0]\n",
        "valid_dataset=dataset_list[1]\n",
        "test_dataset=dataset_list[2]\n",
        "train_dataloader=DataLoader(train_dataset,batch_size=64,collate_fn=collate_fn_padd,shuffle=True)\n",
        "valid_dataloader=DataLoader(valid_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn_padd,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9HodJbvKeMe"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptycyS7FWE4b"
      },
      "source": [
        "## Transformer Encoder followed by LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oeWr0HDhJfo"
      },
      "source": [
        "### transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_bACPajeQx"
      },
      "outputs": [],
      "source": [
        "def get_mask(bath_size,sequence_length,lengths,device):\n",
        "  mask=torch.zeros(bath_size,sequence_length).to(device)\n",
        "  for i, length in enumerate(lengths):\n",
        "    mask[i,length:]=1\n",
        "  return mask.bool()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsaggvjghDsq"
      },
      "source": [
        "#### Positional encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yFXZxqeHMwi"
      },
      "outputs": [],
      "source": [
        "from torch import nn, Tensor\n",
        "class VanillaPositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x = (x.transpose(0,1) + self.pe[:x.transpose(0,1).size(0)]).transpose(0,1)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX-kk1ScG-_n"
      },
      "outputs": [],
      "source": [
        "class LearnablePositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self,d_model: int, dropout: float = 0.1, max_len: int = 500):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.positional_embedding=nn.Embedding(num_embeddings=max_len,embedding_dim= d_model)\n",
        "    @property\n",
        "    def device(self):\n",
        "      return next(self.parameters()).device\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            x: Tensor, shape ``[batch_size,seq_len, embedding_dim]``\n",
        "        \"\"\"\n",
        "        x += self.positional_embedding(torch.arange(0,x.shape[1]).to(self.device))\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzKYvfaWG6cH"
      },
      "outputs": [],
      "source": [
        "def get_PositionalEncoding(d_model: int, dropout: float = 0.1, max_len: int = 2000, learnable=False):\n",
        "  if learnable:\n",
        "    return LearnablePositionalEncoding(d_model, dropout, max_len)\n",
        "  else:\n",
        "    return VanillaPositionalEncoding(d_model, dropout, max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-pr0W6xhOkZ"
      },
      "source": [
        "#### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZQnLdN4UxY"
      },
      "outputs": [],
      "source": [
        "class Encoder_Decoder_Transformer(nn.Module):\n",
        "    def __init__(self,d_model,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "      super().__init__()\n",
        "      self.transformer=torch.nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,  dropout=dropout, batch_first=batch_first)\n",
        "    def forward(self,x,mask,src_key_padding_mask,is_causal):\n",
        "      return self.transformer(x,\n",
        "                       x,\n",
        "                       src_mask=mask,\n",
        "                       tgt_mask=mask,\n",
        "                       memory_mask=mask,\n",
        "                       src_key_padding_mask=src_key_padding_mask,\n",
        "                       tgt_key_padding_mask=src_key_padding_mask,\n",
        "                       memory_key_padding_mask=src_key_padding_mask,\n",
        "                       src_is_causal=is_causal,\n",
        "                       tgt_is_causal=is_causal,\n",
        "                       memory_is_causal=is_causal)\n",
        "\n",
        "\n",
        "\n",
        "def get_Transformer_architecture(d_model,encoder_only=False,num_layers=3,nhead=10,dropout=0.1,batch_first=True):\n",
        "  if encoder_only:\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,batch_first=batch_first)\n",
        "    return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "  else:\n",
        "    return Encoder_Decoder_Transformer(d_model,num_layers,nhead,dropout,batch_first=batch_first)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcCkeqmkhRnT"
      },
      "source": [
        "### feature embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmqQtP0UhZqg"
      },
      "outputs": [],
      "source": [
        "class TimeStampEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.month_embedding = nn.Embedding(num_embeddings=13,embedding_dim=embedding_dim)\n",
        "    self.day_embedding = nn.Embedding(num_embeddings=32,embedding_dim=embedding_dim)\n",
        "    self.hour_embedding = nn.Embedding(num_embeddings=25,embedding_dim=embedding_dim)\n",
        "    self.minute_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "    self.second_embedding = nn.Embedding(num_embeddings=61,embedding_dim=embedding_dim)\n",
        "\n",
        "  def forward(self,dict_batch):\n",
        "    embedding= self.month_embedding(dict_batch['month'])\n",
        "    embedding=+ self.day_embedding(dict_batch['day'])\n",
        "    embedding=+ self.hour_embedding(dict_batch['hour'])\n",
        "    embedding=+ self.minute_embedding(dict_batch['minute'])\n",
        "    embedding=+ self.second_embedding(dict_batch['second'])\n",
        "    return self.dropout(embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCp5Pz6uy-FG"
      },
      "outputs": [],
      "source": [
        "class StationIdEmbedding(nn.Module):\n",
        "  def __init__(self,embedding_dim,nb_of_pos_ids,dropout):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.stationIdEmbedding=nn.Embedding(num_embeddings=nb_of_pos_ids,embedding_dim=embedding_dim)\n",
        "  def forward(self,dict_batch):\n",
        "    embedding=self.stationIdEmbedding(dict_batch[\"pos_id\"])\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bbp1dVXWQs3"
      },
      "source": [
        "#### graph_deepLearning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqW174iJrhFC",
        "outputId": "6ad9be4e-f4cd-40d0-a6fb-0cd7c1b561d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting libpysal\n",
            "  Downloading libpysal-4.9.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.10 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.12.3)\n",
            "Requirement already satisfied: geopandas>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from libpysal) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.25.2)\n",
            "Requirement already satisfied: packaging>=22 in /usr/local/lib/python3.10/dist-packages (from libpysal) (23.2)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from libpysal) (4.2.0)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from libpysal) (1.11.4)\n",
            "Requirement already satisfied: shapely>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from libpysal) (2.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.10->libpysal) (2.5)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (1.9.5)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.10.0->libpysal) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->libpysal) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->libpysal) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.10.0->libpysal) (67.7.2)\n",
            "Installing collected packages: libpysal\n",
            "Successfully installed libpysal-4.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install libpysal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQvAPDZyWNtg",
        "outputId": "c0fe4dde-5968-49ea-c87f-398c269f8a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=3049d4ae756209f0ed6d2a1856d71ae9d30d88b08233d9124d9ba4a0cbc02455\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "if 'IS_GRADESCOPE_ENV' not in os.environ:\n",
        "  torch_version = str(torch.__version__)\n",
        "  scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "  !pip install torch-scatter -f $scatter_src\n",
        "  !pip install torch-sparse -f $sparse_src\n",
        "  !pip install torch-geometric\n",
        "  !pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdSTBOc3sKsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "from libpysal.cg import voronoi_frames\n",
        "from libpysal import weights, examples\n",
        "from torch_geometric.utils import from_networkx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np\n",
        "def get_net(vocab):\n",
        "  x_array=[key[0] for key in vocab]\n",
        "  y_array=[key[1] for key in vocab]\n",
        "  coordinates=np.column_stack((x_array,y_array))\n",
        "  cells, generators = voronoi_frames(coordinates, clip=\"convex hull\")\n",
        "  delaunay = weights.Rook.from_dataframe(cells)\n",
        "  delaunay_graph = delaunay.to_networkx()\n",
        "  positions = dict(zip(delaunay_graph.nodes, coordinates))\n",
        "  nx.set_node_attributes(delaunay_graph,positions,\"coordinates\")\n",
        "  distance=np.linalg.norm(np.concatenate([delaunay_graph.nodes[index[0]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0)-np.concatenate([delaunay_graph.nodes[index[1]][\"coordinates\"][None,:] for index in delaunay_graph.edges], axis=0), axis=1)\n",
        "  nx.set_edge_attributes(delaunay_graph,dict(zip(delaunay_graph.edges,distance)),\"distance\")\n",
        "  net=from_networkx(delaunay_graph)\n",
        "  return net\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, hidden_dim1, hidden_dim2, output_dim,vocab,dropout,device):\n",
        "    super(GCN, self).__init__()\n",
        "    net=get_net(vocab)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.edge_index=edge_index = net.edge_index.long().to(device)\n",
        "    self.distance= net.distance.float().to(device)\n",
        "    self.coordinates=net.coordinates.float().to(device)\n",
        "    mean_distance=self.distance.mean()\n",
        "    std_distance=self.distance.std()\n",
        "    self.distance=(((self.distance-mean_distance)/std_distance)+1)/2\n",
        "\n",
        "    mean_coordinates=self.coordinates.mean(dim=0)\n",
        "    std_coordinates=self.coordinates.std(dim=0)\n",
        "    self.coordinates=(self.coordinates-mean_coordinates.unsqueeze(0))/std_coordinates.unsqueeze(0)\n",
        "    self.conv1 = GCNConv(2, hidden_dim1)\n",
        "    self.conv2 = GCNConv(hidden_dim1, hidden_dim2)\n",
        "    self.conv3 = GCNConv(hidden_dim2, output_dim)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self, dic_batch):\n",
        "    x = self.conv1(self.coordinates, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "    x = self.conv2(x, self.edge_index,self.distance)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, p=0.5, training=self.training)\n",
        "    x = self.conv3(x, self.edge_index,self.distance)\n",
        "    x=torch.cat((x,torch.zeros(1,x.shape[1]).to(self.device)),dim=0)\n",
        "    embedding=x[dic_batch[\"pos_id\"]]\n",
        "    return self.dropout(embedding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozXR4sW0W0Y"
      },
      "source": [
        " #### Combine feature embeddng"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6mU1qWOjRP3"
      },
      "outputs": [],
      "source": [
        "class Feature_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self,d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device):\n",
        "    super().__init__()\n",
        "    self.num_features=2+use_gcn\n",
        "    self.concatenate_features=concatenate_features\n",
        "    self.embedding_dim=d_model\n",
        "    self.keep_input_positions=keep_input_positions\n",
        "    if keep_input_positions:\n",
        "      self.embedding_dim=self.embedding_dim-2\n",
        "    if self.concatenate_features:\n",
        "      self.embedding_dim=int(self.embedding_dim/self.num_features)\n",
        "\n",
        "    list_feature_embedding=[StationIdEmbedding(self.embedding_dim,nb_of_pos_ids,dropout),TimeStampEmbedding(self.embedding_dim,dropout)]\n",
        "    if use_gcn:\n",
        "      list_feature_embedding.append(GCN(hidden_dim1, hidden_dim2, self.embedding_dim, vocab, dropout,device))\n",
        "    self.list_feature_embedding=nn.ModuleList(list_feature_embedding)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "  def forward(self,dic_batch):\n",
        "    if self.concatenate_features:\n",
        "      list_embeddings=[]\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        list_embeddings.append(feature_emebdding(dic_batch))\n",
        "      embedding=torch.cat(list_embeddings,dim=2)\n",
        "    else:\n",
        "      embedding=torch.zeros(*dic_batch[\"pos_id\"].shape,self.embedding_dim).to(self.device)\n",
        "      for feature_emebdding in self.list_feature_embedding:\n",
        "        embedding+=feature_emebdding(dic_batch)\n",
        "    if self.keep_input_positions:\n",
        "      embedding=torch.cat((dic_batch[\"input\"],embedding),dim=2)\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svMRI0xeji-7"
      },
      "source": [
        "### model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSt_zuJRKgBh"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import Embedding, LSTM\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.dim_perceptron=2*d_model\n",
        "    self.linear_perceptron_in=nn.Linear(d_model,self.dim_perceptron)\n",
        "    self.linear_perceptron_out=nn.Linear(self.dim_perceptron,d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.linear_perceptron_out(F.relu(self.linear_perceptron_in(x)))\n",
        "\n",
        "\n",
        "class Transformer_LSTM_Layer(nn.Module):\n",
        "  def __init__(self,d_model,output_regression_size,output_classfication_size,num_layers,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first):\n",
        "    super().__init__()\n",
        "\n",
        "    self.lstm=LSTM(input_size=d_model, hidden_size=d_model,batch_first=batch_first,num_layers=1,dropout=dropout)\n",
        "    self.lstm_layer_with_perceptron=lstm_layer_with_perceptron\n",
        "    self.lstm_layer_with_layer_norm=lstm_layer_with_layer_norm\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      self.mlp=MLP(d_model)\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self,x,batch_sizes,sorted_indices,unsorted_indices,lengths):\n",
        "    x=self.lstm(x)[0].data+x.data\n",
        "    x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.lstm_layer_with_layer_norm:\n",
        "      x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "      x=self.layer_normalisation(x)\n",
        "      x=self.dropout(x)\n",
        "      x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    if self.lstm_layer_with_perceptron:\n",
        "      x=x.data\n",
        "      x=self.mlp(x)+x\n",
        "      x=torch.torch.nn.utils.rnn.PackedSequence(x, batch_sizes, sorted_indices, unsorted_indices)\n",
        "      if self.layer_normalisation:\n",
        "        x,_=torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, padding_value=0.0)\n",
        "        x=self.layer_normalisation(x)\n",
        "        x=self.dropout(x)\n",
        "        x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=lengths,batch_first=True, enforce_sorted=False)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class  Transformer_encoder_LSTM_decoder(nn.Module):\n",
        "  def __init__(self,d_model,nb_of_pos_ids,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,num_layers_transformer,encoder_only,nhead,learnable_pos_encoding,new_station_binary_classification,use_gcn,vocab,hidden_dim1, hidden_dim2,max_len,dropout,batch_first,concatenate_features,keep_input_positions,device):\n",
        "    super().__init__()\n",
        "    self.dropout=nn.Dropout(p=dropout)\n",
        "    self.layer_normalisation=torch.nn.LayerNorm(d_model)\n",
        "    self.feature_embedding=Feature_embedding(d_model,nb_of_pos_ids,use_gcn,vocab,hidden_dim1, hidden_dim2,batch_first,concatenate_features,keep_input_positions,dropout,device)\n",
        "\n",
        "    self.num_layers_transformer=num_layers_transformer\n",
        "    if num_layers_transformer>0:\n",
        "      self.pos_encoder = get_PositionalEncoding(d_model, dropout, max_len,learnable_pos_encoding)\n",
        "      self.transformer_model=get_Transformer_architecture(d_model,encoder_only,num_layers_transformer,nhead,dropout,batch_first)\n",
        "\n",
        "    self.num_layers_lstm=num_layers_lstm\n",
        "    if num_layers_lstm>0:\n",
        "      self.transformer_lstm__list = nn.ModuleList([Transformer_LSTM_Layer(d_model,output_regression_size,output_classfication_size,num_layers_lstm,lstm_layer_with_perceptron,lstm_layer_with_layer_norm,dropout,batch_first) for layer in range(num_layers_lstm)])\n",
        "    self.linear_reg=nn.Linear(d_model,output_regression_size)\n",
        "    self.classifier=nn.Linear(d_model,output_classfication_size)\n",
        "\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.binary_classifier=nn.Linear(d_model,1)\n",
        "\n",
        "  @property\n",
        "  def device(self):\n",
        "    return next(self.parameters()).device\n",
        "\n",
        "\n",
        "  def forward(self,dic_batch,reg):\n",
        "    if self.num_layers_transformer>0:\n",
        "      x=self.feature_embedding(dic_batch)\n",
        "      x=self.pos_encoder(x)\n",
        "      with torch.no_grad():\n",
        "        mask_x = get_mask(x.shape[0],x.shape[1],dic_batch[\"lengths\"],self.device)\n",
        "        causal_mask=torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1],device=self.device)\n",
        "      x=self.transformer_model(x,causal_mask,mask_x,is_causal=True)\n",
        "    if self.num_layers_lstm>0:\n",
        "      if self.num_layers_transformer>0:\n",
        "        x+=self.feature_embedding(dic_batch)\n",
        "      else:\n",
        "        x=self.feature_embedding(dic_batch)\n",
        "\n",
        "    x=torch.nn.utils.rnn.pack_padded_sequence(x, lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    batch_sizes=x.batch_sizes\n",
        "    sorted_indices=x.sorted_indices\n",
        "    unsorted_indices=x.unsorted_indices\n",
        "    if self.num_layers_lstm>0:\n",
        "      for transformer_lstm in self.transformer_lstm__list:\n",
        "        x=transformer_lstm(x,batch_sizes,sorted_indices,unsorted_indices,dic_batch[\"lengths\"])\n",
        "    x=F.relu(x.data)\n",
        "    out={}\n",
        "    out[\"next_station\"]=torch.nn.utils.rnn.PackedSequence(self.classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if reg:\n",
        "      out[\"time_regression\"]=torch.nn.utils.rnn.PackedSequence(torch.exp(self.linear_reg(x)), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    if self.new_station_binary_classification:\n",
        "      out[\"new_station\"]=  torch.nn.utils.rnn.PackedSequence( self.binary_classifier(x), batch_sizes, sorted_indices, unsorted_indices)\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baselines"
      ],
      "metadata": {
        "id": "zZpbR8rG8kBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "class  Baseline_model(nn.Module):\n",
        "  def __init__(self,nb_of_pos_ids):\n",
        "    super().__init__()\n",
        "    self.nb_of_pos_ids=nb_of_pos_ids\n",
        "  def forward(self,dic_batch,reg):\n",
        "    out={}\n",
        "    out[\"next_station\"]=  torch.nn.utils.rnn.pack_padded_sequence(F.one_hot(dic_batch[\"pos_id\"],self.nb_of_pos_ids).float(), lengths=dic_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    return out"
      ],
      "metadata": {
        "id": "jn0xR-ME8tRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Baseline_model(len(vocab)+1)\n",
        "criterion=Total_loss(False)\n",
        "evaluate(model,valid_dataloader,criterion,device,reg=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYH5OMnmELSa",
        "outputId": "fb668202-96cb-41cb-9f16-df7516a96334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'classification': 0.0021053161556483374,\n",
              " 'total': 0.0021053161556483374,\n",
              " 'acc': 0.044894637279486165}"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28s2GCFETdYS"
      },
      "source": [
        "# Trainning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ujoc4c2mQh_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title loss\n",
        "from torch import nn\n",
        "class Loss_next_station_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "\n",
        "  def forward(self, out, target_pos_ids, index_training_element):\n",
        "    loss_classification=self.criterion(out.data[index_training_element],target_pos_ids.data[index_training_element])\n",
        "    return loss_classification\n",
        "\n",
        "class Loss_time_regression(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion = nn.MSELoss(reduction='none')\n",
        "  def forward(self,out,dict_batch):\n",
        "    time_targets=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"time_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    mask_time_targets = (time_targets.data != -1)\n",
        "    loss_regression=self.criterion(out.data,time_targets.data)\n",
        "    loss_regression = (loss_regression * mask_time_targets.float()).mean()\n",
        "    return loss_regression\n",
        "\n",
        "class Loss_new_station_binary_classification(nn.Module):\n",
        "  def __init__(self, ) -> None:\n",
        "    super().__init__()\n",
        "    self.criterion =  nn.BCEWithLogitsLoss()\n",
        "  def forward(self,out,target):\n",
        "    loss_classification=self.criterion(out.data.squeeze(),target.float())\n",
        "    return loss_classification\n",
        "\n",
        "def get_repetition_labels(target_pos_ids,pos_ids):\n",
        "\n",
        "  return (target_pos_ids.data==pos_ids.data).type(torch.LongTensor)\n",
        "\n",
        "def upsampling_strategy(target, epoch, epochs_new_station_only,pourcentage_of_repeat_training_elment):\n",
        "\n",
        "    index_non_repeat =(target==0).nonzero()\n",
        "    coeff=pourcentage_of_repeat_training_elment/(1-pourcentage_of_repeat_training_elment)\n",
        "    index_for_training= index_non_repeat\n",
        "    if epoch>= epochs_new_station_only:\n",
        "      index_repeat = target.nonzero().squeeze()\n",
        "      nb_non_repeat= index_non_repeat.shape[0]\n",
        "      slice_repeat=index_repeat[torch.randperm(index_repeat.shape[0])[:int(coeff*nb_non_repeat)]].squeeze()\n",
        "      index_for_training = torch.cat((index_non_repeat.squeeze(),slice_repeat))\n",
        "    return index_for_training.squeeze()\n",
        "\n",
        "\n",
        "class Total_loss(nn.Module):\n",
        "  def __init__(self,new_station_binary_classification) -> None:\n",
        "    super().__init__()\n",
        "    self.loss_next_station_classification = Loss_next_station_classification()\n",
        "    self.loss_time_regression = Loss_time_regression()\n",
        "    self.new_station_binary_classification=new_station_binary_classification\n",
        "    if self.new_station_binary_classification:\n",
        "      self.loss_new_station_binary_classification=Loss_new_station_binary_classification()\n",
        "\n",
        "  def forward(self, out, dict_batch, upsampling,upsampling_strategy, reg=False):\n",
        "    loss={}\n",
        "    target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "    if self.new_station_binary_classification or upsampling:\n",
        "      pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "      target=get_repetition_labels(target_pos_ids,pos_ids)\n",
        "    else:\n",
        "      pos_ids=None\n",
        "      target=None\n",
        "\n",
        "    if upsampling:\n",
        "      index_training_element=upsampling_strategy(target)\n",
        "    else:\n",
        "      index_training_element=torch.arange(0,target_pos_ids.data.shape[0])\n",
        "\n",
        "    loss[\"classification\"]=self.loss_next_station_classification(out[\"next_station\"],target_pos_ids,index_training_element)\n",
        "    loss[\"total\"]=loss[\"classification\"]\n",
        "    if self.new_station_binary_classification:\n",
        "      loss[\"new_station\"]=self.loss_new_station_binary_classification(out[\"new_station\"],target)\n",
        "      loss[\"total\"]+=loss[\"new_station\"]\n",
        "\n",
        "    if reg:\n",
        "      loss[\"time_regression\"]=self.loss_time_regression(out[\"time_regression\"],dict_batch)\n",
        "      loss[\"total\"]+=loss[\"time_regression\"]\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHCyYC32ToKU",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title evaluation\n",
        "from torch import autocast\n",
        "def evaluate(model,dataloader,upsampling,criterion,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        valid_result=criterion(out,dict_batch,upsampling,None,reg=reg)\n",
        "        valid_results=get_sum_valid_results(valid_results,valid_result)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "    valid_results=get_mean_valid_results(valid_results,nb_points)\n",
        "    valid_results[\"acc\"]=acc/nb_points\n",
        "\n",
        "    return valid_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLu25E-eTcbT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title training\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autocast\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "def train(\n",
        "          epochs_classifcation_only,\n",
        "          epochs_complete_problem,\n",
        "          input_size,\n",
        "          num_heads,\n",
        "          d_model,\n",
        "          nb_of_pos_ids,\n",
        "          num_layers_lstm,\n",
        "          lstm_layer_with_perceptron,\n",
        "          lstm_layer_with_layer_norm,\n",
        "          num_layers_transformer,\n",
        "          encoder_only,\n",
        "          output_regression_size,\n",
        "          output_classfication_size,\n",
        "          nb_batchs,\n",
        "          dropout,\n",
        "          max_len,\n",
        "          weight_decay,\n",
        "          lr,\n",
        "          learnable_pos_encoding,\n",
        "          new_station_binary_classification,\n",
        "          use_gcn,\n",
        "          vocab,hidden_dim1, hidden_dim2,\n",
        "          batch_first,\n",
        "          concatenate_features,\n",
        "          keep_input_positions,\n",
        "          upsampling,\n",
        "          upsampling_strategy,\n",
        "          epochs_new_station_only,\n",
        "          pourcentage_of_repeat_training_elment,\n",
        "          save_best_model,\n",
        "          path_best_model,\n",
        "          batch_size,\n",
        "          device):\n",
        "\n",
        "  epochs=epochs_complete_problem+ epochs_classifcation_only\n",
        "  model=Transformer_encoder_LSTM_decoder(d_model=d_model,\n",
        "                                         nb_of_pos_ids=nb_of_pos_ids,\n",
        "                                         output_regression_size=output_regression_size,\n",
        "                                         output_classfication_size=output_classfication_size,\n",
        "                                         num_layers_lstm=num_layers_lstm,\n",
        "                                         lstm_layer_with_perceptron=lstm_layer_with_perceptron,\n",
        "                                         lstm_layer_with_layer_norm=lstm_layer_with_perceptron,\n",
        "                                         num_layers_transformer=num_layers_transformer,\n",
        "                                         encoder_only=encoder_only,\n",
        "                                         nhead=num_heads,\n",
        "                                         learnable_pos_encoding=learnable_pos_encoding,\n",
        "                                         new_station_binary_classification=new_station_binary_classification,\n",
        "                                         use_gcn=use_gcn,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=hidden_dim1,\n",
        "                                         hidden_dim2=hidden_dim2,\n",
        "                                         max_len=max_len,\n",
        "                                         dropout=dropout,\n",
        "                                         batch_first = batch_first,\n",
        "                                         concatenate_features = concatenate_features,\n",
        "                                         keep_input_positions = keep_input_positions,device=device\n",
        "                                         ).to(device)\n",
        "  if save_best_model:\n",
        "    os.makedirs(path_best_model,exist_ok =True)\n",
        "  optimizer_encoder = optim.Adam( model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "  criterion = Total_loss( new_station_binary_classification = new_station_binary_classification)\n",
        "  train_losses, valid_results = {},{}\n",
        "  best_results={}\n",
        "  for epoch in range(epochs):\n",
        "    reg=epoch >= epochs_classifcation_only\n",
        "    epoch_losses={}\n",
        "    model.train()\n",
        "    i=0\n",
        "    for dict_batch in train_dataloader:\n",
        "      optimizer_encoder.zero_grad()\n",
        "      i+=1\n",
        "      if i>=nb_batchs:\n",
        "        break\n",
        "      dict_batch=set_dic_to(dict_batch,device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch, reg)\n",
        "        loss=criterion(out, dict_batch,upsampling,lambda target: upsampling_strategy(target,epoch,epochs_new_station_only,pourcentage_of_repeat_training_elment) ,reg)\n",
        "        loss[\"total\"].backward()\n",
        "        optimizer_encoder.step()\n",
        "      epoch_losses=update_epoch_losses(epoch_losses,loss)\n",
        "      dict_batch.clear()\n",
        "      loss.clear()\n",
        "      out.clear()\n",
        "      del out, loss,dict_batch\n",
        "    epoch_loss=get_epoch_loss(epoch_losses,batch_size)\n",
        "    train_losses=update_train_losses(train_losses,epoch_loss,epoch)\n",
        "    valid_result = evaluate(model,valid_dataloader,upsampling,criterion,device)\n",
        "    best_results = update_best(model,valid_result,best_results,save_best_model,path_best_model)\n",
        "    valid_results = update_valid_results(valid_results,valid_result)\n",
        "    print_results(epoch_loss,valid_result,epoch)\n",
        "\n",
        "  return best_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title utils\n",
        "\n",
        "def set_dic_to(dict_batch,device):\n",
        "  for key in dict_batch:\n",
        "    if key!=\"lengths\":\n",
        "      dict_batch[key]=dict_batch[key].to(device)\n",
        "  return dict_batch\n",
        "\n",
        "def is_better(valid_result,best_result,key):\n",
        "  match key:\n",
        "    case \"acc\":\n",
        "      return valid_result>best_result\n",
        "    case _:\n",
        "      return valid_result<best_result\n",
        "\n",
        "def update_best(model,valid_result,best_results,save_best_model,path_best_model):\n",
        "  if best_results:\n",
        "    for key in valid_result:\n",
        "      if is_better(valid_result[key],best_results[key],key):\n",
        "        best_results[key]=valid_result[key]\n",
        "        if save_best_model:\n",
        "          save_model(model,path_best_model,key)\n",
        "  else:\n",
        "    for key in valid_result:\n",
        "      best_results[key]=valid_result[key]\n",
        "      if save_best_model:\n",
        "        save_model(model,path_best_model,key)\n",
        "  return best_results\n",
        "\n",
        "def save_model(model,path_best_model,key):\n",
        "  path=os.path.join(path_best_model,key)\n",
        "  torch.save(model.state_dict(), path+\".pth\")\n",
        "\n",
        "def get_sum_valid_results(valid_result,valid_result_batch):\n",
        "  if valid_result:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]+=valid_result_batch[key].item()\n",
        "  else:\n",
        "    for key in valid_result_batch:\n",
        "      valid_result[key]=valid_result_batch[key].item()\n",
        "  return valid_result\n",
        "\n",
        "def get_mean_valid_results(sum_valid_result,nb_element):\n",
        "  for key in sum_valid_result:\n",
        "    sum_valid_result[key]/=nb_element\n",
        "\n",
        "  return sum_valid_result\n",
        "\n",
        "def update_epoch_losses(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key].item())\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key].item()]\n",
        "  return dict_of_list\n",
        "\n",
        "def update_valid_results(dict_of_list,dic):\n",
        "  if dict_of_list:\n",
        "    for key in dic:\n",
        "      dict_of_list[key].append(dic[key])\n",
        "  else:\n",
        "    for key in dic:\n",
        "      dict_of_list[key]=[dic[key]]\n",
        "  return dict_of_list\n",
        "\n",
        "def get_epoch_loss(epoch_losses,batch_size):\n",
        "\n",
        "  epoch_loss={}\n",
        "  for key in epoch_losses:\n",
        "    epoch_loss[key]=np.array(epoch_losses[key]).mean()/batch_size\n",
        "  return epoch_loss\n",
        "\n",
        "def print_results(epoch_loss,valid_result,epoch):\n",
        "\n",
        "  print(\"\\nepoch: \",epoch)\n",
        "  print(\"train :\", end=\"\\t\")\n",
        "  for key in epoch_loss:\n",
        "    print(key,epoch_loss[key], end=\"\\t\")\n",
        "  print(\"\\nvalid :\", end=\"\\t\")\n",
        "  for key in valid_result:\n",
        "    print(key,valid_result[key], end=\"\\t\")\n",
        "\n",
        "def update_train_losses(train_losses,epoch_loss,epoch):\n",
        "\n",
        "  if train_losses:\n",
        "    for key in epoch_loss:\n",
        "      if key in train_losses:\n",
        "        train_losses[key].append(epoch_loss[key])\n",
        "      else:\n",
        "        train_losses[key]=[float('nan')]*(epoch+1)+[epoch_loss[key]]\n",
        "  else:\n",
        "    for key in epoch_loss:\n",
        "      train_losses[key]=[epoch_loss[key]]\n",
        "  return train_losses"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8OL2WGr7cGZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QF4FEU_h1YtX"
      },
      "source": [
        "## Instance of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "HL0AZ-YJChyE",
        "outputId": "ccfdf139-20a5-452c-fe6c-6e36be5a95fd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-eb8eac0884b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title Titre par défaut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model=train(\n\u001b[0m\u001b[1;32m      3\u001b[0m           \u001b[0mepochs_classifcation_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs_complete_problem\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-d40eb1db95f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs_classifcation_only, epochs_complete_problem, input_size, num_heads, d_model, nb_of_pos_ids, num_layers_lstm, lstm_layer_with_perceptron, lstm_layer_with_layer_norm, num_layers_transformer, encoder_only, output_regression_size, output_classfication_size, nb_batchs, dropout, max_len, weight_decay, lr, learnable_pos_encoding, new_station_binary_classification, use_gcn, vocab, hidden_dim1, hidden_dim2, batch_first, concatenate_features, keep_input_positions, upsampling, upsampling_strategy, epochs_new_station_only, pourcentage_of_repeat_training_elment, save_best_model, path_best_model, batch_size, device)\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0mdict_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_dic_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupsampling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mupsampling_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_new_station_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpourcentage_of_repeat_training_elment\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-2dbafaf65fc1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, dic_batch, reg)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mmask_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdic_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcausal_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_square_subsequent_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_lstm\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers_transformer\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-a8f6ed0bd9ad>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m      4\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_decoder_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       return self.transformer(x,\n\u001b[0m\u001b[1;32m      7\u001b[0m                        \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                        \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    204\u001b[0m         memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask,\n\u001b[1;32m    205\u001b[0m                               is_causal=src_is_causal)\n\u001b[0;32m--> 206\u001b[0;31m         output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n\u001b[0m\u001b[1;32m    207\u001b[0m                               \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                               \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    461\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    863\u001b[0m     def _mha_block(self, x: Tensor, mem: Tensor,\n\u001b[1;32m    864\u001b[0m                    attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 865\u001b[0;31m         x = self.multihead_attn(x, mem, mem,\n\u001b[0m\u001b[1;32m    866\u001b[0m                                 \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5438\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5440\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5441\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 7.06 MiB is free. Process 61264 has 14.74 GiB memory in use. Of the allocated memory 13.99 GiB is allocated by PyTorch, and 604.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "# @title Titre par défaut\n",
        "model=train(\n",
        "          epochs_classifcation_only=100,\n",
        "          epochs_complete_problem =100,\n",
        "          input_size=2,\n",
        "          num_heads=12,\n",
        "          d_model=1200,\n",
        "          nb_of_pos_ids=len(vocab)+1,\n",
        "          num_layers_lstm=2,\n",
        "          lstm_layer_with_perceptron=False,\n",
        "          lstm_layer_with_layer_norm=False,\n",
        "          num_layers_transformer=6,\n",
        "          encoder_only=False,\n",
        "          output_regression_size=2,\n",
        "          output_classfication_size=len(vocab)+1,\n",
        "          nb_batchs=100,\n",
        "          dropout=0.1,\n",
        "          max_len=100,\n",
        "          weight_decay=0,\n",
        "          lr=1e-3,\n",
        "          learnable_pos_encoding=True,\n",
        "          new_station_binary_classification=False,\n",
        "          use_gcn=False,\n",
        "          vocab=vocab, hidden_dim1=128, hidden_dim2=256,\n",
        "          batch_first= True,\n",
        "          concatenate_features = True,\n",
        "          keep_input_positions = False,\n",
        "          upsampling=False,\n",
        "          upsampling_strategy=upsampling_strategy,\n",
        "          epochs_new_station_only=0,\n",
        "          pourcentage_of_repeat_training_elment=0.1,\n",
        "          save_best_model=True,\n",
        "          path_best_model=\"test_0.5\",\n",
        "          device=device,\n",
        "          batch_size=64\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_repeat(model,dataloader,device,reg=True):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    acc=0\n",
        "    nb_points=0\n",
        "    repeat=0\n",
        "    not_repeat=0\n",
        "    correct_not_repeat=0\n",
        "    correct_repeat=0\n",
        "    incorrect_not_repeat_as_repeat=0\n",
        "    incorrect_not_repeat=0\n",
        "    valid_results={}\n",
        "    for dict_batch in dataloader:\n",
        "      for key in dict_batch:\n",
        "        if key!=\"lengths\":\n",
        "          dict_batch[key]=dict_batch[key].to(device)\n",
        "      with autocast(device_type=device.type):\n",
        "        out=model(dict_batch,reg=reg)\n",
        "        target_pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id_target\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        acc+=(out[\"next_station\"].data.argmax(dim=1)==target_pos_ids.data).sum().item()\n",
        "        nb_points+=out[\"next_station\"].data.shape[0]\n",
        "        pred=out[\"next_station\"].data.argmax(dim=1)\n",
        "        pos_ids=torch.nn.utils.rnn.pack_padded_sequence(dict_batch[\"pos_id\"], lengths=dict_batch[\"lengths\"],batch_first=True, enforce_sorted=False)\n",
        "        for i in range(len(target_pos_ids.data)):\n",
        "          if target_pos_ids.data[i]==pos_ids.data[i]:\n",
        "            repeat+=1\n",
        "\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_repeat+=1\n",
        "          else:\n",
        "            not_repeat+=1\n",
        "            if target_pos_ids.data[i]==pred[i]:\n",
        "              correct_not_repeat+=1\n",
        "            if target_pos_ids.data[i]!=pred[i]:\n",
        "              incorrect_not_repeat+=1\n",
        "\n",
        "          if pred[i]==pos_ids.data[i] and target_pos_ids.data[i]!=pos_ids.data[i]:\n",
        "            incorrect_not_repeat_as_repeat+=1\n",
        "    print(nb_points,\"repeat: \",repeat,\" not_repeat: \",not_repeat,\" correct_repeat/repeat: \",correct_repeat/repeat,\" correct_not_repeat/not_repeat: \",correct_not_repeat/not_repeat,incorrect_not_repeat_as_repeat/incorrect_not_repeat)\n",
        "    return valid_results"
      ],
      "metadata": {
        "id": "HClPVCKb59Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=768,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=0,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKyqswVm-Flq",
        "outputId": "4f8a2d24-1d1d-4d5e-da84-30957736cbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6509191238701378  correct_not_repeat/not_repeat:  0.2762021385930769 0.4746223564954683\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=12,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufqL-c1e27Vm",
        "outputId": "5e436410-45a5-4c71-da92-f5b40a030112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.572683570872406  correct_not_repeat/not_repeat:  0.24545712973693992 0.38929461542920074\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=10,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqLMoo9hpA82",
        "outputId": "cc34ad4e-47d5-4efd-d99e-5c06837a607e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5703307491790515  correct_not_repeat/not_repeat:  0.22293411471430757 0.40792435839711844\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=600,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYc14HpTkU2z",
        "outputId": "36544e2a-7d0b-4787-df90-4160a18680cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.44894038389925184  correct_not_repeat/not_repeat:  0.29502962979160746 0.2873538261112317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "tI1PRax29Fqw",
        "outputId": "d9d5ea9f-4378-49ec-be25-2fe2ce5e37f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d5fdb97ca0b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-17df1714a1da>\u001b[0m in \u001b[0;36mevaluate_repeat\u001b[0;34m(model, dataloader, device, reg)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpos_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pos_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mtarget_pos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mpos_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrepeat\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucdMSE-0T3-s",
        "outputId": "4f766291-2702-431e-d059-241a1a1ccfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7462507193879279  correct_not_repeat/not_repeat:  0.23080623646979073 0.5669490561746645\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=0,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEt-Bw9FgvDq",
        "outputId": "70b7fed7-4cc6-495a-de11-57721bf5d02d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4839957344527574  correct_not_repeat/not_repeat:  0.35011261507511315 0.2974764468371467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=1008,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=6,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test_0.5/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaphByRZOIo",
        "outputId": "13cfb863-5bb6-43a5-f104-7ce207f93c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6290835844138258  correct_not_repeat/not_repeat:  0.26852681988148086 0.44345460524349045\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=2,\n",
        "                                         num_layers_transformer=5,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGE1rXtIIEpE",
        "outputId": "775055f6-3dae-4158-fdc4-078b4adba7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.5781001387995531  correct_not_repeat/not_repeat:  0.3046073779274453 0.4137291280148423\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer_encoder_LSTM_decoder(d_model=888,\n",
        "                                         nb_of_pos_ids=len(vocab)+1,\n",
        "                                         output_regression_size=2,\n",
        "                                         output_classfication_size=len(vocab)+1,\n",
        "                                         num_layers_lstm=3,\n",
        "                                         num_layers_transformer=6,\n",
        "                                         encoder_only=False,\n",
        "                                         nhead=12,\n",
        "                                         learnable_pos_encoding=True,\n",
        "                                         new_station_binary_classification=False,\n",
        "                                         use_gcn=True,\n",
        "                                         vocab=vocab,\n",
        "                                         hidden_dim1=128,\n",
        "                                         hidden_dim2=256,\n",
        "                                         max_len=100,\n",
        "                                         dropout=0.1,\n",
        "                                         batch_first = True,\n",
        "                                         concatenate_features = False,\n",
        "                                         keep_input_positions = False,device=device\n",
        "                                         ).to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"test/acc.pth\"))\n",
        "evaluate_repeat(model,test_dataloader,device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 788
        },
        "id": "QPBs0PUrKUra",
        "outputId": "196fe7cd-25e3-479d-8c4b-b6a83ccd3977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-cfd962976380>:15: FutureWarning: `use_index` defaults to False but will default to True in future. Set True/False directly to control this behavior and silence this warning\n",
            "  delaunay = weights.Rook.from_dataframe(cells)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.linear1.weight\", \"transformer_model.transformer.decoder.layers.5.linear1.bias\", \"transformer_model.transformer.decoder.layers.5.linear2.weight\", \"transformer_model.transformer.decoder.layers.5.linear2.bias\", \"transformer_model.transformer.decoder.layers.5.norm1.weight\", \"transformer_model.transformer.decoder.layers.5.norm1.bias\", \"transformer_model.transformer.decoder.layers.5.norm2.weight\", \"transformer_model.transformer.decoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.norm3.weight\", \"transformer_model.transformer.decoder.layers.5.norm3.bias\", \"transformer_lstm__list.2.layer_normalisation.weight\", \"transformer_lstm__list.2.layer_normalisation.bias\", \"transformer_lstm__list.2.lstm.weight_ih_l0\", \"transformer_lstm__list.2.lstm.weight_hh_l0\", \"transformer_lstm__list.2.lstm.bias_ih_l0\", \"transformer_lstm__list.2.lstm.bias_hh_l0\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_in.bias\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.weight\", \"transformer_lstm__list.2.mlp.linear_perceptron_out.bias\". ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-10bf83ce1050>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                          ).to(device)\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test/acc.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mevaluate_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2153\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Transformer_encoder_LSTM_decoder:\n\tMissing key(s) in state_dict: \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.encoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.encoder.layers.5.linear1.weight\", \"transformer_model.transformer.encoder.layers.5.linear1.bias\", \"transformer_model.transformer.encoder.layers.5.linear2.weight\", \"transformer_model.transformer.encoder.layers.5.linear2.bias\", \"transformer_model.transformer.encoder.layers.5.norm1.weight\", \"transformer_model.transformer.encoder.layers.5.norm1.bias\", \"transformer_model.transformer.encoder.layers.5.norm2.weight\", \"transformer_model.transformer.encoder.layers.5.norm2.bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.self_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.in_proj_bias\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.weight\", \"transformer_model.transformer.decoder.layers.5.multihead_attn.out_proj.bias\", \"transformer_model.transformer.decoder.layers.5...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7cIy9KQFjv8",
        "outputId": "3ea5d475-a56f-4118-d568-2c020806ed64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8564101696062832  correct_not_repeat/not_repeat:  0.09422492401215805 0.8021341316208778\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.643730131126935, 2.2387092113494873, 4.385555267333984)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K06MhCDWcaF9",
        "outputId": "4a84b9d7-b594-41b9-bca7-acacf62010b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.6269508107925116  correct_not_repeat/not_repeat:  0.24020904856661782 0.4275024463247568\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5190344566683142, 3.329756021499634, 2.0473709106445312)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonRxU5b96oc",
        "outputId": "baba1be5-861e-44f1-95fb-808d9cd6b5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7019025694844104  correct_not_repeat/not_repeat:  0.2555815529946863 0.5174044590664747\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5773612306040138, 2.5278093814849854, 2.7385761737823486)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz2zFYSsAroq",
        "outputId": "10b15668-6b3b-44d9-eb4f-c28a02cbd09d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.7179745421307424  correct_not_repeat/not_repeat:  0.2436203013273272 0.5562012142237641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.5856108172094187, 2.1441447734832764, 1.2037302255630493)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNlGehwEQtfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "378de12f-1ebf-44f6-b4b6-e57d19a2bb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8789227800534886  correct_not_repeat/not_repeat:  0.11254947409853272 0.8136211314803864\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6650741059388481, 1.82258939743042, 2.1341636180877686)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejz7LOwNTZ-k",
        "outputId": "5d16ae15-48ea-4ff8-a2e7-718c1be73c7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.8187565591252243  correct_not_repeat/not_repeat:  0.1462683956178522 0.7372573126376722\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6311055788439596, 2.027265787124634, 2.6414260864257812)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9yHv6HjJ-N",
        "outputId": "ba6f7b9e-b88f-407c-d22b-1e6913f0e56b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.3789143166661024  correct_not_repeat/not_repeat:  0.3284642802475345 0.28316509280364704\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3648367472709855, 2.700303077697754, 3.602348566055298)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_classification=torch.nn.CrossEntropyLoss(ignore_index=len(vocab))\n",
        "criterion_regression=mse_loss = nn.MSELoss(reduction='none')\n",
        "evaluate(model,test_dataloader,criterion_classification,criterion_regression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkLWBTILrc94",
        "outputId": "c04e1913-1724-4df4-e563-1801d24ca813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched src_key_padding_mask and mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163887 repeat:  118156  not_repeat:  45731  correct_repeat/repeat:  0.4562527506009005  correct_not_repeat/not_repeat:  0.30379829874702063 0.3240153275959545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4137118868488654, 2.806699752807617, 3.5753602981567383)"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtscCJTUkD0R",
        "outputId": "d2e3c16a-065e-4503-fa5c-4f991dea2ff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7411692032"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if None:\n",
        "  print(1)"
      ],
      "metadata": {
        "id": "42I-z2ls4-Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision tree classifier"
      ],
      "metadata": {
        "id": "wIjYebkGJ8i8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_without_pos_id=[]\n",
        "pos_id_targets=[]\n",
        "pos_ids=[]\n",
        "time_targets=[]\n",
        "for user in list_users:\n",
        "  inputs_without_pos_id.append(user[\"input\"])\n",
        "  pos_id_targets.append(user[\"pos_id_target\"])\n",
        "  time_targets.append(user[\"time_target\"])\n",
        "  pos_ids.append(user[\"pos_id\"])\n",
        "print(inputs_without_pos_id[0].shape,pos_id_targets[0].shape,pos_ids[0].shape)\n",
        "\n",
        "#delete inputs which have less than 5 elements\n",
        "inputs_without_pos_id=[inputs for inputs in inputs_without_pos_id if inputs.shape[0]>=10]\n",
        "pos_id_targets=[pos_id for pos_id in pos_id_targets if pos_id.shape[0]>=10]\n",
        "time_targets=[time for time in time_targets if time.shape[0]>=10]\n",
        "pos_ids=[pos_id for pos_id in pos_ids if pos_id.shape[0]>=10]\n",
        "\n",
        "print(len(pos_id_targets))"
      ],
      "metadata": {
        "id": "5qdHh6Ts5Bn1",
        "outputId": "9645164c-661a-4dec-a12b-fee490c09e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11, 2]) torch.Size([11]) torch.Size([11])\n",
            "1218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_pos_id=[]\n",
        "for inp,pos in zip(inputs_without_pos_id,pos_ids):\n",
        "  inputs_with_pos_id.append(torch.cat([inp, pos.unsqueeze(1)],dim=1))"
      ],
      "metadata": {
        "id": "1j_YoqAlmcXk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs_with_pos_id_and_time_target=[]\n",
        "# for inp,tim in zip(inputs_with_pos_id,time_targets):\n",
        "#   inputs_with_pos_id_and_time_target.append(torch.cat((inp, tim), dim=1))"
      ],
      "metadata": {
        "id": "Cj4TyCpDm_Zt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_pos_id_user_id=[]\n",
        "for idx,inp in enumerate(inputs_with_pos_id):\n",
        "  inputs_with_pos_id_user_id.append(torch.cat([torch.tensor([idx]*inp.shape[0]).unsqueeze(1),inp],dim=1))\n",
        "  # inputs_with_pos_id_user_id.append(torch.cat([inp, torch.ones(inp.size(0),1)*idx],dim=1))\n",
        "inputs_with_pos_id_user_id[0]"
      ],
      "metadata": {
        "id": "Hqe9GmrNKHpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59dfb252-6833-4cac-bfe8-16c27c73f848"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02],\n",
              "        [ 0.0000e+00, -3.6774e-02,  7.9402e-02,  8.7400e+02],\n",
              "        [ 0.0000e+00, -3.0587e-02,  7.7556e-02,  2.3700e+02],\n",
              "        [ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02],\n",
              "        [ 0.0000e+00, -3.1970e-02,  8.3483e-02,  5.8000e+02],\n",
              "        [ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02],\n",
              "        [ 0.0000e+00, -3.0587e-02,  7.7556e-02,  2.3700e+02],\n",
              "        [ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02],\n",
              "        [ 0.0000e+00, -2.5953e-02,  7.6560e-02,  1.1020e+03],\n",
              "        [ 0.0000e+00, -3.0587e-02,  7.7556e-02,  2.3700e+02],\n",
              "        [ 0.0000e+00, -3.6774e-02,  7.9402e-02,  8.7400e+02]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs_with_pos_id_user_id_target=[]\n",
        "# for idx,inp in zip(inputs_with_pos_id_user_id, pos_id_targets):\n",
        "#   inputs_with_pos_id_user_id_target.append(torch.cat([idx, inp.unsqueeze(1)],dim=1))\n",
        "# inputs_with_pos_id_user_id_target[0]"
      ],
      "metadata": {
        "id": "jl3UvwCh0Cfv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_pos_id = inputs_with_pos_id_user_id"
      ],
      "metadata": {
        "id": "_Iov5qQCKOEf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the data into training and testing sets\n",
        "#Get the inputs and targets\n",
        "inputs = torch.cat(inputs_with_pos_id,dim=0)\n",
        "pos_id_targets = torch.cat(pos_id_targets,dim=0)\n",
        "print(len(inputs), len(pos_id_targets) )\n",
        "print(inputs[0])\n",
        "print(pos_id_targets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HQymYz7K0Z9",
        "outputId": "05c96f8d-e8e5-477d-98a3-5cfa21ada599"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50108 50108\n",
            "tensor([ 0.0000e+00, -2.6449e-02,  8.2146e-02,  5.2800e+02])\n",
            "tensor(874)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "ExA8oZy4KjUW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do cross validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputs, pos_id_targets, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "id": "DCa_zL0Oe3hI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "#got best decision tree classifier\n",
        "# 'criterion': ['gini', 'entropy'],\n",
        "parameters = { 'criterion': ['gini', 'entropy'], 'max_depth': [ i for i in range(10,100,20)], 'min_samples_split': [ i for i in range(10,100,20)], 'splitter':['best', 'random']}\n",
        "best_score = -1\n",
        "# for criterion in parameters['criterion']:\n",
        "for criterion in parameters['criterion']:\n",
        "  for max_depth in parameters['max_depth']:\n",
        "    for min_samples_split in parameters['min_samples_split']:\n",
        "      for splitter in parameters['splitter']:\n",
        "        clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, splitter=splitter, random_state=12)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        score = f1_score(y_test, y_pred, average='weighted')\n",
        "        if score > best_score:\n",
        "          best_score = score\n",
        "          best_parameters = {'criterion': criterion, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'splitter':splitter}"
      ],
      "metadata": {
        "id": "gLpUTDQXKkg4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "6OCspM3UK62N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81bd0464-38ad-4072-faf3-c487ae3a7c74"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score:  0.3939867748501307\n",
            "{'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10, 'splitter': 'random'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross score:  [0.19957234 0.19900214 0.18987883 0.18446187 0.18717035]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Créer un modèle d'arbre de décision\n",
        "# model = RandomForestClassifier(n_estimators=30, random_state=42)\n",
        "model = DecisionTreeClassifier()\n",
        "# model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "# Entraîner le modèle sur les données d'entraînement\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Faire des prédictions sur l'ensemble de test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculer l'exactitude\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Exactitude :\", accuracy)\n",
        "cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "print(\"cross score: \",cv_scores)"
      ],
      "metadata": {
        "id": "6lIeX2pDQx9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb85fb8-88f8-48f7-89df-8a99e7df1f0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exactitude : 0.4115612319563627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross score:  [0.41539558 0.4209551  0.42252316 0.41553813 0.41853172]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Encode the target\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "# y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "# model = xgb.XGBClassifier() #, gpu_id=0\n",
        "# model.fit(X_train, y_train_encoded)\n",
        "# score = model.score(X_test, y_test_encoded)\n",
        "\n",
        "# print(\"score: \",score)\n",
        "# print(best_parameters)\n",
        "\n",
        "# #Predict the next position\n",
        "# y_pred = model.predict(X_test)\n",
        "\n",
        "# #Evaluate the model\n",
        "# print(\"Accuracy:\", metrics.accuracy_score(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUXy-DJKpyPx",
        "outputId": "fd24f4db-43f3-4936-cada-f4f6b9c0032f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score:  0.038114953726254265\n",
            "{'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 10, 'splitter': 'best'}\n",
            "Accuracy: 0.038114953726254265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOmBJAknRSHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a dictionary with the number of data for each class\n",
        "class_count={}\n",
        "print(vocab)\n",
        "for pos_id in pos_ids:\n",
        "  for id in pos_id:\n",
        "    if id.item() in class_count:\n",
        "      class_count[id.item()]+=1\n",
        "    else:\n",
        "      class_count[id.item()]=1\n",
        "print(class_count)\n",
        "print(len(class_count))\n",
        "print(max(class_count.keys()))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#See the tendency of the number of connections for each class\n",
        "print(\"min number of connections for each class\"\n",
        "      ,min(class_count.values())\n",
        "      ,min(class_count,key=class_count.get))\n",
        "print(\"max number of connections for each class\"\n",
        "      ,max(class_count.values())\n",
        "      ,max(class_count,key=class_count.get))\n",
        "\n",
        "plt.plot(list(class_count.values()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2UOJ4O63xDu",
        "outputId": "3f5b1046-7e58-44a9-c9f7-d557fa6b5586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(30.935314, 121.733322): 1, (31.144812, 121.119606): 2, (31.381763, 121.301878): 3, (31.056648, 121.404326): 4, (31.100034, 121.663489): 5, (31.040378, 121.255563): 6, (31.231508, 121.577691): 7, (31.036429, 121.324198): 8, (31.33775, 121.306733): 9, (31.298668, 121.541944): 10, (31.187254, 121.536259): 11, (30.889377, 121.176142): 12, (29.526266, 119.910488): 13, (31.327319, 121.462047): 14, (31.113744, 121.341878): 15, (31.250826, 121.578689): 16, (31.338059, 121.289933): 17, (31.349748, 121.506246): 18, (31.246059, 121.362706): 19, (31.414397, 121.481621): 20, (31.078791, 121.422082): 21, (31.129955, 121.336848): 22, (31.210033, 121.405714): 23, (31.355465, 121.412867): 24, (31.254534, 121.34772): 25, (31.202912, 121.712449): 26, (31.319806, 121.526248): 27, (31.282271, 121.524511): 28, (31.215246, 121.604329): 29, (31.025227, 121.44251): 30, (30.808567, 121.29074): 31, (34.638922, 119.467017): 32, (31.094143, 121.488864): 33, (31.242157, 121.568785): 34, (31.319501, 121.584673): 35, (31.247519, 121.322265): 36, (31.300084, 121.504034): 37, (31.318646, 121.53899): 38, (31.021462, 121.321255): 39, (31.255582, 121.363523): 40, (31.285741, 121.626244): 41, (31.05815, 120.970188): 42, (41.677262, 125.960124): 43, (31.263678, 121.468663): 44, (31.273451, 121.681542): 45, (31.301602, 121.487289): 46, (31.116021, 121.287621): 47, (31.205839, 121.462046): 48, (31.140384, 121.620758): 49, (31.387987, 121.495216): 50, (31.082701, 121.448694): 51, (31.114131, 121.394475): 52, (31.134254, 121.280533): 53, (31.152157, 121.514858): 54, (31.420016, 121.44113): 55, (30.716187, 121.349498): 56, (31.124664, 121.323455): 57, (30.866289, 121.103027): 58, (30.914669, 121.471638): 59, (31.215664, 121.46829): 60, (31.239025, 121.551622): 61, (31.174661, 121.52822): 62, (31.031459, 121.300841): 63, (31.066368, 121.2082): 64, (31.158843, 121.127268): 65, (30.898595, 121.030128): 66, (31.242627, 121.730079): 67, (31.230933, 121.347295): 68, (31.253601, 121.4782): 69, (31.117844, 121.402074): 70, (31.232877, 121.48753): 71, (30.920156, 121.488943): 72, (31.184888, 121.510295): 73, (31.111396, 121.379353): 74, (31.16691, 121.115166): 75, (31.19143, 121.411411): 76, (31.219114, 121.423499): 77, (31.117005, 121.271194): 78, (31.171101, 121.507145): 79, (30.731903, 121.334173): 80, (31.307268, 121.521916): 81, (31.370336, 121.265507): 82, (31.03333, 121.427385): 83, (31.159704, 121.457647): 84, (31.444014, 121.25661): 85, (31.183857, 121.637321): 86, (31.236856, 121.40554): 87, (31.078561, 121.564629): 88, (30.943662, 121.155665): 89, (31.264146, 121.532591): 90, (31.109649, 121.402441): 91, (31.31394, 121.382812): 92, (30.97935, 121.815249): 93, (31.380093, 121.517779): 94, (31.123394, 121.278015): 95, (31.146522, 121.352501): 96, (31.17939, 121.754087): 97, (30.74139, 121.380141): 98, (31.142218, 121.413767): 99, (30.948712, 121.365593): 100, (31.158515, 121.1384): 101, (31.28711, 121.226904): 102, (31.062583, 121.44986): 103, (31.057696, 121.207516): 104, (31.221591, 121.351862): 105, (31.269868, 121.533249): 106, (31.150797, 121.728702): 107, (31.242948, 121.608734): 108, (31.131416, 121.429704): 109, (31.114038, 121.415096): 110, (31.228014, 121.477667): 111, (31.32712, 121.254377): 112, (31.236104, 121.375177): 113, (31.217074, 121.510123): 114, (30.899117, 121.175731): 115, (31.042021, 121.204506): 116, (31.109666, 121.052906): 117, (31.103853, 121.810034): 118, (31.456615, 121.221022): 119, (31.36124, 121.451839): 120, (31.321416, 121.215): 121, (31.27975, 121.497791): 122, (31.011886, 121.234971): 123, (31.273767, 121.222048): 124, (30.746286, 121.344028): 125, (31.254912, 121.371838): 126, (29.263844, 115.023159): 127, (30.751838, 121.360056): 128, (31.300119, 121.532041): 129, (30.716044, 121.291037): 130, (31.188696, 121.181232): 131, (31.265321, 121.503333): 132, (31.100465, 121.401931): 133, (31.042149, 121.539262): 134, (31.272151, 121.375814): 135, (31.038201, 121.216468): 136, (31.353347, 121.441038): 137, (31.241274, 121.36272): 138, (31.116103, 121.43571): 139, (31.357773, 121.508693): 140, (31.17255, 121.363745): 141, (31.382497, 121.548578): 142, (31.246106, 121.443663): 143, (36.406412, 102.003965): 144, (31.200783, 121.446011): 145, (31.180753, 121.749174): 146, (31.00153, 121.235235): 147, (30.892415, 121.023553): 148, (31.155403, 121.57642): 149, (31.342336, 121.605688): 150, (31.177646, 121.555355): 151, (31.013227, 121.552205): 152, (31.221052, 121.503967): 153, (31.050708, 121.217094): 154, (31.150662, 121.434931): 155, (31.269517, 121.498812): 156, (31.305522, 121.483135): 157, (30.919334, 121.475563): 158, (31.162868, 121.448422): 159, (31.292661, 121.305822): 160, (31.30092, 121.16892): 161, (31.250271, 121.406561): 162, (31.337091, 121.543621): 163, (31.339114, 121.321261): 164, (31.038291, 121.194794): 165, (31.370654, 121.459272): 166, (31.360292, 121.583955): 167, (30.915722, 121.763195): 168, (31.203978, 121.423912): 169, (31.235929, 121.449652): 170, (31.108567, 121.766873): 171, (31.162282, 121.426688): 172, (31.127217, 121.335543): 173, (31.312192, 121.456456): 174, (30.991472, 121.430726): 175, (31.260959, 121.562594): 176, (31.209182, 121.752433): 177, (31.098558, 121.377147): 178, (31.240601, 121.442307): 179, (30.824298, 121.363989): 180, (31.112344, 121.480257): 181, (31.182198, 121.390214): 182, (31.019691, 121.249497): 183, (31.24822, 121.296297): 184, (31.112548, 121.538798): 185, (31.242841, 121.391309): 186, (31.254166, 121.224476): 187, (31.152177, 121.172752): 188, (31.154923, 121.36045): 189, (31.301132, 121.177821): 190, (31.166408, 121.569203): 191, (31.247155, 121.23112): 192, (31.146311, 121.399951): 193, (31.054594, 121.339133): 194, (31.273019, 121.307617): 195, (31.355885, 121.375057): 196, (31.119557, 121.036331): 197, (31.268642, 121.332479): 198, (31.357217, 121.439635): 199, (22.522803, 114.218796): 200, (31.069261, 121.375484): 201, (31.279012, 121.514374): 202, (31.203552, 121.371544): 203, (31.19206, 121.438501): 204, (31.120864, 121.389141): 205, (31.280262, 121.364633): 206, (31.266666, 121.598298): 207, (31.343106, 121.455682): 208, (31.198093, 121.126168): 209, (31.27795, 121.538718): 210, (31.232095, 121.20583): 211, (31.207125, 121.299924): 212, (30.739797, 121.32739): 213, (31.10888, 121.390919): 214, (31.155848, 121.383402): 215, (30.936752, 121.470783): 216, (31.030308, 121.650922): 217, (31.254308, 121.459288): 218, (31.321039, 121.453913): 219, (31.232639, 121.705254): 220, (31.147783, 121.366475): 221, (31.378285, 121.436137): 222, (31.178359, 121.279284): 223, (31.209025, 121.616514): 224, (31.175781, 121.643301): 225, (31.160079, 121.37079): 226, (31.308964, 121.403345): 227, (30.718302, 121.314644): 228, (31.190389, 121.457678): 229, (31.252404, 121.435035): 230, (31.202075, 121.187355): 231, (31.251584, 121.4917): 232, (31.235454, 121.172523): 233, (31.205759, 121.26316): 234, (30.939177, 121.223182): 235, (31.325382, 121.553285): 236, (31.176998, 121.423586): 237, (31.069946, 121.719641): 238, (31.254981, 121.623923): 239, (31.031203, 121.65009): 240, (31.115176, 121.36579): 241, (31.2077, 121.669382): 242, (31.314038, 121.509762): 243, (31.148848, 121.314851): 244, (31.260465, 121.461228): 245, (31.211485, 121.403238): 246, (31.274169, 121.221302): 247, (31.262756, 121.625447): 248, (31.230109, 121.434393): 249, (31.286022, 121.497436): 250, (31.163705, 121.503747): 251, (30.910136, 121.843044): 252, (30.960682, 121.447242): 253, (31.263651, 121.573401): 254, (31.042056, 121.720903): 255, (30.919796, 121.459621): 256, (31.202395, 121.6278): 257, (31.045101, 121.223676): 258, (31.227795, 121.254172): 259, (31.326648, 121.304442): 260, (31.099642, 121.449578): 261, (31.368461, 121.372259): 262, (30.891482, 121.739107): 263, (31.051898, 121.769904): 264, (31.262048, 121.458128): 265, (31.159756, 121.525433): 266, (30.88883, 121.130607): 267, (31.402828, 121.48779): 268, (31.270626, 121.5908): 269, (31.276718, 121.371696): 270, (31.355737, 121.403678): 271, (31.337218, 121.465181): 272, (31.215428, 121.660427): 273, (31.015724, 121.40907): 274, (31.324464, 121.387532): 275, (31.273045, 121.546148): 276, (31.134786, 121.444969): 277, (31.175318, 121.181299): 278, (30.934254, 121.279662): 279, (31.143965, 121.573343): 280, (31.201259, 121.40143): 281, (30.967774, 121.447499): 282, (31.252105, 121.56398): 283, (31.201251, 121.561484): 284, (31.030356, 121.535809): 285, (31.260814, 121.490337): 286, (31.423448, 121.433415): 287, (30.988441, 121.87486): 288, (31.086136, 121.507054): 289, (31.211591, 121.499553): 290, (31.268894, 121.692432): 291, (30.900013, 121.049649): 292, (31.247688, 121.528106): 293, (31.176978, 121.499522): 294, (31.303976, 121.474504): 295, (31.294508, 121.537685): 296, (31.018176, 121.418004): 297, (31.228755, 121.523626): 298, (30.723654, 121.330778): 299, (30.970389, 121.451558): 300, (31.359311, 121.310645): 301, (31.260835, 121.366953): 302, (31.234496, 121.521516): 303, (31.137509, 121.40768): 304, (31.211598, 121.439406): 305, (31.104981, 121.360938): 306, (31.188821, 121.387044): 307, (31.143774, 121.519955): 308, (31.168853, 121.394657): 309, (31.346682, 121.49352): 310, (34.689694, 112.407126): 311, (31.053362, 121.431405): 312, (30.999098, 121.447826): 313, (31.311329, 121.495756): 314, (31.255942, 121.38141): 315, (31.246345, 121.45371): 316, (31.096325, 121.341209): 317, (31.382812, 121.242351): 318, (31.19706, 121.095538): 319, (26.139329, 103.078562): 320, (31.210891, 121.3779): 321, (31.125695, 121.46401): 322, (30.889117, 121.319998): 323, (31.242837, 121.455083): 324, (31.142941, 121.443942): 325, (31.163459, 121.574025): 326, (31.250332, 121.594504): 327, (31.233973, 121.41641): 328, (30.847557, 121.226776): 329, (31.26484, 121.498029): 330, (31.273183, 121.488391): 331, (31.216359, 121.397227): 332, (31.297527, 121.376): 333, (31.286632, 121.552094): 334, (31.141245, 121.500503): 335, (31.192915, 121.682688): 336, (31.121932, 121.33481): 337, (31.069676, 121.773361): 338, (31.06623, 121.321933): 339, (30.983162, 121.054057): 340, (31.206194, 121.456569): 341, (30.887807, 121.346419): 342, (31.204999, 121.471193): 343, (31.131231, 121.398758): 344, (30.995864, 121.568444): 345, (31.218201, 121.487151): 346, (31.090076, 121.406999): 347, (31.007327, 121.431151): 348, (31.182483, 121.5762): 349, (31.111607, 121.324796): 350, (31.2355, 121.417315): 351, (31.276178, 121.553996): 352, (31.184319, 121.230946): 353, (31.267266, 121.385292): 354, (31.261563, 121.376836): 355, (31.214683, 121.551574): 356, (31.318661, 121.194317): 357, (31.238593, 121.450062): 358, (31.019246, 121.250749): 359, (31.000658, 121.659325): 360, (31.176379, 121.397706): 361, (31.13107, 121.298444): 362, (31.214561, 121.298904): 363, (31.2676, 121.541803): 364, (31.273242, 121.499598): 365, (30.79706, 121.431713): 366, (31.231687, 121.32576): 367, (30.932475, 121.692651): 368, (31.233761, 121.670586): 369, (31.26604, 121.399004): 370, (31.140979, 121.586245): 371, (31.284148, 121.481157): 372, (31.194194, 121.456589): 373, (30.739639, 121.373695): 374, (31.288777, 121.390078): 375, (31.405231, 121.506638): 376, (31.31724, 121.237444): 377, (31.056596, 121.721736): 378, (31.348418, 121.427796): 379, (31.133956, 121.526289): 380, (31.224385, 121.143509): 381, (31.456258, 121.413525): 382, (30.900531, 121.399787): 383, (30.986462, 121.409757): 384, (31.092058, 121.38753): 385, (31.04139, 121.741651): 386, (31.018376, 121.40507): 387, (31.095884, 121.350551): 388, (31.105352, 121.438535): 389, (31.230287, 121.557116): 390, (31.149677, 121.729053): 391, (31.246527, 121.534982): 392, (31.395915, 121.363062): 393, (31.27431, 121.448589): 394, (31.102445, 121.413911): 395, (31.174053, 121.282683): 396, (31.189335, 121.781953): 397, (31.287814, 121.355886): 398, (31.242514, 121.493696): 399, (31.225266, 121.444323): 400, (31.370388, 121.187891): 401, (31.214209, 121.659493): 402, (31.109806, 121.242241): 403, (31.119806, 121.071074): 404, (31.206547, 121.306923): 405, (31.190168, 121.467092): 406, (31.306089, 121.474725): 407, (31.260555, 121.410883): 408, (31.173856, 121.350522): 409, (31.243013, 121.416207): 410, (30.877822, 121.548349): 411, (31.184979, 121.31046): 412, (31.337262, 121.473928): 413, (31.33167, 121.440751): 414, (30.73407, 121.350673): 415, (31.395438, 121.472743): 416, (30.98383, 121.235077): 417, (31.181381, 121.688934): 418, (31.073061, 120.970637): 419, (31.378631, 121.277965): 420, (31.319077, 121.331853): 421, (30.90387, 121.433071): 422, (31.326138, 121.48973): 423, (31.255905, 121.430231): 424, (31.157536, 121.203337): 425, (31.238561, 121.455041): 426, (31.120602, 121.581473): 427, (31.365408, 121.245027): 428, (31.206089, 121.105074): 429, (31.108876, 121.345256): 430, (31.262746, 121.517701): 431, (31.229334, 121.368291): 432, (31.260165, 121.541462): 433, (31.209905, 121.37472): 434, (31.198952, 121.70244): 435, (31.189701, 121.543143): 436, (31.344291, 121.487858): 437, (31.265, 121.48958): 438, (31.208108, 121.41483): 439, (31.280339, 121.402957): 440, (31.015959, 121.227509): 441, (31.197909, 121.426542): 442, (30.938138, 121.327668): 443, (31.19232, 121.142889): 444, (31.009423, 121.245898): 445, (31.163432, 121.354451): 446, (35.379598, 116.072359): 447, (31.207205, 121.482691): 448, (31.019982, 121.244295): 449, (31.086916, 121.387535): 450, (31.233201, 121.469528): 451, (31.311942, 121.54204): 452, (31.217372, 121.460724): 453, (30.908583, 121.656169): 454, (31.204614, 121.432408): 455, (31.057069, 121.599172): 456, (31.164195, 121.330502): 457, (31.200882, 121.446511): 458, (31.269929, 121.453419): 459, (31.182415, 121.381272): 460, (31.276317, 121.524794): 461, (31.12408, 121.716759): 462, (31.277512, 121.287969): 463, (31.25007, 121.546845): 464, (31.080824, 121.390896): 465, (31.196266, 121.537132): 466, (31.13756, 121.339005): 467, (31.263762, 121.481215): 468, (31.356238, 121.568226): 469, (31.266679, 121.539067): 470, (31.167947, 121.670231): 471, (30.87706, 121.856429): 472, (31.075231, 121.436269): 473, (31.252264, 121.38269): 474, (30.844397, 121.528845): 475, (31.186465, 121.429887): 476, (30.998611, 121.216212): 477, (31.270362, 121.612773): 478, (31.396351, 121.44222): 479, (30.874287, 121.67603): 480, (31.344675, 121.443963): 481, (31.211107, 121.59082): 482, (31.455174, 121.413981): 483, (31.241739, 121.537288): 484, (30.941447, 121.293964): 485, (31.371694, 121.443832): 486, (31.25393, 121.455087): 487, (31.19107, 121.554321): 488, (31.237872, 121.470259): 489, (31.125467, 121.581832): 490, (30.715526, 121.351073): 491, (31.259633, 121.594477): 492, (31.058092, 121.235964): 493, (30.903508, 121.331287): 494, (31.214059, 121.37764): 495, (31.025763, 121.541983): 496, (31.24448, 121.37062): 497, (30.93817, 121.865652): 498, (31.352538, 121.490272): 499, (31.208346, 121.544892): 500, (31.195139, 121.385973): 501, (31.285698, 121.612262): 502, (31.265326, 121.394456): 503, (31.246097, 121.39928): 504, (31.143297, 121.132883): 505, (31.251239, 121.428578): 506, (31.244102, 121.437744): 507, (31.155384, 121.771585): 508, (30.867598, 121.198612): 509, (31.17023, 121.337413): 510, (31.274536, 121.097082): 511, (31.382771, 121.291756): 512, (31.154875, 121.131226): 513, (31.296405, 121.392803): 514, (31.116903, 121.595022): 515, (30.983141, 121.540945): 516, (31.282514, 121.343407): 517, (31.163201, 121.11139): 518, (31.058407, 121.330724): 519, (31.294393, 121.497298): 520, (31.150483, 121.491401): 521, (30.860668, 121.470193): 522, (31.190301, 121.566862): 523, (31.158559, 121.319801): 524, (31.29992, 121.669771): 525, (31.201279, 121.709485): 526, (31.073134, 121.317814): 527, (31.179835, 121.429502): 528, (31.268663, 121.428101): 529, (31.134548, 121.39095): 530, (30.900529, 121.246804): 531, (31.148773, 121.387158): 532, (31.151539, 121.41109): 533, (31.152182, 121.564661): 534, (31.132785, 121.413457): 535, (31.272162, 121.459351): 536, (31.452354, 121.33861): 537, (31.213015, 121.547755): 538, (31.312291, 121.46631): 539, (31.03732, 121.18963): 540, (31.220649, 121.35994): 541, (31.118723, 121.248449): 542, (31.236728, 121.42407): 543, (31.223004, 121.465956): 544, (31.222233, 121.623547): 545, (31.042415, 121.482986): 546, (31.315825, 121.532151): 547, (31.220434, 121.408625): 548, (31.16346, 121.405127): 549, (31.036623, 121.377512): 550, (31.258255, 121.471554): 551, (31.045516, 121.216817): 552, (31.29092, 121.359453): 553, (31.236862, 121.355595): 554, (31.196135, 121.558528): 555, (31.26338, 121.433161): 556, (31.142865, 121.330843): 557, (31.159304, 121.358718): 558, (31.10114, 121.181649): 559, (31.323091, 121.364299): 560, (31.223665, 121.458791): 561, (31.248587, 121.277434): 562, (31.211574, 121.366182): 563, (31.276982, 121.460649): 564, (31.324905, 121.29416): 565, (30.87366, 121.082631): 566, (31.327488, 121.437837): 567, (31.327049, 121.443238): 568, (31.072213, 121.658885): 569, (31.020065, 121.23311): 570, (31.015452, 121.154814): 571, (31.384186, 121.484871): 572, (31.197573, 121.454035): 573, (31.048845, 121.231719): 574, (38.052584, 114.484137): 575, (31.311763, 121.21253): 576, (31.044065, 121.537499): 577, (30.705762, 121.33512): 578, (30.982046, 121.207872): 579, (31.17605, 121.431225): 580, (31.111821, 121.385462): 581, (31.488248, 121.349833): 582, (31.099755, 121.009542): 583, (31.366638, 121.43543): 584, (31.387389, 121.431922): 585, (31.213659, 121.504661): 586, (31.145387, 121.61623): 587, (31.044667, 121.46191): 588, (31.230395, 121.46599): 589, (31.320121, 121.459018): 590, (31.190185, 121.443475): 591, (31.155297, 121.573504): 592, (31.130862, 121.091425): 593, (31.254111, 121.408424): 594, (31.403259, 121.447841): 595, (31.209644, 121.56447): 596, (30.935837, 121.561311): 597, (31.277766, 121.433047): 598, (31.216371, 121.601926): 599, (31.158202, 121.389411): 600, (31.05799, 121.626154): 601, (30.996346, 121.297687): 602, (31.215381, 121.439262): 603, (31.155344, 121.2025): 604, (31.094914, 121.43682): 605, (31.395973, 121.258897): 606, (31.367578, 121.31858): 607, (31.210843, 121.491168): 608, (31.124501, 121.32791): 609, (30.999472, 121.378674): 610, (30.922611, 121.473581): 611, (31.232002, 121.359292): 612, (31.168938, 121.54333): 613, (31.177169, 121.445517): 614, (31.045313, 121.844791): 615, (31.175621, 121.317835): 616, (31.23593, 121.220028): 617, (31.13623, 121.77848): 618, (31.235177, 121.451385): 619, (31.228847, 121.338653): 620, (31.262448, 121.504717): 621, (31.217476, 121.494788): 622, (31.234004, 121.276012): 623, (31.311631, 121.354326): 624, (31.217968, 121.41467): 625, (31.241014, 121.48529): 626, (31.224226, 121.43839): 627, (31.224718, 121.344451): 628, (31.159569, 121.145966): 629, (31.023902, 121.350511): 630, (31.220809, 121.305363): 631, (31.294194, 121.316828): 632, (31.222119, 121.454078): 633, (31.265705, 121.420959): 634, (31.430211, 121.28239): 635, (31.180466, 121.441251): 636, (31.281813, 121.354285): 637, (31.121363, 121.369095): 638, (31.245624, 121.522839): 639, (31.125764, 121.562057): 640, (31.195131, 121.418788): 641, (30.829105, 121.180127): 642, (31.3397, 121.298473): 643, (30.799332, 121.268825): 644, (31.16807, 121.488657): 645, (31.047975, 121.785062): 646, (30.832601, 121.445485): 647, (31.039301, 121.593594): 648, (31.374842, 121.264224): 649, (31.275929, 121.473484): 650, (31.181485, 121.520026): 651, (30.895699, 121.478289): 652, (31.252974, 121.502684): 653, (31.074013, 121.407487): 654, (30.960478, 121.057529): 655, (31.102, 121.462298): 656, (31.204343, 121.407018): 657, (31.402222, 121.493387): 658, (31.153746, 121.504388): 659, (31.137143, 121.545715): 660, (30.832683, 121.481868): 661, (31.01294, 121.637001): 662, (31.191674, 121.699103): 663, (31.11674, 121.586334): 664, (31.248146, 121.441241): 665, (30.945561, 121.150834): 666, (31.318695, 121.410448): 667, (31.235453, 121.653548): 668, (31.32899, 121.323419): 669, (30.926464, 121.682732): 670, (31.24641, 121.548541): 671, (31.034322, 121.601024): 672, (31.145439, 121.417433): 673, (31.226966, 121.53174): 674, (31.182371, 121.526439): 675, (31.219133, 121.524846): 676, (31.459469, 121.412299): 677, (30.954604, 121.333773): 678, (31.331242, 121.225906): 679, (30.884387, 121.408648): 680, (31.203031, 121.443586): 681, (31.128227, 121.45441): 682, (31.30647, 121.492925): 683, (31.214767, 121.444427): 684, (31.292717, 121.482235): 685, (31.239726, 121.139516): 686, (31.294378, 121.567937): 687, (31.121152, 121.141999): 688, (31.186628, 121.372116): 689, (31.213701, 121.41802): 690, (31.063923, 121.353076): 691, (31.227933, 121.45361): 692, (30.797557, 121.356113): 693, (31.280867, 121.656939): 694, (31.294155, 121.418063): 695, (31.423181, 121.387917): 696, (31.098701, 121.582178): 697, (31.300693, 121.496182): 698, (31.214115, 121.406884): 699, (31.22121, 121.411568): 700, (31.250674, 121.47282): 701, (31.232857, 121.475333): 702, (31.22135, 121.391319): 703, (31.17665, 121.132941): 704, (30.950067, 121.449083): 705, (31.077825, 121.33981): 706, (46.777465, 131.812182): 707, (31.2317, 121.546506): 708, (31.082614, 121.523874): 709, (31.127837, 121.380654): 710, (31.414938, 121.486809): 711, (31.194792, 121.466884): 712, (31.318974, 121.412838): 713, (31.24143, 121.475172): 714, (31.274535, 121.48034): 715, (31.122623, 121.433813): 716, (31.318369, 121.407014): 717, (30.918205, 121.509992): 718, (31.13264, 121.32443): 719, (31.299261, 121.578516): 720, (31.276268, 121.683198): 721, (31.292008, 121.36941): 722, (30.730296, 121.342643): 723, (31.01613, 121.203009): 724, (31.19382, 121.49524): 725, (31.342417, 121.40922): 726, (31.23301, 121.442524): 727, (31.28023, 121.543164): 728, (31.32719, 121.53013): 729, (31.373346, 121.135761): 730, (31.037985, 121.752781): 731, (31.140855, 121.360032): 732, (30.898527, 121.22367): 733, (31.012212, 121.413472): 734, (31.224452, 121.511537): 735, (31.274564, 121.51979): 736, (31.205813, 121.413544): 737, (31.202883, 121.476296): 738, (31.18851, 121.403115): 739, (31.177858, 121.346956): 740, (30.719537, 121.355948): 741, (31.160199, 121.557134): 742, (31.25563, 121.508513): 743, (31.132311, 121.362691): 744, (31.171972, 121.140685): 745, (30.866245, 121.343018): 746, (31.217516, 121.452872): 747, (31.146551, 121.235394): 748, (31.298752, 121.491919): 749, (31.204179, 121.538588): 750, (31.191847, 121.381117): 751, (30.909885, 121.195459): 752, (31.281567, 121.557955): 753, (30.814573, 121.352883): 754, (31.012926, 121.423939): 755, (31.260429, 121.426787): 756, (31.371366, 121.236234): 757, (31.256151, 121.557845): 758, (31.122978, 121.704554): 759, (31.035283, 121.246093): 760, (31.201303, 121.434409): 761, (31.183323, 121.447822): 762, (31.281363, 121.459935): 763, (31.397749, 121.267217): 764, (31.285861, 121.507007): 765, (31.274417, 121.257132): 766, (31.219239, 121.418564): 767, (31.045068, 121.232113): 768, (31.120822, 121.29001): 769, (31.198048, 121.604048): 770, (31.301877, 121.353206): 771, (31.255293, 121.49343): 772, (31.236935, 121.106513): 773, (31.225092, 121.38464): 774, (31.051247, 121.480621): 775, (31.225207, 121.452948): 776, (31.303478, 121.55082): 777, (30.8406, 121.251681): 778, (31.156874, 121.348458): 779, (31.245814, 121.411472): 780, (30.918282, 121.639066): 781, (31.072138, 121.661831): 782, (31.312494, 121.157796): 783, (31.282237, 121.42091): 784, (31.225831, 121.412695): 785, (31.239319, 121.265377): 786, (30.920347, 121.057024): 787, (31.278892, 121.710661): 788, (31.085146, 121.589124): 789, (31.127879, 121.452728): 790, (31.185228, 121.1697): 791, (31.401391, 121.463758): 792, (31.312825, 121.491112): 793, (30.984047, 121.726879): 794, (31.00953, 121.052076): 795, (31.207679, 121.489691): 796, (30.816155, 121.298392): 797, (31.192975, 121.449755): 798, (31.126339, 121.573213): 799, (31.113159, 121.06421): 800, (31.240587, 121.402322): 801, (31.317294, 121.460019): 802, (31.324375, 121.433451): 803, (31.19332, 121.430385): 804, (31.220416, 121.529149): 805, (31.216526, 121.326846): 806, (31.122449, 121.582926): 807, (31.195907, 121.410659): 808, (31.194567, 121.453929): 809, (31.266743, 121.481644): 810, (30.975655, 121.228194): 811, (31.268263, 121.394134): 812, (31.049556, 121.600672): 813, (31.208384, 121.466365): 814, (31.335151, 121.590721): 815, (31.241036, 121.374063): 816, (30.7834, 121.299618): 817, (31.242991, 121.515376): 818, (31.299851, 121.163211): 819, (31.01217, 121.265575): 820, (30.831909, 121.397355): 821, (31.304918, 121.497837): 822, (31.215081, 121.481286): 823, (31.043159, 121.420296): 824, (31.307198, 121.511929): 825, (31.159667, 121.515041): 826, (31.038911, 121.226883): 827, (31.161027, 121.722478): 828, (31.088099, 121.430111): 829, (31.300545, 121.170082): 830, (31.030806, 121.396875): 831, (31.273276, 121.1303): 832, (31.276349, 121.39526): 833, (31.230201, 121.308739): 834, (31.087306, 121.275545): 835, (31.240478, 121.526884): 836, (31.284042, 121.583676): 837, (31.017385, 121.715431): 838, (31.24708, 121.419525): 839, (31.197656, 121.463533): 840, (31.213728, 121.386102): 841, (31.388641, 121.444174): 842, (31.039193, 121.604274): 843, (31.242365, 121.503571): 844, (31.15228, 121.118001): 845, (31.144212, 121.50825): 846, (31.193409, 121.393885): 847, (31.185812, 121.457923): 848, (31.258677, 121.392956): 849, (31.016895, 121.42971): 850, (31.198836, 121.215066): 851, (31.241362, 121.395822): 852, (31.023331, 121.315006): 853, (31.260742, 121.399188): 854, (31.337749, 121.579688): 855, (31.149689, 121.531301): 856, (30.948533, 121.465504): 857, (31.257553, 121.311711): 858, (31.00948, 121.4998): 859, (31.26748, 121.57859): 860, (31.211617, 121.486703): 861, (31.244559, 121.724977): 862, (31.245663, 121.449725): 863, (30.947965, 121.640593): 864, (31.10366, 121.420217): 865, (31.23423, 121.119889): 866, (30.983966, 121.737727): 867, (31.354948, 121.496863): 868, (31.358748, 121.52806): 869, (31.201647, 121.420098): 870, (30.843004, 121.438792): 871, (31.25901, 121.65797): 872, (30.823953, 121.528367): 873, (31.172757, 121.425965): 874, (31.019533, 121.388705): 875, (31.052986, 121.506423): 876, (31.301307, 121.33124): 877, (31.295347, 121.515155): 878, (31.234545, 121.743402): 879, (31.324066, 121.536866): 880, (30.910085, 121.504947): 881, (31.248482, 121.53958): 882, (31.059755, 121.389723): 883, (31.318389, 121.653955): 884, (31.157241, 121.418919): 885, (30.92668, 121.466784): 886, (31.218812, 121.401309): 887, (31.288102, 121.122551): 888, (31.334449, 121.536876): 889, (31.221001, 121.731159): 890, (31.266813, 121.512658): 891, (31.298177, 121.473198): 892, (31.045834, 121.408322): 893, (30.865062, 121.662776): 894, (30.795602, 121.152307): 895, (31.294598, 121.206157): 896, (31.096053, 121.518157): 897, (30.977517, 121.669761): 898, (30.858416, 121.035215): 899, (30.93561, 121.517165): 900, (31.057816, 121.413943): 901, (31.327484, 121.406697): 902, (31.425786, 121.422506): 903, (31.250734, 121.487731): 904, (31.264477, 121.44249): 905, (31.38336, 121.420672): 906, (31.318281, 121.447698): 907, (31.032565, 121.409407): 908, (31.312069, 121.172955): 909, (31.313954, 121.499987): 910, (30.924981, 121.710847): 911, (31.139657, 121.298915): 912, (31.157677, 121.424531): 913, (30.904757, 121.467941): 914, (31.219088, 121.428934): 915, (31.149628, 121.43924): 916, (31.297281, 121.505124): 917, (31.315501, 121.535127): 918, (31.25821, 121.479271): 919, (31.069757, 121.399847): 920, (31.360948, 121.595293): 921, (31.351256, 121.519123): 922, (31.295335, 121.555781): 923, (30.872205, 121.530081): 924, (31.390234, 121.254572): 925, (31.277576, 121.460905): 926, (31.316557, 121.516135): 927, (31.120714, 121.42693): 928, (31.235072, 121.518805): 929, (31.255173, 121.403569): 930, (30.831429, 121.242309): 931, (31.30835, 121.657013): 932, (31.333408, 121.552846): 933, (31.253394, 121.389864): 934, (31.139721, 121.673912): 935, (31.261924, 121.568031): 936, (30.929919, 121.206039): 937, (31.070388, 121.532962): 938, (31.221319, 121.166272): 939, (31.383625, 121.268069): 940, (31.189723, 121.356104): 941, (31.460409, 121.401649): 942, (31.167279, 121.113649): 943, (31.242808, 121.420927): 944, (31.054324, 121.793035): 945, (31.081475, 121.523285): 946, (31.138371, 121.416975): 947, (31.180763, 121.476049): 948, (31.267654, 121.473299): 949, (31.260903, 121.618966): 950, (31.088801, 121.536584): 951, (31.313065, 121.417971): 952, (31.185065, 121.503655): 953, (31.219945, 121.285654): 954, (30.766975, 121.362906): 955, (30.934396, 121.369499): 956, (31.39034, 121.272165): 957, (30.741272, 121.3348): 958, (31.43737, 121.432451): 959, (31.283201, 121.665402): 960, (31.046524, 121.749155): 961, (31.370457, 121.470415): 962, (30.908378, 121.634574): 963, (31.255025, 121.199554): 964, (31.064763, 121.396913): 965, (31.005717, 121.202014): 966, (30.894639, 121.21016): 967, (30.893975, 121.016922): 968, (31.364407, 121.561457): 969, (30.997412, 121.247239): 970, (31.168922, 121.412651): 971, (30.834815, 121.197836): 972, (31.251358, 121.485526): 973, (31.174203, 121.431404): 974, (31.253808, 121.483756): 975, (31.262941, 121.602924): 976, (31.236381, 121.484201): 977, (31.239655, 121.478097): 978, (31.199721, 121.325753): 979, (31.468523, 121.425585): 980, (30.978506, 121.447689): 981, (31.176489, 121.188906): 982, (31.271472, 121.494721): 983, (30.989121, 121.127308): 984, (30.796696, 121.200272): 985, (31.150398, 121.537521): 986, (31.361645, 121.475443): 987, (31.166007, 121.457766): 988, (31.1744, 121.285083): 989, (30.855412, 121.569454): 990, (31.260816, 121.36983): 991, (31.136777, 121.546762): 992, (30.915122, 121.560642): 993, (31.301749, 121.457406): 994, (31.302, 121.670498): 995, (31.287632, 121.458734): 996, (31.317987, 120.619907): 997, (31.120753, 121.381579): 998, (31.039158, 121.239632): 999, (31.236177, 121.493181): 1000, (31.163393, 121.381735): 1001, (31.453478, 121.256429): 1002, (31.202118, 121.395128): 1003, (31.176807, 121.251063): 1004, (31.035151, 121.544702): 1005, (31.294885, 121.244609): 1006, (30.929412, 121.460403): 1007, (31.106759, 121.398956): 1008, (30.915686, 121.462098): 1009, (31.350616, 121.589116): 1010, (31.427978, 121.433301): 1011, (31.109271, 121.617421): 1012, (30.966714, 121.771375): 1013, (31.22714, 121.391029): 1014, (31.28386, 121.530324): 1015, (31.178278, 121.606216): 1016, (30.774911, 121.259927): 1017, (31.364503, 121.391608): 1018, (31.293561, 121.362372): 1019, (31.040309, 121.308498): 1020, (31.036987, 121.386554): 1021, (31.243446, 121.675041): 1022, (31.165339, 121.427732): 1023, (31.171648, 121.110276): 1024, (31.116535, 121.678055): 1025, (31.230748, 121.485488): 1026, (31.290434, 121.425041): 1027, (31.054947, 120.995325): 1028, (31.07195, 121.150279): 1029, (31.373432, 121.490334): 1030, (31.456227, 121.359857): 1031, (31.046945, 121.764826): 1032, (30.763589, 121.264689): 1033, (31.011339, 121.296295): 1034, (31.441487, 121.179013): 1035, (31.434069, 121.448741): 1036, (31.233674, 121.498294): 1037, (34.556383, 118.79231): 1038, (31.323113, 121.616609): 1039, (31.277484, 121.382842): 1040, (31.360977, 121.444195): 1041, (30.985081, 121.691229): 1042, (31.240412, 121.147662): 1043, (31.235461, 121.498056): 1044, (31.04822, 121.231928): 1045, (31.251883, 121.418603): 1046, (31.145641, 121.449009): 1047, (31.148334, 121.420448): 1048, (31.317417, 121.48383): 1049, (31.190828, 121.420662): 1050, (31.291192, 121.65263): 1051, (31.053198, 120.907525): 1052, (39.612987, 118.20604): 1053, (31.036404, 121.272345): 1054, (31.244292, 121.495265): 1055, (31.305433, 121.435633): 1056, (31.08906, 121.722984): 1057, (31.241762, 121.50497): 1058, (31.234259, 121.52572): 1059, (31.033659, 121.577846): 1060, (31.28627, 121.599397): 1061, (31.256559, 121.43557): 1062, (31.195709, 121.424093): 1063, (31.358014, 121.37617): 1064, (31.421831, 121.349166): 1065, (30.891157, 121.186557): 1066, (31.219455, 121.098597): 1067, (31.236138, 121.463199): 1068, (30.993883, 121.129758): 1069, (31.270302, 121.398224): 1070, (31.242078, 121.331128): 1071, (31.459342, 121.319364): 1072, (30.733903, 121.341904): 1073, (30.912905, 121.801126): 1074, (31.281917, 121.435906): 1075, (31.163339, 121.469291): 1076, (31.203473, 121.445984): 1077, (31.099085, 121.386207): 1078, (31.058968, 121.787072): 1079, (31.426865, 121.351198): 1080, (31.301954, 121.54744): 1081, (31.232729, 121.566935): 1082, (30.969483, 121.53972): 1083, (31.231068, 121.4021): 1084, (31.200224, 121.316463): 1085, (31.270279, 121.531241): 1086, (31.002291, 121.873286): 1087, (31.237022, 121.546184): 1088, (31.121127, 121.243829): 1089, (30.771941, 121.373524): 1090, (31.132833, 121.17281): 1091, (31.22255, 121.440445): 1092, (31.116791, 121.166516): 1093, (31.249162, 121.487899): 1094, (31.225318, 121.437806): 1095, (31.087507, 121.398717): 1096, (30.85108, 121.855994): 1097, (31.256071, 121.462096): 1098, (31.004198, 121.067198): 1099, (31.270369, 121.443633): 1100, (31.370992, 121.177459): 1101, (31.180175, 121.422303): 1102, (30.798179, 121.426469): 1103, (31.051162, 121.311884): 1104, (31.209913, 121.425292): 1105, (31.178033, 121.487803): 1106, (31.242893, 121.465382): 1107, (31.213582, 121.38377): 1108, (31.002479, 121.390889): 1109, (31.17762, 121.511042): 1110, (31.126777, 121.367557): 1111, (31.237944, 121.693732): 1112, (31.3065, 121.429743): 1113, (30.945306, 121.729327): 1114, (31.021822, 121.590566): 1115, (30.747024, 121.287991): 1116, (31.316181, 121.405014): 1117, (31.235247, 121.383101): 1118, (31.243338, 121.253922): 1119, (31.228047, 121.425893): 1120, (30.978588, 121.581677): 1121, (31.105808, 121.038742): 1122, (31.312672, 121.593011): 1123, (31.308902, 121.502016): 1124, (31.147903, 121.403379): 1125, (31.201965, 121.452457): 1126, (31.345443, 121.596458): 1127, (31.320829, 121.389518): 1128, (31.283959, 121.416644): 1129, (31.267478, 121.5468): 1130, (31.221876, 121.687381): 1131, (31.068209, 121.373076): 1132, (30.875057, 121.324697): 1133, (31.204349, 121.693531): 1134, (31.289489, 121.341656): 1135, (31.070388, 121.520258): 1136, (31.041762, 120.928733): 1137, (31.20901, 121.478403): 1138, (31.307215, 121.486962): 1139, (31.29099, 121.697503): 1140, (31.396687, 121.378844): 1141, (30.852446, 121.262447): 1142, (31.08111, 121.263537): 1143, (31.3691, 121.415317): 1144, (31.22258, 121.428599): 1145, (31.255256, 121.500499): 1146, (30.900763, 121.246509): 1147, (31.048503, 121.141872): 1148, (30.933365, 121.873737): 1149, (31.244859, 121.489305): 1150, (31.288233, 121.304952): 1151, (31.281665, 121.42621): 1152, (31.331055, 121.52962): 1153, (30.72456, 121.344043): 1154, (31.285059, 121.406807): 1155, (31.217237, 121.432824): 1156, (31.176813, 121.439114): 1157, (31.337637, 121.446597): 1158, (31.232407, 121.652986): 1159, (31.296656, 121.483976): 1160, (31.159536, 121.1442): 1161, (31.323598, 121.638883): 1162, (31.239454, 121.46057): 1163, (31.291143, 121.472012): 1164, (31.197573, 121.37641): 1165, (31.413754, 121.202263): 1166, (31.376717, 121.539395): 1167, (32.902077, 109.42358): 1168, (31.233516, 121.491431): 1169, (31.078365, 121.404647): 1170, (31.110093, 121.371253): 1171, (31.050344, 121.770082): 1172, (31.168241, 121.437559): 1173, (31.161635, 121.286602): 1174, (31.065206, 121.803371): 1175, (31.473557, 121.331298): 1176, (31.228927, 121.486896): 1177, (30.967591, 121.656531): 1178, (31.150193, 121.45419): 1179, (31.192275, 121.462826): 1180, (31.247009, 121.47942): 1181, (30.802409, 121.205114): 1182, (31.130891, 121.472384): 1183, (31.171686, 121.441221): 1184, (31.273982, 121.512661): 1185, (31.223114, 121.522857): 1186, (31.170097, 121.608906): 1187, (31.221777, 121.558508): 1188, (31.294609, 121.526886): 1189, (31.160355, 121.454785): 1190, (31.278008, 121.492049): 1191, (31.080002, 121.798389): 1192, (31.373175, 121.438741): 1193, (31.431346, 121.222574): 1194, (31.253346, 121.448039): 1195, (31.396658, 121.496972): 1196, (31.141982, 121.459872): 1197, (31.243709, 121.197504): 1198, (31.068626, 121.108326): 1199, (31.222168, 121.378951): 1200, (31.334186, 121.50238): 1201, (31.19868, 121.485666): 1202, (31.173163, 121.356571): 1203, (31.157022, 121.498865): 1204, (31.289793, 121.455017): 1205, (30.844717, 121.528288): 1206, (30.736093, 121.355957): 1207, (31.126179, 121.378046): 1208, (31.280857, 121.169253): 1209, (31.333315, 121.188402): 1210, (30.955208, 121.732358): 1211, (31.197114, 121.442853): 1212, (31.191818, 121.538713): 1213, (30.866409, 121.188512): 1214, (31.233629, 121.532011): 1215, (31.144653, 121.478451): 1216, (31.227167, 121.633295): 1217, (31.28915, 121.229167): 1218, (31.304036, 121.448053): 1219, (31.234867, 121.46325): 1220, (31.290887, 121.435064): 1221, (30.957633, 121.343012): 1222, (31.168296, 121.588625): 1223, (31.059304, 121.53566): 1224, (31.250339, 121.440196): 1225, (31.11794, 121.457471): 1226, (31.235682, 121.487831): 1227, (31.410741, 121.442349): 1228, (31.218716, 121.568667): 1229, (31.013082, 121.391811): 1230, (31.222609, 121.378494): 1231, (31.264433, 121.541398): 1232, (31.232518, 121.465177): 1233, (31.23271, 121.455345): 1234, (31.309342, 121.580807): 1235, (30.873326, 121.284335): 1236, (31.252495, 121.390194): 1237, (31.109998, 121.164842): 1238, (31.304576, 121.522177): 1239, (31.271819, 121.546358): 1240, (31.184883, 121.430689): 1241, (31.26338, 121.517162): 1242, (31.437104, 121.31285): 1243, (31.037068, 121.686977): 1244, (31.216752, 121.476401): 1245, (31.284953, 121.489071): 1246, (31.119287, 121.204986): 1247, (31.22622, 121.506261): 1248, (31.240869, 121.481603): 1249, (31.224731, 121.43464): 1250, (31.221346, 121.201454): 1251, (30.92643, 121.782514): 1252, (30.948124, 121.381501): 1253, (31.300338, 121.526827): 1254, (31.235847, 121.536101): 1255, (31.127539, 121.389284): 1256, (31.021245, 121.226791): 1257, (31.355364, 121.537096): 1258, (31.209872, 121.409627): 1259, (31.417324, 121.491388): 1260, (31.245393, 121.705496): 1261, (30.956624, 121.334663): 1262, (31.309645, 121.4911): 1263, (30.86683, 121.83302): 1264, (31.335141, 121.413142): 1265, (31.23543, 121.447652): 1266, (31.152749, 121.182589): 1267, (30.879349, 121.094749): 1268, (31.302288, 121.510464): 1269, (31.026816, 121.426094): 1270, (31.280389, 121.467635): 1271, (31.398623, 121.409041): 1272, (31.310614, 121.476617): 1273, (31.17247, 121.404203): 1274, (31.254124, 121.605142): 1275, (31.143835, 120.935676): 1276, (31.288583, 121.428686): 1277, (30.905367, 121.832974): 1278, (31.221201, 121.497347): 1279, (31.223175, 121.184646): 1280, (31.208589, 121.524447): 1281, (30.749894, 121.270796): 1282, (31.203987, 121.520591): 1283, (30.975236, 121.195313): 1284, (31.305269, 121.397452): 1285, (31.327299, 121.452936): 1286, (31.252241, 121.57094): 1287, (31.098359, 121.073778): 1288, (31.228627, 121.480756): 1289, (31.071955, 121.234457): 1290, (31.08093, 121.159291): 1291, (31.155342, 121.8105): 1292, (31.33454, 121.46611): 1293, (31.213849, 121.43028): 1294, (31.259319, 121.494159): 1295, (31.286501, 121.363758): 1296, (31.379455, 121.372876): 1297, (31.162395, 121.353836): 1298, (31.46933, 121.246736): 1299, (31.145654, 121.375598): 1300, (31.169796, 121.52847): 1301, (31.106089, 121.039819): 1302, (31.256426, 121.61001): 1303, (30.9152, 121.664095): 1304, (30.960195, 121.398632): 1305, (30.910239, 121.419143): 1306, (30.990942, 121.12541): 1307, (31.42225, 121.355698): 1308, (31.406426, 121.476433): 1309, (31.432457, 121.359456): 1310, (31.177067, 121.370844): 1311, (30.897085, 121.617832): 1312, (31.240874, 121.518086): 1313, (30.965531, 121.208487): 1314, (31.278808, 121.419722): 1315, (30.874563, 121.067902): 1316, (31.202262, 121.564812): 1317, (31.058805, 121.593285): 1318, (31.267263, 121.086225): 1319, (31.319422, 121.469372): 1320, (31.228726, 121.482748): 1321, (31.195321, 121.445546): 1322, (31.300493, 121.323029): 1323, (31.113798, 121.382691): 1324, (31.307142, 121.362041): 1325, (30.930023, 121.011969): 1326, (30.99055, 121.126163): 1327, (31.285944, 121.44442): 1328, (30.894923, 121.319564): 1329, (31.023064, 121.534267): 1330, (31.236405, 121.432864): 1331, (31.240561, 121.459431): 1332, (31.195613, 121.40425): 1333, (31.227762, 121.495598): 1334, (31.099126, 121.813976): 1335, (31.217682, 121.499709): 1336, (31.212718, 121.447853): 1337, (31.155349, 121.375527): 1338, (31.217512, 121.533844): 1339, (30.830149, 121.35652): 1340, (31.292462, 121.271212): 1341, (31.27328, 121.47376): 1342, (30.853798, 121.344272): 1343, (31.3038, 121.450721): 1344, (30.846647, 121.431405): 1345, (30.718108, 121.283169): 1346, (31.219437, 121.374523): 1347, (31.382369, 121.258579): 1348, (31.212113, 121.444193): 1349, (30.827125, 121.329382): 1350, (31.022923, 121.806437): 1351, (30.956973, 121.097333): 1352, (31.236009, 121.478941): 1353, (30.94602, 121.106186): 1354, (31.415226, 121.349387): 1355, (31.284675, 121.516006): 1356, (31.166058, 121.403691): 1357, (31.030657, 121.236586): 1358, (31.387989, 121.396749): 1359, (31.259763, 121.089338): 1360, (31.209182, 121.535827): 1361, (39.084494, 117.236165): 1362, (31.081917, 121.062248): 1363, (31.307911, 121.518921): 1364, (30.810888, 121.234763): 1365, (35.24116, 113.276351): 1366, (31.33876, 121.437747): 1367, (31.232893, 121.380242): 1368, (30.977355, 121.74808): 1369, (31.242867, 121.490229): 1370, (30.893492, 121.315973): 1371, (31.221325, 121.448881): 1372, (31.202103, 121.445912): 1373, (31.259267, 121.369254): 1374, (31.217323, 121.753758): 1375, (31.183859, 121.415119): 1376, (31.184347, 121.441392): 1377, (31.159523, 121.435774): 1378, (31.25423, 121.461072): 1379, (31.157362, 121.221594): 1380, (31.168519, 121.804669): 1381, (31.065995, 121.314532): 1382, (31.073706, 121.680729): 1383, (31.251614, 121.464007): 1384, (30.91796, 121.543057): 1385, (31.012108, 121.074488): 1386, (30.889117, 121.317123): 1387, (30.801265, 121.395878): 1388, (31.344797, 121.500999): 1389, (31.230277, 121.448573): 1390, (31.219533, 121.427561): 1391, (31.180994, 121.377204): 1392, (31.492475, 121.328439): 1393, (31.08532, 121.237002): 1394, (31.134917, 121.334281): 1395, (31.191702, 121.449143): 1396, (31.209479, 121.702069): 1397, (30.80226, 121.311879): 1398, (31.278028, 121.583992): 1399, (31.304081, 121.326517): 1400, (31.318258, 121.167266): 1401, (31.231663, 121.428471): 1402, (31.213061, 121.483541): 1403, (26.215115, 109.744661): 1404, (31.283733, 121.149832): 1405, (31.260888, 121.340352): 1406, (31.406017, 121.240101): 1407, (31.256232, 121.498254): 1408, (31.151421, 121.279419): 1409, (31.264503, 121.715846): 1410, (31.412982, 121.496282): 1411, (31.162111, 121.566673): 1412, (30.92405, 121.648972): 1413, (31.209904, 121.234203): 1414, (31.266413, 121.351003): 1415, (31.2716, 121.465223): 1416, (31.060659, 121.399957): 1417, (31.328545, 121.538592): 1418, (31.010467, 121.237408): 1419, (31.299799, 121.530915): 1420, (31.24478, 121.505751): 1421, (31.143754, 121.434357): 1422, (31.157151, 121.431115): 1423, (31.294057, 121.171316): 1424, (31.087224, 121.466989): 1425, (31.346801, 121.577398): 1426, (31.283289, 121.510576): 1427, (31.108792, 121.061126): 1428, (31.007639, 121.416247): 1429, (31.162463, 121.364262): 1430, (31.200249, 121.443811): 1431, (31.224154, 121.43807): 1432, (31.051731, 121.423756): 1433, (31.412391, 121.491247): 1434, (31.30048, 121.469206): 1435, (31.181785, 121.529159): 1436, (31.21852, 121.44019): 1437, (31.047441, 121.470836): 1438, (31.224117, 121.4732): 1439, (31.222272, 121.438997): 1440, (31.405792, 121.489703): 1441, (31.242584, 121.449606): 1442, (31.171916, 121.718029): 1443, (30.946702, 121.624426): 1444, (31.251894, 121.464026): 1445, (24.284812, 102.999068): 1446, (31.253206, 121.459958): 1447, (31.178962, 121.388264): 1448, (31.240806, 121.481992): 1449, (31.225275, 121.47164): 1450, (31.237635, 121.476519): 1451, (31.353624, 121.155902): 1452, (31.168125, 121.503534): 1453, (31.259739, 121.488421): 1454, (31.30685, 121.519579): 1455, (30.790922, 121.348126): 1456, (30.889897, 121.320413): 1457, (31.221662, 121.539905): 1458, (31.242152, 121.609858): 1459, (31.190483, 121.434806): 1460, (31.020849, 121.759757): 1461, (31.152045, 121.420017): 1462, (31.254042, 121.464758): 1463, (31.483764, 121.274813): 1464, (31.044697, 121.22462): 1465, (31.276567, 121.483483): 1466, (30.846072, 121.32313): 1467, (31.310089, 121.640057): 1468, (31.273024, 121.42572): 1469, (28.738742, 120.640606): 1470, (31.288667, 121.527375): 1471, (31.299749, 121.170321): 1472, (30.953524, 121.483682): 1473, (31.240013, 121.487148): 1474, (34.798608, 116.090395): 1475, (31.200719, 121.519009): 1476, (31.126748, 120.934108): 1477, (31.210582, 121.529652): 1478, (31.251161, 121.489743): 1479, (31.139255, 121.409759): 1480, (31.227666, 121.431017): 1481, (31.224551, 121.547983): 1482, (30.908341, 121.870538): 1483, (31.200432, 121.447381): 1484, (30.880562, 121.486561): 1485, (31.113501, 121.393037): 1486, (30.948258, 121.324472): 1487, (31.198799, 121.383701): 1488, (31.230363, 121.5056): 1489, (31.240686, 121.480699): 1490, (31.415595, 121.188844): 1491, (31.060038, 121.195223): 1492, (30.838466, 121.228977): 1493, (30.874727, 121.250912): 1494, (31.010124, 121.538644): 1495, (31.211634, 121.520845): 1496, (31.164838, 121.410811): 1497, (31.229071, 121.452449): 1498, (31.214958, 121.527054): 1499, (31.018662, 121.420209): 1500, (31.207348, 121.70185): 1501, (31.269521, 121.153127): 1502, (31.258903, 121.441965): 1503, (31.304737, 121.522227): 1504, (31.210579, 121.384648): 1505, (31.359545, 121.18156): 1506, (31.074814, 121.713455): 1507, (31.218022, 121.504024): 1508, (31.238742, 121.530031): 1509, (31.318438, 121.497035): 1510, (31.231468, 121.537215): 1511, (31.042348, 121.28911): 1512, (31.244568, 121.441817): 1513, (31.279322, 121.466136): 1514, (31.223446, 121.432052): 1515, (31.228852, 121.541643): 1516, (31.303224, 121.308726): 1517, (31.299874, 121.298501): 1518, (31.207247, 121.572933): 1519, (31.265054, 121.456413): 1520, (31.201064, 121.438699): 1521, (31.213903, 121.410198): 1522, (31.199522, 121.44697): 1523, (31.294834, 121.431554): 1524, (31.241577, 121.485099): 1525, (31.251986, 121.456773): 1526, (31.174287, 121.435995): 1527, (31.330908, 121.45516): 1528, (31.228017, 121.444777): 1529, (31.263495, 121.496653): 1530, (31.260217, 121.525816): 1531, (31.400853, 121.483671): 1532, (31.046753, 121.062205): 1533, (31.19124, 121.507961): 1534, (31.25037, 121.488549): 1535, (31.218516, 121.463689): 1536, (31.234499, 121.463079): 1537, (31.30446, 121.426575): 1538, (31.185189, 121.445449): 1539, (31.229057, 121.452254): 1540, (31.161066, 121.354642): 1541, (31.339206, 121.453188): 1542, (31.293519, 121.22306): 1543, (31.233644, 121.525549): 1544, (31.37273, 121.546117): 1545, (31.16872, 121.768142): 1546, (31.232853, 121.486408): 1547, (31.302355, 121.515812): 1548, (31.302746, 121.376994): 1549, (31.231585, 121.517218): 1550, (30.773575, 121.14093): 1551, (31.215347, 121.134685): 1552, (31.236828, 121.514196): 1553, (31.224539, 121.550163): 1554, (31.305114, 121.41898): 1555, (31.16584, 121.530742): 1556, (31.180581, 121.527653): 1557, (30.895162, 121.094602): 1558, (31.206958, 121.591146): 1559, (31.231369, 121.403139): 1560, (31.242261, 121.507912): 1561, (31.123621, 121.266561): 1562, (31.27369, 121.537119): 1563, (31.210887, 121.315496): 1564, (31.024911, 121.255017): 1565, (31.251619, 121.486701): 1566, (30.962059, 121.244644): 1567, (31.277147, 121.514954): 1568, (31.238815, 121.559352): 1569, (31.32087, 121.393055): 1570, (30.811433, 121.309343): 1571, (31.430745, 121.187414): 1572, (30.974035, 121.554233): 1573, (31.0887, 121.513069): 1574, (31.039411, 121.819385): 1575, (31.188514, 121.49841): 1576, (31.214193, 121.376945): 1577, (31.243974, 121.555758): 1578, (30.891109, 121.903749): 1579, (31.276315, 121.564096): 1580, (31.240837, 121.409164): 1581, (31.039155, 121.241352): 1582, (30.796356, 121.199264): 1583, (31.214342, 121.376038): 1584, (31.365913, 121.177965): 1585, (31.25372, 121.437778): 1586, (31.224373, 121.364172): 1587, (31.275331, 121.286529): 1588, (31.201237, 121.669336): 1589, (30.923134, 121.481921): 1590, (31.246946, 121.513919): 1591, (31.243738, 121.486223): 1592, (31.289509, 121.447288): 1593, (31.297689, 121.447357): 1594, (30.919514, 121.45936): 1595, (31.224126, 121.547664): 1596, (31.280348, 121.592755): 1597, (31.391131, 121.251789): 1598, (31.180897, 121.463904): 1599, (31.188512, 121.437591): 1600, (30.994999, 121.264867): 1601, (31.397648, 121.28551): 1602, (31.293762, 121.442609): 1603, (31.055129, 121.770244): 1604, (31.200811, 121.444401): 1605, (31.246387, 121.458214): 1606, (31.012788, 121.228073): 1607, (30.736314, 121.354852): 1608, (31.246573, 121.509144): 1609, (31.25079, 121.48933): 1610, (31.21245, 121.637072): 1611, (31.201966, 121.545756): 1612, (31.356868, 121.555161): 1613, (31.166046, 121.642137): 1614, (31.221705, 121.642288): 1615, (31.024417, 121.471297): 1616, (31.23936, 121.488752): 1617, (30.741693, 121.367052): 1618, (31.236991, 121.441554): 1619, (31.291242, 121.489761): 1620, (31.122643, 121.734179): 1621, (31.041182, 121.739041): 1622, (31.032045, 121.783775): 1623, (31.235154, 121.533056): 1624, (30.919873, 121.464262): 1625, (31.146285, 121.518893): 1626, (31.247569, 121.41575): 1627, (31.137582, 121.083916): 1628, (31.224552, 121.482535): 1629, (31.313444, 121.309439): 1630, (31.209598, 121.437679): 1631, (31.235043, 121.523389): 1632, (31.188068, 121.699562): 1633, (31.227165, 121.480195): 1634, (31.252877, 121.148181): 1635, (31.223981, 121.478274): 1636, (31.238801, 121.386263): 1637, (30.734301, 121.25722): 1638, (31.261132, 121.093167): 1639, (31.266232, 121.402951): 1640, (31.175983, 121.381611): 1641, (31.272774, 121.436412): 1642, (31.18026, 121.406339): 1643, (31.162714, 121.772242): 1644, (31.00816, 121.274391): 1645, (31.284086, 121.212427): 1646, (31.057607, 121.030715): 1647, (31.067235, 121.762108): 1648, (30.889225, 121.177092): 1649, (31.311405, 121.553882): 1650, (31.252485, 121.549613): 1651, (31.225653, 121.480878): 1652, (30.993467, 121.234309): 1653, (31.243463, 121.4907): 1654, (30.89659, 121.281933): 1655, (31.240412, 121.45723): 1656, (30.912581, 121.465142): 1657, (30.800042, 121.417303): 1658, (31.237435, 121.455917): 1659, (31.253657, 121.443859): 1660, (30.860242, 121.79388): 1661, (46.247857, 128.762232): 1662, (31.289282, 121.378597): 1663, (31.154675, 121.81374): 1664, (31.178088, 121.413515): 1665, (31.273966, 121.429374): 1666, (31.341417, 121.271089): 1667, (31.284292, 121.273506): 1668, (31.219656, 121.49234): 1669, (31.323457, 121.57815): 1670, (31.299073, 121.552202): 1671, (31.258982, 121.494994): 1672, (31.4163, 121.496785): 1673, (31.244555, 121.5099): 1674, (31.344366, 121.51915): 1675, (31.409644, 121.303936): 1676, (31.410089, 121.244982): 1677, (31.234796, 121.533464): 1678, (31.292699, 121.478195): 1679, (31.209704, 121.368486): 1680, (31.278161, 121.486144): 1681, (31.207139, 121.449578): 1682, (31.058917, 121.770251): 1683, (31.429482, 121.184088): 1684, (31.492988, 121.292044): 1685, (30.850119, 121.499982): 1686, (31.084634, 120.989297): 1687, (31.132602, 121.095863): 1688, (30.814394, 121.18284): 1689, (31.262032, 121.117499): 1690, (31.367392, 121.276877): 1691, (31.256595, 121.493953): 1692, (31.214719, 121.624972): 1693, (31.010331, 121.381275): 1694, (31.291773, 121.203129): 1695, (31.045964, 121.420076): 1696, (31.258656, 121.472538): 1697, (30.97533, 121.271449): 1698, (30.9269, 121.467678): 1699, (30.963494, 121.160188): 1700, (31.308192, 121.667576): 1701, (31.223134, 121.43817): 1702, (31.101299, 120.918138): 1703, (31.295004, 121.516367): 1704, (31.288044, 121.356098): 1705, (30.88378, 121.573239): 1706, (31.385543, 121.469415): 1707, (31.210579, 121.453849): 1708, (31.326418, 121.501541): 1709, (31.245242, 121.479617): 1710, (31.253405, 121.091261): 1711, (31.469563, 121.384552): 1712, (31.193515, 121.531662): 1713, (31.241223, 121.45891): 1714, (31.045842, 121.756555): 1715, (30.7653, 121.292527): 1716, (31.278478, 121.420747): 1717, (30.903041, 121.922883): 1718, (31.301967, 121.515226): 1719, (31.194928, 121.366998): 1720, (31.272659, 121.601779): 1721, (30.90618, 121.178287): 1722, (30.976182, 121.750557): 1723, (31.174217, 121.289644): 1724, (31.326619, 121.493433): 1725, (31.19183, 121.59097): 1726, (31.174304, 121.436093): 1727, (31.134484, 121.42764): 1728, (31.244229, 121.475231): 1729, (31.303373, 121.4429): 1730, (31.317584, 121.626266): 1731, (31.407391, 121.490586): 1732, (31.301057, 121.388942): 1733, (31.223107, 121.454329): 1734, (31.283403, 121.541263): 1735, (30.898697, 121.172576): 1736, (31.467705, 121.201637): 1737, (31.198372, 121.355749): 1738, (31.054506, 121.460002): 1739, (31.154909, 121.436989): 1740, (31.307614, 121.526541): 1741, (31.182189, 121.394088): 1742, (30.953548, 121.636227): 1743, (31.486771, 121.360405): 1744, (31.280452, 121.480426): 1745, (31.148785, 121.115114): 1746, (31.15702, 121.106352): 1747, (31.276616, 121.593917): 1748, (31.233452, 121.480931): 1749, (31.24256, 121.490511): 1750, (30.824719, 121.471519): 1751, (31.240453, 121.511773): 1752, (31.214641, 121.525998): 1753, (30.89577, 121.697282): 1754, (30.904648, 121.173227): 1755, (31.052639, 121.761696): 1756, (31.285391, 121.497527): 1757, (31.4193, 121.285094): 1758, (31.166365, 121.419792): 1759, (31.420803, 121.280523): 1760, (31.217162, 121.411803): 1761, (31.159771, 121.58896): 1762, (31.035335, 121.117893): 1763, (31.294215, 121.527852): 1764, (30.882016, 121.532008): 1765, (30.933573, 121.55651): 1766, (31.380359, 121.304458): 1767, (31.410197, 121.500761): 1768, (30.932543, 121.716591): 1769, (31.312117, 121.474859): 1770, (31.48259, 121.348661): 1771, (31.249918, 121.47846): 1772, (31.248195, 121.490929): 1773, (31.060535, 121.274528): 1774, (31.307339, 121.534119): 1775, (30.918284, 120.995861): 1776, (31.181629, 121.510713): 1777, (31.229094, 121.480266): 1778, (31.119755, 121.577952): 1779, (31.304538, 121.526838): 1780, (30.956183, 121.908274): 1781, (31.21899, 121.492002): 1782, (31.20172, 121.743118): 1783, (31.006332, 121.579916): 1784, (25.222206, 117.086322): 1785, (31.18078, 121.423677): 1786, (31.265918, 121.428644): 1787, (31.314684, 121.522729): 1788, (31.211641, 121.474534): 1789, (31.038964, 121.2261): 1790, (31.257756, 121.601633): 1791, (31.19458, 121.436178): 1792, (31.441726, 121.246593): 1793, (31.233151, 121.530498): 1794, (31.298418, 121.539277): 1795, (31.332113, 121.451373): 1796, (31.233984, 121.524823): 1797, (31.378141, 121.503801): 1798, (31.436148, 121.224416): 1799, (31.139656, 121.378972): 1800, (31.235631, 121.567852): 1801, (31.012479, 121.240994): 1802, (31.018041, 121.055413): 1803, (31.165389, 121.119468): 1804, (31.327657, 121.544364): 1805, (31.317512, 121.404518): 1806, (30.990721, 121.803969): 1807, (30.758612, 121.320541): 1808, (31.179594, 121.55951): 1809, (31.264874, 121.459338): 1810, (31.292519, 121.549839): 1811, (31.436901, 121.294601): 1812, (31.169488, 121.349667): 1813, (31.247955, 121.495452): 1814, (31.184426, 121.15811): 1815, (31.193811, 121.373792): 1816, (31.306306, 121.28988): 1817, (31.217261, 121.391532): 1818, (31.039725, 121.225427): 1819, (30.935029, 121.186594): 1820, (30.77648, 121.358227): 1821, (31.258551, 121.584794): 1822, (31.257123, 121.503291): 1823, (31.192397, 121.384407): 1824, (30.901978, 121.452316): 1825, (31.010877, 121.240031): 1826, (31.216472, 121.413336): 1827, (31.174748, 121.39247): 1828, (31.297887, 121.62077): 1829, (31.234688, 121.502656): 1830, (31.275801, 121.484797): 1831, (31.395694, 121.391302): 1832, (30.837343, 121.275733): 1833, (30.957104, 121.146044): 1834, (31.369006, 121.25552): 1835, (31.243266, 121.443875): 1836, (31.156928, 121.121902): 1837, (30.893084, 121.252519): 1838, (31.124812, 121.508263): 1839, (31.340851, 121.63262): 1840, (31.450256, 121.254671): 1841, (31.209424, 121.530284): 1842, (31.267737, 121.529422): 1843, (30.802978, 121.397313): 1844, (31.219647, 121.446669): 1845, (31.248715, 121.481228): 1846, (31.32755, 121.15343): 1847, (31.267826, 121.521146): 1848, (31.261655, 121.474716): 1849, (31.038725, 121.464203): 1850, (31.234225, 121.525481): 1851, (31.217249, 121.504369): 1852, (31.348473, 121.384381): 1853, (31.402193, 121.367083): 1854, (31.274336, 121.39328): 1855, (30.844718, 121.256228): 1856, (31.266646, 121.609157): 1857, (31.186057, 121.44872): 1858, (31.085713, 120.966216): 1859, (31.091117, 121.549904): 1860, (31.19979, 121.389906): 1861, (30.903899, 121.906187): 1862, (31.154462, 121.123826): 1863, (30.838141, 121.189035): 1864, (31.265902, 121.537589): 1865, (30.993285, 121.36769): 1866, (31.21138, 121.728982): 1867, (31.212027, 121.491516): 1868, (31.156454, 121.815297): 1869, (30.898128, 121.536836): 1870, (30.876032, 121.466173): 1871, (31.262331, 121.563212): 1872, (31.038898, 121.23984): 1873, (30.897457, 121.36324): 1874, (31.129482, 121.192266): 1875, (31.320837, 121.542217): 1876, (31.039851, 121.603151): 1877, (31.139017, 121.545465): 1878, (30.869886, 121.901843): 1879, (31.397562, 121.15864): 1880, (31.107362, 121.020176): 1881, (31.21309, 121.531691): 1882, (31.358732, 121.508333): 1883, (31.052253, 121.347624): 1884, (30.781368, 121.418953): 1885, (31.281994, 121.631556): 1886, (31.341175, 121.618019): 1887, (31.41351, 121.367585): 1888, (31.310235, 121.437455): 1889, (31.09599, 121.238111): 1890, (30.931335, 121.472783): 1891, (30.949238, 121.021134): 1892, (31.070433, 121.467199): 1893, (30.992078, 121.31207): 1894, (31.054051, 121.741771): 1895, (31.186698, 121.44855): 1896, (31.267542, 121.685192): 1897, (31.023241, 121.226517): 1898, (31.186176, 121.53911): 1899, (31.280091, 121.519684): 1900, (31.42718, 121.33925): 1901, (31.200179, 121.551597): 1902, (30.935874, 121.692581): 1903, (30.873376, 121.040678): 1904, (31.008378, 121.431928): 1905, (31.370243, 121.576881): 1906, (30.975316, 121.349759): 1907, (31.189366, 121.443482): 1908, (31.217315, 121.635295): 1909, (31.307814, 121.504168): 1910, (30.903117, 121.146985): 1911, (31.285798, 121.546427): 1912, (30.889354, 121.488288): 1913, (31.180526, 121.349428): 1914, (31.198268, 121.383605): 1915, (31.211846, 121.475732): 1916, (31.180824, 121.525363): 1917, (31.20694, 121.459659): 1918, (31.200248, 121.474414): 1919, (31.172778, 121.53453): 1920, (31.266257, 121.506521): 1921, (31.238943, 121.498554): 1922, (31.259098, 121.373134): 1923, (31.198673, 121.479377): 1924, (31.182725, 121.41492): 1925, (31.217729, 121.472441): 1926, (31.288572, 121.534233): 1927, (31.231119, 121.484436): 1928, (31.340297, 121.26652): 1929, (31.236369, 121.476308): 1930, (30.991693, 121.639711): 1931, (31.254643, 121.569349): 1932, (31.18713, 121.451332): 1933, (31.501733, 121.334549): 1934, (30.781759, 121.168333): 1935, (31.196975, 121.440602): 1936, (31.251746, 121.488779): 1937, (31.054254, 121.087156): 1938, (31.200293, 121.400145): 1939, (30.86245, 121.124035): 1940, (30.855815, 121.311882): 1941, (31.302901, 121.553485): 1942, (31.279258, 121.474153): 1943, (31.2798, 121.459715): 1944, (31.291293, 121.559335): 1945, (31.242512, 121.481432): 1946, (31.172866, 121.434994): 1947, (31.292718, 121.410291): 1948, (31.248429, 121.463698): 1949, (31.252347, 121.459317): 1950, (31.268106, 121.49354): 1951, (31.204601, 121.484087): 1952, (31.29962, 121.536202): 1953, (31.327041, 121.661258): 1954, (31.256767, 121.438713): 1955, (31.1634, 121.396418): 1956, (31.150303, 121.430423): 1957, (30.897459, 121.151158): 1958, (31.25142, 121.449186): 1959, (31.444231, 121.225828): 1960, (31.267528, 121.490264): 1961, (31.111061, 121.395692): 1962, (31.223297, 121.430839): 1963, (30.978325, 121.70767): 1964, (31.234221, 121.462296): 1965, (31.112806, 121.462647): 1966, (31.420118, 121.349715): 1967, (30.828106, 121.187081): 1968, (31.19907, 121.517372): 1969, (31.290758, 121.514018): 1970, (31.182122, 121.377455): 1971, (31.206201, 121.085991): 1972, (31.062883, 121.744224): 1973, (31.188332, 121.405187): 1974, (30.918525, 121.74387): 1975, (31.220133, 121.471187): 1976, (31.196602, 121.461665): 1977, (31.248325, 121.158122): 1978, (31.256807, 121.518545): 1979, (31.299376, 121.670896): 1980, (31.211337, 121.413801): 1981, (31.223117, 121.504741): 1982, (31.111358, 121.020928): 1983, (31.241188, 121.483341): 1984, (31.092304, 120.912151): 1985, (30.956486, 121.40597): 1986, (31.203019, 121.443614): 1987, (31.24583, 121.392643): 1988, (31.26905, 121.627732): 1989, (31.243292, 121.518833): 1990, (31.223411, 121.428559): 1991, (31.233617, 121.523688): 1992, (31.356305, 121.293716): 1993, (31.251005, 121.418683): 1994, (31.292709, 121.497635): 1995, (30.738649, 121.382934): 1996, (31.371973, 121.278978): 1997, (31.325552, 121.473746): 1998, (31.286066, 121.474323): 1999, (31.229191, 121.453189): 2000, (30.807278, 121.304136): 2001, (31.219947, 121.467027): 2002, (31.211492, 121.571765): 2003, (31.260723, 121.490166): 2004, (31.195442, 121.754406): 2005, (31.175943, 121.424363): 2006, (30.841016, 121.482504): 2007, (30.940176, 121.086747): 2008, (30.963144, 121.39936): 2009, (31.247628, 121.510061): 2010, (31.1171, 121.391692): 2011, (30.921604, 121.471336): 2012, (31.243738, 121.475784): 2013, (31.392798, 121.246646): 2014, (30.852916, 121.364997): 2015, (31.296619, 121.497055): 2016, (31.404161, 121.47109): 2017, (31.237961, 121.477253): 2018, (31.30361, 121.492602): 2019, (31.242355, 121.490982): 2020, (31.233645, 121.53018): 2021, (31.116086, 121.488253): 2022, (31.38417, 121.344688): 2023, (30.841553, 121.161299): 2024, (31.220424, 121.383546): 2025, (31.268884, 121.44402): 2026, (31.045319, 121.846169): 2027, (31.259841, 121.618221): 2028, (31.150287, 121.471783): 2029, (31.289072, 121.481426): 2030, (31.25867, 121.428896): 2031, (31.260365, 121.454262): 2032, (31.290502, 121.434311): 2033, (31.234105, 121.34044): 2034, (31.332279, 121.286543): 2035, (31.220334, 121.367655): 2036, (31.339843, 121.232133): 2037, (30.85104, 121.524116): 2038, (31.262045, 121.446891): 2039, (31.236742, 121.385193): 2040, (30.902581, 121.167864): 2041, (31.206573, 121.701043): 2042, (31.278711, 121.514214): 2043, (31.252686, 121.491508): 2044, (31.11447, 121.460831): 2045, (31.142588, 121.756503): 2046, (31.243532, 121.330166): 2047, (31.024958, 120.918706): 2048, (30.442177, 120.618727): 2049, (30.815977, 121.436091): 2050, (41.835279, 123.498927): 2051, (31.286253, 121.531109): 2052, (31.195623, 121.398393): 2053, (30.721392, 121.345766): 2054, (31.155148, 121.597536): 2055, (30.872829, 121.424028): 2056, (31.314113, 121.110731): 2057, (31.183064, 121.407773): 2058, (31.242039, 121.687875): 2059, (31.247026, 121.448423): 2060, (31.31764, 121.496784): 2061, (31.246573, 121.495702): 2062, (30.793053, 121.176224): 2063, (31.259114, 121.376147): 2064, (31.272769, 121.57809): 2065, (31.217073, 121.524494): 2066, (31.298712, 121.617334): 2067, (31.225269, 121.491889): 2068, (31.204036, 121.437644): 2069, (31.260715, 121.622366): 2070, (31.267571, 121.5032): 2071, (31.250978, 121.44639): 2072, (31.241054, 121.449606): 2073, (31.206668, 121.449678): 2074, (31.260709, 121.087935): 2075, (31.268862, 121.434292): 2076, (31.348925, 121.178176): 2077, (31.384742, 121.277311): 2078, (30.993594, 121.348984): 2079, (30.94469, 121.299291): 2080, (31.23909, 121.474138): 2081, (31.128344, 121.629697): 2082, (31.497375, 121.324955): 2083, (31.242082, 121.522779): 2084, (31.50595, 121.282812): 2085, (31.279562, 121.467095): 2086, (30.779826, 121.376806): 2087, (31.222754, 121.459663): 2088, (31.415037, 121.260851): 2089, (31.291587, 121.511561): 2090, (31.242967, 121.51048): 2091, (31.312979, 121.545236): 2092, (31.204763, 121.526768): 2093, (31.239615, 121.489026): 2094, (30.929761, 121.587992): 2095, (31.332214, 121.21858): 2096, (31.239474, 121.495166): 2097, (31.225477, 121.448153): 2098, (31.268538, 121.49713): 2099, (31.246371, 121.513215): 2100, (31.216686, 121.493011): 2101, (31.193686, 121.423417): 2102, (31.030606, 121.225475): 2103, (31.262004, 121.350318): 2104, (31.244701, 121.514543): 2105, (31.285418, 121.47179): 2106, (31.250986, 121.460075): 2107, (31.234709, 121.436318): 2108, (31.201321, 121.404211): 2109, (31.236743, 121.36086): 2110, (31.038799, 121.216584): 2111, (31.216659, 121.391301): 2112, (31.226998, 121.556375): 2113, (31.193643, 121.402511): 2114, (31.270785, 121.465575): 2115, (31.110606, 121.517194): 2116, (31.248657, 121.537875): 2117, (31.188145, 121.432275): 2118, (31.02877, 121.456878): 2119, (31.249226, 121.448568): 2120, (31.234382, 121.531106): 2121, (31.22278, 121.450373): 2122, (31.292867, 121.42296): 2123, (31.150587, 121.499019): 2124, (31.362367, 121.461011): 2125, (31.227544, 121.537532): 2126, (31.241242, 121.517216): 2127, (31.20367, 121.543991): 2128, (31.168501, 121.402505): 2129, (31.203981, 121.483421): 2130, (31.29711, 121.519384): 2131, (31.188877, 121.43412): 2132, (31.258321, 121.496097): 2133, (31.194107, 121.399831): 2134, (31.190103, 121.485774): 2135, (31.216092, 121.508679): 2136, (31.016211, 121.412788): 2137, (31.184951, 121.409317): 2138, (31.185044, 121.414303): 2139, (31.19927, 121.6228): 2140, (30.750457, 121.376839): 2141, (31.255248, 121.416487): 2142, (31.198113, 121.590353): 2143, (31.200428, 121.437655): 2144, (31.08705, 121.503604): 2145, (31.10346, 121.419988): 2146, (31.243213, 121.504331): 2147, (31.238774, 121.496149): 2148, (31.310425, 121.439368): 2149, (31.264391, 121.519923): 2150, (31.245488, 121.513767): 2151, (31.302086, 121.510034): 2152, (31.244931, 121.50832): 2153, (31.238392, 121.428006): 2154, (31.233051, 121.455285): 2155, (31.225831, 121.36681): 2156, (31.271455, 121.513192): 2157, (31.462643, 121.331794): 2158, (31.112899, 121.367429): 2159, (31.121996, 121.393805): 2160, (31.273189, 121.472745): 2161, (31.284385, 121.511361): 2162, (31.364338, 121.251014): 2163, (31.178673, 121.521509): 2164, (31.218016, 121.568018): 2165, (31.24018, 121.506063): 2166, (31.195877, 121.597043): 2167, (31.239763, 121.540092): 2168, (31.25863, 121.490496): 2169, (31.24062, 121.428563): 2170, (31.245005, 121.517227): 2171, (31.106732, 121.290566): 2172, (31.250767, 121.458177): 2173, (31.111986, 121.38521): 2174, (31.216528, 121.439617): 2175, (31.229645, 121.481533): 2176, (31.213561, 121.634923): 2177, (30.990222, 121.082742): 2178, (30.918528, 121.113765): 2179, (31.247971, 121.513421): 2180, (31.18458, 121.420921): 2181, (31.236173, 121.466734): 2182, (31.23537, 121.522706): 2183, (31.2106, 121.411728): 2184, (31.21535, 121.46606): 2185, (31.241765, 121.496726): 2186, (31.236197, 121.485283): 2187, (31.389487, 121.259676): 2188, (31.237926, 121.46594): 2189, (31.143681, 121.353698): 2190, (31.197512, 121.470198): 2191, (31.188258, 121.301297): 2192, (31.10221, 121.277743): 2193, (31.087108, 121.371206): 2194, (31.195887, 121.591747): 2195, (31.301584, 121.490912): 2196, (31.098201, 121.323149): 2197, (31.214741, 121.427168): 2198, (31.211757, 121.46438): 2199, (31.266483, 121.545252): 2200, (31.207251, 121.407122): 2201, (31.185126, 121.451633): 2202, (31.463063, 121.345403): 2203, (31.4189, 121.295725): 2204, (31.27726, 121.460289): 2205, (31.169703, 121.412163): 2206, (31.225675, 121.364645): 2207, (31.25879, 121.627202): 2208, (31.143351, 121.352765): 2209, (31.256584, 121.4705): 2210, (31.343386, 121.208271): 2211, (31.250428, 121.460956): 2212, (31.230981, 121.542285): 2213, (31.20414, 121.406657): 2214, (31.212328, 121.407081): 2215, (31.283777, 121.379482): 2216, (31.245183, 121.484965): 2217, (31.475425, 121.356951): 2218, (31.222328, 121.461659): 2219, (31.236576, 121.465894): 2220, (31.234168, 121.473173): 2221, (31.170967, 121.40072): 2222, (30.770905, 121.407376): 2223, (31.198953, 121.516066): 2224, (31.09201, 121.395696): 2225, (31.103957, 121.456645): 2226, (31.236339, 121.52576): 2227, (31.314136, 121.549429): 2228, (31.230406, 121.482326): 2229, (31.22132, 121.438917): 2230, (31.447176, 121.385936): 2231, (31.290912, 121.498587): 2232, (31.277632, 121.380997): 2233, (31.278753, 121.505741): 2234, (31.299613, 121.442959): 2235, (30.975347, 121.032558): 2236, (31.232268, 121.541982): 2237, (31.365012, 121.504701): 2238, (31.391154, 121.246483): 2239, (31.270809, 121.543999): 2240, (31.307358, 121.524928): 2241, (31.235341, 121.477273): 2242, (31.326513, 121.490258): 2243, (31.189816, 121.430881): 2244, (31.140348, 121.210964): 2245, (31.192457, 121.443575): 2246, (31.210657, 121.474053): 2247, (31.209934, 121.604472): 2248, (31.265413, 121.564718): 2249, (31.238839, 121.438593): 2250, (31.280003, 121.361419): 2251, (31.284348, 121.564303): 2252, (31.229151, 121.484838): 2253, (31.235908, 121.57067): 2254, (31.248866, 121.478438): 2255, (31.270571, 121.476897): 2256, (31.227254, 121.402789): 2257, (31.231328, 121.487479): 2258, (31.217037, 121.43649): 2259, (31.229203, 121.449776): 2260, (31.256537, 121.625952): 2261, (31.207094, 121.713617): 2262, (31.2754, 121.485024): 2263, (31.30379, 121.455146): 2264, (31.240865, 121.426827): 2265, (31.379546, 121.260455): 2266, (31.333313, 121.333805): 2267, (31.240405, 121.569186): 2268, (31.206761, 121.399923): 2269, (31.23787, 121.543207): 2270, (31.236531, 121.491325): 2271, (31.281993, 121.464732): 2272, (31.156811, 121.236841): 2273, (31.317624, 121.405225): 2274, (31.193771, 121.546521): 2275, (30.901446, 121.904248): 2276, (31.214969, 121.586763): 2277, (31.234737, 121.513936): 2278, (30.930847, 121.490112): 2279, (31.07252, 121.404373): 2280, (31.305124, 121.454071): 2281, (31.265604, 121.567367): 2282, (31.036007, 121.46539): 2283, (31.224855, 121.480154): 2284, (30.967758, 121.08302): 2285, (31.240661, 121.512436): 2286, (31.243917, 121.496829): 2287, (31.223017, 121.465143): 2288, (30.979419, 121.022464): 2289, (31.262993, 121.08814): 2290, (31.18158, 121.295188): 2291, (31.225663, 121.452017): 2292, (30.940039, 121.548407): 2293, (31.258441, 121.461361): 2294, (31.217655, 121.534591): 2295, (30.891706, 121.171358): 2296, (30.763912, 121.333802): 2297, (31.177291, 121.405153): 2298, (31.152563, 121.460663): 2299, (31.199535, 121.456715): 2300, (31.195642, 121.446626): 2301, (31.399445, 121.464161): 2302, (31.161442, 121.507559): 2303, (31.270493, 121.353356): 2304, (31.211534, 121.510558): 2305, (31.299183, 121.628904): 2306, (31.136936, 121.325876): 2307, (31.267888, 121.46753): 2308, (31.197152, 121.444999): 2309, (31.296101, 121.505474): 2310, (35.522852, 102.0076): 2311, (31.169078, 121.433389): 2312, (31.233659, 121.565806): 2313, (30.97167, 121.185692): 2314, (31.177439, 121.108654): 2315, (31.235501, 121.402602): 2316, (31.299291, 121.454034): 2317, (31.169494, 121.302234): 2318, (31.301105, 121.430139): 2319, (31.15562, 121.130864): 2320, (31.22656, 121.490417): 2321, (31.219856, 121.568149): 2322, (31.283793, 121.518888): 2323, (31.236431, 121.460781): 2324, (31.113675, 121.383752): 2325, (31.236926, 121.454604): 2326, (30.981236, 121.488197): 2327, (31.054777, 121.166179): 2328, (31.237789, 121.461058): 2329, (31.240979, 121.677316): 2330, (31.220745, 121.438858): 2331, (31.23301, 121.556388): 2332, (31.17255, 121.405133): 2333, (31.263879, 121.49436): 2334, (30.987088, 121.588932): 2335, (31.1823, 121.394288): 2336, (31.235419, 121.486195): 2337, (31.217217, 121.558644): 2338, (30.747936, 121.348822): 2339, (31.234835, 121.527767): 2340, (31.237696, 121.382534): 2341, (31.269535, 121.4975): 2342, (31.25841, 121.392912): 2343, (31.228282, 121.556676): 2344, (31.264405, 121.489289): 2345, (31.22978, 121.540987): 2346, (31.238517, 121.429012): 2347, (31.292582, 121.508354): 2348, (31.206458, 121.562768): 2349, (31.23095, 121.537305): 2350, (31.138666, 121.398483): 2351, (31.246038, 121.531441): 2352, (31.187712, 121.432106): 2353, (31.232527, 121.530359): 2354, (31.182032, 121.388195): 2355, (31.193646, 121.449596): 2356, (31.202004, 121.591317): 2357, (31.208153, 121.41083): 2358, (31.025516, 121.30169): 2359, (31.239743, 121.512257): 2360, (31.235554, 121.479641): 2361, (31.419608, 121.431596): 2362, (31.266108, 121.341404): 2363, (31.19622, 121.468181): 2364, (31.233112, 121.442667): 2365, (31.17625, 121.418829): 2366, (31.21759, 121.40829): 2367, (31.180043, 121.609028): 2368, (31.182633, 121.439015): 2369, (31.248135, 121.539233): 2370, (31.150865, 121.336324): 2371, (31.234787, 121.498324): 2372, (31.218928, 121.402968): 2373, (31.233983, 121.455055): 2374, (31.177836, 121.408499): 2375, (31.348039, 121.147744): 2376, (31.386162, 121.255053): 2377, (31.215028, 121.438872): 2378, (31.256803, 121.522183): 2379, (31.241234, 121.460559): 2380, (31.258629, 121.491056): 2381, (30.806653, 121.257345): 2382, (31.228956, 121.481687): 2383, (31.258745, 121.370293): 2384, (31.23282, 121.468049): 2385, (31.228634, 121.536498): 2386, (31.209857, 121.644884): 2387, (31.264302, 121.428277): 2388, (31.210053, 121.464569): 2389, (31.296226, 121.494005): 2390, (31.328439, 121.384246): 2391, (31.179351, 121.31286): 2392, (31.187859, 121.444801): 2393, (31.235982, 121.466191): 2394, (31.37219, 121.448243): 2395, (31.213332, 121.619545): 2396, (31.23423, 121.436418): 2397, (31.237667, 121.434224): 2398, (31.293848, 121.528021): 2399, (31.039424, 121.225364): 2400, (31.244479, 121.429372): 2401, (31.237221, 121.467334): 2402, (31.200668, 121.480775): 2403, (31.230519, 121.520449): 2404, (31.281772, 121.484273): 2405, (31.228956, 121.535003): 2406, (31.203194, 121.46865): 2407, (31.216146, 121.472933): 2408, (31.192935, 121.39968): 2409, (31.270144, 121.354637): 2410, (31.199096, 121.401958): 2411, (31.234888, 121.447606): 2412, (31.228906, 121.451142): 2413, (30.972355, 121.610007): 2414, (31.214689, 121.426655): 2415, (31.21176, 121.45331): 2416, (47.35092, 130.301233): 2417, (31.249796, 121.582523): 2418, (31.359987, 121.417748): 2419, (31.06954, 120.928583): 2420, (31.260348, 121.542138): 2421, (30.881092, 121.162611): 2422, (31.246782, 121.528695): 2423, (30.941552, 121.1676): 2424, (31.154497, 121.501351): 2425, (31.043246, 120.924071): 2426, (31.239726, 121.510008): 2427, (31.156954, 121.146623): 2428, (31.252588, 121.463263): 2429, (31.315817, 121.359172): 2430, (30.899462, 121.164967): 2431, (31.211781, 121.405437): 2432, (31.012103, 121.414993): 2433, (31.186202, 121.54793): 2434, (31.193331, 121.599017): 2435, (31.249356, 121.410275): 2436, (31.161896, 121.135143): 2437, (31.251094, 121.416046): 2438, (31.249067, 121.543826): 2439, (31.256654, 121.599709): 2440, (31.170526, 121.34996): 2441, (31.209454, 121.524245): 2442, (31.232395, 121.564673): 2443, (31.149062, 121.392526): 2444, (31.21882, 121.436387): 2445, (31.236897, 121.363985): 2446, (31.1917, 121.440205): 2447, (31.294781, 121.494788): 2448, (31.231316, 121.54242): 2449, (30.83237, 121.439654): 2450, (31.200779, 121.533917): 2451, (31.236842, 121.433815): 2452, (31.275472, 121.436799): 2453, (31.23672, 121.488128): 2454, (31.260173, 121.348277): 2455, (31.193608, 121.448072): 2456, (31.221688, 121.462858): 2457, (31.242363, 121.480168): 2458, (31.238453, 121.377045): 2459, (31.209344, 121.414036): 2460, (31.306637, 121.523563): 2461, (31.17148, 121.411456): 2462, (31.290316, 121.457814): 2463, (31.186411, 121.402496): 2464, (31.265994, 121.519353): 2465, (31.276741, 121.514196): 2466, (31.278997, 121.510278): 2467, (31.308523, 121.423052): 2468, (31.240977, 121.494087): 2469, (31.188183, 121.440258): 2470, (31.233708, 121.454373): 2471, (31.243061, 121.517168): 2472, (31.248836, 121.463246): 2473, (31.177098, 121.525845): 2474, (31.201009, 121.610969): 2475, (31.063518, 121.215539): 2476, (31.212034, 121.476918): 2477, (31.191496, 121.41093): 2478, (31.227864, 121.638217): 2479, (31.295365, 121.16604): 2480, (31.242706, 121.49688): 2481, (31.123026, 120.926117): 2482, (31.218858, 121.465451): 2483, (31.303333, 121.455058): 2484, (30.879413, 121.704723): 2485, (31.198912, 121.444187): 2486, (31.052413, 121.218957): 2487, (31.192182, 121.466139): 2488, (31.154144, 121.118793): 2489, (31.236055, 121.434759): 2490, (31.256113, 121.510151): 2491, (31.26153, 121.582783): 2492, (31.111905, 121.05229): 2493, (31.197367, 121.479142): 2494, (31.233107, 121.417598): 2495, (31.129363, 121.425144): 2496, (31.267858, 121.495135): 2497, (31.410254, 121.496122): 2498, (31.203742, 121.461251): 2499, (31.338453, 121.449399): 2500, (31.216462, 121.677078): 2501, (31.061709, 121.771457): 2502, (31.173828, 121.781108): 2503, (31.228542, 121.538488): 2504, (31.210354, 121.595268): 2505, (31.275255, 121.45991): 2506, (31.258454, 121.440744): 2507, (31.272257, 121.513471): 2508, (31.233059, 121.458379): 2509, (31.241674, 121.485882): 2510, (31.226676, 121.555069): 2511, (31.211339, 121.406552): 2512, (31.201121, 121.445016): 2513, (31.295646, 121.498045): 2514, (31.156365, 121.41831): 2515, (31.234916, 121.447878): 2516, (30.866941, 121.595653): 2517, (31.261499, 121.457989): 2518, (31.238385, 121.521449): 2519, (31.246512, 121.514772): 2520, (31.213397, 121.413655): 2521, (31.241084, 121.467265): 2522, (31.289799, 121.455117): 2523, (31.146112, 121.504322): 2524, (31.200865, 121.448273): 2525, (31.230671, 121.552848): 2526, (31.249854, 121.497793): 2527, (31.350476, 121.519951): 2528, (31.205115, 121.492127): 2529, (31.211021, 121.569582): 2530, (30.910821, 121.465194): 2531, (31.404925, 121.473328): 2532, (30.864276, 121.112687): 2533, (31.276778, 121.542361): 2534, (31.115773, 120.907942): 2535, (31.25495, 121.739898): 2536, (31.242864, 121.426322): 2537, (31.375832, 121.389413): 2538, (31.244865, 121.549508): 2539, (31.299734, 121.531717): 2540, (30.719644, 121.345988): 2541, (31.246935, 121.49406): 2542, (31.194719, 121.448676): 2543, (31.246271, 121.425156): 2544, (31.17216, 121.347748): 2545, (31.258483, 121.368346): 2546, (31.334437, 121.640763): 2547, (31.234093, 121.475011): 2548, (31.257724, 121.595342): 2549, (30.939279, 121.074396): 2550, (31.262311, 121.449241): 2551, (31.316634, 121.657988): 2552, (31.200875, 121.481499): 2553, (31.234528, 121.528657): 2554, (31.19565, 121.439802): 2555, (31.259353, 121.427018): 2556, (30.940804, 121.073937): 2557, (31.145979, 121.376079): 2558, (31.2114, 121.476689): 2559, (31.233185, 121.481944): 2560, (31.233011, 121.523809): 2561, (31.239149, 121.488022): 2562, (31.239862, 121.490491): 2563, (31.239492, 121.486003): 2564, (31.21351, 121.3728): 2565, (31.235309, 121.529189): 2566, (31.25316, 121.459116): 2567, (31.184752, 121.387556): 2568, (31.213212, 121.470589): 2569, (31.223367, 121.46595): 2570, (31.267673, 121.346341): 2571, (31.264645, 121.446818): 2572, (31.103599, 121.416145): 2573, (29.151779, 120.985563): 2574, (31.230405, 121.534439): 2575, (31.19601, 121.444227): 2576, (31.297053, 121.52596): 2577, (31.311026, 121.456026): 2578, (31.203621, 121.393816): 2579, (31.242132, 121.488632): 2580, (31.139021, 121.323325): 2581, (31.014893, 121.047477): 2582, (31.39714, 121.507729): 2583, (31.246176, 121.494923): 2584, (31.23478, 121.51668): 2585, (31.32589, 121.477565): 2586, (31.351912, 121.58074): 2587, (31.197034, 121.391103): 2588, (31.241675, 121.441049): 2589, (31.237596, 121.488614): 2590, (31.24672, 121.38645): 2591, (31.246871, 121.409978): 2592, (31.199128, 121.540588): 2593, (31.215311, 121.523574): 2594, (31.24188, 121.487961): 2595, (31.224841, 121.440899): 2596, (31.226708, 121.481109): 2597, (31.203225, 121.399229): 2598, (31.263574, 121.586365): 2599, (31.247886, 121.391323): 2600, (31.279298, 121.485756): 2601, (30.735389, 121.358248): 2602, (31.258361, 121.594427): 2603, (31.192395, 121.748164): 2604, (31.223306, 121.378382): 2605, (31.239804, 121.545118): 2606, (31.189897, 121.354305): 2607, (31.301024, 121.509195): 2608, (31.224391, 121.489143): 2609, (31.212572, 121.537172): 2610, (31.160324, 121.508559): 2611, (31.215893, 121.558154): 2612, (31.238616, 121.477514): 2613, (31.226286, 121.600124): 2614, (30.679943, 104.067923): 2615, (31.194364, 121.400164): 2616, (31.253467, 121.447489): 2617, (31.242324, 121.221984): 2618, (31.255872, 121.493382): 2619, (31.012822, 121.051558): 2620, (31.276883, 121.507533): 2621, (31.212793, 121.494061): 2622, (31.189, 121.431937): 2623, (31.249972, 121.422237): 2624, (31.251211, 121.491022): 2625, (31.034225, 121.254609): 2626, (31.240285, 121.546323): 2627, (31.231815, 121.464062): 2628, (31.203422, 121.526246): 2629, (31.191178, 121.46648): 2630, (31.216269, 121.435171): 2631, (31.27495, 121.506381): 2632, (31.233679, 121.494765): 2633, (31.240588, 121.379145): 2634, (31.229886, 121.536598): 2635, (30.95485, 121.328077): 2636, (31.087196, 121.505435): 2637, (31.247703, 121.536476): 2638, (31.152571, 121.412981): 2639, (31.203911, 121.590509): 2640, (31.082557, 121.430065): 2641, (31.224487, 121.452917): 2642, (31.23976, 121.482648): 2643, (31.180728, 121.386832): 2644, (31.210101, 121.414922): 2645, (31.208403, 121.441631): 2646, (31.187492, 121.452395): 2647, (31.135275, 121.765517): 2648, (30.867838, 121.737782): 2649, (31.243189, 121.515482): 2650, (31.187384, 121.470686): 2651, (31.218005, 121.430759): 2652, (30.831962, 121.534676): 2653, (30.730789, 121.343497): 2654, (31.33881, 121.604229): 2655, (31.27688, 121.420858): 2656, (31.18674, 121.380533): 2657, (31.251313, 121.488307): 2658, (31.190941, 121.420437): 2659, (31.271086, 121.549327): 2660, (31.030352, 120.990008): 2661, (31.229148, 121.495962): 2662, (31.19914, 121.438003): 2663, (31.241369, 121.458067): 2664, (31.229998, 121.538807): 2665, (31.230429, 121.468099): 2666, (31.376534, 121.563363): 2667, (30.735272, 121.380359): 2668, (31.287524, 121.51338): 2669, (31.211093, 121.409091): 2670, (31.239094, 121.519821): 2671, (31.259135, 121.616618): 2672, (31.233431, 121.471698): 2673, (31.210632, 121.477007): 2674, (31.23923, 121.483027): 2675, (31.095422, 121.393617): 2676, (31.287554, 121.539): 2677, (31.242208, 121.509707): 2678, (31.245215, 121.601813): 2679, (31.331954, 121.451325): 2680, (31.231035, 121.555602): 2681, (30.929541, 121.616421): 2682, (31.260724, 121.469924): 2683, (31.226563, 121.483486): 2684, (31.184327, 121.381012): 2685, (31.085245, 121.533086): 2686, (31.038935, 121.465427): 2687, (31.232548, 121.478971): 2688, (30.735262, 121.351726): 2689, (31.278145, 121.452159): 2690, (31.264003, 121.489834): 2691, (31.232971, 121.47882): 2692, (31.191797, 121.442896): 2693, (31.274419, 121.475879): 2694, (31.183039, 121.434356): 2695, (31.151008, 121.324166): 2696, (31.245569, 121.5156): 2697, (31.177248, 121.51249): 2698, (31.123763, 120.901878): 2699, (31.242744, 121.425797): 2700, (31.187829, 121.452956): 2701, (31.21953, 121.455395): 2702, (31.167548, 121.425927): 2703, (31.237955, 121.493402): 2704, (31.027984, 121.267724): 2705, (31.260191, 121.617048): 2706, (31.23341, 121.483804): 2707, (31.169908, 121.418183): 2708, (31.203252, 121.441646): 2709, (31.2146, 121.522908): 2710, (31.236337, 121.490883): 2711, (31.40369, 121.255993): 2712, (31.234238, 121.521932): 2713, (31.106411, 121.450195): 2714, (28.812629, 115.952954): 2715, (31.26999, 121.472205): 2716, (31.23811, 121.493447): 2717, (30.995671, 121.095344): 2718, (31.003024, 121.254796): 2719, (31.2419, 121.506952): 2720, (31.168688, 121.487892): 2721, (31.216531, 121.541197): 2722, (31.19053, 121.445372): 2723, (31.198273, 121.445101): 2724, (31.224758, 121.446392): 2725, (31.279618, 121.439189): 2726, (31.250629, 121.598677): 2727, (31.22903, 121.3909): 2728, (31.201959, 121.61095): 2729, (31.231339, 121.543694): 2730, (30.976672, 121.459345): 2731, (31.201817, 121.452116): 2732, (31.110847, 121.509512): 2733, (31.410799, 121.48934): 2734, (31.250145, 121.450312): 2735, (31.22716, 121.386851): 2736, (31.269951, 121.336766): 2737, (31.22626, 121.551311): 2738, (31.315735, 121.534779): 2739, (30.829073, 121.470254): 2740, (31.231401, 121.497622): 2741, (31.234885, 121.529933): 2742, (31.223721, 121.482534): 2743, (31.244249, 121.675329): 2744, (31.309, 121.525098): 2745, (31.213659, 121.455113): 2746, (31.250814, 121.449225): 2747, (31.204322, 121.455438): 2748, (31.294157, 121.432501): 2749, (31.571904, 121.158978): 2750, (31.174223, 121.436719): 2751, (31.170804, 121.278846): 2752, (31.216682, 121.468605): 2753, (31.223648, 121.467523): 2754, (31.245938, 121.422548): 2755, (31.343354, 121.59331): 2756, (30.880213, 121.907656): 2757, (31.198142, 121.446688): 2758, (31.350554, 121.391849): 2759, (31.289528, 121.412608): 2760, (31.243869, 121.440506): 2761, (31.395845, 121.450008): 2762, (31.246433, 121.443184): 2763, (31.231749, 121.486203): 2764, (31.224006, 121.423572): 2765, (31.286523, 121.538876): 2766, (31.239836, 121.497502): 2767, (31.20907, 121.436851): 2768, (31.254745, 121.556081): 2769}\n",
            "{528: 41, 874: 32, 237: 25, 580: 61, 1102: 45, 180: 20, 764: 20, 1638: 29, 558: 137, 652: 96, 158: 73, 446: 63, 1325: 45, 59: 24, 1595: 18, 1009: 21, 886: 16, 285: 32, 1310: 2, 713: 6, 211: 37, 82: 10, 1585: 44, 757: 23, 428: 48, 1194: 52, 1407: 71, 1101: 56, 906: 13, 222: 73, 3: 12, 377: 50, 161: 17, 1549: 18, 260: 200, 2096: 7, 1602: 30, 1572: 12, 120: 16, 2267: 33, 100: 2, 471: 26, 169: 50, 1023: 3, 172: 24, 870: 8, 641: 10, 442: 21, 761: 47, 737: 12, 1241: 12, 2368: 4, 802: 84, 151: 44, 86: 111, 257: 75, 2387: 1, 725: 202, 114: 64, 778: 24, 798: 35, 762: 25, 591: 23, 1105: 5, 1294: 10, 2688: 5, 978: 41, 2242: 2, 305: 9, 1933: 6, 765: 25, 1139: 64, 531: 43, 1658: 19, 654: 38, 389: 65, 385: 183, 865: 133, 139: 115, 70: 87, 2573: 2, 224: 39, 555: 29, 284: 28, 596: 23, 1217: 28, 545: 30, 273: 84, 770: 14, 1134: 53, 320: 156, 668: 30, 807: 11, 242: 60, 264: 166, 225: 40, 917: 45, 749: 48, 698: 68, 520: 82, 1160: 46, 157: 100, 822: 42, 46: 52, 1620: 8, 444: 58, 704: 112, 1422: 58, 214: 102, 673: 98, 77: 48, 1404: 13, 109: 128, 947: 99, 275: 25, 842: 100, 166: 72, 486: 102, 1707: 41, 458: 45, 1030: 33, 173: 129, 1395: 16, 1263: 12, 1273: 26, 407: 42, 800: 36, 1430: 45, 115: 50, 1649: 17, 523: 46, 229: 23, 1396: 2, 848: 5, 1180: 6, 1599: 4, 1126: 45, 712: 5, 977: 23, 614: 11, 948: 10, 453: 36, 747: 8, 1390: 13, 1858: 1, 1581: 6, 1046: 21, 801: 16, 1195: 14, 1737: 8, 780: 27, 2438: 2, 219: 102, 208: 110, 272: 87, 1129: 21, 81: 93, 1528: 21, 907: 55, 586: 51, 1770: 7, 372: 50, 414: 168, 1634: 15, 1158: 24, 1664: 7, 178: 104, 1309: 71, 711: 49, 1411: 109, 376: 51, 1532: 14, 20: 53, 1706: 6, 2485: 2, 491: 104, 2541: 7, 313: 117, 1539: 1, 958: 76, 581: 143, 998: 39, 968: 1, 1270: 40, 99: 21, 306: 72, 74: 48, 221: 54, 231: 82, 1280: 21, 234: 113, 429: 12, 851: 103, 363: 110, 631: 79, 954: 68, 233: 2, 1414: 55, 1022: 41, 1159: 17, 957: 24, 2078: 19, 1011: 25, 2059: 36, 1112: 46, 1397: 69, 1261: 74, 388: 80, 317: 35, 519: 51, 706: 40, 745: 48, 339: 104, 1499: 31, 1756: 22, 1589: 29, 1042: 26, 336: 70, 418: 29, 2501: 1, 2003: 1, 1229: 7, 218: 62, 2036: 8, 1553: 4, 2648: 1, 618: 11, 663: 70, 450: 99, 979: 45, 1088: 35, 441: 64, 123: 100, 477: 43, 595: 42, 903: 45, 382: 31, 980: 16, 1712: 4, 483: 32, 942: 12, 1967: 3, 1065: 14, 1308: 12, 1685: 6, 1144: 31, 379: 170, 24: 57, 71: 19, 1490: 5, 1249: 16, 1286: 26, 611: 28, 721: 13, 1625: 18, 1605: 28, 479: 12, 1031: 13, 537: 6, 2231: 8, 2203: 8, 290: 171, 36: 109, 195: 252, 198: 156, 463: 99, 1415: 1, 858: 161, 769: 108, 15: 53, 468: 24, 680: 4, 14: 75, 390: 131, 1629: 10, 207: 56, 73: 125, 976: 32, 750: 28, 1475: 9, 1778: 4, 1636: 14, 65: 22, 773: 36, 928: 87, 110: 71, 1055: 25, 399: 28, 304: 107, 1037: 27, 844: 28, 1486: 51, 1378: 20, 703: 15, 699: 6, 1981: 14, 246: 40, 690: 48, 2157: 2, 41: 20, 627: 33, 1437: 49, 370: 19, 439: 11, 52: 76, 1048: 5, 1423: 3, 1962: 3, 2011: 4, 793: 89, 638: 46, 1480: 39, 205: 96, 241: 33, 616: 19, 133: 72, 496: 37, 325: 28, 535: 70, 344: 109, 885: 83, 650: 52, 926: 6, 564: 34, 1277: 12, 1342: 49, 1231: 2, 394: 25, 1587: 11, 1593: 18, 1825: 27, 1590: 5, 1699: 9, 1871: 80, 1306: 52, 193: 137, 1184: 21, 1125: 46, 532: 72, 1336: 35, 922: 7, 37: 47, 1049: 55, 331: 28, 983: 11, 715: 17, 1246: 50, 27: 30, 1709: 29, 810: 19, 1000: 11, 1675: 6, 927: 42, 243: 66, 226: 178, 1145: 61, 1156: 15, 1353: 27, 159: 8, 1515: 2, 156: 51, 365: 47, 132: 42, 609: 113, 719: 65, 22: 73, 467: 182, 606: 30, 925: 70, 1135: 19, 121: 15, 1506: 60, 679: 32, 415: 105, 2283: 13, 1462: 3, 1154: 104, 127: 30, 1331: 23, 825: 42, 878: 44, 1269: 6, 2090: 1, 1548: 16, 727: 14, 311: 16, 1619: 15, 1068: 21, 2032: 2, 949: 17, 919: 20, 1446: 38, 1230: 16, 348: 114, 387: 53, 405: 44, 21: 92, 91: 56, 1369: 21, 1252: 15, 171: 57, 1973: 8, 794: 32, 867: 8, 961: 8, 1032: 19, 1013: 9, 56: 123, 80: 140, 1885: 10, 1844: 4, 1103: 42, 741: 78, 2223: 8, 723: 27, 2297: 4, 98: 65, 125: 70, 557: 53, 350: 17, 1379: 11, 1442: 2, 658: 56, 358: 23, 249: 17, 1250: 26, 1402: 6, 476: 14, 7: 44, 1413: 44, 877: 54, 28: 46, 202: 69, 1356: 27, 1900: 2, 364: 6, 461: 23, 1185: 23, 1357: 14, 590: 24, 1421: 3, 1058: 38, 1603: 18, 396: 115, 527: 70, 337: 103, 572: 24, 281: 24, 50: 33, 707: 44, 521: 29, 1216: 11, 701: 31, 1445: 38, 1463: 39, 245: 67, 44: 35, 505: 10, 2429: 5, 1107: 22, 551: 45, 639: 109, 2352: 4, 440: 20, 230: 19, 60: 59, 1202: 7, 714: 36, 1949: 6, 1006: 23, 1040: 17, 2002: 17, 1924: 2, 1311: 19, 648: 8, 1017: 46, 1033: 43, 577: 5, 1224: 28, 13: 32, 775: 7, 51: 36, 876: 23, 946: 51, 134: 12, 549: 20, 971: 22, 254: 25, 182: 5, 457: 57, 530: 36, 913: 45, 1496: 15, 1008: 40, 1441: 28, 753: 30, 2252: 2, 837: 25, 261: 107, 395: 100, 1305: 1, 300: 11, 1728: 14, 605: 210, 1455: 7, 1364: 7, 1239: 38, 836: 102, 1059: 47, 1734: 21, 1432: 5, 548: 10, 1120: 7, 1561: 16, 176: 47, 936: 35, 882: 55, 758: 17, 1255: 15, 625: 29, 341: 30, 48: 20, 1942: 3, 777: 1, 592: 6, 49: 50, 371: 13, 170: 91, 1686: 22, 1206: 25, 475: 17, 2007: 13, 873: 5, 1743: 86, 672: 26, 1723: 3, 748: 16, 2273: 1, 2245: 3, 425: 111, 989: 32, 403: 38, 189: 93, 1298: 7, 1021: 81, 550: 48, 883: 58, 920: 89, 4: 41, 2637: 5, 993: 96, 1312: 56, 83: 78, 588: 74, 734: 89, 755: 59, 1429: 17, 2433: 1, 312: 65, 1433: 41, 850: 98, 1791: 16, 456: 12, 669: 66, 421: 108, 565: 70, 9: 69, 771: 43, 634: 35, 398: 74, 1019: 48, 2377: 1, 401: 120, 164: 36, 92: 89, 17: 126, 1630: 18, 1400: 57, 632: 33, 589: 33, 923: 42, 474: 41, 432: 50, 186: 26, 497: 61, 816: 6, 1200: 12, 934: 73, 113: 15, 2175: 1, 367: 37, 433: 27, 702: 27, 149: 116, 1383: 28, 1057: 53, 1996: 7, 938: 61, 1115: 15, 1784: 8, 152: 19, 283: 115, 651: 88, 32: 50, 1283: 76, 466: 27, 1516: 21, 298: 22, 626: 31, 1592: 5, 1617: 17, 2769: 1, 252: 89, 498: 3, 6: 95, 64: 71, 820: 57, 760: 40, 445: 24, 1985: 1, 2253: 1, 1170: 85, 640: 78, 813: 17, 427: 59, 613: 35, 62: 17, 1436: 6, 742: 25, 1368: 48, 289: 14, 490: 16, 1152: 1, 1018: 11, 1359: 12, 1997: 18, 1993: 7, 1242: 13, 1531: 18, 1086: 36, 122: 53, 633: 10, 544: 45, 1295: 15, 413: 67, 1245: 54, 1450: 9, 61: 68, 392: 53, 1293: 14, 324: 9, 1077: 8, 1484: 40, 451: 45, 1633: 5, 1438: 33, 829: 118, 1132: 51, 965: 46, 347: 46, 1739: 20, 889: 43, 720: 22, 16: 25, 1313: 40, 1990: 2, 763: 23, 996: 15, 1: 60, 1211: 60, 963: 117, 1769: 15, 1114: 24, 1964: 18, 25: 69, 138: 65, 19: 68, 1679: 10, 554: 4, 454: 161, 781: 81, 1883: 4, 1304: 40, 2295: 6, 1537: 12, 1234: 25, 1458: 62, 1361: 24, 891: 29, 1110: 48, 1643: 27, 1497: 10, 1274: 34, 1508: 9, 2006: 15, 2101: 5, 579: 64, 1665: 24, 493: 23, 361: 49, 570: 39, 827: 13, 1647: 7, 1024: 4, 513: 8, 2620: 1, 1405: 23, 404: 6, 845: 22, 1837: 2, 872: 105, 326: 131, 843: 57, 662: 29, 478: 24, 1330: 7, 892: 18, 568: 99, 2390: 22, 562: 105, 623: 89, 834: 35, 806: 10, 184: 70, 1119: 34, 766: 71, 192: 57, 786: 55, 974: 11, 1362: 4, 967: 35, 509: 16, 267: 7, 841: 12, 563: 9, 659: 63, 635: 27, 85: 44, 1758: 10, 1812: 10, 1793: 16, 1760: 4, 1999: 13, 1667: 21, 346: 11, 69: 6, 649: 27, 1078: 44, 183: 77, 187: 84, 964: 27, 124: 63, 277: 19, 716: 167, 322: 19, 682: 80, 1966: 29, 309: 30, 181: 12, 103: 16, 1166: 9, 119: 25, 352: 53, 1130: 8, 1399: 7, 2282: 6, 276: 56, 410: 14, 2471: 3, 1266: 35, 751: 36, 179: 17, 1467: 5, 1941: 11, 746: 46, 1340: 29, 1371: 8, 1655: 15, 1133: 52, 1350: 24, 78: 127, 209: 57, 908: 72, 710: 35, 2074: 4, 375: 82, 1285: 44, 1323: 81, 57: 89, 861: 8, 1626: 2, 826: 50, 251: 97, 1869: 10, 1137: 54, 732: 109, 1096: 50, 656: 22, 315: 48, 973: 14, 1479: 1, 2714: 5, 302: 18, 2546: 3, 1071: 11, 1193: 1, 987: 22, 962: 10, 1613: 14, 516: 64, 1495: 28, 256: 56, 345: 26, 696: 59, 1035: 37, 524: 47, 859: 51, 1616: 53, 282: 132, 881: 22, 901: 20, 1501: 18, 288: 2, 1718: 2, 369: 22, 526: 65, 68: 51, 1939: 6, 612: 22, 1705: 3, 637: 75, 351: 16, 1157: 15, 1676: 29, 893: 93, 95: 134, 1111: 10, 2305: 3, 1786: 7, 2407: 2, 53: 39, 244: 13, 593: 39, 1562: 8, 2393: 4, 1171: 8, 1977: 6, 1236: 38, 1608: 14, 1697: 27, 1736: 16, 1151: 33, 1341: 21, 1817: 15, 1518: 26, 318: 93, 1646: 29, 247: 75, 2211: 28, 1210: 40, 2077: 6, 1391: 5, 201: 70, 1183: 36, 567: 66, 1003: 6, 1412: 5, 2029: 4, 803: 30, 402: 16, 1615: 6, 815: 24, 1426: 36, 1127: 48, 150: 29, 1010: 36, 167: 6, 1670: 17, 855: 3, 933: 46, 1650: 19, 2756: 1, 969: 23, 818: 24, 1457: 37, 1329: 26, 875: 61, 297: 54, 831: 22, 238: 83, 217: 22, 435: 27, 89: 8, 666: 31, 1834: 19, 1284: 36, 1406: 10, 2363: 1, 88: 18, 1327: 11, 473: 32, 541: 13, 47: 106, 500: 35, 356: 45, 538: 19, 2307: 2, 517: 47, 607: 93, 717: 92, 952: 7, 1889: 43, 1056: 5, 1201: 22, 869: 14, 729: 73, 38: 38, 759: 55, 31: 16, 797: 18, 506: 22, 1896: 1, 424: 23, 1465: 2, 409: 11, 1143: 43, 835: 44, 87: 30, 162: 52, 1627: 5, 1640: 9, 852: 38, 147: 8, 154: 24, 574: 90, 2056: 35, 522: 78, 72: 26, 1766: 3, 1282: 8, 299: 96, 1861: 3, 569: 42, 601: 28, 1060: 21, 1025: 2, 731: 11, 1461: 4, 1172: 16, 1223: 18, 492: 42, 34: 34, 540: 24, 362: 64, 1409: 24, 542: 37, 1751: 26, 1367: 95, 1730: 15, 481: 147, 1542: 63, 507: 8, 1233: 23, 2187: 1, 944: 9, 400: 19, 692: 57, 700: 14, 785: 11, 740: 17, 1014: 11, 2559: 2, 144: 38, 534: 20, 1485: 38, 924: 93, 1192: 31, 945: 17, 744: 47, 1226: 14, 743: 8, 1979: 6, 1227: 11, 1146: 5, 1082: 23, 887: 10, 1818: 2, 332: 37, 148: 8, 2654: 2, 1522: 7, 657: 6, 1403: 20, 1789: 11, 814: 14, 1012: 12, 617: 62, 430: 48, 1394: 13, 236: 10, 58: 10, 1316: 5, 1892: 9, 1722: 22, 12: 25, 1443: 5, 26: 16, 508: 41, 1682: 12, 684: 13, 1618: 49, 128: 42, 374: 21, 160: 82, 1372: 10, 2122: 5, 863: 4, 1880: 25, 1684: 6, 112: 4, 896: 8, 1773: 1, 1147: 37, 1113: 15, 902: 34, 174: 19, 1538: 16, 1251: 17, 101: 41, 1746: 10, 1004: 86, 1178: 20, 314: 87, 310: 75, 423: 57, 436: 25, 1321: 19, 18: 32, 438: 18, 271: 29, 792: 13, 196: 16, 163: 24, 416: 9, 2091: 1, 206: 57, 556: 17, 594: 24, 200: 29, 54: 30, 1188: 54, 1898: 2, 1419: 4, 966: 14, 274: 63, 1866: 26, 194: 111, 1694: 67, 2079: 14, 1588: 10, 1131: 10, 1290: 10, 738: 24, 2076: 1, 1642: 6, 910: 41, 2094: 1, 735: 5, 1622: 2, 1489: 2, 1810: 1, 1424: 21, 459: 16, 1919: 19, 1248: 14, 1520: 26, 145: 42, 2133: 1, 718: 50, 1767: 30, 301: 65, 1691: 7, 890: 24, 1867: 7, 1887: 7, 1272: 7, 512: 54, 1820: 11, 752: 12, 1358: 7, 1601: 15, 1354: 1, 235: 21, 1352: 2, 384: 6, 677: 36, 393: 62, 1141: 33, 959: 30, 1854: 33, 1355: 14, 1888: 21, 582: 6, 265: 25, 2218: 20, 1771: 19, 999: 8, 472: 19, 1093: 23, 1983: 6, 645: 12, 1047: 13, 294: 23, 266: 10, 278: 101, 212: 31, 912: 31, 45: 88, 11: 39, 1554: 4, 805: 6, 1905: 1, 1472: 20, 1089: 10, 373: 31, 1320: 6, 539: 19, 2205: 3, 578: 70, 1493: 32, 2001: 10, 1571: 10, 213: 40, 1808: 5, 817: 11, 821: 27, 342: 33, 1657: 20, 494: 54, 2015: 29, 2339: 2, 323: 32, 598: 14, 693: 7, 1929: 5, 23: 8, 155: 14, 2300: 6, 840: 13, 916: 6, 2288: 2, 518: 42, 2: 31, 688: 12, 1628: 18, 1747: 6, 1360: 1, 117: 5, 1199: 1, 559: 14, 55: 17, 286: 14, 683: 35, 191: 94, 2427: 1, 1678: 17, 2147: 4, 2550: 3, 1635: 10, 1986: 18, 2335: 13, 1253: 1, 2009: 16, 1209: 27, 511: 6, 1639: 3, 1319: 7, 279: 20, 2641: 5, 1366: 21, 986: 10, 2098: 3, 1109: 18, 1104: 27, 259: 61, 135: 23, 587: 21, 1510: 23, 1725: 16, 1466: 5, 1198: 17, 788: 11, 1140: 43, 63: 47, 1663: 35, 333: 50, 560: 44, 1668: 7, 1492: 32, 116: 41, 165: 12, 1763: 5, 130: 6, 860: 33, 1469: 1, 1315: 4, 784: 15, 1524: 18, 1075: 20, 250: 43, 1666: 6, 1213: 14, 1586: 3, 2408: 7, 1094: 10, 988: 5, 1931: 7, 1783: 10, 240: 54, 253: 19, 981: 111, 1687: 2, 1262: 37, 510: 63, 1123: 15, 1954: 5, 465: 78, 223: 26, 2185: 3, 1673: 33, 354: 9, 1221: 16, 1027: 8, 726: 44, 137: 7, 129: 31, 628: 20, 514: 43, 296: 12, 1215: 12, 529: 12, 111: 21, 1726: 9, 1281: 13, 502: 41, 681: 4, 420: 49, 929: 9, 1680: 13, 2188: 10, 2204: 3, 1901: 2, 1754: 18, 670: 10, 695: 4, 644: 31, 604: 60, 96: 137, 1653: 3, 1238: 55, 287: 51, 1444: 9, 597: 11, 2095: 1, 1121: 6, 1385: 9, 447: 58, 10: 85, 1081: 13, 1811: 22, 426: 2, 408: 20, 1671: 16, 1451: 7, 1525: 2, 561: 44, 462: 53, 1729: 11, 1416: 24, 1926: 6, 1007: 14, 2596: 1, 1205: 34, 460: 12, 619: 9, 689: 5, 1708: 1, 914: 41, 1264: 6, 1661: 55, 1002: 7, 1299: 8, 1237: 11, 503: 12, 321: 9, 1347: 20, 767: 8, 1714: 1, 603: 13, 1918: 7, 489: 16, 1498: 10, 2073: 5, 1142: 3, 1470: 13, 1755: 3, 1494: 4, 756: 7, 239: 55, 2040: 3, 1118: 2, 722: 24, 1856: 11, 2382: 4, 536: 22, 1344: 5, 2039: 7, 1700: 1, 1659: 6, 904: 13, 232: 8, 1535: 6, 1564: 4, 849: 39, 854: 7, 790: 12, 355: 42, 29: 22, 1275: 31, 146: 15, 1375: 17, 177: 5, 1303: 43, 708: 11, 1701: 13, 655: 36, 694: 75, 291: 55, 1410: 7, 1517: 14, 733: 27, 215: 58, 921: 7, 1546: 9, 386: 4, 799: 8, 1644: 5, 2327: 4, 5: 8, 838: 18, 1039: 20, 955: 18, 1529: 12, 1677: 43, 126: 29, 897: 4, 856: 11, 796: 14, 76: 45, 30: 83, 2325: 3, 1052: 5, 175: 143, 1029: 26, 104: 17, 782: 24, 204: 2, 1398: 3, 1365: 5, 1583: 6, 67: 48, 220: 20, 2081: 1, 2536: 13, 862: 39, 258: 67, 984: 22, 1069: 47, 1148: 15, 552: 33, 1785: 8, 1872: 2, 768: 7, 642: 2, 1116: 35, 1090: 30, 2130: 4, 1913: 32, 1333: 32, 501: 69, 2219: 14, 107: 47, 1045: 28, 1247: 33, 1302: 37, 1349: 37, 2552: 4, 1291: 29, 1257: 3, 960: 73, 819: 1, 783: 5, 2197: 1, 1925: 1, 1256: 12, 1972: 6, 335: 17, 674: 97, 546: 13, 1850: 30, 1324: 7, 504: 35, 866: 2, 1452: 4, 935: 13, 411: 31, 1765: 39, 1870: 7, 1346: 5, 280: 24, 533: 61, 2410: 2, 1874: 30, 956: 18, 1085: 15, 1099: 21, 340: 5, 795: 1, 2500: 2, 1806: 1, 1175: 16, 2012: 2, 1838: 12, 2378: 1, 900: 49, 661: 17, 270: 4, 2068: 1, 664: 11, 2089: 3, 349: 31, 1187: 9, 1155: 10, 584: 32, 199: 20, 2736: 1, 353: 112, 188: 18, 2141: 5, 2054: 4, 2251: 13, 585: 32, 2395: 1, 1832: 10, 2484: 2, 772: 6, 2049: 8, 2166: 1, 1505: 1, 328: 31, 1186: 8, 499: 71, 437: 42, 1067: 17, 982: 4, 1847: 5, 868: 106, 1196: 60, 2048: 4, 1136: 23, 1957: 3, 2274: 1, 269: 16, 1912: 8, 1015: 54, 1735: 35, 884: 7, 1503: 5, 378: 7, 1702: 1, 1481: 3, 1189: 17, 1292: 11, 1153: 15, 905: 8, 1565: 6, 136: 7, 482: 17, 1596: 7, 1511: 6, 140: 34, 1464: 3, 620: 14, 643: 15, 615: 19, 515: 23, 452: 13, 1471: 23, 1258: 9, 880: 43, 1001: 70, 1338: 11, 1300: 27, 1884: 22, 1521: 13, 1645: 43, 1212: 28, 168: 41, 1975: 5, 610: 27, 602: 24, 1072: 14, 1478: 3, 1016: 6, 2183: 12, 1597: 1, 1339: 9, 2037: 2, 576: 8, 39: 21, 1041: 24, 94: 7, 1363: 18, 909: 11, 1388: 18, 1393: 4, 2461: 1, 1328: 8, 2734: 1, 636: 56, 2376: 4, 75: 6, 1243: 12, 2527: 1, 1418: 2, 2092: 3, 2691: 2, 90: 40, 210: 53, 1563: 66, 728: 31, 106: 3, 1865: 2, 368: 29, 839: 16, 624: 61, 975: 1, 262: 2, 1149: 10, 2146: 3, 1733: 31, 1594: 44, 2496: 3, 888: 4, 832: 20, 939: 4, 488: 22, 553: 33, 697: 30, 1779: 3, 2022: 2, 1218: 16, 327: 21, 1757: 2, 779: 10, 316: 8, 1606: 10, 1334: 3, 1169: 8, 1550: 7, 1287: 5, 2189: 1, 2220: 5, 1163: 12, 754: 23, 1821: 16, 105: 5, 1431: 20, 102: 3, 575: 17, 455: 20, 1100: 7, 1530: 3, 2660: 1, 812: 4, 1600: 2, 804: 7, 573: 18, 1559: 6, 1401: 8, 830: 15, 228: 28, 1288: 3, 1689: 15, 1968: 9, 1182: 7, 1073: 3, 203: 5, 1681: 2, 1875: 22, 1662: 8, 1054: 6, 1092: 27, 1191: 4, 898: 11, 864: 30, 1833: 3, 1064: 3, 1799: 22, 248: 36, 1674: 3, 1740: 4, 1380: 27, 449: 10, 2388: 3, 1150: 9, 268: 6, 2020: 2, 1168: 10, 990: 5, 2517: 2, 2293: 3, 329: 45, 422: 23, 383: 4, 1318: 3, 685: 12, 2030: 1, 2236: 7, 2027: 11, 833: 8, 1575: 14, 547: 14, 1482: 2, 2127: 27, 334: 12, 1044: 60, 366: 20, 871: 24, 2050: 4, 647: 13, 1748: 2, 1220: 3, 1932: 3, 2119: 17, 2137: 24, 216: 4, 709: 2, 84: 17, 1063: 4, 2114: 9, 847: 21, 307: 77, 1742: 43, 808: 14, 1908: 4, 1534: 7, 2730: 1, 1976: 8, 2753: 1, 292: 2, 2213: 1, 2449: 1, 2254: 3, 2401: 1, 1652: 1, 2014: 12, 487: 8, 2173: 10, 1098: 20, 970: 16, 2342: 4, 1948: 8, 1573: 22, 255: 6, 1720: 1, 1190: 2, 724: 17, 1846: 5, 2086: 4, 1578: 1, 2754: 1, 360: 27, 2414: 7, 2509: 2, 2453: 3, 2633: 1, 1951: 5, 2512: 1, 600: 47, 2444: 4, 1084: 45, 1987: 1, 543: 11, 263: 5, 131: 40, 2063: 2, 1509: 6, 622: 19, 1823: 4, 2181: 1, 972: 9, 571: 9, 2038: 9, 1864: 5, 2422: 4, 1268: 4, 1911: 4, 2533: 3, 1698: 14, 2024: 3, 1488: 7, 1824: 34, 1641: 1, 1969: 1, 789: 9, 940: 11, 1381: 6, 143: 25, 293: 24, 79: 78, 1556: 38, 2583: 7, 1732: 6, 1091: 7, 943: 17, 1428: 9, 2489: 1, 1545: 14, 1648: 17, 1083: 15, 621: 21, 330: 6, 1621: 23, 1176: 5, 1768: 36, 1796: 2, 1992: 2, 675: 10, 2477: 1, 1301: 30, 2434: 4, 1998: 2, 646: 23, 295: 42, 1244: 18, 2280: 1, 359: 42, 1207: 6, 2535: 2, 1507: 13, 1079: 22, 823: 21, 338: 25, 118: 12, 1279: 1, 2301: 4, 1204: 3, 1557: 3, 853: 9, 2344: 1, 380: 3, 1036: 10, 1434: 2, 434: 2, 1197: 33, 190: 3, 774: 6, 1228: 3, 1828: 13, 1917: 11, 1713: 8, 2374: 7, 1106: 5, 2135: 3, 2488: 3, 2435: 5, 1460: 1, 1138: 6, 1420: 3, 1903: 7, 1020: 25, 2057: 2, 2129: 1, 824: 6, 2217: 1, 2071: 4, 1759: 10, 1117: 7, 1173: 10, 1937: 1, 412: 6, 791: 26, 2225: 9, 2476: 1, 1271: 3, 66: 2, 1982: 2, 1314: 33, 2314: 4, 630: 42, 2372: 7, 153: 3, 1408: 7, 1576: 7, 671: 32, 1902: 4, 2313: 3, 42: 27, 419: 56, 1028: 4, 1859: 12, 1895: 1, 227: 31, 417: 20, 953: 6, 1693: 8, 1519: 4, 2018: 3, 141: 5, 1208: 11, 1798: 9, 2238: 1, 1772: 1, 1855: 7, 1574: 8, 608: 36, 1897: 5, 1792: 5, 899: 4, 448: 25, 1566: 1, 1278: 18, 1448: 6, 2016: 2, 776: 3, 2499: 2, 525: 37, 1980: 6, 995: 7, 1710: 3, 1326: 1, 480: 40, 1845: 3, 2258: 2, 43: 1, 2131: 2, 185: 46, 1296: 16, 1940: 2, 566: 5, 1540: 4, 1074: 23, 1097: 16, 931: 35, 1934: 2, 1080: 5, 730: 4, 2668: 1, 1370: 6, 1179: 15, 676: 7, 2160: 8, 1752: 3, 2472: 2, 2601: 2, 2061: 1, 2562: 2, 997: 1, 660: 8, 2450: 4, 828: 31, 391: 36, 1345: 2, 1533: 1, 1502: 2, 994: 29, 1831: 1, 1745: 13, 1807: 10, 1351: 15, 1623: 5, 93: 5, 1122: 10, 895: 7, 1607: 9, 2075: 2, 932: 17, 2354: 1, 2580: 2, 319: 24, 381: 10, 1552: 22, 2371: 5, 2381: 1, 1804: 2, 1604: 39, 142: 14, 1167: 2, 33: 3, 1066: 7, 1958: 12, 2031: 2, 2725: 1, 930: 20, 1567: 16, 937: 21, 1800: 1, 2595: 1, 2289: 11, 1222: 8, 678: 3, 2112: 2, 2158: 3, 1960: 2, 1841: 1, 2750: 1, 2356: 5, 2456: 1, 1297: 14, 1526: 1, 1656: 3, 2478: 19, 2621: 1, 1440: 10, 1164: 17, 2548: 2, 1816: 3, 1165: 5, 846: 2, 108: 5, 2557: 3, 1776: 1, 787: 4, 1043: 2, 686: 22, 1062: 2, 2190: 21, 2209: 6, 8: 3, 1034: 8, 2359: 13, 1835: 19, 1456: 29, 1921: 9, 1439: 3, 1513: 1, 1624: 3, 2340: 1, 1427: 1, 2346: 4, 1551: 6, 1609: 2, 1881: 10, 1061: 17, 495: 11, 705: 15, 691: 19, 2285: 2, 1343: 41, 2448: 1, 857: 1, 1128: 11, 2673: 1, 2692: 1, 1879: 5, 1387: 1, 2178: 3, 2599: 1, 1914: 1, 1527: 3, 1727: 1, 811: 22, 485: 8, 443: 2, 1894: 4, 1978: 4, 2350: 1, 2066: 1, 197: 35, 1203: 2, 1491: 3, 2309: 7, 2758: 1, 2097: 8, 2717: 1, 1076: 5, 406: 2, 1809: 1, 464: 32, 1254: 5, 2694: 1, 2380: 1, 2689: 1, 397: 3, 1953: 1, 1307: 14, 2576: 2, 303: 1, 2162: 2, 665: 14, 2013: 2, 1660: 1, 1944: 2, 911: 9, 1174: 3, 1459: 3, 2270: 2, 1232: 5, 1805: 2, 2276: 1, 879: 20, 1731: 2, 1569: 17, 1651: 2, 1373: 2, 2000: 1, 2044: 2, 1909: 15, 1890: 1, 1848: 1, 2064: 1, 2343: 1, 1715: 1, 1504: 1, 1267: 12, 2538: 1, 2391: 1, 1570: 11, 1614: 1, 1181: 4, 1780: 5, 1580: 3, 1794: 3, 1873: 4, 2228: 2, 2128: 1, 1240: 1, 951: 4, 2328: 1, 1989: 1, 1579: 3, 2104: 8, 2047: 1, 2111: 5, 35: 2, 1514: 1, 1541: 3, 1598: 2, 1289: 14, 97: 2, 1386: 1, 2266: 2, 1468: 2, 1026: 1, 1547: 3, 2058: 5, 629: 1, 484: 2, 343: 7, 2364: 1, 2224: 4, 2604: 1, 2284: 2, 1337: 4, 1435: 1, 2087: 3, 1842: 1, 1750: 2, 2161: 1, 2208: 2, 2159: 6, 2676: 1, 2568: 1, 1882: 9, 2600: 1, 2142: 3, 2025: 1, 2269: 1, 991: 8, 2113: 7, 2622: 4, 1523: 1, 2035: 1, 1852: 1, 918: 18, 2560: 1, 1830: 1, 2539: 6, 739: 1, 1214: 8, 1695: 1, 894: 73, 2053: 1, 1950: 1, 1558: 20, 1276: 3, 2628: 9, 2246: 1, 2767: 1, 469: 2, 2419: 3, 2682: 1, 2430: 3, 1920: 1, 2534: 1, 2431: 3, 1265: 7, 2351: 2, 1317: 1, 736: 14, 1782: 4, 2070: 4, 1787: 1, 1764: 2, 2677: 1, 2326: 3, 2051: 6, 2619: 1, 1005: 1, 2136: 1, 2532: 9, 1744: 1, 1868: 2, 1984: 1, 1259: 1, 2413: 11, 992: 2, 2103: 3, 1857: 1, 687: 3, 1417: 10, 2623: 1, 2613: 2, 1703: 1, 1688: 1, 1795: 1, 2460: 2, 2510: 1, 1749: 1, 2740: 1, 2260: 5, 2591: 1, 2469: 1, 653: 2, 2639: 1, 2421: 2, 2415: 1, 2713: 1, 1235: 2, 2194: 2, 1927: 1, 2264: 1, 1970: 1, 2085: 1, 667: 1, 1654: 1, 2588: 1, 2582: 7, 1819: 1, 950: 34, 2005: 1, 2763: 3, 2262: 1, 2464: 1, 1839: 3, 1916: 1, 2716: 1, 1815: 18, 2177: 2, 2226: 1, 1425: 1, 599: 1, 2505: 2, 1335: 19, 1893: 1, 1051: 8, 2631: 1, 2603: 1, 2360: 3, 1384: 1, 1683: 1, 1738: 1}\n",
            "2183\n",
            "2769\n",
            "min number of connections for each class 1 2387\n",
            "max number of connections for each class 252 195\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.colors as colors\n",
        "# print(list_users[0])\n",
        "group_of_10 = [[list_users[i] for i in group_users_id[j]] for j in range(len(group_users_id))]\n",
        "# print(group_of_10)\n",
        "# create a color map\n",
        "print(len(group_users_id))\n",
        "number_user_plot = 3\n",
        "number_group_to_plot = 6\n",
        "print(group_users_id[6])\n",
        "cmap = plt.cm.jet\n",
        "norm = colors.Normalize(vmin=0, vmax=number_user_plot)\n",
        "group_3 = [list_users[443], list_users[532], list_users[249]]\n",
        "print(len(list_users))\n",
        "\n",
        "# plot the itinerary of 10 users in the dataset\n",
        "fig, ax = plt.subplots()\n",
        "for i in range(number_user_plot):\n",
        "    user = group_3[i]\n",
        "    # user = np.random.choice(group_of_10[number_group_to_plot])\n",
        "    pos_id = user[\"pos_id\"]\n",
        "    color = cmap(norm(i))\n",
        "    ax.plot([i for i in range(len(pos_id))], pos_id,  'x', color=color)\n",
        "ax.set_xlabel('evolution')\n",
        "ax.set_ylabel('pos_id')\n",
        "#     ax.plot(pos_id, 'x', color=color, label=f'user {i}')\n",
        "# ax.set_ylabel('pos_id')\n",
        "#     x = user['input'][:,0].numpy()\n",
        "#     y = user['input'][:,1].numpy()\n",
        "#     color = cmap(norm(i))\n",
        "#     ax.plot(x, y,'x', color=color)\n",
        "# ax.set_xlabel('x')\n",
        "ax.set_title('Itinerary of ' + str(number_user_plot)+ ' users in the dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5cRrQ5xg8D0H",
        "outputId": "ffba5dca-9f2b-4ecf-815a-1dba221da452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n",
            "[123, 157, 249, 289, 318, 440, 443, 468, 542, 561, 618, 639, 650, 661, 713, 747, 769, 775, 815, 835, 900, 917, 950, 963, 979, 1000, 1026, 1088, 1096, 1176, 1234, 1244, 1265, 1278, 1363, 1382, 1422, 1424, 1449, 1472, 1523, 1647, 1691, 1694, 1759, 1862, 1867, 1937, 2057, 2081, 2139, 2141, 2176, 2211, 2221, 2231, 2322, 2324, 2336, 2338, 2375, 2411, 2476, 2547, 2549, 2582, 2583, 2595, 2611, 2683, 2725, 2729, 2744, 2775, 2807, 2820, 2846, 2856, 2885, 2906, 2917, 2975, 3032, 3047, 3064]\n",
            "3072\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.cm as cm\n",
        "def plot_itinerary(list_users):\n",
        "    fig, ax = plt.subplots()\n",
        "    for i in range(10):\n",
        "        user = np.random.choice(list_users)\n",
        "        # color = cm.nipy_spectral(random.choice(range(0, 10)))\n",
        "        norm = colors.Normalize(vmin=0, vmax=10)\n",
        "        for j in range(len(user['pos_id'])):\n",
        "          ax.plot(user['input'][j,0], user['input'][j,1], 'o', color=color)\n",
        "    plt.show()\n",
        "\n",
        "plot_itinerary(list_users)"
      ],
      "metadata": {
        "id": "4VTCPvjr9d50",
        "outputId": "c84b1b97-7dcb-449a-b036-87ebe3848907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Approach"
      ],
      "metadata": {
        "id": "la6oh-00LxD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "oEoCaV7OLBmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(inputs, pos_id_targets, test_size=0.3, random_state=1)"
      ],
      "metadata": {
        "id": "zINuSgqbMD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Encode the target\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "#go to GPU\n",
        "# X_train = X_train.to('cuda')\n",
        "# X_test = X_test.to('cuda')\n",
        "# y_train_encoded = torch.tensor(y_train_encoded).to('cuda')\n",
        "# y_test_encoded = torch.tensor(y_test_encoded).to('cuda')\n",
        "#Search for the best parameters\n",
        "# 'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
        "# 'learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
        "# 'n_estimators': [100, 200, 300, 400, 500],\n",
        "# 'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
        "# 'min_child_weight': [1, 2, 3, 4, 5]\n",
        "# parameters = {'max_depth': [30, 40, 50, 60, 70, 80, 90, 100], 'learning_rate': [0.05, 0.1, 0.15, 0.2], 'n_estimators': [50, 70, 100, 130, 170], 'gamma': [0, 0.1, 0.2, 0.3, 0.4], 'min_child_weight': [5, 10, 20, 30, 40]}\n",
        "parameters = {'max_depth': [30, 40], 'learning_rate': [0.1, 0.2], 'n_estimators': [50, 70, 100], 'gamma': [0.2, 0.3, 0.4], 'min_child_weight': [10]}\n",
        "best_score = -1\n",
        "\n",
        "for max_depth in parameters['max_depth']:\n",
        "    for learning_rate in parameters['learning_rate']:\n",
        "        for n_estimators in parameters['n_estimators']:\n",
        "            for gamma in parameters['gamma']:\n",
        "                for min_child_weight in parameters['min_child_weight']:\n",
        "                    model = xgb.XGBClassifier(max_depth=max_depth, learning_rate=learning_rate, n_estimators=n_estimators, gamma=gamma, min_child_weight=min_child_weight, gpu_id=0) #, gpu_id=0\n",
        "                    model.fit(X_train, y_train_encoded)\n",
        "                    score = model.score(X_test, y_test_encoded)\n",
        "                    print(score)\n",
        "                    if score > best_score:\n",
        "                        best_score = score\n",
        "                        best_parameters = {'max_depth': max_depth, 'learning_rate': learning_rate, 'n_estimators': n_estimators, 'gamma': gamma, 'min_child_weight': min_child_weight}\n",
        "\n",
        "print(\"best score: \",best_score)\n",
        "print(best_parameters)\n",
        "# cv_scores = cross_val_score(model, X_train, y_train_encoded, cv=5)\n",
        "print(\"cross score: \",cv_scores)\n",
        "\n",
        "#Predict the next position\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test_encoded, y_pred))"
      ],
      "metadata": {
        "id": "R-25EdbFMF0i",
        "outputId": "0f5c072c-d353-4443-d32f-c23a5695ebe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [23:01:50] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [23:01:50] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009161470925054078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [23:09:50] WARNING: /workspace/src/context.cc:44: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-ba5beb96aa84>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mmin_child_weight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_child_weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_child_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, gpu_id=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1517\u001b[0m             )\n\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m             self._Booster = train(\n\u001b[0m\u001b[1;32m   1520\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m             _check_call(\n\u001b[0;32m-> 2051\u001b[0;31m                 _LIB.XGBoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   2052\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2053\u001b[0m                 )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Séparation des données par utilisateur et organisation en séquences temporelles\n",
        "user_sequences = {}\n",
        "for entry in inputs_with_pos_id_user_id_target:\n",
        "    user_id, position, time = entry[0], entry[1::2], entry[2::2]\n",
        "    if user_id not in user_sequences:\n",
        "        user_sequences[user_id] = []\n",
        "    user_sequences[user_id].append((position, time))\n",
        "\n",
        "# Normalisation des données\n",
        "scaler = MinMaxScaler()\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    positions = [pos for seq in sequences for pos in seq[0]]\n",
        "    times = [time for seq in sequences for time in seq[1]]\n",
        "    positions_scaled = scaler.fit_transform(np.array(positions).reshape(-1, 1))\n",
        "    times_scaled = scaler.fit_transform(np.array(times).reshape(-1, 1))\n",
        "    user_sequences[user_id] = [(positions_scaled[i], times_scaled[i]) for i in range(len(positions_scaled))]\n",
        "\n",
        "# Padding des séquences\n",
        "max_sequence_length = max(len(seq) for seq in user_sequences.values())\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post', dtype='float32')\n",
        "\n",
        "# Encodage des séquences (si nécessaire)\n",
        "\n",
        "# Fractionnement des données\n",
        "train_data, val_data, test_data = {}, {}, {}\n",
        "for user_id, sequences in user_sequences.items():\n",
        "    train_size = int(0.7 * len(sequences))\n",
        "    val_size = int(0.15 * len(sequences))\n",
        "    train_data[user_id] = sequences[:train_size]\n",
        "    val_data[user_id] = sequences[train_size:train_size+val_size]\n",
        "    test_data[user_id] = sequences[train_size+val_size:]\n"
      ],
      "metadata": {
        "id": "t24nQ6zkMSyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "f5D9IoBtGX6R",
        "dDvAwpD4GrJu",
        "S_mzoE-MHqLa",
        "ptycyS7FWE4b",
        "3Bbp1dVXWQs3",
        "zZpbR8rG8kBn"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}